== best paradigms

_a discussion of bad, ubiquitous programming practices, and their superiors. this document serves to help one recognize then unlearn bad practices._

.TODO
* fusion system should make map, filter, concat (any alse? why or why not?) primitives. map is obvious: it's just function application in the context of an array. filter is reduction (one element becomes none). concat is expansion (one element becomes many). indeed, all these generalize to a->{a}, or subset->subset.
* note: the distinction between non-/executable code (e.g. proglangs vs desclangs) is false: _executable_ code _describes_ the things to be executed, and the execution is really just processing by some processor/executor. the same can be said of "non-executable" code: the executing code processes it, and thereby affects the program's behavior. one may (falsly) argue that non-executable code does not affect what's being done, but rather upon what it's being done e.g. `cat` always prints a file independent of which file it's given. however, a precise way of phrasing this is that the subset of ``cat``'s behavior, printing, is independent of its file parameter. indeed, extra functionality can be relied upon if the thing to be printed is assumed to be printed to a terminal, and the file contains the ^D charcter, which will exit the terminal, and thus is functionality beyond ``cat``'s assumed use, despite this functionality being reliable. similarly, we may parameterize any aspect of a program by choosing a behavior based on a condition. a simple case is `switch`/`case`, which accepts mere integers, yet those integers determine which branches are taken. so are the integers mere data, or subprograms? alone they're obviously mere data without particular meaning, but that's _always true of all numbers._ so the obvious question is what their meaning is in a given context. when programs interpret them, they may or may not affect branching, but they do affect how the program runs, regardless. is a program that prints "hello" 5 times "really" different from one that prints it 10 times? i surely do not know, nor do i think that such a distinction is useful, but it certainly isn't necessary. all code is code, not the very fact of execution itself, and the very fact of code being processed by an executor is what makes it "executable;" therefore executability is not a property of code, but instead of how the code relates to some other thing, namely whether that thing executes the code. ultimately execution is implemented physically, typically on a cpu, but can also be done on tpu, gpu, an analog computer, an electrical circuit, chemical circuit, or even a rube goldberg machine. _any_ physical system supporting abstract patterns of behavior can be called a computer and the things upon which it acts and is assembled can be called _code_.
* revise this document in light of digitstrings.
* consider symbolic code, namely eventualy refinement of predicated symbols, and expressing structures/outputs as relations & holes that are eventually filled (literally free variables that are eventually bound (or not, if they're to remain somewhat free)); things with holes can be composed, and the composition retains the original holes. as always, the holes only need references, which may be numerical indices, names, or most generally a predicate (though predicates alone lack indexing that enables efficient traversal.)
* i've lambasted functions. i still must lambast data structures.
* thinking that modularization is achievable is foolishly naïve. let's mathematically describe _modularity_: it's independence, polymorphism, interchangeability, literally relation `∀a∈A,b∈B (a,b)`; there are no constraints on any particular relations among subsets of `A` or `B`. surely this naturally occurs often, but naturally does not occur often. modularity is not something that we decide; it's an intrensic property of relation, and relation is the definition of any system that we're coding. therefore trying to enforce modularity is simply adding complexity in transformations (from the natural, non-modular space to the modularized space/representation) and masking the program's nature by trying to fool a coder into thinking that there's independence when there actually is not. as always, both extra complexity and non-natural expression complicate working with the code.
  ** the inevitability of non-modularity is yet another example of why organic programming reflects truth of code, and why we need tools to work with organic code, rather than trying to use constrained systems. ideally we should be able to visualize in/dependence, e.g. as a hexagonal or rectilinear grid vs a more organic graph.

=== introduction: what's wrong with programming today?

the majority of work in common programming is:

* dealing with linguistic constraints. it usually sounds like, "how do i do <computation> in <lang> given <constraint>?" and the answer is always a work around, usually considered idomatic. this gives rise to so-called "design patterns."
* dealing with execution/computational model constraints, such as strict vs non-strict eval
* choosing and converting among data structures / types, usually for efficiency, though sometimes for elegance, and other times to provide whatever arbitrary structure some function takes
  ** commonly one makes new types not for their structure, but for their assumed semantic meaning. this is seen in oop langs and haskell. in these, the types are not considered for their mathematical structure, but instead are effectively just JSON schemata checked by the type checker.
* specifying symmetry, usually verbosely. e.g. factor has `find`, not `findAll`, and has `cut` but not `cutAll` (`u;.1` in j). thus loops and their termination conditions or domains must be specified _everywhere_. because loops are specified explicitly, they require a nontrivial amount of refactoring to change their traversal logic or domain. also nesting loops or traversing multiple structures in tandem is hell. excepting array languages and prolog, proglangs don't have builtin considerations of symmetry. prolog's symmetry is implication which derives total sets from predicates, and array langs are symmetric of axes. this is arguably the greatest of all defects.
* abstraction devices—such as oop, subroutines, combinators—that serve only to factor/chunk the program for brevity or nicer _representation_, which usually means *syntax*, though sometimes factoring programs reveals underlying arrangements of logical aspects of the program and suggests a model.

to my knowledge, only early uiua (sadly uiua's had some ugliness introduced since) and prolog don't suffer these issues. they both:
* have implicit symmetry of their data
* are only notation rather than a language, and so their syntax is arbitrary and has not arbitrary constraints/asymmetries, and their syntax represents programs directly (homoiconicity. uiua would be homoiconic if its higher-order fns' argument fns needed to be quoted. that would be a pain, though, and we can still quote them (delay their evaluation via the `^` operator) if we want.)
* have one data structure
* prolog has non-strict evaluation, and uiua has `^`

even lisps (except pico lisp) has the "multiple data structure" issue. consider racket scheme. it has both the stream type (srfi 41) and the _sequence_ type. they do the same thing, but sequences are a little bit different and are a bit more efficient, i think. they are a different type. there exist functions to convert between the two types. closures (namely `closure : inputs ... -> (a . closure-or-nil)`) can accomplish the same behavior. they're all different types. `car` & `cdr` work on only the closure one, and stream functions work only on streams, and sequence functions only on sequences. each has basically the same behavior but in some places have tiny technical differences that have nothing to do with the program logic that they implement. why does this happen? it's because the problem is with the underlying single data structure with the execution model: the linked list with strict evaluation. the idea of a sequence/stream is non-strict evaluation and abstract representation of a sequence.

.example
[source,scm]
----
;; represents the infinite sequence `[i..]` by the "generator" pattern (as is commonly called in python)
((cdr ((letrec ([L (λ (i) (cons i (λ () (L (add1 i)))))]) L) 0)))
'(1 . #<procedure>)
----

did you know that racket scheme's virtual machine is more efficient for pure code but that chicken scheme's translates to c before execution and thus has no vm and so executes impure code faster?

.example
[source,scm]
----
((cdr ((letrec ([L (λ (i) (cons i (begin (set! i (add1 i)) (λ () (L i)))))]) L) 0)))
'(1 . #<procedure>)
----

wow! so many ways to encode a simple idea, right!? haskell's `[i..]` is basically the tersest possible expression. it's built into the language. there's not much to computing, but what all there is should be implemented by the vm, not left to programers to implement, since they'll make all sorts of designs that disagree. the fact is that the language itself is the single point of design unification, and all programmers will implement different designs, so the only hope of decent synergy among many people's code is a vm with an interface (i.e. its opcodes/builtins/primitives) that doesn't force the programmer to worry about implementation details—namely efficiency and syntax. this is what makes j and prolog good.

=== interactivity, and program editors rather than programming languages

programs are commonly written like initialization-until-termination, rather than, like prolog, being a collection of information storable on disk or in volatile memory, queryable like a server is, modifiable while running. aside from that, however, is a further concern of interactivity: cursors & views. the repl is a very poor, limited form of interactivity. text—the need for syntax and having it evaluated—is often, but not always, ill-suited. it's powerful because we can express arbitrarily complex queries quite well; but it is good for expressing queries, not designing them! design occurs in an interactive environment, such as the kakoune text editor.

kakoune is one of the most interactive editors that i've found. it allows multiple selections simultaneously and saving them to registers, and in this way of manipulating multiple data simultaneously, follows the array paradigm. however, like array _languages_, the kakoune _editor_ shows the selections which the queries identify _as the query is being written_ and show the operations _as they're entered by the user_.

editing data like this is at least as wonderful an improvement on the repl as the repl was on compiled programs. of course, there's no reason to confine ourselves to terminal emulators, text editors, or textual representations of data.

suppose that i copy tabular data from a webpage, and when i paste it into text, it looks like this:

----
103.19.58.84	4145	Indonesia Banyuwangi	

3980 ms
	SOCKS4	High	1 minutes
93.116.57.4	4153	Moldova Ungheni	

380 ms
	SOCKS4	High	1 minutes
119.82.251.250	31678	Cambodia Phnom Penh	

2140 ms
	SOCKS4	High	1 minutes
190.182.88.214	30956	Colombia 	

2020 ms
	SOCKS4	High	1 minutes

[...]
----

my task is to turn it into a queryable sql database for both programmatic use and readability. any programmer would probably want to write regex or a parser. in kakoune, however, it's natural to select whitespace then replace with spaces. now we get the following output:

----
103.19.58.84 4145 Indonesia Banyuwangi 3980 ms SOCKS4 High 1 minutes 93.116.57.4 4153 Moldova Ungheni 380 ms SOCKS4 High 1 minutes 119.82.251.250 31678 Cambodia Phnom Penh 2140 ms SOCKS4 High 1 minutes 190.182.88.214 30956 Colombia 2020 ms SOCKS4 High 1 minutes
----

ok. i see that each record ends with "minutes". i'll insert a newline after those:

----
103.19.58.84 4145 Indonesia Banyuwangi 3980 ms SOCKS4 High 1 minutes
 93.116.57.4 4153 Moldova Ungheni 380 ms SOCKS4 High 1 minutes
 119.82.251.250 31678 Cambodia Phnom Penh 2140 ms SOCKS4 High 1 minutes
 190.182.88.214 30956 Colombia 2020 ms SOCKS4 High 1 minutes
----

i see some leading whitespace. let's remove that then identify fields. there's a variable, here: locations may be any number of names. they're always followed by regex `[0-9]+ ms`. however, rather than bothering with regex, we can simply go to the end of each line, do `6b`, and insert a field separator there:

----
103.19.58.84 4145 Indonesia Banyuwangi |3980 ms SOCKS4 High 1 minutes
93.116.57.4 4153 Moldova Ungheni |380 ms SOCKS4 High 1 minutes
119.82.251.250 31678 Cambodia Phnom Penh |2140 ms SOCKS4 High 1 minutes
190.182.88.214 30956 Colombia |2020 ms SOCKS4 High 1 minutes
----

the remainder of field separators are obvious:

. one after each of 1st & 2nd words
. one before the 2nd word from the end
  .. one before the word before that
    ... one before the word before that

----
103.19.58.84   |  4145 | Indonesia Banyuwangi | 3980 ms | SOCKS4 | High | 1 minutes
93.116.57.4    |  4153 | Moldova Ungheni      |  380 ms | SOCKS4 | High | 1 minutes
119.82.251.250 | 31678 | Cambodia Phnom Penh  | 2140 ms | SOCKS4 | High | 1 minutes
190.182.88.214 | 30956 | Colombia             | 2020 ms | SOCKS4 | High | 1 minutes
----

i've added spaces around delimiters and aligned them (using kakoune's `&` operator) for readability, just because i can effortlessly. "word" here means a substring of non-whitespace characters. notice how some locations depend on others.

kakoune has many primitives that j has but many cursor-based ones, too:

[options="header"]
|==================================================================================
| kak                          | j                | op
| `/`                          | `i.`             | find
| `"rZ`                        | `r=:<selection>` | store value in register `r`
| `|`                          | `2!:1`           | exec cmd
| `S`                          | `;.±[12]`        | split
| `|.`                         | `<a-(>`, `<a-)>` | rotate
| `p`, `P`, `A`, `a`, `i`, `I` | `,`              | append/prepend/insert
| <a number n before cmd>      | `cmd^:n`         | execute a fixed number of times
|==================================================================================

kak does not support arithmetic nor conditional actions. for sorting we do `|sort`, and for grouping we: select, filter, delete (which also yanks), then paste elsewhere. this must be done for each group, though a macro makes that easy. its data & execution model are not nearly as efficient as bqn, rust, or go, but that's fine considering that the point of interactivity is to _derive_ dataflow programs, not execute them. one can easily write an efficient vm that executes kak commands i.e. programs.

kakoune does not have fold but all loops (including fold) can be expressed easily in terms of a macro. the only shortcoming is that kakoune does not support looping until a predicate is false.

ones that kakoune has that j does not are pretty much all selection-based ones—that is, kakoune's interactive aspect:

[options="header"]
|====
| key                         | command
| s                           | select
| u                           | undo
| d                           | delete
| Z                           | store selection
| j,k,l,h,J,K,L,H,f,t,E,e,b,B | modify selection / move cursor
| ?                           | extend selection to next found search term
| q                           | record macro (define subroutine interactively, though the user must not make mistakes lest they mis-record a macro!)
|====

the reason that kakoune's select is different from j's is that kakoune's does not have a return value; it selects without changing the buffer(s) (which, along with register values, is the program state). we can extract with `d`, or copy with `y`, or replace with `c`, or pipe to a program or a whole ton of other things, easily changing the buffer or not, seeing how we're changing it as we do it.

the reason that j does not have kakoune's `d` is that, again, kakoune deletes what's selected; j has no facility for removing a single slice `[m:n]`, let alone multiple slices, nor for selecting them in the first place! of course j's regex library does support these.

* obviously kakoune does not work on multidemensional arrays well, but then again, who actually uses rank 3+ arrays anyway? and if they do, why? as sql shows, we can do well with tables or vectors.
* you don't really get runtime errors in kakoune

note that neither vim nor emacs is any good for this, since they aren't interactive. they do still improve on repls, however: to execute commands, they use keybinds rather than language syntax, and they have undo! hell, even sql has undo: transactions!

also note that interactive editing shares something in common with stack langs: both modify state. what in a stack lang is `dup`, in kak is `f<delim>yP`. yet obviously text buffers are a far more complex state than a stack! they also act as arrays, allowing subarray selection. comparing kak to stack machines is a bit silly, though, when stack machines are considered for their efficiency.

all this to say: frontending langs with kakoune and its json-rpc is better than repls, but a generally configurable interactive environment that goes beyond the confines of text is even better. until text is deprecated in favor of opengl-based arbitrary data editors, repls should be deprecated in favor of kakoune/json-rpc. in fact, i'll take it even further: these things should be used for _all_ computer interactivity! do away with POSIX repls! use text editors instead, and simply enable choosing keybind profiles while the shell is open! at such a point, there's no concept of programming language vs interacting with a computer vs editing or viewing data: it's all just arbitrary manipulation & display profiles as the user prefers, though of course the underlying functionality must still be programmed. but the actual manipulation & display of data should no longer be "dead" programs and syntax, or even text! in a world with webgl and html tables, the idea that anyone confines themselves to displaying multidimensional data as text tables with double-newlines (looking at you, j), or that anyone writes code to align text columns is, if you really think about it, pathetic. in fact, i can in kakoune do `%<a-k>regex<ret>5WEd` to, for lines matching `regex`, delete their 5th word _in place_, which would be gold-only-knows-what in j, is something to think about. the fact that i don't immediately know how to express it in j is already enough! that the expression is longer is another thing, and the cherry on top is that i can't see what i'm doing in j as i type-out the syntax! i'm far more likely to make a mistake in j both because i can't see what i'm doing and because i must manage a long syntax blob!

a kak/json-rpc sql editor would be wonderful.

here's a good shell example: say i'm in a directory in kakoune (kakoune knows its current working directory). then i have a keybind to change directory; it just does `!find . -maxdepth 1 -type d` then `gkxd` to delete the 1st line, which is always `.` (great design, `find` authors!) now that all the cwd's subdirectories are listed, i just do `/` then start typing any regex, to identify the dir that i want to change to, then press enter. who needs bash completion? all i have to do is get to a given line then `xd_` to get the line without whitespace, then `|cd` to pipe the selection to `cd`. what if i want to do anything with multiple files? just `%<a-s><a-k>regex<ret>|rm`, and you actually get to see exactly which files you'll delete. it's very j-like how we select masks then passing them to functions (such as `rm`) rather than providing selection functions to other functions. it's very lisp-like (its macros) how we build-up commands and data to execute, then actually execute them—except that we do it on-the-fly and interactively and with strings.

it's basically like "what if all commands that you execute in the shell were like `massren(1)` but if we didn't have to write massren?" here's how we do the same functionality as `massren` in kakoune:

. `!ls` to paste the list of files of the working directory
. `<a-s><a-k>regex<ret>` to filter only the files that we want to modify (or just manually delete or arrange files however you like)
. `+"` (a keybind that i've made with the `surround.kak` script) to quote the filepaths
. `ypi <esc>` to turn `"filepath"` into `"filepath" "filepath"`
. modify filepaths as you want
. `Imv ` as part of building-up the command to execute. prepends "mv " to every selected line
. `<a-|>sh` to execute each command, ignoring its output, or to be safer, `$sh` to do the same but delete selected text on non-0 exit status

aside from interactivity, *what makes kakoune such a good programming interface is that it has a very powerful selection mechanism.* selection is one half of programming: the other is relating selections. the selection may be related to another and both of a common datum (e.g. saving multiple selections into multiple registers, all of a common text buffer)—this sees relations of both positions & their corresponding values altogether—or the relation may be between a selection and a different selection—for example, deletion or replacement: i select a slice (of a string [array]) `[m:n]` then replace it by a new string that has a new length, at slice `[m:n2]`. this relates subsets of the buffer to each other and thus implicitly relates one buffer state to another.

for good computer use we must have interactive programs interact with each other well. kak and kitty are good examples; they expose their states through environment variables and have syntaxes for external program invocation s.t. the running program can be passed arguments to their command line, and both kitty and kakoune listen for commands on sockets.

=== functions

a function is a particular variety of (sub)program that takes inputs and gives outputs. that inputs are free enables the subprogram much more than having fixed values. that being said, it could use fixed addresses, have those addresses mutated, then read from them. this is common in assembly, seen as setting registers values before invoking an instruction or reading output from a fixed register after invoking an instruction. however, like in assembly, pushing parameters to the 1-dimensional stack avoids some problems present in 0-dimensional registers, namely when two segments of code need access to common registers at different times; one is bound to overwrite the other's value. the error here is that independent data are kept in a common place that cannot simultaneously store them both.

functions solve this collision problem in the same way that the stack does: by introducing _scopes_, analagous to stack frames: scopes are nested i.e. ordered just like stack frames or elements in a linked list. however, for convenience, a scope contains multiple _variables_, analagous to registers. the variables are named like registers, but unlike registers usually informative names are chosen per function. these variables scoped to the function are the function's _inputs_. a function _returns_ one or more output values; however, unlike its inputs, its output is unnamed; its output may be named in the scope in which the function is invoked, but this is not necessary if the function's output is passed directly as an input to another function's input. functions in stack langs elegantly allow returning multiple return values, except that all values must be used by successive functions and must be ordered properly. a freer, better alternative to functions (returning & binding values) is to mutate data such that successive reads use the new information. this way we immediately store _alongside_ (or otherwise clearly related to, in the relational model) original data (what would be fn inputs) new data (what be fn outputs). any procedure may check for that extra data or not. the data may be modified in ways that affect only desired procedures. this method is most feasable when efficient encodings are used, so that storing new data is efficient. again the relational model shines in that, given a relation `a`, we can freely modify other relations `B` without or without regard to `a`; `a`'s relations to any `b` in `B` are not stored in any relation itself; they're `implied` by any joins _when the join is performed_; it's dynamic yet efficient & free relation! this is no surprise when we recall that [virtual] relations generalize functions.

functions are ubiquitous, but they certainly are not needed and there are other expressions [encodings] of (sub)programs. common examples are `eval` in lisp or ruby, called `load` in lua, though lua still reads into a function, or `call` in factor. generally, however, any program permits an infinite number of encodings, and any decoder can evaluate the program, assuming that the execution environment's primitive functions can accomodate the functionality. as ever, the general method is superior when tailored to its use.

particularly, functions' shortcomings are:

* function bodies cannot be examined nor modified (except picolisp, but there lambdas are fexprs, which generalize functions & macros)
* functions definitions reference inputs by name, yet invokation specifies inputs by position. this inconsistency is awkward enough, but there's even more awkwardness:
  ** some languages like python or racket allow specifying inputs by name, though when no name is given, use position to match values to inputs.
  ** many languages accept variadic inputs as sequences. we're already accepting inputs positionally, so why the redundant asymmetry^*^?
    *** python's model also accepts variadic inputs as a dictionary! i suppose that that's consistent with accepting variadic sequences; now for both positional and named arguments we can accept inputs by names given by the invoking function rather than the defining one.
* dual to the silliness of variadic args, and for langs that actually allow multiple return values, one may as well return a single structure (e.g. list) of data to return.
* functions force input names even when no particular name is appropriate
* dual to functions requiring names for their inputs, invoking code must bind output(s) to names if 1. multiple outputs are returned; or 2. an output is used multiple times. binding clauses are crufty.
* we're constrained to the function's/language's scoping model and passing data around subprograms only by inputs & outputs and the language's facilitating syntaxes, which are often inelegant & verbose.
* being forced into naming causes unnecessary datum ambiguity.
* functions unnaturally encourage partitioning data into subsets each of which corresponds to some idea
  ** partitioning data is, in the function model, always done into multiple scopes, and relating data across scopes is inelegant
* functions' inputs are scoped and function bodies usually invoke other functions; thus functional programs are nested & recursive. this naturally forces one to manage scopes, just to keep data in scope in order to relate data across scopes. this clumsiness is not present in sql nor procedural langs.

^*^ i never thought that i'd see the phrase _redundant asymmetry_!

clearly function users cannot decide whether to accept arguments positionally or by name, nor whether to accept them by position or by name, nor whether the function or the invoking code should specify the names or positions of the inputs. obviously functions are poor design, but adding together distinct-but-equivalent designs for identifying data is not a solution! concatenative langs don't have these issues, and use subprograms as quoted programs, which are inspectable & modifiable and don't name their inputs nor outputs, nor do they require binding clauses. their critical downfall, however, is that:

* they're verbose and/or confusing (though not as verbose as common applicative langs as they're commonly used)
* their code's meaning is not apparent at a glance. in fact, identifying its meaning takes quite a bit of work!

the relational model (namely as per sqlite) is very good. it uses queries instead of functions to express (sub)programs, which has benefits:

* _all_ data are kept in relations
  ** relations are the sole scoping mechanism. in sql, all scopes are named.
    *** relations cannot be nested. therefore there are no nested scopes due to relations^*^.
* subprogram outputs are named by the subprogram itself. possible inputs are specified as data in the relation or constants.

^*^scopes are nested when queries are nested, but nesting is rare and practically never more than 1 level deep.

the only disambiguation needed is `as`. `join` can avoid most query nesting, and avoids all relation nesting; instead of nesting relations' scopes, it unions them.

all programs are: cond (asymmetric exception), loop (when considering program as state machine), group, select, extract, put. _extract_ removes from a structure and generalizes `split-at`, `partition`, &c. `put` generalizes insert & create.

a/symmetry can be seen in a map (alist) vs the `map` function (one fn over all data, where application to each datum does not affect other data in the structure being mapped over.) you can combine them: a `cond` inside a loop body. think of the `cond` clauses as exceptions to the norm, expressed by the `else` clause.

it's best to write a usable yet slower tool that produces a faster version of the tool.

why not combinators? they must be used in moderation. one musn't shoehorn their thinking into the particular models bulit of combinators, or even the structures that they operate on. it's a slippery slope, never immediately bad, but always eventually bad.

[quote,mercury crash course,https://mercury-in.space/crash.html#org250e070]
----
This might sound like a lot of quibbling over nothing, but there's a definite mental burden that goes unnoticed due to how omnipresent the burden is. It's why style guides often discourage very long functions. They note that it becomes hard to understand what the function is doing. What, precisely, makes it hard? It's that in these languages you can't just focus on the meaning of any single statement of the function; you always have to consider each statement in the full context of the rest of the function, and a larger context is more to have to keep in mind. In Mercury, when state variables aren't involved, only the goal you're looking at matters to the meaning of the goal. Even any variables referenced by the goal are like 'single-assignment' variables that you can be sure aren't having their meaning changed elsewhere in the function.
----

consider a simple function, common in functional style: `map`. we'll consider it by a common variety: as it is in the factor proglang—namely that it accepts _one_ sequence argument. want two? use `2map`. want three? use `3map`. want more than that? write your own. fortunately scheme's definition is actually sensible: it accepts an n-ary fn and _n_ lists, mapping over them in tandem. this is allowed in lisp but not in factor because lisp functions may be variadic. in fact, this gets us some *relatively* apl-levels of implicit symmetry: `(curry map +)` pointwise-adds any number of lists, since `+` itself is variadic, the same as j's `+/`. sadly that's as "array" as a non-array (or relational) language gets.

[source,scm]
----
(define xs (list (range 6)
                 '(0 6 7 -2 1 6)
                 '(100 200 56 27 35 1)))
(apply map + (map (λ (x) (map cdr (filter (λ (x) (= 0 (modulo (car x) 2))) (map cons (range (length x)) x)))) xs))
----

resulting in `'(100 65 40)`. i used `map` 4 times, and each of `apply` and `filter` once.

or in j:

[source,j]
----
xs=:(i.6),0 6 7 _2 1 6,:100 200 56 27 35 1
+/((#$1 0"_)#])"1 xs NB. the version that i prefer. exploits shape, boolean masks, & rank.
+/((0=2&|@i.@#)#])"1 xs NB. literal translation of the scheme code. exploits boolean masks & rank.
((0=2|i.@#)#])x+y+z['x y z'=.xs NB. j version of sql solution #1 except that it uses iota instead of abusing the coincidence that x=i.6
(0=2|x)#x+y+z['x y z'=.xs NB. literal translation of sql solution #1
(](((1&{)@$@[#:i.@#@]){{x+//.&(f&#)y[f=.0=2|x}}]),)xs NB. sql solution #2 translated to j, as tacitly as i could muster. filtering & grouping may be done in either order
cols+//.&(m&#)flat[m=.0=2|cols=.1{"1($xs)#:i.#flat=.,xs NB. same, explicit. notice the elegance of m defined defined of cols which is defined of flat. it would be elegantly expressed in a stack lang.
+/((0=2|[:i.1{$)#"1])xs NB. effectively the same b/c +/ effectively sums columns
((1 0$~#)#])+/xs NB. the easy way. easy b/c +/ relates by column, and returns a single row so there's no need for "1
(#~1 0$~#)+/xs NB. same thing, refactored
----

this solution exploits j's rank and reshaping/extension rules! rather than pairing `i.@#` then filtering that for evens, we directly encode that result as `(#$1 0"_)` then filter by that mask by sticking a `#]` on the left: `((#$1 0"_)#])`. then i tack on `"1` to do it for each row. within the `"1` i tack on `+/` to sum each row. this is much easier to write in j not only because the syntax is terser and thus more manageable, but also because j's array model allows us to discuss structure in terms of rows easily. lisp's list model is too general to be useful. its generality means that the programmer must be more explicit/verbose, and that the vm can't optimize as much. it also forces the user to make their own models, but its sequential (vs random [arbitrary]) access is limited and encourages creating data structures rather than exploiting arrays' symmetry (structural regularity).

also it's a real pain not being able to, wherever, push stuff to a stack for use later. the stack has not scope. that's nice. in applicative langs, though, we have scoped "local" binds. shoehorning everything into some set of combinators is generally poor design.

in factor:

[source,factor]
----
: xs ( -- x y z ) 6 <iota> [ 0 6 7 -2 1 6 ] [ 100 200 56 27 35 1 ] ;
0 xs [ [ [ 1 + ] [ 2 rem 0 = ] bi ] 3dip + + and ] 3map sift nip
----

my factor solution exploits the stack: that i can use the stack in any way that i want within `3map`; it's free access to implicit program state. i used an external counter variable instead of zipping with `i.6`.

i'm kind of cheating by using `3map`, but i don't consider that b/c factor should have written a single `map` which, like scheme, accepts a list of _n_ lists and applies an n-ary fn to them in tandem.

a simpler solution:

[source,factor]
----
xs [ + + ] 3map ! [ + + ] 3map is analagous to x+y+z in the 1st sql solution below
dup length <iota> [ 2 rem 0 = ] map
swap [ and ] 2map sift
----

this shows how high-order programming usually shouldn't be used. higher-order fns means nested fns, which means complexity. this flat style, though it iterates twice, is much clearer and is idiomatic of j. higher-order fns are especially difficult in the stack paradigm because we must track many levels of nested execution simultaneously, tracking which inner fns modify the stack, and how they return values to outer layers. in the sql versions below, `sum` is the only aggregate fn. `where` (`filter`) can be done with a predicate or as i've used it here, with the identity program, filtering-out falses. `group by` may be higher-order or not, as j's `u/.` may be considered higher-order, or may be `</.` which is not higher-order, and a verb may be applied afterward.

TODO: when are higher-order functions required? technically or practically? what does this consideration of higher-order fns imply about metaprogramming? should mp be avoided, too? just as higher-order fns can be flattened into a sequence of first-order fns, can mp be analagously flattened? under which models or not?

also, the explicit j solution translated into factor:

[source,factor]
----
! helpers
: AND ( arr mask -- filtered ) swap [ and ] 2map sift ; inline
: hs>arr ( hs -- arr ) array>> [ tombstone? ] reject ; inline ! why hash sets don't implement the seq protocol, idk.
! assume that groupby is defined, which is j's "key" (/.)

: xs ( -- X ) { { 0 6 7 -2 1 6 } { 100 200 56 27 35 1 } } 6 <iota> prefix ; ! now a list of lists
xs [ flatten dup length <iota> [ [ 2 rem 0 = ] map ] keep ] [ first length [ rem ] curry ] bi map over AND [ AND ] dip groupby >alist [ second hs>arr sum ] map
----

TODO: re-write this now that i've considered exploiting coupling and a/symmetries, as the beans/jeans example below shows.

* `/mod` would've given a seq of (listNum,eltNum), vs `rem`.
* this is actually incorrectly defined, even though i happen to get the same answer here. the defect is that i used a hash set when i should have used a list; /. allows duplicates, which is generally what we want here!
* `over AND [ AND ] dip` associated the mask with two vectors then applied it to each in two different steps. in the array model i'd have the mask as one argument then AND it with each of any number of vectors. i can express this is factor by putting the arrays to filter into a sequence (here `2sequence`) then ``map``ping over it with `[ mask AND ]`. of course, better than just array mappings is more general mappings, like i imagine that k uses. i've yet to compare k to sql.
* the ability to pass around a pointer is uncommon in functional langs, such as in scheme, j, haskell. it's probably uncommon in python, perl, ruby, etc, and i imagine it's still commonly done in langs that were originally non-functional such as c++ or java.
* TODO: how can passing around pointers make things easier? e.g. if we append pointers rather than values then we can we are _relating_, not just accumulating values into a structure! e.g. `: H ( -- h ) H{ } ; HS{ } "hs1" HS{ } "hs2" 4dup [ H set-at ] 2bi@ drop nip [ "beans" over adjoin ] [ "jeans" over adjoin ] bi* H .` modifies the hash table and thus the hash table is seen as a relation of the two input hash sets! we modify the hash sets after we add them to the hash table, which means that the hash table relates unidimensional relations. pointers are actually extremely useful: they relate actual objects rather than just values of objects! with pointers, we have actual _objects_ whereas in a purely functional framework we have only identifiers for values. (in prolog this isn't an issue, b/c symbols are abstract, not mere placeholders for values.) sadly, pointers are, like functions, unidirectional arrows. see my sql notes about using relations to encode digraphs.
* using `4dup` here is silly. it's there so that after i set H, i still have the vectors on the stack to modify afterward. it ends-up retaining the strings "hs1" & "hs2", too, which i remove by `drop` & `nip`. we immediately know that `4dup` is silly here, but do you see why? it's because there's obvious *symmetry*: the pattern `(V{ } k) ...`; thus we should use a symmetry primitive (namely `map` in factor, or just use j's implicit symmetry) rather than `4dup`. factoring _code_ is not the only factoring to do! factor things into symmetries, too! to factor-out symmetries of a relation [expression] is to identify them separately while identifying the asymmetry relation of them both: `expr` factors into `asymm(symm1,symm2)`.
  ** i would say that "any time that you go from one thing to multiple, put those multiple in an array then map over it" but why not always have _everything_ in an array and map over it?! use singleton arrays which are equivalent to their single member, and we can even encode the empty object by using the empty array! no need for `f`, _and_ our empty object retains type symmetry! we can use relations instead of arrays; any relation works pretty well, though virtual relations (set-theoretic predicates in prolog) is freeest. virtual tables in sql are either as good or nearly so.
  ** `bi`, `2bi@`, `tri*`, &al relate fns & data. if factor were better-designed, all these combinators would be discarded and an equivalent to j's rank would be used instead. like in j, plurality would be implicit, so there'd be no syntax bloat of encapsulating everything in array brackets.
  ** how i'd actually write it: `[ HS{ } HS{ } ] { "hs1" "hs2" } [ over [ H{ } set-at ] dip ] [ 2map ] keep second first dup . [ { "beans" "jeans" } [ over adjoin ] 2map . ] dip .`

NOTE: in factor, _vectors_ have variable size and _arrays_ have fixed size.  _shaped arrays_ (`arrays.shaped` vocabulary) is like arrays as in typed racket or array langs.

in sql:

[source,sql]
----
% sadly we can't assign indices to sql rows. fortunately i.6 is equivalent.
with t(x,y,z) as (values(0,0,100),(1,6,200),(2,7,56),(3,-2,27),(4,1,35),(5,6,1))
select x+y+z from t where x%2=0;
----

that version sees the lists as being pointwise associated with each other, each being a separate attribute. it is fixed to three lists. a more general version, which accepts any number of lists, is as follows:

[source,sql]
----
create table t(listNum,eltNum,value);
insert into t values(0,0,  0),(0,1,  1),(0,2, 2),(0,3, 3),(0,4, 4),(0,5,5),
                    (1,0,  0),(1,1,  6),(1,2, 7),(1,3,-2),(1,4, 1),(1,5,6),
                    (2,0,100),(2,1,200),(2,2,56),(2,3,27),(2,4,35),(2,5,1);
select * from t;
┌─────────┬────────┬───────┐
│ listNum │ eltNum │ value │
├─────────┼────────┼───────┤
│ 0       │ 0      │ 0     │
│ 0       │ 1      │ 1     │
│ 0       │ 2      │ 2     │
│ 0       │ 3      │ 3     │
│ 0       │ 4      │ 4     │
│ 0       │ 5      │ 5     │
│ 1       │ 0      │ 0     │
│ 1       │ 1      │ 6     │
│ 1       │ 2      │ 7     │
│ 1       │ 3      │ -2    │
│ 1       │ 4      │ 1     │
│ 1       │ 5      │ 6     │
│ 2       │ 0      │ 100   │
│ 2       │ 1      │ 200   │
│ 2       │ 2      │ 56    │
│ 2       │ 3      │ 27    │
│ 2       │ 4      │ 35    │
│ 2       │ 5      │ 1     │
└─────────┴────────┴───────┘
select sum(value) from t where eltNum%2=0 group by eltNum;
┌────────────┐
│ sum(value) │
├────────────┤
│ 100        │
│ 65         │
│ 40         │
└────────────┘
----

* if sql had a bit of abstraction then this would be elegant. i like sql's data storage & query model, though.
* very interestingly, `listNum` was not part of the query! we expect `(listNum,eltNum)` to not be a multiset, but for this query, we can simply say that `eltNum` is a multiset and still get the result that we want!
* this is the general data encoding method: sparse table where each spatial dimension has a column plus a column for each type of value associated with that point. you can simply associate multiple values with a single point by e.g. `values(x,y,v1),(x,y,v2)`. the ability to be sparse and non-unique very powerfully enables us in a way that apls do not. it's also very much easier to specify axes by name rather than using rank.
  ** whereas factor & lisp (and most langs) see the three lists as separate, j and sql see them as part of one arrangement that can freely change shape.
* *the relational model maintains relations throughout all computations*. in the j version, i had to manually and meticulously maintain relations among intermediate values within computations. i effectively had to ensure that each operation had the correct shape, which is fine, but unnecessary in sql b/c, rather than shapes, we have relations—*inextricable* associations among attributes. in sql we can pretty much say exactly what we want without needing to convert our thought into steps in an execution model. the *invariance* of relations is what enables us such freedom. it's exactly what makes prolog so nice: prolog programs are exactly statements of variance and invariance. it's implication, tacit logic. functions (or data structures) satisfy the same effect in non-relational programming, though we must be explicit in our use of them—a form of boilerplate code. relation actually literally _is_ invariance! relation is represented in all code that's factored of other code.
  ** invariance is seen in sql triggers and frp. invariance is 1. less to specify (tacit!) and 2. less to manage (less reasoning, safer)
  ** see next section for more on this

ideally we'd write it in something like prolog:

----
%pointwiseAssoc/2 takes seqs & rets a seq. all/1 converts all valid values into a seq. thus all(y) is a seq of seqs.
x in X, i=iota(length(x)), r=pointwiseAssoc(i,x), r(j,y), even(j), result=fold(+,map(pointwiseAssoc, all(y))).
x∈X, i=i.#x, r=i<->x, r(j,y), j|2==0, result=+/|:all(y). % better, apl-like notation. instead of +/|:all(y) we could've done map(+,all(y)) assuming that map works like in scheme.
----

.fns, frp, relations, and implications

functions can mimic relations to an extent if everythng is tacit e.g:

----
x1 is "abc",p is last(x1),x2 is append(x1,reverse(x1)).
% p=:'z'. % how to make this set x1's last to 'z'?
----

if `x2` is defined of `x1` as an expression, not beta-reduced to a value, then `x2` implicitly has a different value whenever `x1`'s value is changed. this is easily accomplished by defining `x2` as a function of `x1` rather than as the result of `append(x1,reverse(x1))`. in j:

[source,j]
----
   x1=:'abc'
   p=:{{{:x1}}
   x2=:{{x1,|.x1}}
   x2''
abccba
   x1=:'bats'
   x2''
batsstab
   NB. there's still no way to modify p such that x2's output/value changes! we're using fns, not relations!
   NB. however, we can define p as a function that returns indices of a structure s, then `for idx in p(s): s[idx]=f(s[idx])`
----

so we can mimic reactive programming by defining everything as functions i.e. defining them as relations rather than values, but dependencies are only one way. thus in the functional model we're still unable to identify a substructure by predicate, name it, then change the value of the named thing in order to change the value of the structure. we can easily do this in c, however:

[source,c]
----
#include <stdio.h>
#define P for(int i=0;i<3;i++)printf("%i ",x[i]);printf("\n");
int main(){int x[]={10,22,30},*p=x+1;P *p=55; P return 0;}
----

prints

--------
10 22 30
10 55 30
--------

sadly c can only refer to single indices, not substrings nor any arbitrary (sub)structure.

summary:
* if all expressions are evaluated only at query time then that's frp.
* unlike relations, frp is unidirectional
* if a structure supports efficient arbitrary indexing, then we can define fns that return indices, then use those indices to get values, then apply a function to them, then use those outputs to set the original structure at the indices.

masks are the same as predicates, except that predicates not only filter, but derive sets, too.

TODO: in prolog, how to do `S is 'abc',last(S,B),R is _S_but_with_'z'_instead_of_B`?

''''

.pure bullshit, pt 1

let's take a look at absolute programming assery in practice.

. `"hello" dup length <iota> [ "" 2sequence ] 2map` fails with "Generic word integer>fixnum does not define a method for the vector class.
Dispatching on object: V{ 104 0 }". what the _fuck_ does that mean? WHY is it trying to use integer>fixnum? furthermore, isn't integer already a fixnum?!
. `"hello" dup length <iota> [ "" 2sequence ] V{ } 2map-as` works fine. the problem is that `2map` tried to create a string out of a character and an integer, in some way that failed. i can't imagine how, given that `CHAR: h 0 "" 2sequence` and `104 0 "" 2sequence` both work. after poking around and pondering for about 10 minutes i realize the problem: each iteration of the map produces a 2-element string, and 2map, which, given its inputs, is equivalent to `"" 2map-as`, can't make a string out of strings. why? because, per its definition, it tries to make a string out of ELEMENTS; this confounding error message's useful implication is that a 2-element string cannot be seen as an integer codepoint and thus does not satisfy a fact of string's definition: that its elements are (representations of) codepoints. what we want is for `2map` to concat the strings, not try to match (2-element) strings as characters! our solution, then, is to collect the 2-element strings into a structure that can store anything (i'll use a vector), then flatten that into a single string:
. `"hello" dup length <iota> [ "" 2sequence ] V{ } 2map-as "" [ append ] reduce` is what i want. well, actually, there is one small tweak.
. `"hello" dup length <iota> [ number>string "" 2sequence ] V{ } 2map-as "" [ append ] reduce`. there we go. wait-! nope. guess what error i get: it's our old friend, "Generic word integer>fixnum does not define a method for the string class.
Dispatching on object: "0""! what the fuck. can you guess the problem this time? it's that `2sequence` requires two elements, but number>string returns a singleton sequence, not an element. we can't just replace `"" 2sequence` by `append`, though, because our first parametr is a character, not a string! generally numbers may be any number of characters, so let's turn our character into a string then append it to the string representation of a number:
. `"hello" dup length <iota> [ [ "" 1sequence ] [ number>string ] bi* append ] V{ } 2map-as "" [ append ] reduce` actually fucking works. and look how long that shit is! *101 characters*, not including `"hello" `.

un-fucking-believable. so two questions and their answers:

. why was such a simple idea so difficult to express?
  .. the error messages were unhelpful. this is due to extreme ad-hoc polymorphism (a design that implies multiple data types).
  .. we had to discern among various logically equivalent data types carefully: character vs string, string vs vector; and `append` vs sequence-from-elements
  .. we had to convert among logically equivalent data types: `number>string`, `"" [ append ] reduce`
. why is that expression so horrible?
  .. the fn names are long
  .. we needed a lot of quoting syntax, which is 4 characters extra each time: `[  ]`
  .. simply specifying the type conversions is verbose

so none of these things is a fault of the stack model. however, the stack model is good only for succinct code. when the operations are many and all practically logically worthless, then the stack makes reasoning difficult. tracking the stack through so many operations is difficult. however, when the operations are few, and the stack is actually exploited for its elegance, then it works well! then again, we're probably better-off using relations anyway, since they aren't nested, and are thus easier to write & refactor.

the solution in j is 21 characters: `[;@:(,&.>)<@":"0@i.@#`. it's a bit cruddy itself because it requires boxing just to keep elements separate. or you can say that boxing is required to avoid padding. another solution is `[;@:(([<@,":@])"0) i.@#`, 23 characters long. if you remove the `;@:` and `<@` then you can see a list of values, which seems fine, but it's padded with spaces, thus adding extra erroneous information to the actual computation—the "linguistic constraint" common proglang design flaw. the j version is the same logic but, thanks to j's unified model (the array being its single structure), we don't need to specify as much:

* `;` is "flatten", which in factor was `"" [ append ] reduce`.
* `map` is implicit or modified by `"0`
* atoms are not a different type from arrays; they're only a different rank, which is a difference as (in)considerable as factor sequences being different lengths

.overview

keywords: bound vs free, symmetric vs ad-hoc, axiom, algebra [universal algebra]/model & notation.

within each algebra's set of axioms are predicates defined of particular objects and functions e.g. T = T ∨ F, which implicitly:

. says that ∃ T & F which are special objects of the algebra (since they weren't brought into scope by ∃ or ∀)
. partially defines `∧` (with the total meaning specified by other axioms involving `∧`)
. is a predicate (though this predicate being an _axiom_, by definition it always holds, though that's moot here because _always_ is practically meaningless because the predicate is of only specific elements (viz T & F) vs unbound variable e.g. ∀ a : a ∧ F = F.)

basically, this document asserts that the level of specificity/complexity (measured by the complexity of a definition or the size of a specification) is the degree to which using it is painful and inelegant. common examples of over-specified things:

[options="header"]
|=======================================================================================================
| harmful                                | preferable
| language                               | algebra
| syntax                                 | regular encodings (which _may_ support any representation(s))
| coding                                 | specifying constraints
| semantic structure / strict guarantees | [naming] conventions / _practical_ guarantees)
| lists                                  | relational databases
| data structures                        | abstract structures
|=======================================================================================================

the preferred alternatives are less constrained, more flexible, dynamic, or abstract. basically, everything is context-dependent, so it's foolish to specify anything outside of a context. therefore everything should be abstract, specified (automatically) to any context.

varieties of partition which is harmful:

[options="header"]
|=======================================================================================================
| harmful                                | preferable
| programs (countable)                   | code (mass noun)
| modules                                | a database
|=======================================================================================================

other miscellaneous harmful things:

[options="header"]
|=======================================================================================================
| harmful                                | preferable
| functions                              | contextual mutations. see `purity.adoc`
| [holding] data structures [in memory]  | loops (traversals)
| recursion                              | loops
|=======================================================================================================

languages are bullshit. they're all doing the same thing: putting or (re)moving data from one place from/or/to another. the only thing making them different is 1. their scoping model; 2. any builtin support for some structures [data patterns]; 3. how they sequence operations. oop classes, sql tables, lambdas (especially closures), lua tables, lisp lists, arrays, &c are all grouping & scoping mechanisms, some relative to other scopes (e.g. oop objects), and some indexed by identifier and/or ordinal. regarding ordering actions, most languages are so unsophisticated that they do exactly and only explicitly what you instruct; the statements cannot be confidently assumed to be optimized, and they're expecuted in the exact manner and order in which they're syntactically specified, unlike solver/strategy-based languages like sql or prolog. pure languages like scheme or haskell do not execute exactly as their code reads, but the programmer must know their simple evaluation strategies in order to write effecient code; one cannot write whatever code they wish and still have it be fast, so these languages effectively do exactly as the programmer says; only the syntax does not describe what one would naïvely think.

for a language to be good, it must satisfy some qualities:

. no needless constraint(s) e.g. need to use classes, or overy-particular scoping systems, or referential transparency, or module system (see `eval` below)
. enable defining syntaxes—generally like red/rebol, or factor, e.g. how double quotes are non-builtin syntax for string literals, or brackets are non-builtin syntax for list literals. unlike lisp macros, which are constricted to being sexps.
. `eval` (and i mean a true eval, like in picolisp: loads/imports can be done anywhere in control flow, invoked as ordinary functions rather than special forms.) an example of a bad `eval` is racket's, where one cannot simply call `eval`; they must do some special & particular namespacing/scoping stuff first. furthermore, in racket one uses a special form, `require`, to load files, and files each have their own namespaces ("modules"), which makes `eval` of a file (nb. evaluating a file is often called `load`) complicated. as mentioned, `load` is just `eval` of a file. the fact that code may be in a file, or database, or stored in any other form, is independent of the information being stored, and thus should not concern `eval`. in fact, the programmer should be free to store code however they want, and then `eval` it. the programmer should be responsible for scoping things as appropriate (which should be as simple to specify as a simple tagged list (where the tag is here in the head): `(define module (scope module1 module2))`). again, this is simple in javascript or lua: `x = {mod1: require module1, mod2: require module2}` which is a form of qualified imports, equivalent to haskell `import qualified Module1 as mod1`. lua has no module system; files (like anything) can be evaluated; like any function, the last element is the return value; to effectively be a module, the last statement of a file should be a map from fn names to fn definitions. such a definition is, if one defines a macro, which cannot be done in lua, from identifiers to export to a map (i.e. (list fn ...) -> {fn: fn, ...}) then this is as simple as `(provide fn ...)` in racket.
. grouping operation e.g. `cons` (the relation primitive) or list or hash map literals. asymmetry is expressed by tables and symmetry by sequences. thus a good language should have efficient support of sequences and tables (hash tables, alists, or sql tables, which allow indexing on any attribute.)
. not distinguish between "code" and "data"
  .. not treat code differently dependent on whether it's being statically vs dynamically considered (or being completely dynamic is good). again, if everything's data, then this dichotomy of considerations cannot exist.
. bindings: the ability to associate labels with any part of the program. namely one should be able to label a part of control flow (for use with goto) and must be able to bind identifiers to variables anywhere. c allows these both, but features a cruft: the programmer must declare a variable before they can bind to it. this reflects the fact that, in c, a variable is not just a part of the program [logic], but literally a datum present in memory during runtime (on stack, in register, or in heap.)

a good consideration of a language is as a traversal over a data structure. applicative languages use trees (asts); stack langs obviously use stacks; prolog uses a lattice; sql uses relations. queries are traversals.

* codfns apl compiler demonstrates that we can compute using only a gpu
* _metaprogramability_ is given by ability to produce & evaluate routines/expressions.

looping techniques: modify state with jmp, [tail] recursion, or backtracking.

if you already commonly program, ask yourself: "how much of my intelligence or effort is spent just making my code clever enough to work nicely with the language or framework that i'm using?" if you were to ask a non-programmer how to express the same code, how would they do it? how simple would their code be compared to your code? obviously their code must be correct (namely for edge cases, too.) given that they haven't chosen a particular programming language or toolkit, is their naïve code efficient, or can it feasibly be optimized or expanded into efficient code?

rather than type systems, which only check that code is correct, it's better to have parsers (or macros) which either produce a result or a [compile-time] failure; their superiority over types is that failure is a part of the program just as it is during runtime. to the most extent possible, the *encoding* (incl syntax) or *structure* should make incorrect encodings obvious. for example, if everythnig uses i32's, then accidentally putting a string be obviously incorrect; an example of a structural constraint is that a list have no more than 5 elements; putting more than 5 would be obvious. though this convention should make writing correct code easy, one could, if they find it worth the effort, put-in static checking mechanisms such as lisp macros that check for proper syntax.

conventions avoid a common fault of langs: constraining the lang in many ways, then clumsily trying to compensate by idioms, design patterns, or counter mechanisms, e.g. racket imposes the lexical scoping, but features a special mechanism for dynamic scoping; racket features module phases, but a special token that makes a module's declared symbols persistent across phases. are these options because sometimes one choice is appropriate instead of others, or does the existence of multiple choices show disintegrated design? beware any language that "imposes" a restriction but somewhat discreetly offers a per-occasion opt-out mechanism.

naming conventions are better:

. easy regex for finding symbols for refactoring
. less constraint means more flexibility, though [common] intention is clear

also conceptual representation should not reflect technical encoding. e.g. `(map add1 (map add1 xs))` is...well, bad notation anyway; `map add1 . map +1 $ xs` is better: it represents composing fns. of course, this code should be parsed into one that traverses xs only once. there's a common assumption in how to write code and write code interpreters that must die: that code is broken into _instructions_ rather than general ideas. forget `map +1` as an instruction; consider it generally as an idea. then the question arises of its meaning and grammatical & semantic rules, e.g. that map f . map g = map (f . g). i expect / hope to god that apls do this. such simple & obvious compositions of ideas is taken for granted in natural language (which is logical & semantic) but in programming it's still assumed that the programmer needs to specify everything _literally_, since programs these days are still glorified instruction sequences, excepting prolog: the only language that can be stepped through forwards or backwards, and whose statements' order never matters. often but not generally is a sequence of arbitrary _actions_ a (sub)program. this is expressed in j as link:https://code.jsoftware.com/wiki/Vocabulary/Loopless[expressions rather than statements].

another main idea is to look at the form of data devoid of meaning/interpretation. consider data only as information [information theory]; *generally code should not reflect interpretation.* code always permits multiple models [algebras] simultaneously. only some of these are human readable, even by a programmer. similarly, only some of these models should be considered [reasoned] by a programmer. progarmmers should define axioms, identify structures & operations that implement those axioms (e.g. uncons & cons with lists, or opcodes with registers), then perform arrangements of manipulations that eventually tend data toward a goal while preserving invariants to guarantee sensible future considerations thereof. this is similar to how we use closed form expressions to reason about sums rather than summing every number.

example: traversing a tree: the traversal must not change any element of the tree. let's say that we want any element that matches a predicate. hopefully we've encoded our tree to make this search fast, which means mere parallelism, since the predicate cannot be known a priori; we can't optimize the tree based on the kind of data that it has. this is obviously true if its type is `∀a. Tree a` i.e. *free* in its data's type.

the developer should not care to understand or be able to trace/visualize each step in a process that transforms state, change-by-change. instead, the developer should know, by applying axioms, what the end result of a process (loop or series of operations) will be. all data's axioms, and their average properties, should be considered; for example, ages are never negative, and usually within [5,90]. they're just discriptive—not computable—data, so their encoding only must be lossless; it's not constrained by e.g. needing to support addition. also the distribution of data tells us the average entropy and complexity of getting or storing uncommon data.

this being said, if an implementation or logic error is present, being able to view state is useful. furthermore, especially for large computations, it's nice to not need to redo the whole computation from fresh initial state; a system whose state is known can be brought from that state into the desired state more efficiently. state should be stored in a database, so that the state's internal data structure does not need to be viewed; it can instead be queried like prolog or sql—more than just viewing a subset of the internal structure!

data used in the condition of a loop must support at least one axiom, even if that axiom is mere equality. if the loop is traversing a structure, then that structure must have at least one axiom: that it's constructed by repeated application of some closed operation. usually the inverse thereof is used to traverse the structure. typical loops are application of a fn f to some x where such application tends toward a y : f(y) = y; y is a fixed point of f. for imprecise data (e.g. floats) we can check whether f(y) ≈ y i.e. |f(y) - y| < δ. fixed points are usually the base case / exit condition. loops can break before then, though, if a condition is found to be violated, which means that we discovered _during_ the loop that its input was inappropriate. such checks are sensible only if we can't validate in O(1) before looping.

.todo/discuss
* examine outer vs inner products [apl] and `zipWith` vs cartesian product (pairwise vs for-each) and `join on` vs `join` (cartprod)
  ** `map` is appropriately like unary `zipWith` (seen altogether as variadic `map` in racket): pointwise application of a n-ary function.
* consider metric spaces for program optimization (less redundant encodings and more efficient traversals)
* a _program_ is anything that uses information. it's commonly a state manipulator, or produces a variant, reinterpretation, or implication of a state.
* langs as "models": evaluation as traversals over structures: applicatives: rose tree; stack langs: stack; prolog: predicates. _model_ means the data structure used to represent linguistic constructs, and the evaluation by traversal of those structures. inelegant, irregular langs like java, python, have multiple structures, many ad-hoc rules, and complex models.
* metaprogrammability is important in the same way that recursion is. see a program as a data structure; don't you want to be able to manipulate data structures? that a structure can be evaluated is not special among any of its other properties. lambdas can be evaluated and passed around, so why not programs? in a lambda calculus language like haskell or scheme, is there really any difference between a program and a lambda? even if there is, why care? why define a proglang by some set of mandatory constructs? i want to _code_; why force an encoding on me?! why force _language_ on me? all programs are particular manipulations and particular relations. that, and nothing more particular, is what my programming tool should enable me! i want to *define relations and manipulations.* programs, as all data, must be encoded by some encoding(s), and those encodings must be represented by some syntax [representation scheme], whethrer textual, graphical, audial, or whatever; i must be able to "read" & manipulate the program. anyway, metaprogrammability acknowledges that there is no generally sensible way to consider code separately from other data/information. by no coincidence, all metaprogrammable "languages" are hardly languages; they're just relations e.g. prolog (`rel(vars...)`), factor (`vars ... rel`), scheme (`(rel vars ...)`). the latter two support `quote` & `eval`, and prolog has no need for that b/c programs are just sets of facts; evaluation is not part of the program itself, so quotation would have no meaning or a quoted object would equal it unquoted.
  ** when you consider programs as models (traversals of data structures), a lack of metaprogrammability is as ludicrious as being told to program using only one data structure and not be able to make your own. like if i were to say that you could use only lists and hash tables, because that's just how the language is. you can't define a ring buffer, or deque, or b-tree, unless you define them in terms of lists or hash tables. now that's not terrible, since—mostly—any structure can define any other, but why start by arbitrarily limiting which structures are available?! why not use the least constrained structure: the [binary] relation? indeed, this is what lisp & prolog do! isn't it interesting that the languages whose data primitive is mere relation are metaprogrammable? factor goes a step further by not even having a special construct for relating things; instead of `(cons 1 2)` you just put `1 2` on the stack so that constants are implicitly sequentially related and are not even considered different from the rest of a program.
* _flat_ means _non-recursive_. consider the structure `'(a b c)`. that's a "flat list." however, it is not flat! we know this certainly because lists are recursively defined: `'(a . (b . c . ()))`.
  ** TODO: even in a truly flat array the elements are defined in terms of their relations to other elements; they're defined in terms of their context, but where their context is represented by literals rather than free symbols. when i've been saying "flat" i really meant context-free i.e. the context is given in terms of free & bound variables. this is the only satisfactory definition. it's how prolog does.
* total optimization means compiling, and more constraints means greater potential for more optimization. this is at odds with program generality and general runtime modification. of course this has nothing to do with how facts/codes are _expressed_; again, no code for instructions should be taken literally—only virtually, and _macro_ should mean be pre-runtime metaprogramming, distinguished from runtime metaprogramming, since pre-runtime execution does not affect compiled code's execution speed. macros can be seen as partically-evaluated programs, just like _images_ in factor. partial execution is a form of optimization. to optimize optimization, all code that can be executed should be, leaving only computations dependent on runtime variables yet uncomputed.
* metaprogramming is closure over execution: that the execution of a statement produces a statement that can be executed and so on
* order is not needed; only indexing is. re-ordering is just re-indexing; it's just installing an intermediate map from indices to new ones. this is true of order of arguments (including both applicative and stack langs), order of columns or rows of an array or relation [relalg]. order is useful to make encoding schemes shorter b/c the indexing is implicit; it is unhelpful when indexing must still explicitly be done.
  ** a similar argument can be made for re-grouping, since all things belong to the universe [set]. because all programming is regrouping, reordering, and basic arithmetic, we see that there's hardly anything to programming. at least given current programming tech, this presents a paradox!
* stack langs do not particularly use stack(s) as data structures. the stack (TODO: investigate multistack models) is used for encoding a program only viz control flow (equally including branching and passing to & returning from functions, which technically _are_ control flow, seen in asm as pushing to the stack then jumping)
* though apl, lisp, & factor are axiomatic, they're significantly restricted in being strictly order-based rather than unordered relation of primitives like prolog; ideally we want not need to bother with ordering.
* applangs have asts, a recursive structure. stacklangs use stacks, a linear structure. thus apl fork x(fhg)y (i.e. h(f(x,y),g(x,y))) is expressed in factor as `2dup f g 2bi* h`. stack langs effectively flatten asts. instead of nesting expressions, we leave expressions' outputs on the stack so that later expressions can evaluate.
* indices match the polynomial or number/radix pattern, Σ[i]x_i^i. this is why we can refine single-axis indices into multi-axis indices via modular arithmetic. this matches with polynomials being seen as vectors e.g. <6,3,2,1> to mean 6x^3+3x^2+2x+1.
* nb. stack langs are postfix b/c files are read from left to right b/c files are so read; prefix would require either reading the whole file then evaluating it strictly in reverse order, or a foldr/thunk type eval from left to right.
* stack langs are implicitly curried. one of my favs is n > "for greater than n" `(curry > n)` [scheme] has the opposite effect! haskell gets around this by using a fry-like syntax: `(> 5)` i.e. `(> _ 5)`, or ``[ _ 5 > ]` [factor/fry]
* a nicety of stack langs is that they maintain state across evaluations; in a repl one can see the current state(s) as they consider and evaluate expressions, cf applangs which require binding and/or accumulating one large expression, which is messy or at least inconvenient.
* `if` statements sharing common predicates should be refactored from `(if p x y) ... (if p a b)` into `(if p (list x a) (list y b))`
* when the whole program's state is available, there's no need for functions, since all possible inputs are already in scope. obviously only known data would be used; if a function defines some variable `a` then another function would not use `a`, unless it were sure that `a` is still bound (i.e. that its defining function hasn't already finished and freed `a`.) `a` here is not access-scoped but is still temporally scoped; anything can use it but only if it's still bound. of course "functions" could still be useful & appropriate, but only insofar as being relations of vars, expressable as a simple tuple. then again, that's all that functions ever should be: tuples of an input object (list, vector, table, &c) and some such output object. functions denote relations unrelated to any context, excepting the existence of other functions or constants. there is no strict distinguishment between functions or constants of a particular context vs outside of any context. indeed, everything is in a context of the things by which it's defined; thus only primitives are truly context-free. therefore the very idea of functions is fundamentally flawed. one may as well dispell that error and instead have subroutines all of whose parameters are implicit and identified by their presence in the subroutine body. the only thing is to make a _syntax_ to bind vars e.g. with `f x y := x + y * 2`, `f 3 4` is syntactic sugar for `x y = 3 4; f; restore x y`. this is precisely what picolisp does. _functions_ are an overly specific consideration of the the contexts in which binds exist. the error is in making functions special language builtins rather than some convenience devices for removing or injecting relations into contexts. there are many valid systems for context management which vary among specific programs. again, a good language does not distinguish between `(a,4)` in a hash map and `a = 4` as a program variable. sql somewhat succeeds here, as all vars are stored in tables and manipulated as data; its success is only partial because tables themselves are not manipulable data, except in sqlite, where they are stored in a global table of tables. the most appropriate scoped bind mechanisms simply associate binds with some program subset e.g. `let x = 4 in x + x`, except that `(x 4) {x+x}` is cleaner syntax, showing that we're working only with relations. indeed, this syntax correctly & helpfully shows that `let x = 4 in x + x` is equivalent to `(\x -> x + x) 4`. this syntax supports `y := (x 4); b := x+x; y = ∅ {b}`, which binds `y` to a list and `b` (which can be said to be a function of `x`, since it's defined in terms of `x`) to the expression `x+x`, then says that within this anonymous evaluation of `b`, `y` is unbound. a device to choose immediate vs delayed evaluation (which differ because definition and evaluation contexts (bind sets) differ) has not been specified for this example, but would be integral for any language.
* by reference should always be the default; by value is less efficient and less frequently used. in the functional paradigm there's not even any difference because nothing is ever modified, unless it's in a loop, in which case everything defined in terms of the now-modified thing is now recomputed. really the question should be, for any variable, whether computation should be done once at definition or usage time, or done for each usage. once at usage is lazy eval. once at definition is dereference in the definition. per usage is dereference per usage. dynamic vars are effectively see everything defined with dereference per usage, at least by default; there must be a mechanism to dereference in a definition, though. keep in mind that this is only relevant for vars defined in terms of other vars; none of these concerns matter for vars defined in terms of constants.
* i like for fn seq `× z +` (like stack lang) to compose as \x y -> x × y + z
* set theoretic binds, e.g. {x(t):t>1} or {x(t):t∈{1,-1,0}}. sql does this already (kinda) with `select x(t) from tbl`. most langs accomodate this by `for` loops (e.g. racket's use of sequences in its `for` for). HOWEVER! `for` loops are a sequential/nested (i.e. they've a definite unique location in the AST) whereas sets are not! furthermore {x(t):t>1} is an abstract structure (idea), not a data structure! it's calculable. it's not an operation, and so has not a measure of strictness, since it's not an evaluation, though many possible _considerations_ exist of it. anyway, sets aren't scoped! like prolog, sets are predicates calculated by logical implication of others; sets are predicates, and the whole system is just a collection of constraints/facts that're reconciled. ∃ or ∀ determines whether we're searching for one or traversing all of a set; these complete the control flow operators, having already considered manually-specified facts which naturally imply `cond` selection and structures to be traversed. here ∅ is interperable as void, ⊥, or any empty structure. sets free programmers from concern about _how_ relations are encoded (again, e.g. `(((x . y)) ((x . z)))` vs `((x . (y z)))`). the best encoding is never known in advance, and is trivial for a computer to calculate as it's just a constraint problem like graphviz' graph layout or determining a lattice of sets under subset.
  ** set notation is agnostic of language, data structure/model, encoding, or code organization (scope, whether of a module, method, object, or binding clause). sets specify exactly the program spec w/o implemenation-specific details.
  ** programmer should specify graph as edge set; program should find path therethrough to answer query. this is what prolog does, recognizing that lattice is a specific variety of graph. in fact, unreachable nodes are then obvious and tell the programmer what facts remain to be exactly specified. these graphs are a bit more nuanced, though: edges are labeled by transforms: subsets (which can implement removal by discarding subset) or injections. really, though, an injection is just statement of a relation. a:b injects a into b, and can be phrased as b->b∪{(i,a)} where `i` is an auto-incrementing implicit index. even if indices are non-continuous, they can be traversed in order by sorting by index before traversing. a:b means the relation/fact of a being in b. you may think of it as a being in the table b, but tables are appropriately symmetric & flexible if they may contain other tables, though this may be better technically encoded as all tables being flat, and hierarchies (which may be general graphs, not just DAGs) encoded by tables referencing other tables like how foreign keys or joins do; it's basically, instead of a∈B or A⊂B, `insert into x(val,contained_by) values(a,b)`. it's like c pointers, except that being a pointer or not isn't anything special; a thing is merely _effectively_ a pointer if it can be joined by that value with another table. another example is the nested lists `(a (b c) d e)` encoded flatly as `(de B '(b c)) '(a B d e)`, which can be made flat by splice-unquoting `B`. this decoupling 1. is simpler to implement; 2. allows things to be containd by many things simultaneously, 3. sees containment as an arbitrary binary relation. relations permit traversals or transforms. the _inclusion_ relation can encode control flow. joins can be visualized like `(a b (c) d e)` to join `(a b c)` with `(c d e)`. it can be interpreted literally as lisp.
* what can we learn from T9 digit keypad input method? see how it saves keystrokes and, when ambiguous (as tokens usually are) provides the most likely value first (default) but allows quick selection of subsequently probable values; this combination of *compressed-and-ambiguous lookup token* and *quick disambiguation* makes input very efficient! (similar to probabilistic data structures.) can you imagine programming on a feature phone?
* code should be structural. e.g. using `x` instead of `w` & `s` in `(command-line #:once-each [("-w" "--warn") x (set! warn-thres x)] [("-s" "--sleep") x (set! sleep-thres x)])`. the programmer is not served by `w` & `s` more than by `x`. really the commonality ``(λ (s) `((map string-append '("-" "--") `(,(string-ref s 0) ,s)) x (set! (string-append s "-thres") x)))`` should be factored, but of course, for a quick hack, it's fewer keystrokes (though more text) to use the unfactored version.
* quasiquotation is a perfect example of good syntax: it's notation for structure. the parameterization of qq is good: just use `,` and `,@`/`~`.
* syntax should be composable like hash map union, up to collision (and the programmer must then provide disambiguations for only collisions). consider `for` and `match`. `for match` should be easily to support e.g. `(for-match ([(list* a b c rst) xs]) ...)`. this can easily be done in lisp. however, generally data structures are easier to compose than syntax, since data structures are exactly data, whereas syntax only represents data. in the case of lisp, however, the syntax _is_ the data structure (homoiconicity), so syntax is as easy to work with as axiomatically structured data.
* when should map be implicit? we can use map, fold (loops), and cond to clearly denote 1. application to group whose elements are symmetric on grouping; 2. incremental mutation; and 3. ad-hoc map. but what about applying length to a list of lists? should it be mapped or not?
* `(x (a v) ...)` can be seen as applying attributes a ... of values v ... to x e.g. `(x type float parse #rx"[0-9]+")`. for attributes without values, instead of parsing by car then parsing cdr as (k v) ..., we see cdr as a stack, and loop, taking n items off the stack based on the top of the stack, just like how stack-based langs take n args off the stack where n is specified by the fn on the top of the stack.
  ** rather than literal `(map (λ (field) (cons field (a1 v1 a2 v2))) '(a b c))` to associate common attributes across many data, it's more efficient to just associate them plainly: `(+ (a1 v1 a2 v2) (a b c))` where `+` is an arbitrary symbol denoting combination
    *** this is a good example of using quoting to express ideas instead of merely providing a sequence of instructions
* no language can account for what the developer wants to do, nor how to do it. thus a "language" should support only those things that _must_ be present in all programs, namely relation, β-reduction, and i/o. hmm. looks like there's no language there. yup. languages suck. no, not that all languages _yet_ suck; language itself is a bad idea!
* keep language & documentation separate! if a linguistic construct does not affect the behavior of the program, then it belongs in docs, not lang.
* lisp is only as nice & flexible/dynamic as lispers say when the lisp does not discern between symbols and identifiers, and uses only alists as a scoping mechanism. no identifiers, only symbols. no structs, only alists. all scoping done by data structure bounds. pil does this; it does not distinguish between symbols and identifiers. however, there must still be a way to define items in the list in terms of other items bound in the same list (effectively `let*` & `letrec`); perhaps this means using vectors instead of lists.
  ** the ideal language uses lists (or other structure(s)) as its only way of storing data, there being no _language_: only manipulators of these structures.
  ** `(A data ... | fns ...)` (abstraction over factor's `cleave`, `spread`, and `napply`, where one is chosen by whether #data or #fns == 1 or #data == #fns) works on lists and accomplishes both ad-hoc and symmetric relations: #fn == 1 or #data == 1 => `map` [over axis] which is symmetric over given axis; and #data = #fns => 1:1 ad-hoc map between data & fns e.g. `(A ((3 16) @ add1) @ add1 /) => '(5 1/17)`.
    *** otherwise there're other methods for associating parts of data with facts, e.g. naming symbols by a (set of) convention(s) so that the name reflects some way to interpret or use the data. obviously all methods must _somehow_ relate things a/symmetrically, but beyond that there're arguably infinitely many ways to express that.
  ** the most important fact about this style is that it looks at data by definition/relation exclusively, and thus is the most direct representation of the defining facts of the program itself. contrast this with an encoding of these facts by language syntax. the syntax is not algebraic nor plain. it distracts from the program, fooling the developer into interpreting the actual program in terms of the language—but the program is what it is, not defined by any language [representation]!
* constraint without compression is bad.
* the thing that everyone got wrong about macros is that they considered _syntax_. i've always thouht of lisp as syntaxless i.e. its syntax is merely the very least needed to encode arbitrary data (it includes only grouping and, for functionality, exploits ordinality of sequential items [of the same group] viz head vs tail). however, even lisp said that that should be syntax so that macros can be done! how stupid to include the constraint that the syntax (user-written expression) must directly encode the code-as-stored-in-the-system! homoiconicity is useless; just define syntax as a PEG that parses into a programmatic object. macros are syntax -> syntax. why do that instead of syntax -> object? the syntax isn't _the encoding of a program_! it's _one_ that exists solely for the convenience of the programmer! template haskell made the same syntax -> syntax mistake! programs are not syntax! they're particularly ordered information! syntaxes (plural) only _represent_ these orders. the truth/fact exists independent of any representation, much like an idea has meaning before it's considered.
* no single point of definition. implicit def. do we organize programs like trees or linked lists rather than arrays? i'd like to be able to reorganize programs as easily as apl rotates or transposes arrays.
  ** this is what makes hooks awesome. there's no particular place in code that you need to insert a hook; you specify anywhere a relation between the hook and a routine.
* any expression not ultimately used in a value passed to an action (e.g. print) is useless
* we see lexical scoping become amazingly awkward or verbose when the user [of some fns] isn't the chooser. for example, say that the entry point chooses an implementation of an ad-hoc polymorphic object by a cmdline arg. that choice must be communicated to the user of the choice. usually this is accomplished by passing it as a fn arg. (blocks of code are pamatererized by `let` blocks, btw.) dynamic binding provides a solution, but lexical scoping can still hamper its flexibility, depending on the system (e.g. in picolisp i can define `x` in main and any code run thereafter can use it, whereas in racket i'd need to define `x` as a parameter (dynbind) in a module, then have both main and other modules import it, and have main set its value): namely you may still need to partitioned your modules properly so that the requirement graph is a tree. for example, the cmdline arg `test | backend1 | backend2` chooses a dry or normal run through a program; the dry run implementation sees each abstract fn defined as a print statement. each of the backends implements actual instructions. all choices must be in the user's scope. they need be also in the chooser's scope only if the chooser passes them rather than something that represents them, such as an interned symbol or a string, e.g. `(use "backend2")` in the chooser; in `use`'s scope there's a map from string to actual fn collection. ad-hoc poly is always simply implementable by the shape `((id ((id fn))))`. still, it's nicest to have the fns be directly accessible, as though they'd been defined in the same scope that uses them instead of needing to use `send` [racket] or `assoc`.
  ** pil's symbol/identifier model is very apt: it enables purely non-functional programming (by binding values to all the identifiers used in a function to be called) but also supports the function syntax, which is terser than defining each symbol, and associates a scope with the bound identifiers. for example, `(de f (x) (+ x 2)) (f 3)` is equivalent to `(de f () (+ x 2)) (let (x 3) (f))`.
  ** modules' general scoping problem is easily demonstrated by that f & g can be corecursive but only when they're defined in the same module. seems like modules supporting parameters could fix this issue, but generally being constrained is the issue, which in this case is done by modules not being first-class, and therefore largely asymmetric with all other code.
  ** NB. i mean _context_ to mean a generalization of _scope_ from a delimited section of a program to the whole state of a program or other set of predicates.
  ** with modules scoping there're no truly global vars; they're all scoped by their defining module
  ** generally, ask how important parameterization is; for example, you may write a single file that uses no parameterization devices, but the parameterization is simply using the file as a template; to parameterize, just replace a definition. you need parameterization devices only if you are using multiple parameters either simultaneously or you're frequently running a common computation but with different parameter values. parameterization (which is merely making a variable free to be bound elsewhere) trades simple, static binding for slightly more complex context-dependent binding. thus, each time you consider parameterizing, see it as generalizing the binding from one option to many context-dependent options, which naturally begs the question: "which contexts exist, and what is the map between contexts and the value to which we're binding?"
    *** parameterization breaks symmetry about bind value and context, turning into an ad-hoc set of pairs of context & value.
  ** classes instead of dynbind may be appropriate if multiple instances of a type class are needed simultaneously. i need to consider this throughly before knowing whether generally oop is advantageous over dynbind for this use case.
  ** dynbind makes everything parameters implicitly: anything can be defined anywhere and used anywhere; hence anything may be a parameter of anything else simply by a thing _using_ a symbol assumed to be in scope, without requiring that the symbol be _given_ to it. of course, other binds have this same property, but require particular definition and inclusion of contexts (e.g. A & B require C, A requires B, and x∈C. C exists only to be included by A & B because both use x, but A binds x then B uses it. it'd be nice to define x∈A but that'd imply circular dependency between A & B.
  ** TODO: just like dynbinds are effectively implicit definition of programs, so should there be a way to implicitly define relations, namely for easy ad-hoc polymorphism. this is akin to adding a relation to a dynamically bound map. this is accomplished by prolog, and by prolog embeddings e.g. racklog for racket or pilog for picolisp. reading racklog's tutorial, i'm pleased that control flow is implicit.
  ** my favorite solution to this problem is to continually build the state of the program as it runs instead of passing state in a big fn composition graph. this is basically the RIO [haskell] paradigm except 1. it doesn't require anything to be declared so that it can be imported into both the user's and choosers scopes independently; and 2. it doesn't group objects together [into an ADT], so we can declare attributes anywhere during execution, which may affect later computations (no single point of definition.)
  ** any system that does not feature imports has a good level of flexibility. examples are lua, js, picolisp; in these langs "import" is not a concept; instead, we can simply run other files' code and optionally bind their return value.
  ** it's interesting to think about how all programs are just sequences of instructions or loops, and all of those instructions are either setting/updating values or executing actions. loops either map or change state until it matches a predicate. map is mere multitude and is thus parallelizable. loops, e.g. folds, are not generally parallelizable because the nth computation depends on prior ones i.e. a key difference between map vs other loops is that the loop is stateless. one may argue that map is defined in terms of fold, and fold has state, so therefore map must have state; however, this is incorrect: map's "state" is a list that's being accumulated, but this information is already present in the input list. `map id` is ineffectful. `map f` sees each application of `f` being independent of others, and applying `f` to each element is `map`'s only effect i.e. it's the only new information that `map` provides. in fact, `map` provides different information from `f`. they are independent, though they can be used jointly (think _joint distribution_); *their joint composition is orthogonal.*
  ** all programming can be interpreted as setting things up then using them. setting state / [fn] parameters can be seen as setup. people are used to thinking in terms of fn units, that the fns alone are computable concepts. this is fine, but should be interpreted as each fn being something that adds or removes some definite information; the information transform (representation/encoding) or addition or removal is precisely the fn's definition. two fns that add or remove the same information are equivalent up to transformation/encoding.
    *** of course, fns considered as such must be independent/orthogonal, and their _general_ composition must not be in any particular order, though obviously each particular program may order their composition; here the set of fns with an order is the very definition/signature of the [unique] particular program; thus the selection & order of fns is the program's information, not the functions'.
  ** also all programming can be thought of as sets of rules, each of which has a/symmetry for each of some properties. every structure should, for each of its properties, note their a/symmetries. symmetries give implicit operations. e.g. a list whose order is said to not matter should make `sort` a nop, and lists whose elements are symmetric over application should have map automatically applied e.g. `1 + xs` = `map +1 xs`. i'm yet unsure whether this would imply that `xs + ys` = `map + xs ys`. the problem of elegance of expressing both a/symmetric list operations (e.g. `map` and `head`) is similar to that of composing higher order fns e.g. a loop condition should be tested as-is if not thunked (nullary fn), but if it's thunked then it should be evaluated and _that_ value tested. similarly, `+` should equal `liftA2 +`. such schemes may be possible with clever idea composition mechanisms, but it's untennable if one tries to use fns that compose only if unary, in-order, and may generally support optional or kwargs.
  ** there should be only one ad-hoc poly construct in any single language. this is basically `cond-let`. however! i mean that _any_ ad-hoc poly should be done by this ad-hoc coupling mechanism! we can't have both `if` and classes and hash maps and alists! they all accomplish the same thing: arbitrary relations! that a language may treat a collection of identifiers differently from an equivalent alist or hash table is immensely troubling.
    *** alists (not linked lists—just n-groupings of binary sets) are the simplest, most natural ad-hoc relation representation.
    *** fns are not a flexible enough encoding of arbitrary relation. they _are_ exactly that, though: nullary fn (A) becomes unary fn (A param) or binary (A p1 p2) which merely associates `A` with a set of parameters (usually indexed by ordinal position (list) or key (dict)). this is ideal when each invocation of A has different parameters, but is inappropriate otherwise. this sees "function" as an inappropriate term; really there're just _things_ that may or may not be free in any of their parameters. this directly corresponds to using sexps to encode arbitrary data. suppose a list of things (a b c). to parameterize b, just make `b` the head and give it a tail of attributes: (a (b prop1 val1) c).
      **** to be symmetric with syntaxless programming, rather than fns there are only data which may permit many interpretations, some of which may be in terms of actions. instead of an n-ary fn F, there should be a thing called `F` that may be optionally associated with other data. this may be called parameterizing, but association is commutative; F parameterized by X is no different from X parameterized by F, since it's just an association of X & F. furthermore there's no difference in `(F X)` and `X` being in scope when F is evaluated, and language / encoding / evaluation system should reflect that.
  ** problems with current non-simple ad-hoc binding mechanisms:
    *** constrained by their arbitrary definitions, and thus less flexible. e.g. java accomplishes ad-hoc poly via interfaces & instances, or haskell by type classes and their instances. ok, but that means that we inherit type classes' limitations like failing to well accomodate multiple parameters; or the limitations of java classes; i can't even think of an example of how they may be limiting (b/c i'ven't touched java in like 6 years,) but the fact that we must use them instead of something else is inherently limiting. *generally _anything_ predefined may not be exactly what we want, and so we'd find workarounds or clever exploitations. thus all parts of any programming system should be optional* or support arbitrary user-definable alternatives.
  ** _factoring_ is another term for _coproduct_; it's dual to product/combination. all asymmetry implies coproduct/cond. product may preserve independence of information or not, which determines whether an inverse coproduct exists; e.g. (α,ω) does and supports (α,ω) -> α and (α,ω) -> ω, whereas α+ω does not support either of those coproducts. α+ω still supports morphisms to α or ω _given_ ω or α (consider like conditional probability distributions) because addition supports 1. inversion (viz subtraction or addition with an inverse element) and 2. uniqueness (i.e. in a+b=c, any one variable can be determined when given only the other two.)
  ** example of generalization: the initial design says that f & g return A, and h processes values from f & g. during implementation it's discovered that g must return B : A ∩ B = ∅; now h must be changed to handle both A & B, right? it seems that the trouble is that the inclusion of one more case requires changing more of the rest of the system than necessary. conversely, using exception rules seems less easy to manage; however, it _is_ appropriate! exceptions are asymmetries, and a/symmetry should be obviously shown in code! regardless, h must handle B or something that uses h must handle B &c. it seems that either A∪B is the "true" type that our program should consider or else that B should be converted to A. the existence of this choice inevitable; it directly reflects the program's information. however, there does not need to be a choice about where to put these ad-hoc relations/rules! in prolog all rules are in the same context and are unordered. in other langs there exist (usually many) particular constrained constructs for specifying rules. the need to choose a construct and deal with entailed constraint is needless encumberment. this is, yet again, the _needless ordering problem_: needing to care about ordering even when it doesn't matter, simply because we're using ordered structures. storing knowledge graphs in text files has this problem; we must choose in which section to store knowledge in a text file, even if multiple appropriate locations exist. indeed, a truly good language is merely an unordered (though often context-specific) set of rules. context is nothing more than a _choice_ of rule. i use _choice_ to mean cond block (from predicate to result) i.e. a partitioning fn. generalize fns to nodes in a state machine. nodes are subroutines that make the program's state more desireable. when desirability is maximized, the program terminates or loops back to a place where it awaits more input, thus moving back to a less desirable state, ready to increase desirability again.
    *** *_location_ and _ordering_ are each horrible. _relation_ is necessary.* many langs incorrectly forcably constrain that partitions of code (into modules) (usually for mere organization/navigability) correspond to separate execution/logical contexts! the programmer is forced to put everything in one file, wherein everything exists in the same context and so we don't need scoping mechanisms (e.g. fn args, dynbinds, object modifier methods [oop], closures) just to include objects in current scope or modify objects unincludable in current scope (both of which together support a unified representation in c#: that all vars can be in any scope, but only in some scopes can they be mutated. generally this pattern is plain inclusion of an object in scope but where the possible operations on it is related to context.)
      **** manually ordered module importing is preferred over importing whose ordering is implied by the import DAG e.g. `req A req B` is better than `req A B C` where C defines x which is set by A and used in B when A calls f∈B.
        ***** that we've organized our code into separate modules should not imply that each module exists on its own! _loading a module_ is a bad idea; _including_ a module is useful! _including_ here means inclusion of that module's code in the whole program, not that the module is evaluated then accessible to whatever module imported it. again, code should encode _ideas_, and scoping should exist *only* for name collision resolution i.e. context-sensitive names. in fact, _always all_ code should be evaluated (considered) altogether then optimized. obviously dependence naturally begets an order, and independent things can be evaluated as early as desired, to reduce memory use during optimization. in racket, `(require (only-in "a.rkt" a)) (set! a 3)` fails: `set!` does not allow setting imported identifiers. that surely should not be a technical limitation! thus it's an unnatural restriction. racket fails developers in not having its scoping being exactly renaming imported identifiers to avoid ambiguity.
  ** fns are decent for writing libs, but not programs. in libs, each fn corresponds to a unique functionality independent from other functionalities, or if they're related, then they can usually easily enough be written in terms of a common "helper" fn. but programs' objects are much more interrelated, so fns can be quite inelegant!
    *** libs' fns are separate. mains are complex. this is mostly because main has much more complex *control flow* (not functionality) than lib fns. this is a fault of using procedural programming; this problem does not exist in a flexible language that does not regard order such as a query language or prolog.
  ** in fp langs, there are only four scope inclusion mechanisms: fn args, accessing variables defined/declared in the same scope, imports, and exceptions. in haskell all data is immutable. in racket fn args' values can be set, but this does not affect them outside of the fn; args defined in common scope can be `set!` but those injected into scope by importing cannot be `set!`, though imported parameters (dynbind vars) can have their values set for the current thread/continuation (and cannot affect the parameter's value in other threads/continuations.) unsure how other functional langs handle variable binds across contexts (viz pass by reference vs value.)
    *** racket also supports mailbox thread message passing, another async (in addition to exceptions) method for scope inclusion
    *** the way to scope in fp langs is then to parameterize data by generalizing them from data (or nullary fns) to n-ary fns or by module vars (if available, e.g. dynvars in racket) or else by partial application e.g. `f = f 3` if shadowing is allowed like that, else `let f = f 3 in ...`, which is pretty annoying. like damn the problem tendancy with fp is that, one way or another, one must specify information (parameters/relations) redundantly over multiple contexts, even if all those contexts have a large intersection.
* x∈{a,b,c}<=>x:=⨿(a|b|c) where a, b, or c can be products (lists)
* like rank invariance, fn app invariance, so that `f + g` (when f & g are unary) is just as well as `a + b`. i just don't want to need to make specific combinators for all sorts of things. TODO: test whether, in factor, i'm satisfied with the generality of napply, cleave, & spread.
* metaprogramming can't exist in a syntaxless coding model, right? at that point all programming is equally meta or not: at that point it's just truth by any preserving encoding/representations—graphical, syntactic, textual, compressed, encoded by any of numerous bases, &c.
* i like how, in joy, f g h composes them; to do f g h [haskell] (i.e. g & h are params to f) in joy, you quote the fns: f 'g 'h.
* rather than fns & macros, it may be more sensible to define syntax patterns (where _syntax_ means the representation of data that the user/programmer sees) and semantics; thus a program is parsed not according to a dynamic rather than static set of syntaxes. this means that we don't have "a language" but instead a collection of maps from syntaxes to semantics. multiple separate models may be used simultaneously, e.g. lisp & j can be used together; anything that can't be parsed as lisp is tried to be parsed as j and vice versa. obviously one may define parser combination rules.
  ** i think this is what red/rebol call _dialects_. it's similar to racket's `#lang` construct.
  ** this is the same as pil macros, except without being constrained to fexprs; rather than pattern matching to a list, we pattern match to any general structure.
  ** while a powerful and interesting paradigm, it should not be used much, if ever; a single good basis accomodates all data. we can immediately see in red/rebol that the language is a large collection of ad-hoc constructs. there's practically no symmetry. even link:http://re-bol.com/rebol_quick_start.html#section-11[the documentation only promotes features] and no algebraic basis nor spec is given for the language. anyone can make a big library of functions & data, invoking and using them. to be able to compose functions or constructs elegantly and freely is very good design, and rebol is good in that respect, but note how rebol is a case study in the result of indiscriminate use of ad-hoc syntaxes! this is separate from the design principles of 1. separating data from its representation(s); 2. using highly-symmetric structures to express many encodings & transforms. indeed, syntax is occasionally particularly useful or nice, but usually only gets in the way of actual programming; syntax is a necessary evil that enables *the user to specify* _encoding_ [transforms or arrangements [relations] of data], and syntax is needed only to express relations, which transforms and data, as transforms merely relate an input to an output, e.g. the pair `[x (+ x 2)]` which expresses the function `λx. x+2`. generally, as pico shows, fns relate formals to an output. i emphasize "the user to specify"; the user does not need to use syntax to specify every relation directly; they can express relations as functions that produce those relations. syntax is not truth; it's an interface between truth and ; it merely facillitates communication. therefore syntax should 1. not constrain ability to encode ideas; 2. should express ideas so that they may be understood easily; naturally syntax should clearly show relations, and the a/symmetry of each.
  ** homoiconicity sucks. syntax should be terse, but again, syntax has nothing to do with what the syntax represents! and all concepts permit multiple syntaxes [representations]! a cool thing would be to enable syntaxes for delimited parts of a program, just like is done for math papers: "in this section, <syntax> refers to <concept>." have no language: just a dynamic collection of syntaxes; the semantics will be asm-like: just mutable variables/registers, dynamic binding—just pil's semantics. and metaprogramming is still useful, though not necessary; it's conceiveable that the option to `eval` syntax that's been produced rather than specified literally is nice, though surely often it'd be just as easy or easier to construct an object then pass it to some fn.
* "high/low level" is precisely expressed as the number of axes of symmetry. the more aos, the higher level; the fewer, the more low-level. one may say that the number of all relations determines, but ad-hoc relations are just as cumbersome as relating multiple low-level objects. there is never reason to _choose_ a level; aos are properties of data structure; they are _determined_ exclusively by the predicates/constraints that define the form/shape of information—the very facts that distinguish the information from random noise.
* predicates are pointfree forms of sets: A = {s|a(s)} can be encoded as a. for B & b defined similarly, a AND b is much more efficient than A intersect B. this method doesn't support sets of ad-hoc elements. but, if *anything* is known about the elements, then at least some of the set can be expressed in terms of those properties [predicates], making set operations more efficient.
* how can programming benefit from knot theory? seems like there're "pointfree" ways to undo knots rather than unraveling it all.
* explore encoding programs non-textually, e.g. graphically or audially (though these are isometric with classes of text schemes with matching degrees of freedom)
* metric spaces should be useful in designing efficient lookup structures
* distinguish code trichotomy: _descriptive_ (not necessarily exist instructions of which this code can be an argument,) _computable_ (instructions exist whose args are this code,) and _executable_ (instructions themselves). executable ⊂ computable ⊂ descriptive.
* rather than use a paradigm that's always good, and thus uniform, i prefer one that's usually good, but that when occasionally is insufficient, we use a natural extension, much like how lazy eval is related to strict eval, or enclosing a datum in a heap memblock (contiguous or otherwise) is a natural extension of storing it in a register.
* linked lists should sometimes be replaced by skip lists.
* [where i discuss grouping or common contexts (as an alternative to wrapping _explicitly_ in data structures)] illustrate how common this is in math by example of subscripts: Y~i~|x~m,i~ ~ Bernoulli(p~i~) : m ∈ [1,m]. while it's often nice to use matrices to express relations w/o indices, that's wholy a notational advantage, not a technical one. these contexts will be created automatically, e.g. their (max) size will be identified (when static initialization is appropriate) before compile time, and a block of memory will be created for that context in the asm's `data` section. operations concerning the context will be automatically arranged, too.
* the advantage of tagged data in a set, instead of inserting into an organized sequence (or more generally graph): identifying proper insert points becomes expensive as the graph becomes complex. with sets, which are inherently unordered, we just insert into the set without regard to location. in fact, auto-organize this document by expressing it as a queryable/organizable set. an _organizing query_ is a fold from a set to a set of sets where each iteration of the loop extracts a subset then inserts it into the output set.
  ** e.g. rather than write a non-computational yet organized text file (e.g. [asciidoc] markup), wherein i may make a terminology section, it's better to just define terms as an alist. this is *computable* and perhaps even more importantly can be *queried rather than looked-up.* queries do lookup for you; that's why people opt to query google for answers or ask other people questions; they want the answer immediately, easily, not to move some abstract "cursor" to the answer's "location", then look there! i want to not use a text editor to work with notes! i want a knowledge editor that may work with text among other data formats; the importance here is that whereas text is ordered unilaterally in a text file (which is a stream of bytes,) knoweldge is not ordered linearly; it's ordered [structured as opposed to unstructured; not as in _total ordering_] by a graph. thus the expectation of looking up a definition is merely typing the query and getting an answer *without changing my current view of some knowledge* i.e. i don't "lose my place" when looking-up something else, and i can do this without keeping tabs or new windows. this can be done in text editors by using marks, but that's a bit clunky: we use arbitrary letters or numbers as marks, and must remember which mark corresponds to which location; they require a single location, which is not generally sensible since linear ordering of data is not generally possible; the data may permit multiple useful orders; also the keystrokes are inconvenient: they must be 3 keys, usually `"<mark>z`. a graph of marks would be most sensible. at first i was going to say "stack," but then i thought, "why not generalize to a tree" but then realized that graph is the most general, so let's use that. relations are practically constraints, and constraints are always kept in an [unordered] set. knowledge is stored in a graph, and all relations are *implicit*. to insert into an rdf, you insert a triple; you don't need to find a vertex then add to it an edge!
* reconsider notes on "lists vs arrays" now that i'm ignoring "lists" as a concept, having replaced it by pairs, which generally beget trees rather than lists specifically.
* the ideal "language" is really just a notation for operations on an algebraic or categorical structure e.g. stack, array, register, i.e. programming is merely an optimized notation for expressing programs as mathematical processes. good languages are not designed; they're identified by careful consideration of mathematical objects.

a common control flow elegance problem is implied by the design constraint of plain composition i.e. that f(g(x)) will always be f(_) regardless of g(x). consider the following:

----
for x in X:
  g $ let y = f x
       in if | y == 0 -> continue loop # discard x
             | y <  0 -> e1
             | y >  0 -> e2)
----

the syntax & semantics are a mix of haskell (with ghc extension `MultiWayIf`), python (`for` & hash comment), and java (`continue`).

this relies on `continue`, which is not common functional style, though it is technically expressible functionally by continuation passing style.

i'm pretty sure that this logic isn't legal in any language. the problem is that `g` is always invoked, even if the conditional decides to `continue`, and `g` must always have an argument, which is not obeyed if we `continue`. to make the code legal, we'd need to move passing to `g` to each of the non-`continue` branches. generally, if one branch of _n_ needs different control flow, then n-1 branches must be made less elegant just to be legal code. a much better semantic is for `continue` to propogate, such that applying `g` is skipped and the loop continues. this is indeed how reality functions; we try something, assuming that it'll work, and only if it fails do we try something else. prolog appropriately uses this control flow strategy of trying what's expected, only backtracking upon failure. in haskell this is done by using the `Alternative` type class.

again, the design bumble is the constraints that 1. the code is evaluated as it appears and 2. that it always takes the form of a tree. prolog does not so bumble, because its code is not structured in a tree; it's structured implicitly in a graph (specifically a lattice), where each horn clause represents an edge (when computed only over two nodes) or more generally a collection of paths (when computed over multiple edges.)

another, simpler example: give the scan of the sum of numbers in a list that we're building-up, or ∅ if any is even. the two common solutions are to 1. build the list then check if any element is even, which is inefficient; or 2. as we're building the list, use a short-circuiting construct to return the empty list. the solution that i propose is that we define a rule to describe a non-even thing; we then specify that we're building a list of these and that the list is ∅ if any non-even element is added. aside from how we produce the possibly-conformant elements, these three *rules imply control flow* for building the list then returning the scan or ∅.

programs have a dual nature: data pipelines and logical rules. i've yet to see a language that accomodates both well. commonly pipelines are easy, but loops disturb that pipeline simplicity, and facts are encoded as pipeline forms rather than as universal laws; or in prolog it's all rules and no pipeline. _functions_ are a strange mix of pipelines and facts. typically violation of facts can be found in dynamic or static errors, but the error is all that we get; i've never seen a program suggest restructures of itself that would conform to a set of provided facts. some facts can be enforced but strangely and inelegantly, such as making type checking work by using a refinement filter like, in typed racket, `(max 0 n)` to ensure that `n` is non-negative. ok,...but what if `n` was negative? is it really appropriate to assume 0? that depends on the nature of `n`. this is a low-pass filter that ensures that the further remainder of the program is logically consistent, but it cannot comment on the nature of inputs. likely the best approach is to use refinement-typed `if`: `(if (< n 0) (error "n < 0") (f n))`. however, this is again a fact being encoded as a composed function. it would be more appropriate to have unrelated statements of fact then pipelining: `(assert (>= n 0)) (f n)`.

in summary: graphs appropriately encode control flow, but the subset of graphs called _trees_ cannot elegantly (or naturally) encode all forms of control flow. the ad-hoc inelegance is seen syntactically by there being separate syntaxes for 1. function composition (which generally encode trees), 2. looping constructs (which generally encode loops), and 3. binding statements (which generally encode dags). technically, we can use combinators and recursion to uniformly encode all these as function composition, but that's generally ugly as sin, and still, though it may be hidden, ultimately relies on a branching operator. if the language supports first-class continuations, then we may elegantly have a variable that stores a continuation, so that the program can be control flow is not given statically by the code's syntactic structure alone. however, this is possibly difficult to reason about. it's certainly not functional style.

conclusion: though there might exist many even equally good ways to encode control flow, we must acknowledge that programs are not nested compositions of building blocks. they are not layered together like onions. they are general, complex, arbitrary, like brains, neural networks, general graphs. any language that disregards this truth is bound to inelegance. certainly refactoring should be as easily programmed as factoring an algebraic expression, according to that algebra's defining rules. this is another call for programs to be constructed of algebras. it's certainly sad to require a human to not only rearrange code into isomorphisms that follow simple & regular rearrangement rules, but especially to require rearranging its syntax in a text editor! we need _logic_ editors, not _text_ editors, as obviously programs are composed of logical objects, not text objects, though they are currently commonly so _expressed_. expressions are constrained by their definitions as are all things; carefully consider whether these constraints cripple working with the thing being expressed!

=== minimum [wip]

TODO: replace "e.g."'s by total consideration.

a/symmetry is measured by whether the elements of the domain must be individually stated, or if instead multiple can be stated by a single statement.what are the necessary components of the minimal practical programming language? turing machines are the simplest, but impractical. here's what we'll do:


. look at languages' models only. e.g. lisp uses lists, factor uses stacks, and link:http://www.om-language.org/[om] uses function composition.
  .. lists are exactly stacks; they're just used a bit differently in lisp vs factor, but even that could be said to be a difference between two languages rather than a difference of paradigm. also technically lisp is built on _pairs_, not lists. in fact, there's no reason to prefer proper lists over improper ones. `null` (`()`) should not be thought of as "the empty list" any more than `false`. there's the thought that mapping over the empty list is sensible, and returns the empty list, but seeing as `null` is its own value, and not a list (because _list_ is not a lisp data type; there's no primitive predicate that checks that; however there is `atom`, which is a primitive predicate that checks whether an object's type is _pair_), we can define `map false = false; map xs = [...]` which is exactly the same as `map () = (); map xs = [...]`. i do agree that `()` should be used instead of `false` because it's terse and represents the empty set, which is always the null value, cf `false`, which is a non-empty/bottom value of the boolean semiring. this being said, we must, even today, as lisp is, unlearn `()` as "the empty list", and instead recognize it as "null", the special empty value. indeed, this must be recognized in tandem with recognizing that lisp is not of lists, but pairs, and that therefore "lisp" meaning "list processing [language]" is inappropriate. in fact, even "pair lang" is a stupid definition, since "pair" means "ad-hoc relation", which are needed for all mathematical expressions and therefore all programs; to say that anything features relation is redundant. null termination is needed only for static arrays on contiguous memory partitionable into equally-sized cells where each cell corresponds to one object. linked lists do not share such assumptions, and do not require a _cell_ to hold the value of a termination symbol; termination is encoded, in the case of c &c, in the pointer pointing to `null`; or, in the case of lisp, haskell, &c, [the `cdr` of] an object satisfying `atom`.
    ... even the empty set, though it makes sense, is inconsistent; we'd expect the empty set to contrast with a non-empty set; however, truthy values are usually not lists, and again, there's no reason that they would be, since lists are just a common (recursive) pattern of pairs. the sensible truthy contrast to ∅ for a datum is a singleton set of that datum. there's no singleton pair; by definition it can't exist. assuming `cdr` to be `null` is not a symmetric solution; null is here a dummy value; any other value, e.g. `0` or `647274706`, would equally suffice. in fact, because lisp uses `null` for `false`, there's nothing distinguishing `null` as a terminator from the falsy value being in a list, except that we test `(null (cdr x))` to determine whether we're reached the end of a list, cf `(null (car x))` which tests whether the current value is falsy. however, again, this is just a needlessly verbose method that relies on the arbitrary null-termination convention rather than the natural case: `(not (atom (cdr x)))`. so the question remains: how do we symmetrically represent truthy values as non-empty sets? it turns out that we actually do not need to do anything; the solution is that _there are no sets, no lists._ a pair is a data type just like string, number, or symbol, and we branch on type to determine how to handle it when appropriate. it is inappropriate to consider lists, or to consider pairs different from other types. we can store multiple data in a single string (though usually there're no efficient operations to consider their multiple data,) or single number (e.g. numbers as bitstrings or masks.) pairs are not special. they support the `map` function, and `map` is not special; we can easily write a function that maps an n-ary function over an n-bitstring, or fold over a string as a sequence of characters. `map` should not need to deal with _lists_; it should be general, of pairs, traversing pairs as binary trees; this: 1. is potentially more efficient than traversing lists (because lists are linear, so don't support parallel traversal); 2. preserves structure of whatever tree you have; 3. has (a generalization of) the exact same base case as traversing an improper list (viz `(atom (car x))` and `(atom (cdr x))`). in conclusion, the "solution" to the _empty vs non-empty set_ problem is to recognize that there was never any empty set to contrast! there's only an arbitrary symbol for a falsy value and everything else, and this difference is meaningful *only* wrt the `if` special form, but is also conveniently useful generally to represent a variety of zero different from `0`, which is useful as a number and may be usefully contrasted with the lack of a value.
    ... we see that "lisp" is really "bitp", _binary tree processing_, since trees are the structure created by recursing on `cons`, though really rose trees &al structures are not binary trees; they recurse on `cdr` only (a rose tree being (root,children) encoded as `(cons root children)`.
    ... WAIT, no, homoiconicity & metaprogramming requires that null be the empty list; if we want functions and their args to be encoded in sexps, and we want to metaprogram by outputting sexps, then we must, for nullary functions, distinguish between reference to a it vs invoking it: f vs (f). non-nullary functions aren't ambiguous whether they're being referenced vs invoked, e.g. for unary f, f vs '(f . 1). you may argue that f refer to invoking and 'f be non-invoking, but this would be inconsistent with notation for non-nullary functions. null-termination makes nullary and non-nullary functions symmetric about reference vs invocation. you may still say that (f) is (f . ()) i.e. a pair of f and the falsy value, but at this point, since we've already established that 1. all pairs, to be outputs of macros, must be null-terminated, and 2. that functions and their args may as well be called a list of function and its args; then we arrive at the fact that lists are null-terminated. for consistency's sake, _all_ lists, regardless of whether they'll be macro outputs, should be null-terminated. yet non-null-terminated lists can still exist; their existence is naturally implied by the language axioms; therefore it's appropriate to give them their own name, _improper lists_.
      .... this suggests the question: should we use sexps? mexps are the same but without redundant parens. again, we can use 'f and f to distinguish a fn vs evaluating a fn, and we can still use parens to delimit otherwise-ambiguous nested mexprs. it's obviously insensible to ever have 'f not as an argument to something, and that can be inferred! haskell does away with the need for quoting by being non-strictly evaluated.
      .... this suggests a related question: should code be homoiconic? indeed, there maybe a language with a different model that cannot reasonably afford homoiconicity, or even differently, should try _not_ to be homoiconic, because homoiconicity limits brevity! yet without homoiconicity metaprogramming can still be done, and structures visualized, modified, produced, and evaluated as code!
      .... certainly we don't need macros; picolisp's fexps demonstrates that merely choosing to not evaluate the argvec suffices. the only property that makes macros useful is delayed evaluation. we can just as well quote then selectively eval. in fact, homoiconicity per se isn't even valuable; what's actually valuable is _evaluable structures_. macros are overconstrained: they have the constraint that code must be encoded as syntax. more sensibly, however, there should be no constraints beyond 1. the structure is mutable; 2. the structure is executable. many langs allow evaluation of strings as syntax, which satisfies (2) but not (1), since modifications to syntax do not describe modifications to the semantics that the syntax represents. so the real sole concern of metaprogramming is that we can execute code, where _code_ means literally _the encoding of information_.
. consider these models as mathematical structures, then we'll describe them by graphs with unlabeled edges and valueless nodes
. compare these graphs. e.g. _list_ is synonymous with _stack_ (and arrays are equivalent but with different efficiency for random access and reshaping,) functions are representable by lambdas, which can be described by lists, so functions are isomorphic to lists (composing lists is (via `cons`) is equivalent to composing lambdas.) actual "computation" is called `eval` or β-reduction.

what do we have so far, then?

. things can be stored in stacks / lists / lambda formals or registers / alists / maps / defines (as lua shows us, `x = 4` is the same as setting key `x` to value `4` in the global map (`_G` in lua)). *these are generally seen as lists whose elements may be a pair of key & value.*
. data inevitably have scope, even if global. they can be accessed only by procedures that have access to their scope. lacking global scope, scope endowment is accomplished via scoped binds, naturally accomplished by parameter passing (`let` is just alternative syntax to lambda composition.)

it should be enough to simply define data as relations of other data; this is merely lists, generally graphs, which specify constraints/relations of data, including access and mutation, thereby coding synchronization or other relations of puts & gets. then it should be enough to define programs as mutations (regardless of whether in-place mutation or function) thereof. this is pretty easy currently in any lisp for which mutation is faster than recursion: make some few global vars in a module. allow access to it wherever necessary. you can use such simple, few globals as stack(s) or registers. now we _could_ make this work for recursion-preferring langs, but that's a little more difficult for reasons described in the example in `just-use-lists.adoc`.

aside: why recursion isn't necessary: recursion is merely a switch between a base case and recursive case. the base case is a single instruction without particular control flow; there's no element of recursion/iteration. the recursive case is merely splitting a form into multiple forms over a subset of which you'll recurse. to recurse is to apply either a base case or recursive case, which ultimately expands to base cases. therefore recursion is merely iterated splitting (a form of *indexing*) then applying an operation. therefore the essence of looping (iteration or recursion) is indexing: *identifying subsets*. the only difference between iteration and recursion is that, for lexically scoped languages, at least, the subset is strictly enforced by scope; after we select the subset, we pass it to the next call, within whose context only the subset is in scope. by contrast, iterative loops have invariant scope. scope is a bounding mechanism. bounds are present in finite sets, too. because each data structure can be indexed, we can use finite sets of indices to refer to subsets (iteration, stateful) instead of actually pruning-away subsets (recursion, pure.) furthermore, indices can refer to multiple structures that share common indices; their relation is implicit via indices, rather than being specified explicitly by cons. there may be very many things sharing indices or not, so, again like dynamic vs lexical scope, any subset of things may be easily identified by indices rather than specifying every possible combination of relations, which are always ordered and so cumbersome, but may be stored in a way that permits nonsequential access e.g. alists. another way of looking at it is types vs predicates; predicates implicitly work over many things whereas types must be specified & bound manually and are more restrictive.

what all good models have in common is minimizing every variable's scope. however, the language must make doing so elegant. many langs fail here; it's often preferable to use larger-than-necessary scopes for convenience.

==== notation

. to help us remember that we're just using the lambda calculus, let's call "eval" `β`
. lambdas are pairs of an input list and output list a la picolisp
. quotation will always be quasiquotation (henceforth _qq_), because it's only more capable than quotation without substitution. there's no need to terminate a list with `()`; if you want to recurse, just use `atom`.
. with such qq & λ, `cons` is redundant and thus omitted 
. `car`, `cdr`, `if`/`cond`, `atom`, `eq`, and lambdas will be altogether replaced by a common generalization of them: parsers. they're like `cond` on steroids: maps from predicates to values, but with syntax for extracting data and expressing predicates neatly, not obviously distinguishing between equality vs predicate matches, nor predicate satisfacton of form vs value. every lambda's formals supports parser syntax.
. though `apply` might not be strictly necessary (and can be considered a convenience macro,) we don't need to consider it; we're already using qq, and unquote is a part of that. we can use unquote instead of `apply` a la janet.

thus our language's grammar is: qq, β, λ [parser]; and its vocabulary is interned symbols, words/bytes, and pairs. this is purely functional; adding `set` would change that.

* macros (as a lang feature) aren't needed; for metaprogramming (mp) just use qq & β. also, mp is nice like racket instead of messy like `defmacro` because lambdas are parsers.
* lambdas will accept an optional label, for easy recursion/goto/continuation. there will not be a "define" form. all binds will be accomplished by parsers.

with stack based langs, we've only positional parameters, not named ones, and we access them by `pop` [factor, elisp], so we don't even need identifiers (including ordinals) to reference them. however, this is just a less convenient version of parsers.

TODO: what about coparsers to help ppl write valid programs?

=== the few necessary aspects of every language

_static_ (or _early_) means "determinable before runtime;" _dynamic_ (or _late_) means "determined only during runtime." think of static vs dynamic arrays in c: static ones' addresses can be known without running the program; dynamic ones aren't knowable, and even the size isn't knowable before running the program, and even then, it changes throughout the program!

dynamic indicates polymorphism (or variation, instead of being constant): one of many values will be chosen, and we don't know which without tracing program state, whereas static means that only one value is possible. for example, if `run` is an ad-hoc polymorphic identifier (e.g. a method of an abstract class in c++ or java) then its value is determined by an object, e.g. `obj:run()`. this lua syntax is akin to oopy `obj.run()` in java, but is actually syntactic sugar for `run(obj)`—an equivalent haskell-style functional approach. this example of _dynamic_ connoting polymorphism is specifically one of dynamic binding. static or dynamic of a binding refers to the bind's value.

* static is concrete enough that we can use non-algebraic rewrite rules.
* static is always potentially faster than dynamic.

==== scope & binds

firstly, neither scope nor binds is necessary, as assembly language demonstrates. however, it's arguable that the names of registers are identifiers bound to given values, and that the scope is totally global—a "zero" scope, so to speak. there's no avoiding the facts that:

. values are addressed somehow, whether by address in a heap, or position on a stack, &c
. in all expressions, subexpressions are either bound or free, and there must be a rule for determining which

===== definitions

[cite wikipedia, cite late binding]
----
_dynamic binding_ is sometimes used [to refer to late binding], but is more commonly used to refer to dynamic scope.
----

consider the following picolisp code:

[source,lisp]
----
(de f () (+ x 4)) ; (1)
(f) ; NIL         ; (2)
(de x . 65)       ; (3)
(f) ; 69          ; (4)
----

TIP: this shows another of picolisp's good features: NIL propogation instead of crashing on free identifier.

the combination of dynamic binding and dynamic scope allows this code to be valid: in line 1, the dynamic binding allows `x` to accept whatever value it's bound to when `f` is invoked. dynamic scope allows `x` in `f` to inherit the top-level value of `x`.

NOTE: being top-level isn't praticularly relevant; all that matters is that the `x` in f is an inner-more scope than `x` outside of `f`.

though dynamic binding makes parameter passing unnecessary, parametrs are still nice, since they allow unbound (anonymous) expressions to be given to functions for use; it frees us of the need to bind everything to names.

if we'd used dynamic binding and lexical scope, then `f`'s definition would be invalid; `x` isn't in its scope. lexical scope is known for closures; let's look at a practical dyn.bind/lex.scope example:

[source,lisp]
----
(de f () ; closure of x
  (de x . 4)
  (+ x y))
(f) ; NIL; y isn't bound
(de y . 6)
(f) ; 10. dynamic binding: f can use y, and as per lexical scoping rules, y is in f's scope.
x ; NIL. x is bound only within f
----

except that that doesn't actually happen in picolisp since link;https://software-lab.de/doc/faq.html#closures[picolisp doesn't use lexical scoping]. i've yet to learn (let alone understand) picolisp's bind & scope mechanisms and patterns of their elegant usage.

* it seems that dynamic scope implies dynamic binding, since scope determines binds' values.
* scoping relates to:
  ** deallocation in gc langs
  ** context delimitation
  ** semantics of free variables. usually illegal, but
    *** in lua and picolisp free vars are null
    *** theorem provers (e.g. agda,) type checkers (e.g. haskell,) or logical deduction systems (e.g. datalog) could use them as part of a reïfication engine.
* generally every binding syntax has its own associated scoping rule, even if many use the same rules. for example, the `for` (&al loop) syntaxes in algol languages bind where the scope is the loop body.
* lexical scoping is more natural to function composition (applicative style;) dynamic scoping is more natural to mutation.

===== wise use

of course, scope & binds are concerns only if they're used, which they aren't in concatenative paradigms, aside from possibly defining functions, which always (i think, at least in apl & factor) have scopes exactly their parameters (ɑ [& ω] or the stack.)

for a statement in one context to be able to modify another context is a grave mistake, completely confused and senseless. one should have either [pure] functions or subroutines (which do not return a value; they're pure mutation.) within either a function or subroute (collectively _subprograms_) definition one may bind; these binds are valid only within the body (and *not* in subprograms called within the subprogram) and are freed upon the subprogram terminating. subprograms are then merely delimited sections of the whole program.

what makes programming difficult is when expectations about program behavior aren't clear. the ability to merge multiple different rules is a primary cause of such difficulty & danger. thus multiparadigm is good if there's also separation of paradigms, such as purely functional or purely mutative. a clear violation of this design principle is languages featuring a `local` keyword, implying that there's no single consistent scoping rule, which means that we as programmers generally need to read through every single subprogram just to know its behavior. haskell uses the `IO` monad as a clever yet overly restrictive solution to clearly *delimiting/marking* pure vs impure functions.

to consider a "single program" as such is foolish; we should be able to add or remove any subprograms and still be left with a valid (though possibly nonsense) program.

==== mutation

in place (_mutation_) or with separate destination (_function_).

==== looping

iteration or recursion. generally goto where dataflow is a cycle [graph theory]. given that goto is just funcall, goto is a useful generalization, suggesting that it should be used for all program _sequencing_ (deriving execution paths from a graph of statements [graph theory]).

''''

the crux of this document: many languages demand constraints for the sake of safety. i say that it's better to demand such simplicity that safety is hardly needed; that the liklihood of someone doing something improper is small because they have few options, and what options exist are always encoded obviously rather than following some special [complex] syntax, convention, or model. simple syntax, conventions, and models are good. for example, stack-based langs or lisp are simple; they each have few rules that define them. this means fewer things for programmers (or compilers or interpreters) to consider. fewer possibilities means higher predictability, and so the programmer's expectation of what's happening is more likely to accurately describe what's actually happening!

* btw fortran is faster than hand-written asm b/c fortran has a very good optimizer

interesting langs not yet considered, (but not necessarily to be considered:)

* roc (potentially better than haskell for programming (cf type algebra.) terser syntax, maybe faster, non-curried though, type checking always succeeds if types are correct, and type annocations are never needed, MUCH improved notation for ADTs, and ADTs are closer to row-polymorphic types)
* rust
* pony
* mercury (based on charity, if memory serves)

and link:https://illumos.org/[illumos]

things like go, zig, and other langs that're basically fast python/ruby/js/v will never be considered unless one is found with particular algebraic language properties or a particularly interesting runtime model.

.introducing erlang & joe armstrong

picolisp: completely hackable (including modifiable during runtime,) uses multiple processes instead of multithreading (thus actor-based concurrency)*
factor: concatenative, monoidal, optimized
j: concatenative, parallel

*as joe armstrong said, "[system] threads are evil anyway because they share resources. you have nice things in operating systems which are actually isolated, so one process can't fuck up another process' internal data structures, but threads are evil, 'cause what's the difference between threads and processes? it's that threads _can_ fuck up each others' internal data structures, so they're absolutely the things you don't want to program with."

all three: simple, based on one data structure (list, stack, array), efficient (both cpu & mem; enabled due to language symmetry,) algebraic (particular patterns of lisp, monoids & stack updates, tensors,) data-based (both picolisp & factor see programs as data to which functions can be applied. i'm unsure how this is with j.)

erlang (to learn:) distributed (built on π-calculus,) fault-tolerant (b/c of agent independence,) 

all of them altogether:

* systems that update others or themselves incrementally such that each increment does not _destabalize_ the system (i.e. the system can recover; yes, it may error, but it can _recover_)
* systems that work together and grow together. yes, some may die, and others may spawn new ones

principles:

* fault-tolerant
  ** isolation (how are things related or unrelated; if unrelated, then one breaking causes the other to break. the surest way to maintain stability is to reduce dependence)
    *** concurrency
      **** implicitly parallel (like haskell's evaluation of applicative do blocks)
* 0 downtime (updates during execution)
* processes repair other processes that are to broken to repair themselves (i.e. processes stabalize destabalized processes. this is an alternative to killing and spawning a new, replacement process)
* upon death, its occurence & reason are sent to a living node, which passes that info to wherever it should go

these principles should be applied to data storage, too.

"each module being a unit of service and a unit of failure. a failure does not propogate beyond the module."

joe armstrong's talk, link:https://www.youtube.com/watch?v=cNICGEwmXLU[systems that run forever self-heal and scale] demonstrates that sequential programming is inherently flawed and is therefore a bad practice (excepting small programs like `cat` that serve one simple function that's merely evaluated once per invocation.) also all erlang processes being concurrent explains the adage, "let it crash." in such systems "crashing" refers to a cell rather than an animal.

of course erlang satisfies all these things, since it's built specifically to model physical and organic systems.

.keep in mind while reading

* _relation_ has the same meaning as _relation of data_, since _data_ just means _stuff_. _data_ or _datum_ is exactly equal to vacuous unqualified mathematical symbols.
* smc means _self-modifying code_

.some big paragraph that i wrote

disregard givens; design from scratch by _first principles_: defining [adj] constraints and their implications. each _problem_ (i.e. thing that needs solving) is partitioned into two classes of constraints: the _desire_ and _universal constraints_. we always seek the (optimized subset of the) the interesction of those constraints. a simple though abstract example is a solution set of linear equations. we may have one solution, none (i.e. the empty set,) or many (particularly in linear algebra, _many_ always implies _infinite_.) a less abstract example: the universal constraints of physics are the laws of physics, and we desire to fly. our solution is then the intersection of mathematical expressions that describe flight and physics' universal constraints. this is obviously a complex example: its solution is not obvious, and many solutions exist, naturally partitioned into flight that's either valid only in fluid or valid otherwise. be it not pretty or simple, it's realistic. if we want to find the best solutions, then we must consider problems in their grand complexity, not artificially approximated in terms of cookie-cutter niceties—such mental tools as [for computer science] _lists_ or other _common_ data structures. *all models more specific than predicate logic skew truth.* such "prefab" solutions must be abolished. they may be easier to reason about for humans, but their inherit arbitration makes them more difficult to systematically reason about. this is particularly consequential when we consider that computers are ideal for solving problems systematically! both humans and computers can reason well by rules rather than easy piecewise composition of seemingly "neat" structures not described by predicates. *algebra* is a study of axioms' implications irrespective of the set over which the axioms hold. this means reasoning only about properties—not mentally tracing dataflow nor the state of a program, which is error-prone, annoying, and unnecessary. example algebraic design are programs _described_ by stacks, arrays, or the lambda calculus. i say _described_ and not _describable_ to mean that the programmer reasons in terms of these structures rather than programs merely permitting expression by such structures. this begets elegance in the same way that an algorithm elegantly expressed in polar coordinates is nicer than one reasoned in descartian coordinates, despite polar/descartian equivalence. we want the user to know how to express programs by an algebra simple enough that the computer can heavily optimize the program; or express a desire in terms of an algebra that a computer can solve in the given context of universal constraints.

there are only two properties to make a program ideal: efficiency and elegance.

structure:: generally means _form_, i.e. arrangement (of data), i.e. particular relation (of data.) i parenthesize "of data" to emphasize that structure is independent of data, but ultimately is useless unless applied to data; structure is abstract over data, and like all abstractions, represents useful truth, but in practice must eventually be reified. pointfree functions are example structure abstracted over data.

.TODO's

* consider lisp basis: `cons`, `car`, `cdr`, `quote`, `lambda`, `def` (which binds to data (incl lambdas) or macros a la pil,) `if`, `set`, `eq`, `atom` (opposite of `pair?`,) `eval`. in additon to pil's lambda shape, car & cdr can be done exclusively via deconstruction: `((a b) (cons 1 2) (+ a b))`. this is the applicative form; the pointfree version is `2`. `map` [haskell, scheme] should be called `2:` and should be an overloaded form of fold (same function, different (default) params.)
  ** to avoid `apply` (which should be done) all functions will take a list of arguments that will be parsed-out; much work will be in optimizing parsing fn args.
  ** problems: encourages recursion, requiring optimizations/translations to stateful version. using continuations (viz named let) should be easily translatable to assembly jump statements.
  ** describes intermediate data. this should be replaced or optimized into pipelines that maximize allocated memory reuse.
  ** how can i merge sexps perfection with photon basis e.g. `a == b => _`? do i so need? no; `cond` covers this perfectly.
* compare link:https://fortran-lang.org/[FORTRAN] against j and picolisp
* revise notes. reserve _function_ for the mathematical concept, and use _continuation_ (or some other possibly more-appropriate term) to refer to memory addresses that the instruction pointer can validly have, i.e. those that can be `goto`'d.
* ensure that i mention the importence of anonymous ADTs: for them to express a program elegantly they must be anonymous, just like functional programming without lambdas (i.e. with only named functions) would be horrible.
* see https://en.wikipedia.org/wiki/Satisfiability_modulo_theories
* reconsider type classes in terms of factor's oop system
* discuss randomized algorithms & probabilistic data structures
* discuss ADT constructors/destructors [destructuring aka pattern matching] vs their functional equivalents: constructors & traversals
* explore arrays as ad-hoc polymorphism e.g. a hierarchy of algebraic type classes can be expressed by a simple spec on arrays: the unit value is stored at position 0; + is stored at 1; × at 2; &c. as in this case, the number may have meaning rather than being arbitrary. the hierarchy is determined (calculable) simply by pointwise addition of arrays, checking which resultant cells are 0/nil. this is really using arrays as tuples that represent abstract structures, then using set-theoretic operations to relate those structures.
* note in the appropriate place that using data structures add only readability to function composition—"let over lambda."
* fully expand (to completeness) §programming mindset
* merge discussions of languages with ./wares-and-langs.adoc
* discuss beauty as a heuristic for elegance. to determine beauty, express code by audial or visual space, e.g. a beautiful FSM graph will appear beautiful. a visual description of syntax (a la link:https://www.sqlite.org/lang_select.html[sqlite]) will appear beautiful if symmetrical and simple enough. or perhaps it may appear beautiful yet infinitely complex like fractals.
* revise section on linearity into one that discusses _units_: 1 as the base case and also the seed for generation, e.g. naturals as (0,1,+), and integers with the addition of inverse, and rationals with addition of division.
  ** n-dim structures are products of (n-1)-dim, for both continuous and discrete spaces; discuss this fact respective to arrays, lists, and continuous spaces, finally seeing them all as relations over universally-qualified variables whose meaning is found once a space is assumed, e.g. "∀x" meaning symmetry about x where x is either an integer or real depending on whether the statement is considered in discrete or analytic mathematics. e.g. 0-dim is a point. introduce one "∀", and now you've added a dimension: 0-dim := ∃p. p = _. 1-dim is ∀[x : 0-dim]. x. 2-dim is ∀[x : 1-dim]. &c. this is _true_ dimensionality. _pseudodimensionality_ is emulation of dimensionality by modulus, which allows reshaping, e.g. all arrays of shape [a][b]...[c] where a × b ... × c is constant can be reshapen into other i.e. reshaping is symmetric about cardinality.
  ** discuss array/list equivalence by matrix representation of tree, and compare to 3d and higher-dimensional structures.
* rearrange this document: 1) overview; 2) common fallacies; 3) what programs must be (we've a lot to consider even when we're considering only the most basic language!), and how lisp is the natural language for programs; 3.x) subsection on "programs" as evaluable relations, and that's implications on how programming relates to general math, language, game theory, &c formal systems; 4) now that we've identified the basis for programs, consider structure of complex programs (this is where (0,1,+) (i.e. monoids) will be discussed, not just wrt programs, but in general, again e.g. constucting numbers); 5) why monoids are not enough (we want to be able to calculate programs rather than merely evaluate them.) this section will consider SMT solvers and hoare's work, evaluating how appropriate each is. however, i must be careful to not consider these systems if they're foolishly concerned with trivialities, such as excluded middle (continuum fallacy; truth can be a real value) or "whether constructivism is correct", and i must avoid formal systems' common nonsensical considerations such as russel's paradox (improper definition.) such things are correctly not the concern of practical programmers! note that such nonsenses are always of logic and never of math: they discuss truth/validity rather than structure! as programmers, we care only about _useful_ ideas/programs. we deal with numbers & relations. yes we use logic & math, but only insofar as it helps us program! any math that won't eventually be implemented by relations of numbers isn't relevant to us.
  ** to introduce both algebra and programming/relation as primitives, first discuss unit & relation, then evaluation, then axioms as a particular variety of relations, and logic as a particular variety of evaluation. this simultaneously introduces the fundamental(s) of mathematics, and demonstrates programs as nothing more than mathematical expressions, where computers can evaluate them.
  ** where should i discuss denotational semantics?
* think about goto vs delimited continuations vs retroactively adding algebraic evaluation rules. this is similar or may include bottom-propogation. consider `(println (+ x (* y (if (= z 0) ⊥ z))))`; if z = 0 then the println statement won't execute b/c the whole expression will be ⊥. the propogation boundary must be defined. for example, if this println were in a `begin` block, we'd need to allow `begin`'s other statements to evaluate, unless they're explicitly linked to the println. such propogation boundary determination may require the partitioning of all expressions as being 1) part of another expression; 2) in a `begin` block; 3) in a `let*` block. propogation would occur for (1) & (3) only, since these are the cases when things are dependent. in the case of a `let` block, when all bound values are assumedly needed, it'd be sensible to have `or` on expressions that can produce ⊥. (or,⊥) is an alternative to `if`. (this is simply nixy trees.) (cond,v) where v is any value which propogates, generalizes and is an alternative to maybe, either, etc insofar as short circuiting or addition (i.e. +0 or ×0) and is also an alternative to synchronous exception handling, and backtracking [parsing].
  ** prolog does this; it's called _backtracking_ and is prolog's single control structure
  ** related: raising exceptions as a control flow mechanism. this is a variety of hook or event-based control flow. perhaps this is what the π-calculus is about?
* discuss [consequences of [technique]] using list instead of maybe, citing ~/programming/nicholaschandoke-me/articles/racket-macros.adoc:§_keyword args:general correct solution_ as example if needed. the idea is simply that list (recursive product with base case 0 = '()) generalizes maybe (coproduct with 0 = Nothing.) because lists feature recursion, they're superior to maybe. one might suggest that (maybe,cons) is just as good as list, but they're probably isomorphic. anyway, list is terser and still familiar, and therefore remains preferable.
  ** discuss algebra of lists and maybe e.g. #f × n = #f, #f + n = n. cite haskell notes on how maybe generalizes link:https://en.wikipedia.org/wiki/Boolean_ring[boolean rings], and see wikipedia on boolean link:https://en.wikipedia.org/wiki/Boolean_algebra_(structure)[lattice] structure. discuss how this structure is homomorphic in maybe and list, and see that article and my haskell notes on maybe+list semirings. consider how the lattice generalizes the link:https://en.wikipedia.org/wiki/Two-element_Boolean_algebra[2-element boolean algebra] so commonly used in cs.
* discuss recursion in terms of unit. algebraically it likely lacks 0, having only the recursion operationo over a set, which would make it a semigroup. is there any useful conception of it as a monoid or more-endowed structure?
* `unquote` should be available anywhere. outside of a qq, it's `eval`. also, mentioning just b/c it's related, `splice` should be useful outside of qq, instead of `apply`
* suggest a syntax (both for natural language and computer langs) for "base case & recursive case," e.g. "int &rec list +" to mean an int wrapped in a list, or that wrapped in a list, .... "int &rec list *" would mean int, or int wrapped in a list, or that wrapped in a list....
  ** perhaps (list ... int) and (list ...+ int)
* as an example...of something probably mentioned elsewhere herein, use lists instead of maybe for optionality/short-circuiting, and show traditional lisps (i.e. those with `t` & `'()`) the empty list as 0: + (append) 0 is identity, and × (cartesian product) [TODO: are these correct? product and coproduct should be dual. can i describe cart-prod as a categorical dual of append? likely not. anyway, monadic join is defined in terms of #f or cartesian product, both of which are practically multiplication by 0, i.e. #f or (). point is: we don't need maybe. list generalizes maybe, and is therefore better.
* (point a00-45) say i've a loop `(let loop ([x 0] [y 0]) ...)`, and for the first _n_ iterations, `x` is used, but thereafter it isn't. usually it'd stick around in memory. we can say `(let loop ([args '(0 0)]) ...)` so that we can reduce the amount of memory used. however, without special optimization, we'll lose time in cons & uncons. still, this is an interesting solution.

.wrt point a00-45

we can refactor

[source,scm]
----
(define (ema p)
  (let ([α (/ 2 (add1 p))])
     (let next ([k 0] [x 0])
       (λ (y) (if (< k p)
                  ;; x accumulates a mean
                  (cons "NaN" (next (add1 k) (+ x (/ y p))))
                  ;; x is the most recent ema value
                  (let ([x (+ (* α y) (* (- 1 α) x))])
                    (cons x (next k x))))))))
----

into

[source,scm]
----
(define (ema p)
  (letrec ([α (/ 2 (add1 p))]
           [next (λ (x y) (+ (* α y) (* (- 1 α) x)))]
           [f (λ (x) (λ (y) (let ([x (next x y)]) (cons x (f x)))))])
    (let loop ([k 0] [x 0])
      (λ (y) (if (< k p)
                 (cons "NaN" (loop (add1 k) (+ x (/ y p))))
                 (let ([x (next x y)])
                   (cons x (f x))))))))
----

both of which are effectively equal. an example invocation:

[source,scm]
----
(void (for/fold ([p (ema 3)]) ([x '(0 0 0 1 0 2 5)])
        (let ([P (p x)])
          (printf "~a " (car P)) (cdr P))))
----

prints `NaN NaN NaN 1/2 1/4 9/8 49/16`. note that we discard for/fold's return value, which is a procedure.

* we not only omit the eventually-redundant `k` parameter, but also the `if` statement that branches upon it.
* we need to use the `next` function in both the average-accumulating and ema-accumulating cases.

.summary

abstractions with (possibly many) numerical _degree(s)_ are ideal. e.g. recursion schemes, tensors, ADTs, functions. all of these structures represent both data structures and transforms.

* algebraic (symmetrical except ad-hoc definition of algebra's rules)
  ** the more symmetry something has, the fewer data are needed to describe it, and the more uniform & predictable its behavior. therefore algebras should be compared by a measure of symmetry in order to identify the best algebra.
  ** pointfree
  ** duals recursion schemes & generation rules (generative functions and/or recursive ADTs)
    *** all structures are mathematically just (recursively) nested relations. any operator that doesn't lose information is a relation: `->`, `cons`, `[a b]` (array). all structures isomorphic to any structure can be used interchangably. therefore the question of which structure to use is purely dependent on the language/runtime's special considerations of those structures. because all graphs can be expressed by linked lists, and graphs are the most general data structure, we know that arrays and functions can each express any data structure.
    *** function & data structure equivalence
* tacit (e.g. group operation notation)
* branchless
* fixed-point arithmetic
* types should be first class & algebraic (like in the lux proglang) i.e. you can write type families just like ordinary lambdas
  ** support & use anonymous data types
* typing should follow type theory convention & arithmetic (seeing types as expressions of numbers and algebraic operators)
  ** see link:https://homotopytypetheory.org/[homotopy type theory]
  ** use types to design programs (namely primitive combinators,) then use an untyped runtime
* how can asymmetric physical devices (e.g. pumps, diodes, sawteeth) suggest digital analogues?

.programming mindset

when designing or programming, have this mindset, think in these terms, ask these questions.

before development:

. what predicates/structure (predicate = axiom, which are the only things that define abstract structures) define the problem & solution?
  .. what, if any, transforms between them do we need to identify? how are they similar i.e. which defining properties or implied properties/behaviors do they exhibit?
. which tools work well with these structures?

during development:

. of a structure
  .. describe it as an element of an algebra or a point in space e.g. binrec is point 2 in the space of recurrence relations.
  .. describe it as both abstract structure and data structure. one defines properties/behaviors; the other implements it in terms of relations [of atoms]. e.g. the λ-calculus is an abstract structure of 3 unary operations (ɑ,β,η) (ɑ is parameterized by the value to rename to, much like ln is unary b/c the base e is implied) over the set of lambda expressions. i haven't identified its structure more specific than merely an abstract structure—seems more lattice-like than field-like, but who knows? anyway, its data structure representation/implementation in fp is...λ's. we get direct translation! cool. that's the most efficient evaluation model. however, if we wanted to manipulate lambdas as data, then our programmatic representation would be lists or macros—whatever method considers lambdas as a relation of a list of formals and an output expressed in terms of the formals. the only reason to express lambdas as lists is because of the language design. fortunately lisps make lambdas exactly equivalent to lists, the only difference between the two being whether, at any point in a program, the list is evaluated as a lambda or not. lisp invested the term _sexp_ to unify lists and lambdas.
  .. of which sets/categories is it a member i.e. what're its types? e.g. binrec is in the set/category of recursion schemes.
  .. what're its properties and to which sets do they belong? for example, binrec has some property of value 2. this property can be called _degree_, _norm_, _rank_, w/e.

TODO: revise this whose section into parts: 1) initial concept; 2) reasoning to that concept's conclusion; 3) reasoning about programs in terms of that conclusion.

these...aren't quite correct considerations; following these considerations to their conclusions, we find that sexps are the natural structure of programs. however, sexps aren't always the best _implementation_. the only alternative is arrays, whose inherent difference is only SIMD support and memory allocation & traversal concerns. both of their _formal semantics_ are identical. in fact, the λ-calculus can be entirely reduced: ɑ-translation isn't needed when all η-reductions have been applied, and β-reduction is equivalent to running a program; therefore programs expressed only as the composition of pointfree functions obviates the λ-calculus; for such programs, the only λ calculus is β-reduction, which is moot since it's the only thing that separates a program from data i.e. it describes programs as executable data and highlights how programs are ultimately merely binary strings that hardware translates into physical phenomena to achieve computation.

obviously programming directly in bitstrs isn't practical. still, we should program in a practical algebra most similar to bitstrs i.e. the least-complex practical algebra. matrices may work. on the principle that everything of degree 2+ can be expressed by compositions of things degree 1 (i.e. every element is expressible in terms of a unit element,) i suggest matrices (_linear_ transforms, i.e. transforms of degree 1) and matrix composition (which may or may not be matrix multiplication, which is function composition when we consider matrices as representations of linear functions.) nonlinear expressions (e.g. x^2^ - x) are expressible by matrices, e.g. in this case [[1 -1]] or the point (1, -1) in the polynomial vector space P(2). *linearity is essential in all contexts; any set whose elements cannot be expressed linearly can itself be expressed linearly* as is this quadratic polynomial example demonstrates. another example is binrec being composed of two linrecs. note that we must consider whether these linear combinations are linearly independent or not! *the linearity is not of the calculation, but of the program itself.* i conjecture that all computations are inherently linear i.e. can be expressed by compositions of linear structures, regardless of structure. we may search for counter-examples simply by trying to express any given algorithm/program/function only by linear structures, requiring as many structures as necessary. also, our consideration of our program is only up to the conditions under which the program halts. our consideration does not concern memory manipulation nor other runtime constraints since we can express in-place updates as a pure function that shadows an identifier e.g. we don't consider whether a swap function returns a new tuple or updates in-place; we care only that both are single steps forward in an algorithm, considering that swap := (a,b) ↦ (b,a) is the same function whether used as `let x = swap x in ...` or `let x' = swap x in ...` or `x <- swap x; ...`. such useless concerns cannot exist in pointfree systems. an example class of systems in which this concept is degenerate/singular is cat-stack-langs such as factor: all factor functions are simultaneously pure and impure, all updating the stack in-place. we may say that functions take from the stack and put new elements, and so they're pure, but what's the difference between updating memory in-place vs popping from the stack and replacing what used to be there with an updated value? the state monad also demonstrates that _purity_ is an illusory concept. therefore fp is no more suited to provability than procedural designs; in fact, *precisely, systems are provable iff they're algebraically described*.

we want a notation that makes the asymmetry (there exists) obviously distinct from the symmetry (for all.) we want self-similar bases—bootstrap-driven development.

if you can't describe a structure by only numbers and the set to which it belongs, then you don't have a good concept. lambdas cannot be, though certain classes of them can, e.g. recursive combinators.

=== commonly misunderstood concepts

TODO: complete this section

. state is unsafe
. safety should be a concern
. that _programming language_ is [should be] a countable noun

==== state

_state_ is program data whose values aren't defined at runtime. for polymorphic programs this can include the program structure.

state is a fact of programming, as suggested by "a program's state." state is easy if you "manage" it intelligently. considering programs as [hyper]graphs (of modules, statements, or whatever all), we can add edges between any hyperedges or vertices and states or state morphisms. thus as we traverse the program, state is modified appropriately. common morphisms are `const`, e.g. in

[source,lisp]
----
(let ([x 3]) ; morphism (const 3)
  (let ([x (+ x 1)]) ; morphism (+1)
    (writeln x)) ; 4
  (writeln x)) ; 3
----

each `let` form applies the stated morphism when its scope is entered. rules like scope nesting can be higher-order morphisms.

.state's problem: state shared among continuations

the commonly-mentioned "problem with state" is really multiple continuations (as defined in racket reference §1) accessing a *common state*, where the state change is supposed to affect only a proper subset of those continuations, but instead affects a superset of that subset. solutions are:

* scope with parameter passing (including threads with mailboxes; see racket's synchronization model)
* purity: only mathematical functions—no effectful subroutines. all binds are immutable function parameters. note that concatenative and applicative languages are not different models; they're different styles. concatenative programming simply prefers higher-order combinators.
* mutex
* meticulous management of simple state (namely implemented by a stack and/or register machine)
* π-calculus or other multi-agent models, producer/consumer, blocking queues, barriers, semaphores, &c

or combinations thereof, e.g. rust's ownership model, which is basically a combination of semaphores and scope with parameter passing.

obviously stacks[concat]/lists[app] (factor or lisp) are more fp and registers (asm) more state. functions implicitly act on the stack or each function uses particular registers. asm is purely stateful (no functions,) though again a sequence of updates is equivalent to applying a sequence of unary functions over a loop, which is equivalent to a single composed function.

in discussions of pure fp, stateful updates are commonly said to be impossible. however, state is used rather obviously: rather than updating a variable in place, instead we call a closure which returns updated state. so funcall instead of `set` and programs are compositions of functions instead of updating a state then reading from that state. again, stack-based langs are the degenerate case of state: they're simultaneously purely functional and purely stateful. they illustrate that `(g . f) a` where `f : a -> b, g : b -> c` (pure fp) is equivalent to `a f g` (stack) or `x = a; f a; g a` (purely stateful; `f` & `g` do not _return_ anything; they merely update `x`'s state.) the ability to shadow variables also demonstrates how state is practically used: (let ([x 4]) (let ([x 5]) (print x)))` is no better than `x = 4 x = 5 print x`.

so...what's the problem with state, again? the problem is _shared, emergent_ state: when the execution of a function affects execution of other functions without our anticipating it. we want to be able to track state, to track emergent behaviors, to know that our system isn't doing anything that we hadn't anticipated. while we're on the subject, type checkers are supposed to guarantee that our program cannot behave outside some specified logical-behavioral constraints. indeed constraining behaviors, and enforcing logical/mathematical structure is good. however, choosing a good canonical basis for programs is a superior alternative to adjoining type systems (or any other systems) onto the actual program—the code that actually produces some effect or result from an initial condition.

whatever you do, but stupidly simple. use registers or a stack, state or function composition. multi-paradigm langs are bad unless the paradigms "work well together" (some reflection of models satisfying common patterns and/or having simple transforms between/among them.) like don't write a pure function in a non-pure lang, 'cause then people will wonder about its purity. don't make people wonder; don't give authors options, because then readers will need to check through all options to see which were used. and don't rely on the author documenting, since it's unnecessary and often authors don't document. even if they do, due to the complexity of natural language, they may explain it in a way that only some people can understand. simple means predictable, meaning that devs don't need as much awareness when coding; coding is then more automatic, easier to learnless likely to have mistakes, and easier to write parsers/tools for.

see my `just-use-lists.adoc` article for use of `set` & `reset` as a stateful alternative to `let`. another example is if `sum a b; inc` [asm] isn't really different from `(add1 (+ a b))` [lisp]. you can make macro or preprocessor that maps procedures to registers that they modify, and thus effectively compose functions. hell, you can just use the stack in assembly, much (or "just?") like you would in factor.

.state's utility

state enables communication among threads (locking mechanism required.) this is fine design. it may require defining a function with a local bind whose access & mutation is regulated by the function, which may use a sync mechanism in addition to more application-specific logic. for systems that support it, a functional alternative is mailboxes or other message passing mechanisms that erlang surely has. state also makes control flow easier to express when multiple branches of a complex dataflow update a common state; this saves us the trouble of refactoring the lambdas within the control flow (which are already altogether complex) to accept or return extra parameters. for an example of this, see `just-use-lists.adoc`.

ironically the very things that people say are bad about state are the very things that make it good, if done properly! just use locking mechanisms and, for state accessed & updated in various branches in a single thread, just scope the state properly, e.g. using `let`!

.scope & state

using the λ-calculus, we don't even need `let`. without `set` the λ-calc is implicitly pure. it's simple but requires λ composition; everything is scoped. commonly it's lexical scoping, but other scoping rules work, too. (see wikipedia's consideration of ɑ-renaming.) however, this problem is solved by all languages: they allow top-level definitions (i.e. those outside any lambdas.)

consider:

[source,haskell]
----
f :: Int -> String -> String -> IO ()
g :: Int -> String -> String -> Int -> String -> String -> (Int, String, String)
----

quick aside: fun fact! `curry` in haskell is like `apply` in lisp! consider how currying is implicit in haskell, as `eval` is implicit in lisp, and haskell tuples are lisp lists/pairs. `curry` is binary and `apply` n-ary, though, since `curry` works on duples, and `apply` on lists.

[source,scm]
----
;; typed racket
(: f (-> (-> Int String String) Void))
(: g (-> Int String String Int String String (List Int String String)))
(f 0 "tom" "nook") or (apply f '(0 "tom" "nook"))
----

aside over. now then, this is obviously horrible, especially to refactor. clearly the triple describes a person and should be grouped:

[source,haskell]
----
data Person = CommonInputs { n :: Int, name :: String, job :: String }
f :: Person -> IO () 
g :: Person -> Person -> Person
----

which is convenient, but requires us to define a type [haskell] or struct [racket]. not bad. in untyped racket's case the only refactoring we'd need is to change the struct's definition. all continuations that use the struct remain as they were. however, what if we want more types? we'd need to add upon the struct or add another struct as a function parameter. such concerns are resolved by using a/lists instead of structs. in this case functions would take the a/list as their one parameter. (this is similar to taking a single row-type parameter in purescript or a map in clojure or an object in js.)

things must be in scope to be referenced! the only way to be more convenient than passing parameters is to use globally-scoped binds. immutable globals (as for all immutable binds) is universally accepted as fine. global mutable binds are usually considered the most dangerous thing ever, but again, depending on the program's needs, adding a locking/sync mechanism or restricting its scope makes it fine, e.g. global static binds, or in scheme, using `define-values` to define multiple functions that use a common state that's been bound within the `define-values` form, acting as a sort of closure. it's not really a closure because because `define-values` is a special form separate from a lambda, but whatever. another solution is using _parameters_ in racket (`fluid-let`) in other lisps.

ultimately lexical scoping solves any unsafe non-concurrent mutability problem(s). TODO: after understanding picolisp's dynamic binding & lexical scoping model, add to this section. the fact that "things must be in scope to be referenced" is always true, but most programmers are so familiar with lexical scoping that suggesting the ability to define a form in terms of variables that may be declared and bound at runtime is as unintuitive as a logic without modus ponens. this is the only thing more dangerous/flexible than mutable globals in static scoping. its advantage is complete orthogonality & flexibility: we don't need to include any modules in order to reference variables undeclared. this is dangerous only if we don't account for the possibility of a variable being unbound, assuming that we can check whether unbound variables are a falsy value; if even trying to reference unbound variables insofar as checking their boundedness (using `if` or some other, more particular special form e.g. `boundp` in elisp) raises an error, then dynamic binding is quite dangerous.

==== safety

==== language cardinality

a language is a tuple of (semantics,syntax). there's no particular reason to couple these things. useful uncoupling is the variety of syntaxes that translate to instruction sets such as jvm, clr, asm. clojure and java both go to jvm. c and abcl to asm. in fact, abcl is an example of how one syntax (common lisp) may compile to multiple semantic/runtime systems, e.g. CL->asm, CL->C.

while we're at it, lisps (most notably picolisp) do not have syntax beyond a literal encoding of data relations, which is the bare minimum needed to express a program, or _any_ structure for that matter! so why have syntax? it clearly isn't needed or even commonly useful. but let's assume that sometimes it may be nice; then we use macros (or, again most notably in the case of picolisp, `eval`.)

so now we've reduced language _out of existence_ (or _beyond a degenerate form_) to the truth of:

. some bare-necessity syntax (viz one that describes relations)
. optional "on-demand" syntax for convenience i.e. the "syntax" is merely a small & particular system of relation. usually the relation is a function or constraint on composition of functions of particular forms. it's important to note that _functions_ aren't special; they're just relations. instead, evaluation is special; it enables reasoning about ideas / enacting instructions. notice i didn't say `eval`! `eval` is only one variety of evaluation. another example variety is type checking, which, rather than executing instructions, parses relations of logical elements, checking for logical consistency or net implication.
. semantics/runtimes

compilers are maps from syntax to semantics: they parse strings obeying syntactical constraints into executable systems obeying semantic constraints. the particularity of syntax is enforced by the compiler and assumed by the programmer. compilers are thus another version of evaluation. compilers evaluation is (usually reductional) translation, whereas interpreters' evaluation is execution. there are an unknown and ever-changing number of various evaluators, and many of them can work together, which means that the number of evaluation systems is the sum of simple evaluation systems and compound ones—a number bound to remaining unknown since it's so large and difficult to calculate.

for example, chicken scheme compiles to C, which can then be compiled to bytecode or machine code by e.g. llvm or cc. in this case we're bound to the union of scheme  and C semantics: we cannot use chicken to express an operation that'd be impossible in C, nor can it express an operation impossible in scheme,...though in this case, C's semantics are probably a superset of scheme's, so we're really compiling to C doesn't further restrict our program's semantics to scheme's.)

finally, let's discuss _semantics_. that's a plural word. why should we group certain semantic properties? an example is strictness. some languages are "lazy-eval" whereas others are "strict-eval"...by default. anyone can delay evaluation by using coroutines, generators, quoting, wrapping in a lambda, &c. what about typing? that's a common semantic property. well, frequently it too is optional! python, racket, & js (to name a few) introduced typing retroactively. furthermore, what exactly is _typing_? it's logical consideration of structure. but we don't need typing in order to have that; contracts are an example alternative mechanism. what about typing being more than mere checks, such as type classes for ad-hoc polymorphism? python and racket (and probably js) already have that. frankly, all typing boils-down to if statements or some equivalent system and—you guessed it!—particular relations.

at this point, i'll just say it: every bit of syntax or semantics is just constraints on relations. *_code_ is just data under some interpretation.* if data has order [structure] (isn't chaotic nonsense) then it has information, and we can extract that information. "code" or "language" is nothing more than how we choose to parse/represent structure. the natural language is then one that only encodes relations, and whose syntax uses the minimal number of characters needed to unambiguously represent relations. a natural consequence of such a primitive language is that more complex and convenient notations (e.g. '(1 2 3) to represent '(1 . (2 . (3 . ())))) can be written as mere relations. as it turns-out, lisp is this language; it is nothing more than homoiconic relations an arbitrary subset of which can be evaluated.

=== algebra-based programming

.general idea

carbon is such an interesting element that an entire branch of chemistry is devoted to carbon-containing compounds. that subfield even uses unique notations. of all the elements, carbon is the only one that has such devotion. compounds are merely relations of elements. however, these relations must obey certain rules; some combinations of elements combine to produce compounds and some do not. this is the chemistry algebra: the set of elements, and the set of axiomatic operatiors that enable the production of compounds. we as programmers should seek the mathematical analogues of carbon: versatile elements of the algebra's set that enable us a great variety of compounds via their great support of various composition rules. that is what i mean by _algebra-based programming_.

.algebras?

the terms _category_, _algebra_, and _space_ are similar and frequently discussions are not so specific that either term is more appropriate than another. an appropriate term that generalizes all of them is _[abstract] structure_. however, i prefer the term _algebra_ because:

. it's simultaneously unambiguous & terse; _structure_ could refer to _data structure_, _abstract structure_, or some general notion of form. the disambiguation _abstract structure_ is lengthy.
. it connotes algebraic [axiomatic] structure & operations. elementary algebra is familiar: permits systematic, axiomatic, deterministic solving algorithms based on iterated inverse operations and re-expressions for certain classes of structures, and naturally suggests other methods, such as galois theory or calculus, to solve problems for which the algorithm fails to produce solutions.

the particular distinctions:

* _categories_ consider particular elements
* _algebras_ consider axiomatic operations irrespective of their sets[' elements]
* _space_ link:https://math.stackexchange.com/questions/174108/difference-between-space-and-algebraic-structure[is vague] and certain spaces may be algebraic structures or not. regardless, _spaces_ are abstract structures that formally generalize notions of geometric space—namely that all spaces have points; all elements of all spaces can be appropriately described by ordered tuples, and a point's numeric value always immediately relates it to other points i.e. all points have neighborhoods.
  ** many abstract structures that feature "space" in their term are not truly spaces, e.g. the suggestion that probability spaces generalize euclidean spaces is at least misleading and at best probably not helpful

spaces are the most common, so by "algebra" i mean "abstract structure satisfying properties (ir)respective of its associated collection where that collection may be defined ad-hoc and/or procedurally, whose elements' relations are described either ad-hoc or by an equivalence relation."

usually we're progarmming for a purpose beyond play or exploration of structure for its own sake. so disregard programming _languages_! we don't want _expressivity_; we want to encode sensible ideas with least effort! we aren't trying to _express ideas_; we're trying to identify solutions to problems! we want a program that satisfies some needs, and we don't care how it's found or what its form is so long as it's easy and works. expressivity just gets in the way; it offers more options than are necessary. expressivity burdens us with the confounding responsibility of identifying expressions that work elegantly together—no mere feat!

.(a)symmetry

[options="header"]
|==============================================
| ad-hoc polymorphism | parametric polymorphism
| asymmetry           | symmetry
| ∃                   | ∀
| no closure          | closure
| enumeration         | generation rule
| finite/bounded      | infinite/unbounded
| closed exprs        | non-closed exprs
|==============================================

* axioms or basic rules (e.g. the peano numeral data type definition) should be the only ad-hoc statements.
* ad-hoc polymorphism appropriately encodes constraints, whereas parametric polymorphism expresses variable freedom.

the combination of ad-hoc & parametric polymorphism enables specification of any mathematical rule with exact specificty. this is true of types as well as functions. for example, bounded recursion is a coupling of an asymmetrical rule (base case) and a symmetrical one (recursive case.) commonly ad-hoc rules limit the extension of generation rules. beware types that seem to be symmetrical, e.g.

[source,haskell]
unfoldr :: (b -> Maybe (a, b)) -> b -> [a]

there's asymmetry in the `case` on `Maybe`'s constructors. "but, wait!" you say? "`Maybe` is simply a monoid!" if that's the case, then we should be able to express it in terms of monoidal functors:

[source,haskell]
unfold :: Applicative f => (b -> f (a,b)) -> b -> [m]

then reify that to `Maybe` and run it. it's impossible to define `unfold` without `case` on `Maybe`'s constructors. asymmetry is required for halting.

programs should be calculated, preferably by a computer, but at least by hand. efficient calculation requires *symmetry*: for everything to obey the same axioms without (*ad-hoc*) exception. ad-hoc (asymmetry) means arbitrary association, and must be accounted for by `cond`—a generalization of `case` or `if`. (see §branchless for examples.) aside from the syntactic cruft of branching blocks, ad-hoc means more rules—more cases for us to account for, which means more varieties of behaviors to reason about, which means more thinking, more code, and greater chance that we'll fail to account for edge cases. precisely the trouble with asymmetry is lack of closure. operations that aren't closed over a set are much more of a hassle to reason about.

every program's only complexity should be exactly its asymmetry, and a language/notation should make very obvious which parts of it are symmetrical or not.

.symmetry example

consider the following definiton of exponential moving average:

[source,scm]
----
(define (ema p ys)
  (let-values ([(α) (/ 2 (add1 p))]
               [(init rst) (split-at ys p)])
    ;; ema is a function not over the past p values, but over the past 1 value.
    (let loop
      ;; ema uses its period only for its first value (which is the mean of same period);
      ;; all successive ema values are functions of only their predecessor.
      ([v (mean init)] [rst rst])
      (if (null? rst)
          null
          (let ([new-v (+ (* α (car rst)) (* (- 1 α) v))])
            (cons new-v (loop new-v (cdr rst))))))))
----

it takes an n-list and returns an (n-p)-list. suppose we'd like to refactor it into a cons-sequence (a form of non-strict list:)

[source,scm]
----
(define (ema p)
  (let ([α (/ 2 (add1 p))])
     (let next ([k 0] [x 0])
       (if (< k p)
           ;; x accumulates a mean
           (cons "NaN" (λ (y) (next (add1 k) (+ x (/ y p)))))
           ;; x is the most recent ema value
           (let ([x (+ (* α y) (* (- 1 α) x))])
             (cons x (next k x)))))))
----

this program is incomplete: it references unbound identifier `y`. in fact, refactoring `if`'s "false" clause into a form such that

. it has the same shape as the "true" clause, namely `(cons value unary-procedure)`
. it returns the updated value of x in car

is impossible!

the problem is when k == p: x is the mean of the first p values, but for k > p x is the most recent ema value. the variation of behavior (program structure/shape) being dependent on or partitioned by sgn(p - k) is asymmetry; rather than being symmetric about (p - k), or even asymmetric about (p - k) on (> 0), but instead (p - k) on sgn(p - k).

NOTE: asymmetry is here described by the syntax "asymmetric about _value_ on _equivalence relation_" where the number of partitions resulting from the equivalence relation is called the _degree_ of asymmetry. if "symmetric about" is used, then the "on" clause must be omitted; this case means that there does not exist any equivalence relation of program structure/shape; exactly one shape describes the program (in reduced/simplified form.)

therefore we must use `cond` (or `case`) instead of merely `if`, since `cond` supports an arbitrary number of degrees of asymmetry, whereas `if` supports only 2. any attempts to represent the desired program:

[source,scm]
----
(define (ema p)
  (let ([α (/ 2 (add1 p))])
     (let next ([k 0] [x 0])
       (cond [(= k (sub1 p)) ;; the last NaN value
              (cons "NaN"
                    (λ (y) (let ([P (next (add1 k) (+ x (/ y p)))]) ;; next returns a pair
                             ;; return P with its first element modified
                             (cons (+ (* α y) (* (- 1 α) (car P))) (cdr P)))))]
             [(< k p) ;; x accumulates a mean
              (cons "NaN" (λ (y) (next (add1 k) (+ x (/ y p)))))]
             [else ;; x is the current ema value
              (cons x (λ (y) (next k (+ (* α y) (* (- 1 α) x)))))]))))
(car (for/fold ([p (ema 3)]) ([x '(0 0 0 1 0 2 5)]) (printf "~a " (car p)) ((cdr p) x)))
----

prints NaN NaN NaN 0 1/2 1/4 9/8 49/16. note that the final value is the one returned by the loop, whereas the others were displayed by the `display` statement inside `for/fold`.

however, if we refactor the whole function instead of just `if`'s "false" clause, then we can transform the program into a symmetric form. the general technique to do this is to identify the shape of the most complex expression, then express the other forms by that same shape.

[source,scm]
----
(define (ema p)
  (let ([α (/ 2 (add1 p))])
     (let next ([k 0] [x 0])
       (λ (y)
         (if (< k p)
             ;; x accumulates a mean
             (cons "NaN" (next (add1 k) (+ x (/ y p))))
             ;; x is the most recent ema value
             (let ([x (+ (* α y) (* (- 1 α) x))])
               (cons x (next k x))))))))

(void (for/fold ([p (ema1 3)]) ([x '(0 0 0 1 0 2 5)])
        (let ([P (p x)])
          (printf "~a " (car P))
          (cdr P))))
----

which we can refactor into

[source,scm]
----
(letrec ([α (/ 2 (add1 p))]
         [next (λ (x y) (+ (* α y) (* (- 1 α) x)))]
         ;; produce ema values
         [f (λ (x) (λ (y) (let ([x (next x y)]) (cons x (f x)))))])
    ;; accumulate mean into x for first p elements
    (let loop ([k 0] [x 0]) (λ (y) (if (< k p)
                                       (cons "NaN" (loop (add1 k) (+ x (/ y p))))
                                       (let ([x (next x y)]) (cons x (f x)))))))
----

you may wonder why we need to transform this program but not the original one which returned a list. the answer is that the original's asymmetry was encoded in the `split-at` statement, which increments the degree of asymmetry, but we also discard the list of NaN's, which decrements the degree.

TODO: cons-sequences vs link:https://srfi.schemers.org/srfi-41/srfi-41.html[scheme streams], and consider backtracking (see the queens problem)

.suggestivity

a good algebra has few operations and few ways to express any program. this means that if a programmer doesn't know how to code a program, or they're unfocused, then glancing the algebra itself, or a list of common patterns/idioms, suggests to the programmer a definition, so that programming happens more automatically.

.algebraic interpretation play

this section suggests identifying consequences of unusual interpretations.

relation equivalnce allows us to consider interesting things: `a -> b -> c` can be described by `[a b c]`. how can we use matrices to compute or reason about types? if we describe types by numbers, e.g. arity, or a sequence of arities (e.g. `(a -> b) -> b -> b` has arity sequence `[1 0 0]`) &c, can we calculate particularly useful function types, then from that derive a suggested function definition? if that sounds like a one-in-a-million shot, remember that functions are just lambdas, and their only supported operation is application/composition. if we consider that all n-ary functions can be described by applications of unary functions (demonstrated by currying) then all functions are trees of unary functions, which may be expressed as tuples or any other binary relation. it's easy to identify calculi about such simple structures.

we can express a graph by an adjacency matrix. we can take eigenvalues of matrices (generally vectors.) what do the eigens of an adjacency matrix represent or tell us? under which varieties of graphs is this operation sensible? is the cardinality of the operation on adjacency matrices any less useful than it is on matrices for which the operation is already assumed useful? if not, why? if not, this must mean that, despite matrices and graphs being isomorphic, there's a difference in either axioms or amount of information between adjacency matrices and matrices for which eigens are useful! if eigns are found to have meaning, perhaps that can give a better implementation of some common structure/method that we've been using.

=== branchless

tl;dr: represent conditionality by the indicator fn.

not only is branching slow, but we must write extra code to account for it. it's also slower for any computer to calculate because it can only predict (often badly, and always entailing extra computation) what the upcoming code is. if there's no branching, then it obviously loads whatever code is next; however, if it branches, then it must load some code from wherever the branch tells it to go (determined during runtime,) which generally is impossible to know in advance.

[source,c]
----
//branching
//faster b/c compiler optimized into 3 instructions
int min (int a, int b) { if (a < b) return a; else return b; }

//branchless. relies on comparison statements returning 0 or 1 instead of true | false
//slower b/c the assembly outputted by the compiler was poor
int min(int a, int b) { return a * (a < b) + b * (b <= a); }

//branching
void upper(char* d, int n) {
  for (int i = 0; i < count; i++)
    if (d[i] >= 'a' && d[i] <= 'z')
      d[i] -= 32;
}

//branchless. 7x faster.
void upper(char* d, int n) {
  for (int i = 0; i < count; i++)
    d[i] -= 32 * (d[i] >= 'a' && d[i] <= 'z');
}
----

*general branchless is a × cond~a~ + ... + z × cond~z~, expressable by a matrix. on a vector processor, or with MIMD, a GPU, or other parallel device, the conditions can be simultaneously evaluated.* `(cond [p r] ... [else e])` can be expressed as `(or (and p r) ... e)` where `and` & `or` short-circuit on ⊥ and ⊤ respectively.

still branching is needed to delay computation:

. speedup: if you're branching over many conditionals each of whose predicates require many cycles to compute, then branching can be more efficient than branchless
. correctness: if any of your predicates are effectful, but should affect only if prior predicates failed, then you'd need branching

i'm unsure whether either of these are necessary concerns, or if these patterns can always be refactored into predicates that support branchless programming—namely that they compute quickly and are pure.

* branchless handcoded assembly is always faster than branching handcoded assembly. one can't generally trust a compiler to produce faster assembly from branchless c.
* array-based programming on some architectures (e.g. intel) avoids branching but still loops, by using `esi` & `edi` special looping-designated registers. instructions like `cmp ...; cmovg ...` are branchless but still obviously use conditionals.
  ** SIMD/AVX branchless is the fastest variety of cpu (cf gpu) programs.

==== eventually branchless

link:https://en.wikipedia.org/wiki/Self-modifying_code#Optimizing_a_state-dependent_loop[self-modifying code] can replace branching code by branchless code once a condition has been met. such conditions may be specified in code, e.g. using a compare statement, or they can arise naturally as the code self-modifies, e.g. if statements from a set of loop instructions are removed upon each loop, eventually leading the loop to collapse into a nop, or otherwise tend toward a state that, upon reaching, guarantees that the loop shall never be evaluated again during the program's runtime.

=== kakoune philosophy

the kakoune philosophy is basically the unix one (*specialized* and *composable* via RPC/IPC/piping/sockets) plus *speed*, *simplicity*, *orthogonality* (ideally only one implementation of each function and functions' functionalities don't overlap,) *language-agnostic (but not form-agnostic!), and *the notation is the language* (as is the case in APL.)

=== general mathematical principles

TODO: read link:https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence#Curry%E2%80%93Howard%E2%80%93Lambek_correspondence[curry-howard-lambek correspondence (wikipedia)]

==== relation & similarity

in mathematics _relation_ is a coupling of 2+ things whereas _similarity_ is a set of common properties held by 2+ things. in mathematics objects like sets can be unordered collections. in programming this is akin to a block of memory being allocated then elements written to it in nondeterministic order. non-ordering implies that any operations on particular elements are less than ideal efficiency. these elements are truly unordered; the access and state of each element is entirely unrelated to any others. non-ordering is enabled by random-access memory. this being said, random access still supports sequential access; we can order array elements.

if items are to be ordered, then they can either be ordered by *contextual* or *intrinsic* property. in the former case, we must use general structures (e.g. list) whereas in the latter case, we can use efficient structures that exploit inherit properties (e.g. heap.)

[options="header"]
|===============================================================================================================
| access     | order type                | example structure | required relations
| random     | n/a                       | array             | 0
| sequential | arbitrary, contextual     | array, list       | n, unpredicated (not constrained to predicate(s))
| determined | intrinsic, total ordering | heap              | 1, predicated (equivalence relation properties)
|===============================================================================================================

* in the case of sequential ordering, n relations are needed whether using a linked list or an array: in both cases each relation is a pointer. though there are pointers in an unordered array, they're irrelevant because they don't signify anything because there's no order to signify.
* because sequential access is determined by unpredicated relations, both cons cells and unary functions exactly represent (binary) relation and are therefore isomorphic. (linked) graphs and recursion are consequent structures. in untyped lisps `cons` is the only primitive except value (number, char) and maybe string. every cons pair is ad-hoc; a universal analogue is a rule for consing, such as an anamorphism.

NOTE: the default assumption for relations is that they'ren't symmetric/commutative. their order is seen as a lack of property rather than having the commutative property.

===== self-relation & self-similarity

TODO: merge with §data/function equivalence

_self_-relation is noteworthy because it's the simplest class of cycles: loops. all unpredicated relations are equal; therefore `(a,b) = (a -> b)` etc. *self-relation on the unpredicated binary relation produces chains of arbitrary order* which is...common in programming? like really? is it more common than unordered or totally ordered structures? maybe it's important only because it describes linked lists, which are then used like unordered arrays but with more flexible (de)allocation? TODO

self relation, if it were to have a degree, would obviously have a degree of 1. as with many things of degree 1, its significance (as its own concept) is its simplicity. self-relation commonly isn't anything special compared to general relation. for example, recursion (a loop) isn't worth distinguishing from corecursion (a cycle.) however, it commonly does make a difference, too. for example, in type theory, it's obviously best to use fully-reduced/simplified types, i.e. a recursive type of one recursive variable rather than many, when possible. self-relation...can describe structures of arbitrary size, composed of units.... TODO
TODO: questions about self-relation usually specifically question equivalence relations: reflexivity, symmetry, transitivity. but this is true specifically of predicated structures.

self-relation goes by many context-dependent names:

[options="header"]
|===================================
| context            | name
| graph theory       | cycle or loop
| definition         | recursion
| geometry           | fractal (maybe always recursive)
| aristotelian logic | posture
| general grammar    | reflexivity
| program runtimes   | reflection
| systems analysis   | feedback
|===================================

and there are many more, such as involution (a function that's its own inverse.) recursion is often treated as a special topic, but the more general self-relation or cycles should be considered instead. consequently the question of "recursive vs iterative" is foolish; they'ren't particularly related and certainly don't form a dichotomy. they're both essentially cycles that may halt if a branch is taken.

====== fixed points

TODO

''''

recursion is the method of traversal: the very fact that the primitive relation (cons) is binary implies that:

. all arbitrary data structures are expressible by cons
. trees (graphs with exactly one path to any node) can be traversed entirely by simply recursively unconsing
  .. this assumes that cons is applied symmetrically: (... . (c . (b . a))...). if a different rule were used—e.g. (d . ((b . a) . c))
  .. traversal by uncons means that all data structures expressed entirely by cons can be traversed entirely by simple applications of `map` (though for graphs with cycles that would map over some elements multiple times)
    ... this applies to structures that aren't really connected, but are expressed as such for convenience. for example, a zipper's two lists may be considered sepapate, but where zipper is a type alias for (cons L1 L2), a single nested map would iterate over both. in other words, in practice in languages like picolisp where cons is the only relation (no arrays,) all structures are graphs all of whose nodes share at least one edge with another node, even if that's not true of the abstract graph that they represent. in this case the structure traversal would need to know which edges are "true" edges vs merely implementation-necessary edges.

thus we see that there are *rules* for *generating* data structures, and *inverse* rules for traversing them—unfolds & folds. therefore types that describe rules for generating structures also describe traversals. recursion is simply closure over application/parameterization. where do we draw the line between recursion and repitition or loops? they're all about self-similarity/factorability. it's about *recurrence*.

''''

TODO: how did i get to talking about types here?

type systems are algebraic. they're abstract structures, not data structures. they're collections of abstract rules, not collections of data.

recursive types' base case examples:

[source, haskell]
----
List a := Nil | Cons a (List a)     -- base case & recursion are both constructors
StateT s m a := Monad m => (s, m a) -- StateT is a monad transformer: a monad parameterized by another monad.
                                    -- it's therefore recursive. the base case is m being not a transformer,
                                    -- i.e. a monad not parameterized by another monad.
----

==== symmetry (universality / rules)

ad-hoc specs are effort; symmetrical is implied. ad-hoc means that you're specifying something specific in order to accomodate a specific problem.

the key is not amount of abstraction, but appropriateness: the basis functions should work *together* well. e.g. semirings are awesome.

abstract structures are better than data structures, too: we can easily compare abstract structures' definitions, whereas data structures are usually defined (and designed) separately, in the manner of, "we're doing such-and-such with some *blocks of memory*; what's a structure that accomodates exactly that and no more, so that it's efficient?" this kind of thinking gets us a large selection of incompatible structures whose definitions are hidden in implementation details. such thinking is entirely devoid of symmetry and mathematical property. by contrast, we can easily relate abstract structures: a monoid and monad are obviously similar, as are semigroups and monoids.

we want:

* freedom from redundancy (e.g. using fold when map would do)
  ** defaults (parameters) are a good way to do this. however, whereas commonly "parameters" regards functions, we should consider structural parameters, e.g. when a category is unspecified, `Identity` is assumed, or in the case of numeric division, 1 is assumed as the numerator if only one arg is specified. in the case of map & fold, we need default parts of the function itself! thta's true abstraction & reification!
* using appropriate basis (e.g. using polar for a cylinder rather than cartesian)

it's been noted that programmers commonly spend significant time fixing things that they've made, which is interpreted as unnecessary difficulty, as opposed to dealing with natural difficulty. while true, there're universal & subtle varieties of this:

* no matter what code we write, whatever designs we use, we must make other code or designs work with them.
* whatever designs we use, we're constrained to them. commonly we must change their internals, augment them, and sometimes split them into parts. efficient autoadaptation or re-solving are ideal. both require a goal [predicate], and respectively require 1) each component being context-sensitive or 2) a (not necessarily deterministic) method for solving the system.

thus we want a small variety of structures that we know fit together easily and still express all programs elegantly (clearly, tersely.)

==== quotient

any separation of data is partitioning, whether it be `a` vs `b` in `(a, b)` or `split(f)-at` or `partition` (which should be called `split-on` aka `group`)

==== modularity

remainder is a partitioning scheme and modulo arithmetic forms a ring. remainders are as common in programming as are partitions / splits / groups / equivalence relations. it's the most fundamental and so most pervasive pattern in programming. for example, we see it in many recursive loops: the structure that we're iterating over is partitioned, and some of that data is consumed into an output, leaving the remainder for the next iteration. this is basic, not insightful. the interesting question is whether recursion also forms a ring.

==== abstract structures

* contrast with data structures
* the subject of universal and/or abstract algebra and/or category theory
* an algebra is axiomatic operations on any set
  ** in universal algebra the set is never particular; it's always free. i.e. it's discussion about the operations alone, where the operations all operate over any arbitrary set
    *** you may think of universal algebra as point-free algebra
  ** in abstract algebra each algebraic structure (e.g. groups) may consider special sets (e.g. vector space over a _field_)
  ** categories' (classes of) objects are ad-hoc/particular/bound rather than parametric/free. the arrows particularly consider classes of objects.
* abstract structures are _interpretations_. abstract structures interpret data structures, but also identify classes of data structures.

.structures

* functors
  ** applicatives (strong lax monoidal functors)
    *** monads
* semigroups
  ** monoids
* ring
  ** modular arithmetic
  ** boolean
  ** semirings
* groups
* vector spaces
* optics
* recursion schemes
* hughes' arrows
* algebraic effects
* barbie/HKDTs (good idea, but flawed in generality & reification under type systems that i know of (scala or haskell.) not even available in typed racket.)

.structure derivation

TODO: this section needs research

Q: what does cons (the primitve relation operator) necessitate about traversals?

trying to implement a doubly-linked list as e.g.

[source,scm]
'((0 . 1) . (1 . 2) . (2 . 3)) : (Listof (Pairof a a))

does not work insofar as given only any element of the structure, we cannot navigate either to the left nor right of the structure. given any element, we want to be able to navigate to the rest of the structure; that implies that the rest of the structure must be present inside the current element. well, the "rest of the structure" means "a traversable cons chain" because every structure is a cons chain. well, lists are a common cons chain. let's try replacing elements by lists: `(Listof (List a) (List a))`. well, that's not right. its rank seems off. `(Pairof (List a) (List a))` (a zipper) works. indeed, it's two lists, which corresponds to the two directions that we want to navigate. furthermore, it's symmetrical, whereas list is a non-symmetrical type: its recursive case is on its RHS. such (a)symmetry and rank determine which recursion schemes to use to traverse structures.

.operation properties

* associativity
* commutativity
* distributivity
* closure/recursion
* inversion (nb. usually called _inverse element_, but the element' inversion property is always relative to an operation. no element is _inherently_ an inversion of any other element.)
  ** involution
* identity
* idempotency/fixedness
* information change [amount] (e.g: integration adds a constant: injections in some sense lose information but bijections don't: forgetful functors. isomorphisms)
* short-circuiting (achieved by multiplication by the additive identity)

.set/category properties

* order

.general properties

* analogue (e.g. homomorphisms)
* uniqueness
* basis [vector spaces] / generating set [groups]

=== numerics

unless you've special need (which i can't imagine, but assume may be possible,) use fixed-point arithmetic (including ratios) instead of floating-point. they're faster and exact rather than approximate.

=== typing

verdict: type theory is good. typed languages are either so poorly typed as to be not considerably better than untyped langs (e.g. java, c++); or are well typed but have flawed type checkers (haskell, typed racket) that ultimately makes them good for most programs, but literally completely obtrusive for certain programs, and merely cumbersome for some programs. therefore it's best to use types and type calculators to design programs, but code the programs in untyped (or latently/dynamically typed) languages.

typing can be its own programming language if done properly. types describe data/functions, which are equivalent, i.e. there's a bijection between types and functions. this is a reflection of the curry-howard-lambek correspondence.

types add (and enforce) structure.

.benefits/uses

* polymorphism
* provability: encoding or guaranteeing specific properties as types rather than verifying by predicates at runtime
  ** especially useful for preventing unobvious invalid values. if a program crashes due to invalid data, then it's obvious where/why. however, handlable invalid data is unobvious, e.g. `(* precondition: x ≠ 0 *) (de [x] (send-to-remote-api "DoThing" (add1 x)))`. usually dependent typing (e.g. ada's) is needed to avoid this class of errors.
  ** lessens probability that a program will crash
* *defines/expresses grammars well*. yes, types, to the extent that they're specific (e.g. dependent typing is more specific than non-dependent) types can implement parsers.
  ** especially useful when many similar but significantly/importantly distinct data & morphisms are used, e.g. git would benefit from types to easily know which similar operations work on branches vs commits vs files.
* identifies improper/incomplete refactoring. e.g. if i change a type's shape but fail to account for that change in functions of that type, then the checker immediately tells me. this is especially useful for polymorphic types.
* we can use types to identify what we know; this is a metric of how well each part of the program is understood. 

.when types are inappropriate

* types are only useful when you're working with distinct types that are valid only in particular relations. for example, types are useless for arithmetic, since only one type (complex numbers) is used.
* types are only (quite a bit of) trouble when we're having trouble identifying structure. typing directly oppose flexibility.
  ** types are uneful exactly insofar as they're specific; (unqualified) general types (the most extreme being `Any`) are not helpful. a qualified general type like `C a => a -> T a -> a` is useful and most powerful.
  ** jack-in repls or eDSLs are good cases of whether types usefully add structure or limit expression

in languages without good type inference (e.g. typed racket:)

* typing syntax adds cruft, which competes with brevity
  ** passing polymorphic functions to higher-order functions (and `->`) is a hassle

NOTE: typed racket is faster than untyped, since types are used instead of contracts! therefore it's better (though possibly with less helpful error messages) to use other lisps for untyped code.

.when types aren't needed

algebra-based progamming does not need typing because all of the operations and valid compositions thereof satisfy laws. therefore the question is no longer typechecking, but rather whether the described program is valid. invalid programs will be caught at compile time. an invalid program can only be one containing any invalid (g ∘ f), i.e. codomain/domain mismatch.

types may be useful if they obey an algebra, again with closure. see §_data/function equivalence_ below.

==== aside: haskell's fatal flaws

haskell is a good case study of a language based on abstract structures with a good type system that nonetheless is not the most preferable language. it currently has the most capable & elegant type system of all languages. here're the reasons that i don't use haskell:

* lacks:
  ** (elegant) row types
  ** (elegant or efficient) dependent types
  ** type sequences (`a ...` in racket)
  ** list types capable enough to iterate over `a b c ...`
  ** refinement types
* ghc (at least) fails to infer multiparameter type class instances
* uses nominal typing
  ** neither isomorphic nor equivalent types are implicitly coercible
  ** by haskell's design, they're needed for type class instance lookup. this is yet another suggestion that type classes have flawed design
  ** suggests seeing a type for its intended purpose rather than for its form
  ** no anonymous types
    *** no anonymous newtypes. we can't bind type classes to type forms on-the-fly

.nominal typing & type classes

simple example problem: the `Eq` type class implies that things can be compared by only one equivalence relation. it doesn't directly imply this; we could define `Eq2`, `Eq3`, &c (even though that's obviously stupid.) still the _real_ trouble comes as functions use types like `Eq t => t -> t -> Bool`. now if i want to use this function with a different equivalence relation, then i'd need to create a new type. oh, wait! that's easy because i can use `newtype`. but this is obviously less elegant than the obvious way: simply saying, `t | a == b = ...`—on-the-fly overrides.

newtypes' only utility is changing a type's type class instances. very strange & inelegant idea compared to the obvious solution:

. types are considered for their form alone
. like lambdas, types are algebraic expressions and may be bound to identifiers; the binding and expression are separate (y'know, like they are in typed racket?)
. types are algebraic: they're composed of only primitive operations. equivalence, isomorphism are considered by the type system and calculated by the same laws of lambda calculus: α-translation, β-reduction, η-reduction. given that haskell's elected to not have row types (and so type composition/application cannot be commutative) the least it can do is make its type compositions follow the same rules as actual haskell code, i.e. the lambda calculus!

e.g. haskell's rose `Tree` type would be a mere `type` alias for `∀a. rec r: a × (List (r a))`, which, considering that `List := ∀a. rec r: 1 + (a × r a)` expands to
`∀a. rec r: a × (rec s: 1 + ((r a) × s (r a)))` which may or may not β-reduce; i've not learned type theory well enough to say. i'm also not sure if these types should be expressed by `rec` or μ.

rather than type classes, we want context-specific _interperations_ or _roles_:

* derive morphisms from types that satisfy some property, e.g. a predicate on a type that refinemes it into (or derives) a type class instance
* instead of type classes, use functions from *types* to output *functions*, e.g. `∀a b. a × b` ↦ `(\(a,b) -> a == b) : (∀a b. a × b) -> 2`  (this example assumes that `==` is defined on ∀a b. a × b)
  * this breaks haskell's separation of types and functions, excepting type families, which are defined ad-hoc, but are recursive, so more complex values can be derived, albeit inelegantly and generally inefficiently (increased compilation times)
  * this point is somewhat incorrect/flawed. i'll come back to it later.

lenses are a perfect example of how a single type replaces a type class of methods `get` & `set`, and instead of instancing a type class, each lens is simply defined as a function. even better, this makes lenses composable! this is possible because, unlike type classes, functions are types and obey symmetric type algebra rather than being ad-hoc. *ad-hoc inherently structures resist axioms.*

==== type-based programming

. languages that don't feature ADTs, like c++ or java, types merely augmentat _actual_ program logic in order to prevent errors
. ADT-based langs like ocaml or haskell use types to help ensure correctness but also to design programs and both attribute & enforce axioms
. type systems with dependent & refinement typing like agda's or f*'s are capable of encoding the entirety of programs "purely as types" because predicates are part of the types and the only other code needed is pattern matching (type deconstruction) and funccalls (which, by including recursion, includes looping.)

the ultimate use of types is their implicit computation of every implied fact, so that the programmer never needs to specify any implied (redundant) information. this entails some kind of solver.

the power of function types is that they describe all λ-exprs and that they're explicit. data types vs functions is a false dichotomy. that's why it's so difficult to decide how abstract to make data types; should they be higher-kinded? should their constructors take function parameters, or should we use functions on the data type whose constructors take data parameters? these questions are unnecessary and can easily lead one to waste time trying to specify the best definition of a data types—ones that're flexible and elegant and work together. functions already satisfy all of those conditions, and are *anonymous*.

NOTE: the following entails a summary of link:https://chrilves.github.io/types/[christophe calvès' series of articles on types]

.example: type that represents currency conversion

    val currencies: Set[String] = Set("EUR", "USD", "JPY")

    final case class Conversion(
      from: String{currencies.contains(from)},
      to: String{currencies.contains(to) && from < to }
    )

    type ConversionRates = Map[Conversion, rate:Double{rate > 0}]

* `rate > 0` is dependent typing
* `currencies` is an enum of strings to effectively identify a subset of all strings (haskell `Currency = EUR | USD | JPY` but more generalizable)
* `Conversion` is an unordered tuple type
  ** the predicate `from < to` ensures that: 1) pairs are of distinct currencies; 2) no pair of currencies can be specified twice e.g. USD->JPY and JPY->USD being defined separately, and so possibly being inconsistent
    *** a map is used instead of tuples to complement point (2)

NOTE: in type theory, the uninhabited type is called `0`; the unit type is `1`, booleans are `2`, &c

===== data/function equivalence

.conceptual

in referentially transparent programs, such as those of haskell, programs are mathematica functions. i'm going to say the same thing 3 times for clarity:

. all data are thus either program inputs or outputs, or inputs or outputs of the functions whose composition is `main`.
. program inputs (hard-coded data) are passed to a function, whose output is passed to another function, ..., whose output is passed to a function upon whose evaluation the program halts.
. a datum `b : b` may be produced from an `a` by a function `f : a -> b`. if `b` is to be used anywhere (which is must, if it's to be useful,) the only way that it can be used is by being passed to another function, say `g : b -> c`. this is equivalent to morphisms `a -> b -> c`—"a to b to c"—expressed by the function `g ∘ f : a -> c`. entire programs are function composition; therefore all intermediate data are function parameters.

again, because all binary relations are isomorphic, and recursing on them produces all structures, all structures are isomorphic independent of relation opreator. many haskell libraries, e.g. lenses, use functions instead of data. *curry & uncurry* demonstrate equivalence of product types and function types by being bijections between the two.

.technical

every data is bijective with a pair of inverse functions; therefore data & functions are equivalent. a common (though only as an implementation detail) example is `build`, which generates a list using a function. another example is `StateT`, which is essentially (i.e. excepting kleislihood) a chain of function compositions evaluate to a final state, like how `build` evaluates to a list.

.unit types (constructors are unparameterized)
[source,haskell]
----
--- 0 as data & function

data Void
type VoidFn = ∀ a. a
d2f :: Void -> VoidFn
d2f x = case x of {}

f2d :: VoidFn -> Void
f2d x = x

--- 1 as data & function

data Unit = Unit
type UnitFn = a -> a
unitFn :: UnitFn
unitFn x = x

d2f :: Unit -> UnitFn
d2f Unit = unitFn

f2d :: UnitFn -> Unit
f2d f = f Unit

--- 2 as data & function

data Bool = True | False
type BoolFn = a -> a -> a

true,false :: BoolFn
true  a _ = a
false _ b = b

d2f :: Bool -> BoolFn
d2f True  = true
d2f False = false

f2d :: BoolFn -> Bool
f2d f = f True False

--- &c
----

* a nullary product type is the unit. this is why unit is written `()`; cf `(A,B)`.

.products
[source,haskell]
----
data Prod a ... = Prod a ... -- constructor is of type a -> ... -> Prod a ...
type ProdFn a ... = ∀ c. (a -> ... -> c) -> c`

constructor :: a -> ... -> ProdFn a ...
constructor a ... f = f a ...

d2f :: Prod a ... -> ProdFn a ...
d2f (Prod a ...) = constructor a ...

f2d :: ProdFn a ... -> Prod a ...
f2d f = f Prod
----

.coproducts (each constructor has different parameters)
[source,haskell]
----
data Coprod a ... = A a | ... -- each constructor is of type t -> Coprod a ...
type CoprodFn a ... = ∀ c. (a -> c) -> ... -> c

-- n represents the nth constructor
injN :: ∃ n ∈ (a ...). n -> CoprodFn a ...
injN n ... _ ... f ... _ = f n -- f :: (n -> c)

d2f :: Coprod a ... -> CoprodFn a ...
d2f = \case
  (A a) -> injN a
  ⋮
  (N n) -> injN n

f2d :: CoprodFn a ... -> Coprod a ...
f2d f = f A ... N
----

an example of non-obvious type equivalence as proven by inverse bijections:

[source,haskell]
----
data N where
  Z :: N
  S :: N -> N

f :: Maybe N -> N
f Nothing = Z
f (Just n) = S n

invF :: N -> Maybe N
invF Z = Nothing
invF (S n) = Just n
----

therefore N ≅ Maybe N. considering that Maybe a ≅ 1 + a, N _is a solution to_ t ≅ 1 + t. in fact, it's the least fixed point of the type-level function `Maybe :: a -> Maybe a`! the greatest fixed point is an infinite peano.

NOTE: μ: (* -> *) -> * is the least fixed point operator, i.e. T ≅ μ(F). μ(Maybe) = N. this example using an alternate λ-like notation: N = μT.(1 + T)

as you'd expect, the function version of N is `a -> (a -> a) -> a`. morphisms between the GADT and such functions is obvious by now. this function is the primitive for all recursive structures.

* each of all recursive types is the smallest solution of some type equation. this isn't a surprise when we consider that `fix` can be easily used to implement recursion.
  ** List a = μ(1 + a × T)
    *** streams are the greatest fixed point
* ADTs are types expressible by relations of 0, 1, +, ×, and μ
  ** BinTree a = μT.(1 + a + (T × T))

TODO: how to express recursive types literally instead of in terms of μ?
TODO: given this, do i want to add anything to the statement that recursion is closure under function application?
TODO: types are inherently for pure programs. how to apply them to stateful programs (for speed, e.g. using vector instead of list)?

.recursion schemes

now that we know function/ADT equivalence and ADTs' basis, we're ready to consider recursion schemes: the factorization of recursive functions.

[source,haskell]
----
-- one base case
s1 :: a -> (Int -> a -> a) -> Int -> a
s1 base rec = f
  where
    f :: Int -> a
    f 0 = base
    f n = let r = f (n-1) -- this is why Int type is present instead of a
           in rec n r

-- tail recursive version
s1 base rec n = aux base 1
  where
    aux res i = if i <= n
                then aux (rec i res) (i + 1)
                else res

fact,sum :: Int -> Int
fact = s1 1  (*)
sum  = s1 0  (+)
list = s1 [] (:)

-- two base cases
s2 :: a -> a -> (a -> a -> a) -> Int -> a
s2 base1 base2 rec = aux
  where
    aux 0 = base1
    aux 1 = base2
    aux n = rec (aux (n - 1)) (aux (n - 2))

-- tail-recursive version
s2 base1 base2 rec = aux bsae1 base2 2
  where
    aux b1 b2 i = if i <= n
                  then aux b2 (rec b1 b2) (i + 1)
                  else b2

fib = s2 1 1 (+)

type bintree a = forall c. (a -> c) -> (Tree a -> Tree a -> c) -> c
data BinTree a = Leaf a | Node (BinTree a) (BinTree a)
tree :: Int -> BinTree Bool
tree = s2 (Leaf False) (Leaf True) Node
----

i used `BinTree` rather than `bintree` because it gives a more elegant definition of `tree`. now i wonder about function types' utility. their beauty is symmetry: they express both functions and ADTs symmetrically, AND they encode ADTs anonymously, thereby focusing on the ADT's form rather than its name or intended purpose. they extend the *algebra* of (function) types, seeing ADTs as their arrows (constructors and dual pattern matching) rather than as categories or choices or structs! therefore function types are the fundamental algebra of computation.

however, they're troublesome to use in current languages (except maybe f*, coq, or agda, as i'ven't learned them yet.) our programming language really should elegantly support algebraic operations on types, including implicitly solving a type-algebraic equation for a type solution. perhaps, however, recursion schemes & optics are together enough to express all programs elegantly.

at least function/data equivalence allows us to systematically derive data types from functions, which may or may not be useful.

==== numeric typing

rather than latent or general typing, by _numeric typing_ i mean using complex numbers as the only data type. complex numbers have many useful algebraic properties and describe much of the natural world, which should describe at least most practical (cf theoretic) programs; usually programs compute things that laypeople can understand, let alone things that can be described by complex numbers! a generalization (albeit losing some algebraic properties) of binary complex numbers is arrays (n-ary numbers,) or even more generally, tensors (arrays of arbitrary nesting patterns.)

benefits of complex numbers:

* great cardinality
* contain the boolean ring
* fast & efficient computation, and ubiquitous (especially regarding both cpu & gpu opcodes)

=== tacit (pointfree)

benefits:

* consider whole program at once. no being lost in detail.

compose pointfree operators:

[options="header"]
|===================================================================================================================
| how                                                                                                      | lang
| threading macro (esp. supporting insertion point via underscore, e.g. `(-> (foldl + _ (range 3)) sub1)`) | lisp
| pointfree composition                                                                                    | haskell
| concatenative programming                                                                                | apl
|===================================================================================================================

ideally the language would infer pointfree, e.g. `(+ car last)` would be shorthand for `(λ (a) (+ (car a) (cdr a)))`. haskell's applicative `->` is decent—`(+) <$> head <*> last`—but lacks elegant generalization (viz nesting.)

no programmatic entities should be given names; they should be given symbols that are either arbitrary, or correspondent (e.g. ∧ & ∨, whose vertical inversion describes their duality,) or common not for their _use_, but for their behavior, e.g. + & × are used when they're defined to obey the common identities, associativity, &c. the reason to never name based on usage is that:

. the name is not as descriptive as the definition itself
. definitions are often modified incrementally as new uses arise, but names do not support such _small/elegant_ alterations where the new name describes its difference from the original
. homomorphisms abound. it should be assumed that in every case where something has some purpose, there's a separate case where the purpose is different but analagous. having separate names for entities with closely related mathematical definitions hides their similarity. finally, there are too many axes of similarity for words to elegantly describe: in addition to homomorphic (a difference of context,) things may differ in abstraction, implementation, arity, axioms, &c. composable symbols are the best (and arguably the only decent) notation that we have.

==== stack

NOTE: functions are called _words_

* purely functional: all functions implictly have the stack as the only argument. thus each function is implictly a stack endomorphism.
* no arguments are named. no local binds.
  ** refactoring functions is practically moot compared to applicative languages
  ** it's like whole programs are implicitly in the threading macro 
* *satisfies algebraic design*; functions are the only elements and composition is the only operation on them. this allows us to see the program for its structure rather than purpose.
  ** e.g. stack words `bi` & `dup` are `\f g -> \x -> [f x, g x]` and `\f g -> \x -> g x (f x)` where x is on the stack.
* plural symmetry: returning or accepting multiple values is no different from one
  ** *composing variadic functions is just as easy as unary ones*. this enables interesting tacit programming.
* prefers more simpler functions than fewer complex ones. this encourages writing higher-order functions and makes programs tacit, again preferring a composition of many small functions to create various composite functions on-the-fly still without requiring much code
* functions are printable
* lisp-like macros (homoiconic)
* continuations (which is a tuple of stacks)
  ** coroutines
  ** exception handling

it's interesting that the word `short` can modify `head` &c to take what's available instead of erroring. i should try to implement that in scheme.

* om seems to be the best catlang. however, it needs funding & development.
* joy, factor, forth, seem to be the best available catlangs. however:
  ** forth is like C: no types, so reflection isn't feasible; fast, low-level, less suggestive of functional paradigm
  ** despite being beautiful and algebraic, joy is apparently, at least currently, slower and less practical than factor.

i'm choosing factor as the language that i'll use at least until om is ready.

==== identifying algebras

as i'ven't yet identified a method for determining an algebra from a set of needs, here i'll fumble with vagries that can be explored.

* goal: all _specific/complex functions_ have small, simple, pointfree definitions. this requires good choice of _common/fundamental functions_.
* generation functions that guarantee certain data forms
  ** implies that other functions don't need to check their inputs

=== data & abstract structures (for general use)

abstract structures are defined by their axioms/behaviors; being algebraic, they aren't defined in terms of particular data. differently, data structures always contain particular arbitrary data, and are defined for fast particular operations, viz search, get, set. an abstract consideration of data structure is concerned with both the algebraic properties of the data, but also storing the data such that desired operations are efficient.

an example is the heap: it requires its elements be totally ordered. the definition [implementation] of the structure is strictly dependent on this property. therefore the structure itself is imbued with algebraic truth, allowing simpler definitions of search, get & set—at least when search is a predicate only of the ordering, e.g. defined in terms of `<`, `not`, and ordered constants. a search for numbers that divide 3 would be no better here than in a data structure defined without regard to algebraic properties. minheaps or maxheaps even enforce O(1) access of a set's min or max. very cool.

it's silly to choose a _structure_; it's more sensible to identify relevant algebraic properties of data, then identify a structure defined about those properties; *data structure should always be derived from abstract structure* unless you're using a probabilistic data structure, in which case obviously the structure should correspond with a probability distribution of certain events.

structures are ranked by their specifity (to a problem) and speed for a set of operations. *there are only two data structure operations: traverse & transform*.

.data structure operations

* traversal (identify a subset of elements)
  ** arbitrary element(s)
    *** traverse a proper subset of elements
    *** traverse all elements
  ** particular element(s) e.g. max of maxheap
  ** particular element(s) as determined by (particular) predicate e.g. predicate `(> 5)` for a heap
* reshape
  ** reindex (e.g. matrix transpose or reversing a sequence)
  ** rearrange (change a graph's edge set)
  ** resize
    *** insert at arbitrary position
    *** add to or extend a side (concat, cons, snoc)
    *** delete

* traverse generalizes get & set from one element to subsets. anything that can be gotten can be set or traversed.
  ** note, however, that some structures, like red/black trees, spend effort to reshape themselves after a set
* each structure permits particular traversals, and, because all structures are relations among data, all traversals can be expressed as recursion schemes

traversal is partitioned into 3 classes because reasoning about each can be quite different depending on the structure. for example, traversing an element in a list is similar to but a bit easier than traversing a substring, traversing the whole list is easiest since that's what we already have, and traversing all but the last element is is the slowest possible traversal of a (singly) linked list. parallelism is no consideration for individual traversals, and easiest to consider for complete traversals.

.always encode traversals as link:https://docs.factorcode.org/content/article-sequences.html[sequences]

for efficiency, all structures should be built & consumed non-strictly; then `consume ∘ produce` allocates no memory, and one can map over the structure multiple times, and have those automatically combined into a single traversal. to implement non-strict in an otherwise strict runtime, use _sequences_: functions from index to element. notable index types are `0`, `Int`, `(Array Int)`. examples of sequence superiority: 1) `range 10` doesn't allocate memory; 2) traversing [2 3 4], then traversing [6 4 7] is faster than traversing [2 3 4] ++ [6 4 7] since it elides O(n) concatenation.

generators are easily represented by closures:

[source,scm]
----
(define (nats [n 0]) (cons n (λ () (nats (add1 n)))))
(nats)         ; '(0 . #<procedure>)
((cdr (nats))) ; '(1 . #<procedure>)

;; print 1 through 10
(let loop ([p (nats)])
  (let ([n (car p)])
    (when (< n 10)
      (displayln n)
      (loop ((cdr p))))))

;; fusion

(define (cs-map f s) (cs-fold cons '() s))
----

i call generators of this style _cons sequences_. i just realized that's an unintentional pun...maybe i'll rename it later. anyway, it's just non-strict looping, just returning the current value and a thunk to loop again (or not loop again.)

* it's like a fold, but
  ** always a left fold
  ** can short circuit
  ** supports infinite sequences
  ** evaluates elements only as needed
  ** using multiple states is just as easy as one. e.g. `foldl (\(s,t,u) a -> (s',t',u')) (s0,t0,u0)` vs `(let loop ([s s0] [t t0] [u u0]) (loop s' t' u'))`. the former is easy because of pattern matching. however, in languages without pattern matching, it'd be a hassle.
  ** semi-stateful: each closure has its own state, but closure states are independent, so closures can be freely duplicated and passed to various other closures
    *** states can be saved and resumed later
  ** terminating generators can simply return any non-pair when they're done. the empty list is a good choice.
* converting a closure to a list does not really make sense; we're using closures _instead_ of lists.
* in a language whose recursion is inefficient and/or a non-functional language, we'd instead return the address of a function to invoke, rather than the function itself
* using non-strict thunks like this is just waste if a strict version would be as well. a good language must default to strict eval, using non-strict only when the code implicitly requires it

another benefit of cons-sequences: on particular iterations of their execution, they can perform stateful actions, which other cons-sequences can use. consider the following code:

[source,scm]
----
(for/fold ([i 1]) ([x X] [y (Y X)]) where X & Y are sequences
  (+ (* i x) y))
----

ideally `X` would be traversed only once. this would be a necessity if `X` performed stateful actions during its execution. (why state instead of returning usual values & rolling state in a list? dunno; maybe this functional approach is always better.) but this would require `Y` to be a cons-sequence, since it'd need to take each x ∈ X as an arg. at least, if not needed nor cleaner, it saves redundant computation.

.the zipping problem

i have multiple loops that generate sequences of differing lengths. the way that i want to use these lists together is particular—not simply shortening lists to the smallest's length, nor padding shorter lists with default values to make them all the length of the longest list.

solution: with sequences or cons-sequences, instead of, on any given iteration, omitting a return value, we can return a zero value; thus padding would be natural; this supports complex padding patterns. it would be interesting to consider how cons-sequences could be redefined to support dependency on other cons-sequences. this immediately seems to relate to frp, non-strict eval langs like haskell, and agent-based models like erlang; such a model may be better for coding in general. of course, the second that you mix functional/applicative and stateful programming, the question "why not use only one" arises, and it's such a valid question that we immediately see that there cannot be a good reason to combine them; they're each orthogonal bases. non-functional applicable programming is just functional programming before people understood how to do it properly. purely stateful programming is what actually physically occurs, and so must always be used. therefore fp is the λ-calculus (plus some extra needless complexity in many languages): a dsl for composing functions, whose final product supports a transform to stateful machine code.

.returning only certain elements
[source,scm]
----
(define (even-nats [n 0])
  (if (even? n)
      (cons n (λ () (nats (add1 n))))
      ;; strict eval if we'ren't returning
      ;; a value on this iteration
      (nats (add1 n))))
----

.`apply`, `unquote`, and `cons` in formals

for a language to feature both `apply` and `.` in formals (e.g. `(define (f x . xs)))`) is redundant. 

[source,scm]
----
;; unary f
(define (mmap f xs)
  (if (null? xs)
      '()
      (cons (f (car xs))
            (mmap f (cdr xs)))))

;; variadic f
(define (mmapm f . xss)
  (if (ormap null? xss)
      '()
      (cons (apply f (map car xss))
            (apply mmapm f (map cdr xss)))))

;; same, but latter is less efficient b/c it maps twice.
;; ideally we'd not have variadic functions; instead we'd
;; use 1. a function that transforms ((1 2 3) (a b c))
;; into ((1 a) (2 b) (3 c)) with 2. apply (or unquote).
(mmapm cons '(0 1 2) '(a b c))
(mmap (curry apply cons) (mmapm list '(0 1 2) '(a b c)))
----

also `apply` (at least as it's defined in racket) is just a less-capable form of `unquote-splicing`:

[source,scm]
----
(apply + '(1 2) '(3 4))         ; fails b/c apply expects only its last argument to be a list
(apply + (append '(1 2) (3 4))) ; works
(+ ,@'(1 2) ,@'(3 4))           ; obviously works
(+ ,@'(1 2) 3 4)                ; we can unquote only where necessary
----

this form of unquote splicing is available only in lisps that allow it outside of a `quasiquotation`.

TODO: see ~/codenotes/plurality-in-lisp.rkt about fusion. the above cons sequences do not support fusion.

.graphs

TODO: all data structures are graphs; how does graph theory relate to identifying data structures? graph theory obviously considers traversal & structure. it's also related to group/galois theory, which concerns symmetries. frankly, link:https://en.wikipedia.org/wiki/Outline_of_discrete_mathematics[all discrete mathematics] should be considered by one structure. see _Graph Theory_ by Russell Merris (Wiley Series in Discrete Mathematics & Optimization.)

you may generally think of data structures as graphs _in the graph theory sense_. for example, a zipper (a duple of lists) would be considered as the disjoint union of two paths. this is a rather strange yet apparently correct description. being not well studied in either group nor graph theory, i can't comment further, but i assume that both disciplines enlighten us to better interpretations of zippers, as with any other data structures. certainly we can intrepret every data structure as a graph, and optimize the structure by

. optimizing traversal: minimizing the shortest path between given nodes
. optimizing balancing: minimizing the difference between a graph and the rebalanced graph

.no particulars

as with everything, data structures should not be seen as "some few things each complex and worthy of study." instead, usually, each structure should be seen as no more particular than one number out of an infinite number of numbers. structures should be implemented on-the-fly just like lambdas. this is almost always feasible because:

. most structures are bulit on few symmetries
. structures can be defined by other structures

''''

typed racket's array library (part of the `math` package) usefully supports three varieties of strictness:

strict:: array wrapping a vector. evaluation changes vector in memory.
non-strict:: function from indexes to element. recomputed on each eval.
lazy:: memoized function

or, as a table:

[options="header"]
|===
| strictness | caches values? | evaluates indexes → value function
| strict     | yes            | on each evaluation
| non-strict | no             | on every evaluation at each index
| lazy       | yes            | on first evaluation at each index
|===

.structure symmetry

*every structure permits terse, elegant traversals & reshapes when these functions are written in terms of the structure's symmetries.* reasoning by symmetry allows easily identifying solutions that would be very difficult to reason about by studying "frame-by-frame" updates to structures. non-coincidentally this is the same as reasoning about recursion: it's difficult to trace every function call, but much easier to understand in terms of closure and base & recursive cases.

for example, it's difficult to understand folds over rose trees unless you understand the rose tree's symmetry.

.compositional definition

rose trees and zippers are simple compositions of lists. rose trees use particular nestings of lists whereas zippers use multiple non-nested lists. how could i identify particular substrings of a list? a substring is defined by the triple (list, start, end). multiple would then be [(list, start, end)], but i know that the list is constant over all substrings, so i can factor it out: (list, [(start, end)]). that's technically a "new" data structure. if we want only to traverse the list of intervals in any order, or fifo, then we're done. however, if we want to traverse in _order_ of substring length, then we're store them in a data structure defined by order, e.g. a heap. for this we'd need to tell the heap to sort on `end - start`. racket's `data-lib`'s binary heaps are constructed over a <= operation.

* array
  ** static
  ** dynamic
* cons/pointer digraph
  ** skip list
  ** DAG
    *** tree
      **** balanced tree
        ***** binary search tree, heap, splay tree
      **** rose tree
      **** finger tree
      **** list
        ***** stack
        ***** ring buffer
        ***** alist
* hashmap
* zipper
* differnce list (purely functional substitute for doubly-linked list)

hashmaps are an interesting solution to making alists faster. however, with ordered keys, splay trees may be better than hashmaps since they grow & rebalance well, whereas, depending on the hashmap implementation, the hashmap may not grow quickly (say, if it uses dynamic arrays under the hood.)

.efficient general-purpose structures

NOTE: this section currently isn't even attempting to be complete

where these structures should be used is debatable. however, they're listed here simply because they're impressive, independent of their suitablity. they're efficient for many applications where not much forethought is put into the nature of their data.

* finger tree
* rope
* skip list (apparently generally superior to balanced trees)

.choosing a structure

_here we're assuming that we're choosing a data structure instead of a generation function or recursion scheme, in case it's worth entertaining._

it's far too easy to assume a structure or framework simply because of its popularity or support from builtin functions. we need to plainly but carefully consider our reasons for using given structures:

. efficiency
  .. speed
  .. memory
. elegance/naturality
. convenience (it and/or functions on it are already implemented)
. recommendation (either explicitly or implicitly, e.g. being a language's builtin type)

instead ponder:

. why are you putting your data in a structure? why do you need to structure unstructured data? what properties should your data have after being structured? what's the least structure that you need to implement in order to achive the desired relations of data?
. how will it be generated? (function output)
. how will it be consumed? (function input)

you may make your own structure (which should be very easy if you follow these best paradigms) or you'll know which of many already-available ones to choose.

structures are for code, not readability! whatever structure you impose—whether data structure, abstract structure, or structured functions, do so expressly to the end of expressing *program logic* better. an example is recursion schemes. *not* an example is a `Person` struct of name, age, &c. that's descriptive data, not programmatic data! all descriptive data should be stored either in a database or by common types, here (again, like a database) as a matrix: an unordered list of vectors of known size. always use basic structures when they'll do. just like you should never assume use of basic structures for implementing program logic, so should you never consider anything beyond basic structures for descriptive data, i.e. data that isn't calculated in a way that significantly affects the program's behavior.

==== arrays vs lists

suppose ads := [a...z] ↦ [b - a, ..., y - x]. the following are various implemenations:

[source,scm]
----
(define (ads s) (map - (cdr s) (drop-right s 1))) ; if (drop-right _ 1) is O(n), then this impl is O(2n)
(define (ads s) (map - (cdr s) s))                ; O(n). assumes that map returns at end of shortest list rather than requiring all same length
(define (ads s) (let r ([p (car s)] [s (cdr s)]) (and s (let ([e (car s)]) (cons (- e p) (r e (cdr s))))))) ; O(n)
(define (ads s) (reverse (cdr (foldl (λ (e t) `(,e . (,(- e (car t)) . ,(cdr t)))) `(,(car s) . ()) (cdr s))))) ; O(2n)
(define (ads s) (and s (cdr s) (cons (- (cadr s) (car s)) (ads (cdr s))))) ; O(n)
_TODO ; stack paradigm, implemented by a zipper

(define-syntax-rule (sp/ a is ...) (array-slice-ref a (list is ...)))
(define (ads a) (array- (sp/ a (:: 1 #f 1)) (sp/ a (:: 0 (sub1 (array-size A)) 1))))
----

NOTE: versions 3 & 5 assumes that the empty list is the false value. in scheme we'd say `(if (or (null? s) (null? (cdr s))) s ...)` instead of `(and s (cdr s) ...)`.

version 5 is the best (fastest & simplest) list implementation.

both:

* relate arbitrary data
  ** support nesting, which means multidimensionality like a matrix (an array (the primitive relation) of arrays) or a matrix of matrices (which supports flattening). the former doesn't increase depth, but the latter does.
* are equally apt for iteration
* run in parallel just the same (on cpus): we can perform multiple `map` operations in separate (virtual) threads.
  ** pointwise ops use one gpu cycle, so arrays are, only on such architectures, faster than lists.
* support n-ary operations (in scheme, `map` is like haskell's `zipNWith`)

neither suggests a traversal; traversals are problem-specific. iteration clearly depends on shape, e.g. a tensor, cycle, general graph with(out) cycles, DAG, tree, list, stack.

arrays:

* random access
  ** a structure that's nothing more than direct data access. it's the simplest arbitrary-size random access data structure possible.
* generalize bitwise operations
* can encode:
  ** graphs
  ** linear transformations
* fixed rectangular shape (determined by the size of each axis)
* O(1) get & set
* O(1) length
* O(1) removal or duplication of axes
* O(1) transpose
* slow addition of shape
* parallelizable pointwise ops (since matrices can be considered as columns or rows)
  ** see blas libs, esp. those official ones optimized for particular cpu architectures
* especially good for combinatronics (for both efficiency/parallelism and expression elegance)
* easily translate to databases (relational, distributed, graph, &c), so you can leverage their parallelism &al distributed computing, and ease of computing over large amounts of data, especially just loading data from disk rather than needing to keep it in memory.

linked list (graphs):

* sequential access
* support ragged matrices
* O(1) push & pop
* O(m) get & set
* O(n) length
* purely functional

zippers:

* eliminate the trouble of choosing take vs spilt-at; they're effectively the same. to split a zipper is the same as searching through a list.
* naturally iterate: input on the right, output on the left if all elements are consumed; else old elements on the left and a separate output list.
* generalize to arbitrary dimensions. xmonad's `StackSet` is an example of a non-linear zipper
* are dual to queues. both are pairs of lists that transform into one list by reversing one list and appending to the other, but queues do so in a slightly different manner, and thus achieve different functionality.
  ** queues and zippers are alternaties to doubly-linked lists.

graphs/arrays are the most general data structure.

zippers are not a particularly fundamental structure; they're given as an example of a possible improvement on a list—namely when one would want to take or split a list. this segways into another point: each graph structure must be designed for particular traversals. however, this is not true of arrays! arrays support all possible traversals over arrays equally well, since arrays are O(1) get & set. therefore arrays are the default solution if you don't know how you'll traverse your data, but you know that it won't change size.

it's impossible to make _strict_ purely functional doubly-linked lists. however, link:https://wiki.haskell.org/Tying_the_Knot[lazy ones] link:https://hackage.haskell.org/package/liboleg-2009.9.1/docs/Data-FDList.html[are possible]. being that even strict languages support non-strict functions by wrapping the original function in a lambda [thunk], construction &al traversals can be non-strict, thus making purely functional doubly-linked lists possible when implemented purely by functions rather than data structures i.e. the "data" of the structure is stored entirely as lambda arguments.

i'm also considering a synchronizable untyped structure `(struct fs (f stack v))` (or encoded as a closure): elements are pushed to it until a condition is met, and then `(begin (set! v (apply f stack)) (set! stack '())`

* arrays are superior for fixed-shape rectangular data. graphs are better for data of irregular pattern and shape
* when you don't know what variety of traversals you'll be doing on your data, but you know that the data's shape won't change, then arrays are best because all their supported traversals (and mutations) are equally fast
  ** typed racket's math's array library (at least when using non-strict arrays) uses relative indices, so:
    *** rotating an array is as simple as changing its starting offset.
    *** slicing, reshaping, removing axes or duplicating axes are O(1)
* if slicing operations seem ineffective, consider them on the transpose of an array

lists aren't as powerful as arrays, but i've yet to identify which powers are useful:

* we can start & end array iteration at arbitrary indices, whereas we must start list iteration at its beginning, and though we can stop iteration after an arbitrary number, we cannot express this number in terms of its length without having O(n) traversal.
* suppose we've a db-style m×n matrix (rows are records, cols are fields.) in this case, the array's shape is known at compile time. we can just as well declare n lists each of length m. if we need fast lookup, we can use a treeset. it's not O(1), but O(log~2~n) isn't bad. a proper B-tree or splay tree or skip list (latter both are implemented in racket's `data-lib` lib) is even better. this being said, matrices don't need balancing!
* vector search always returns an index; if you want the index, then there it is; however, if you want the value, then it's trivial to get the value given an index. lists support search, but would need to return the index and value simultaneously in order to satisfy both needs without writing multiple functions. furthermore splitting lists would require either even more nearly-identical function definitions, or a more complex single function. splits are most elegantly done with zippers, but zippers are still less efficient than arrays, even if they're on the same order of magnitude.

verdict: sequences (lazy lists) and vectors defined by index functions to elemetns are best; these don't require upfront allocation (or, often, _any_ allocation,) and they support fusion. array implementations often come with better functions than lists; such functions shoud be defined & used for lists, too. for example, scalar extension is done on arrays of any shape, but this functionality isn't standardly implemented for lists; for lists you'd need to nest `map` multiple times, and you'd need to make the nesting relative to the nested lists' shape. (optics & recursion schemes solve this.)

*open questions*:

* how commonly do structures sizes change? why?
* when (and how often) do we know the shape of our data when we initialize it?

=== array-based languages

j, apl, burlesque, bqn, sql.

NB. linux comes with apl keymap: us,apl, dyalog variant, option grp:lswitch.

advantages:

* array-based. array algebra, because it works on multiple data automatically, is a simpler alternative to iteration; rather than performing an operation on each element, an operation is performed on all elements at once. furthermore we can consider a whole structure easily rather than using functions that go element-by-element, which are inelegant whenever we need to consider specific substructures.
  ** arrays directly represent axes (where the number of axes is the rank.) a list of 1-data can become a matrix: a list of j-dimensional data; that matrix can then be extended into a rank-3 array, i.e. a list of rank-2 arrays. extruding is obvious by arrays; by lists (pointers) there are multiple equally-elegant implemenations, which introduces the trouble of choice/standardization/(uni/con)formity.
* implicit plurality & mapping
* simple & regular
* polymorphic
* algebraic (viz boolean and [modular] integer rings)
* suggestive. they're small but *particular* languages.
* instead of defining polymorphism or kwargs, array langs uses idioms, which can be modified inline as different functionality is desired.
  ** relieves burden of designing collections of functions modularly, which usually entails careful forethought about polymorphism, default values, and when functions should be split into multiple smaller functions.
  ** an example is apl's grade operators: they break a sort operation into multiple parts, so that one may easily choose only part of a sorting operation, or the whole thing, without any appreciable difference in effort.

=== array langs algebra (WIP)

* reshaping uses modular arithmetic
* rather than have a set of structures, we have rows of vectors; each row is a point, and each column a feature.
* APL HAS A FIXED-POINT PRIMITIVE
* (in apl at least) _rank_ differs from _depth_: a matrix (array of arrays) has depth 1 but rank 2. "The length of the shape of an array is equal to its rank. Therefore, we can find the rank of an array with ⍴⍴—the shape of the shape. Since a scalar is rank 0 (i.e it has no dimensions) the shape of a scalar has length 0 and is an empty vector: ⍬"
* scalar operations are applied to all points without changing shape
* if α f ω, if α or ω is scalar, then f is applied to all elements of ω or α (this is called _scalar extension_). if both are arrays of the same shape, then f is applied pointwise. _incompatible_ shapes raise an error: if shapes differ, then the smaller shape is repeated until it's the same size as the larger array (in t.racket's `math/array` this is called _broadcasting_). this makes expressions like `1 2 3+(3 2 1) (¯1 ¯2 ¯3) ((44 71 11) 1 (32 0.5) )` valid. this example's β-reduction is `(4 3 2) (1 0 ¯1) ((47 74 14) 4 (35 3.5) )`; `1+` is mapped over `(3 2 1)`, `2+` is mapped over `(¯1 ¯2 ¯3)`, and `3+` is mapped over `((44 71 11) 1 (32 0.5))`.
  ** this expression is invalid in t.racket's `math/array` (error "expected rectangular data"). however, for arrays of the same shape, broadcasting is obvious: `(array+ (array [1 2 3]) (array [[3 2 1] [-1 -2 -3] [44 71 11]]))` => `(array [[4 4 4] [0 0 0] [45 73 14]])`. see §6.3.1 _broadcasting rules_ for complete spec. i haven't understood the rules technically enough to want to try to find an array representation that accomplishes the same result as the array langs (viz apl) version.

again, arrays should be sparse. just as sets may be represented by a predicate adjoined with some ad-hoc enumeration of elements, so can an array. consider the identity matrix; it's defined as [xᵢᵢ=1;0] (shorthand for [xᵢⱼ=i==j]). this is a fn (i,j) → {0,1}. I₄+[xᵢ₂=3;0]=[(j==2?3:0)+i==j]] which is encoded by the exact same bits regardless of size. this is a shape-invariant sparse array. all mostly-symmetric arrays should be mere fns from index to value.

==== trees

can store in [[parent] [sibling] [value]] matrix. the de facto apl structure is _inverse tablized form_. _parent vectors_ are standard in high-performance computing; a parent vector (afaict yet) is a pointer from child to parent. of course many children may point to a common parent. by beginning with an array of children you maximize parallelism. each child has (n)one parent; thus at each iteration up the tree, that whole level is in scope.

what's neat about codfns is that in the ast (of the code that it's compiling) the interpretation of the children is given by the parent.

==== case study: fibbonacci

TODO: interpret this section into terms of recursion schemes, then generalize to arbitrary computations, generalizing matrices if/when necessary.

recursive -> iterative -> matrix. credits to https://rybczak.net/2015/11/01/calculation-of-fibonacci-numbers-in-logarithmic-number-of-steps/

.recursive definition

fib n := fib (n - 2) + fib (n - 1), fib 0 := 0, fib 1 := 1

.iterative optimization

TODO: how was this reformulation derived from the recursive form? what subset of recursion schemes support this optimization?

fib n = go n 0 1 where go k a b = (k == 0) * a + go (k - 1) b (a + b) # branchless style

.matrix reformulation

the iterative definition is a linear combination, in the recursive case, even in branching form. therefore it can be expressed by a matrix. being that fibbonacci is a 2nd-degree recscheme (i.e. over 2 recursive variables) our matrix will be a 2x2: [[0 1] [1 1]], which satisfies the recursive identity: T * [[F~n~] [F~n+1~]] = [[F~n+1~] [F~n~]] = [[F~n+1~] [F~n+2~]]. it preserves the original increment between the 1st & 2nd recursive variables, i.e. the difference between (n + 1) - n = (n + 2) - (n + 1) = 1. the transform is applied to the recursive variables column vector.

* it'll always be a square matrix because we aren't adding nor subtracting degrees of freedom / variables.
* notice that 0 & 1 here are not mere scaling factors; they determine abscence or not of variables, which means that they describe the actual code (inclusion of identifiers in code or not.)
  ** this is exactly the same as branchless programming or using predicates as masks.
  ** we can use matrices of 0's & 1's to both describe and _computationally encode_ the structure of algorithms themselves. of course matrices might not be able to well describe any code (though they may perhaps) and so we'd use other structures such as graphs or semilattices to describe program logic. the significance of such encodings is that these structures exhibit known mathematical properties, and so can be systematically optimized/reduced, transmuted, compared, and other goodies. this is in the direction of computationally producing, optimizing, and hacking together programs, as well as formally discussing the structure and therefore *form* of programs independent of each their expected runtime contexts/purposes.
    *** as with all vector spaces, matrices encode linear combination. linalg is neat & useful, but how do matrices compare with other algebraic structures for their description and computability of programs? btw _macros_ don't count; they are programs that produce others, but they don't compute programs algebraically; they just follow arbitrary instructions. it's impossible to write a macro to optimize some source code. syntax trees are not algebraic.

thus the iterative form is reformulated into F~n~ := (matref (T^n^ * [[F~0~] [F~1~]]) [0 0]). we can compute matrix exponentiation, but as is common with any recursion, e.g. factorial, we can preferably omit redundant intermediate operations. in fact, one can study any system's evolution to see common patterns (e.g. self-similarity or other forms of modulus) and then factor-out those patterns and apply any reductions. perhaps rather than "recursion" you may think of 

=== datacode

if a program is described entirely by mutable data structures, such as lists or matrices, then the structure of the code is simple and lends itself to smc, which makes programs adaptable during runtime, and makes their definitions simple if the program is mostly created by bootstrapping, like programming a diploid cell rather than a whole person. obviously like the cell, such programs would take time to "grow;" that growth should be saved at various stages, or should be at least done at "complile time" rather than when the program is run by a user. saving programs is trivial when they're just simple data structures.

.eval, not macros

TODO: see file:///nix/store/90c33f811d9vhlhs9qkr2xycrcv3xgqw-racket-8.0/share/doc/racket/guide/eval.html about making the below code work in racket.

macros are considered different from functions, which is asymmetrical. for example, we infamously can't `apply or`. that's stupid; if we just define `or` as a function (i.e. a manipulation of relations) then obviously there's no problem (since all code must be defined as such):

[source,scm]
----
(define (my-or . xs)
  (if (null? xs)
      #f
      (let ([x (eval (car xs))])
        (if x x (apply my-or (cdr xs))))))
(my-or #f 3 '(error "error"))
(apply my-or '(#f 3 4 (error "error")))
----

should work. however, at least in racket, the bind to `x` fails: "?: literal data is not allowed; no #%datum syntax transformer is bound in: <1st arg here>". i'm unclear on when `eval` works in racket.

TODO: use racket link:file:///nix/store/90c33f811d9vhlhs9qkr2xycrcv3xgqw-racket-8.0/share/doc/racket/guide/eval.html#%28part._namespaces%29[namespaces] properly, then try again

* apply works nicely with the fact that quoting produces a list; it allows me to effectively do `(my-or 'a 'b 'c ...)` without needing to quote each argument: `(apply my-or '(a b c ...))`
* a proper lisp would allow me to say `(if xs)` because `()` would be falsy and `#f` wouldn't exist

=== some misc study

consider the following function. it operates on sequences and lists. that's one degree of asymmetry, as clearly marked by `case-lambda`. why have both? how can we express this one simple idea (take no more than) by one simple function rather than two?

[source,scm]
----
;; 1st form: take m elements if available; else take whole list, whose length must be less than m.
;; 2nd form: e.g. (let-values ([(more? next)] (sequence-generate (in-naturals))) (take-no-more-than/gen 3 more? next))
;; the 2nd form was chosen because, at least currently in typed racket, (-> Natural (Sequenceof a) (Values (Listof a) (Sequenceof a)))
;; isn't neat to implement.
(: take-no-more-than (∀ (a) (case-> (-> Real (Listof a)          (Listof a))
                                    (-> Real (-> Boolean) (-> a) (Listof a)))))
(define take-no-more-than
  (case-lambda
    [(m xs0)        (let loop ([count 0] [xs xs0])
                         (if (or (>= count m) (null? xs))
                             null
                             (cons (car xs)
                                   (loop (add1 count) (cdr xs)))))]
    [(m more? next) (let loop ([count 0])
                         (if (or (>= count m) (not (more?)))
                             null
                             (cons (next)
                                   (loop (add1 count)))))]))
----

we begin by comparing the two. to compare, they should be normalized, which means expressed in common terms & forms, then simplified, namely by factoring and reducing redundancies. the obvious, yet particular solution is to accept only sequences, and convert the list to a sequence before passing it to `take-no-more-than`. this works because an inexpensive morphism from list to sequence exists. generally, though, one case is not a subcase of another, and such lucky, simple refactors aren't possible. anyway, let's compare. their difference:

. in the list case, the state is as a loop var, whereas in the sequence case it's hidden in a closure
. instead of `null?` the sequence version uses `(not (more?))`

but that's not an enlightened description; it's a plain one—one that sees only what the code is, not what the program is. we aren't reversing some hacky code, here; this is code whose purpose we already know. we can reason about its intent rather than its machinery. it's a function that we can reimplement any way we want so long as it fulfills its purpose. a description of it in cs terms:

terminating loop, i.e. there're a variant and an invariant. considering the whole program as a triple of code, data, (each corresponding to `.code` & `.data` sections of assembly code) and presently-allocated memory (the stack, registers, heap), the variant must be represented by data, memory, or code, any of which can be called _state_—that which can be saved & loaded from in order to suspend & resume the program, just like save states in video game emulators, or suspending & resuming an os. iff the state is saved in code, then the code is polymorphic [self-modifying]; else it's state stored in some memory structure.
  .. it's obvious to think of it as being stored in a single memory block whose value is overwritten 
  .. but in fp, state is stored in the parameters to the loop (a lambda/continuation). the loop address is a constant value in memory, which means that, unless there's optimization by the compiler/interpreter, then memory is dynamically allocated during the loop body, then its value is set to the updated state, then that memory address is passed to the subsequent invocation of the loop, and, assuming that the old state is not referenced elsewhere, its memory cell is deallocated. not a memory leak, but inefficient needless (de)allocation.
  .. finally, in polymorphic code the memory state is not significantly different from the code state

the variant is the tuple (count, xs or (next,more?)) the invariant is at least the loop address and the code.

but those're the variant and invariant of the _implementation_. what are the mathematical variant & invariant?

TODO: identify

=== haskell & lisp

[options="header"]
|==========================================================================
| haskell          | lisp
| Π                | `cons`
| ⨿                | `car` & `cdr`
| pat match        | `cond`
| types            | `lists`
| constructors     | alist of interned symbols to lists
| non-strict lists | `(de nats ((i 0)) (cons i (lambda () (nats (+1 i)))))`
|==========================================================================

=== evaluation

mechanism to select whether eval is done at definition vs use. pointers or dynamic binding are such devices: `pos = {<, min, &low}` in c (or w/e syntax is needed to add functions to an array); in picolisp, `(de pos () (list < min low))` (defining a function since functions are always evaluated upon invocation); in factor, which, like haskell or ocaml, there are only words of natural arity, not partitioned into data vs functions:

[source,factor]
----
SYMBOL: low
0 low set
: y ( -- y ) low get 4 + ;
y . ! prints 4
10 low set
y . ! prints 14
----

here `y` is defined as a function instead of a global variable i.e. it's evaluated upon each use. typically languages make it very difficult to evaluate only part of a definition e.g. `y := x + 4 / z` where `x` is evaluated at y's definition time and `z` is evaluated per use of `y`. such complexity is usually unjustified, bad design. metaprogramming, redesign, or runtime or compiler optimization usually makes this a non-issue, so that, for things defined by both constants and variables, the constants or majority of output are not re-evaluated upon each use. functional paradigm makes this easy: `y := λz. x + 4 / z`. just like that, `x` is certainly evaluated at `y`'s definition time and `z` is certainly evaluated upon each invocation. unfortunately this comes at the cost of needing to pass some `z` as a parameter to `y`, except in picolisp, which, by its dynamic binding, uses whatever values `x` & `z` have at `y`'s invocation. this is good, but, afaik there's no way to evaluate `x` at `y`'s definition with `z` evaluated at `y`'s invocation. i'm very much a fan of optional parameters e.g. in picolisp:

[source,lisp]
----
(de y (x) (+ x 20))
(y 10) # 30
(y) # NIL
(let x 20 (y)) # NIL. actually i thought that it'd eval to 40
(let x 20 (y x)) # works, but it's inelegant
(de y () (+ x 20)) # redefine y
(let x 20 (y)) # works, but inelegant; i'd prefer the syntax (y 20)
(de y (z) (+ (or x z) 20)) # works. elegant invocation but inelegant definition.
----

i can define an fexpr to make `(de f (sym1 ... symn) (g sym1) ...)` (example's first definition form) expand to `(de f (sym1* ... symn) (f (or sym1 sym1*)) ...)` (example's last definition form).

=== case study: redbean

TODO: likewise investigate justine's other code

* tiny
* dynamic (lua)
* built from ground-up for efficiency &al 1st principles
* naturally gzip-encoded responses

==== ape (actually portable executable)

* runs on bare metal
* runs on 6 oses
* avoids syscalls

==== cosmopolitan

TODO
