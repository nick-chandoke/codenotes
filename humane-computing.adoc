= human(e) computing, inc

coding is modelling; it's like a mental model, perception, laid in front of us for display. it's an external view of the mind's eye. thus everything should be naturally automated simply by people using computers as their mind, as a way to think, replacing paper. it's that we use the computer for everything, which has the side effect over time of incrementally building modules, which we do to make our thinking easier. to note is to code. to develop a thought is to develop a codebase. to test our assumptions, we test code. to determine conclusions, we run programs. in any universe, everything is coded matter. thought is abstract, code generalizing the logical tool called _language_, and for thought to actually occur, it must be coded by corresponding physical phenomena. _computers_ would just as well be called "code machines" or "simulators". like our minds, they are malleable, and we can create our own little universes in them. i'm enabled this perspective by having asked what computers (essentially) were, rather than what tasks they could complete. even programmers typically look for features, capabilities; they see programming languages as tools rather than expressions of thought. anyway, eventually i wanted to strip away all language that i could while retaining a program, and eventually realized that bitstrings were an encoding of the essential facts of math: relations of symbols. bitstrings are relations of the lowest-information object.

computers are simulation machines. they are the simplest, most abstract characterizations of the universe. they are *particular action machines*, whereas the universe is generally non-particular action. therefore computing is practically a holy art. it is play & study of nature itself, but as a microcosom. coding is about meaning & existence themselves! thus i demand that proglangs be close to bit sequences or some other structure that minimally encodes relation. i demand that languages be "low-level" because the bits are truth, whereas the "higher-level" languages are interpretations—unnecessary assumptions and mental biases! i want the raw facts alone! i demant to code low-level because i perceive the universe in such terms! importantly, to see only the truth and not be biased by interpretation (an accompanying entourage of truths) enables us to imagine new, novel representations & interpretations, which may make working with them in particular ways very easy or natural! computing should be a human right. it's an achievable one given how inexpensively computers can be made. olpc failed, but that's ok because the raspberry pi foundation succeeded, what with their pico model being USD $5 _and_ being totally hackable with gpio! and even still, handheld computers are everywhere, in great variety. computing is accessible to everyone! anyway, it's (past) time to see coding & computers not as a particular task for some particular class of devices, but as essential to living as are mathematics or language, and for computers to be afforded to people as readily as books!

with this perspective, it's only natural that coding & computing should be as accessible as math or reading. (indeed, code is very closely related to language—basically the most minimal form of language). we have arrived there; we have pretty affordable smartphones & tablets that are no less powerful than paper—the _de facto_ mental aid for math, and the _de facto_ medium for the _de facto_ encoding of language: text. and hell, we even have the second-most popular encoding of language: speech, in the form of audiobooks! we have video which we can watch on the go! that is impressive and excellent; however, despite that we can do this anywhere, it mostly requires our exclusive focus, so we aren't experiencing the rest of the world around us. our senses are still limited to only two—audial & visual—and we must constrain our vision to a small screen (small field of view), and constrain our posture to one of mostly being still, either looking at a screen on a stand or in our hand. these systems cost us a great deal of freedom! we should adapt these systems, or create new ones, to enable us to do all that we want to do, with all the freedom of movement, and all the joyful variety of physical stimulation that we have (or maybe _had_; i was born in '93, long before any kid had any kind of portable computer) as kids!

i define code as "non-abstract meaning". it is a primitive (axiom) of reality, like the word "thing" and so it cannot be explained. it's time to stop thinking about "code" as "programming language", and to stop using the term "programming language" at all, in fact! we may code steps, or databases, or human-readable text. we may mix these. they are not siloed. code may be expressed as glyhs on a screen or on paper, or electrons or voltages in a semiconductor, or as sounds, or as a variety of non-textual visuals, or holes in a punch-card, or any combination of these! as humanity has studied information, and the field of computer science has come about, we have enough perspective to change our concept of language to make it much more powerful, elegant, terse, flexible, and less misleading. language evolved for describing non-abstract things: time, location, people, physical objects, and things of the earth. it was never suited for abstract concepts, and indeed we see an astounding deal of fallacy simply because natural language's mechanisms for describing precise, abstract thought are poor. because the language is not terse, we resort to metaphors to try to quickly communicate AND REASON, and these fail. remember, btw, that language is the _de facto_ tool of logic, and so miscommunication & fallacy often occur together. anyway, we should devise a communication & persistent storage protocol for code without interpretation: pure truth, definition; and we should make commonplace the practice of pondering relations & interpretations of data. *this is necessary to fully know any truth without bias. when we improve our coding, we improve our reasoning and understanding both.*

of course, for using audio, there's especially no reason to use text, since, even with `cursorless`, using audio to use a cursor to modify text in a buffer is clumsy. given that we're using audio, _there's no reason to have an audio interface to a visual design_; just make a new audio-focused design!

notable is what andreas stefik called link:https://www.youtube.com/watch?v=uEFrE6cgVNY["evidence-oriented programming"]. he's completely wrong in his method because his method assumes that usability is "ease of use for an untrained person" but proglangs are not appropriately measured by how productive people are with them, unless we know that they've been correctly trained to use the language! i've already written a book talking about how natural language is terrible and ultimately causes _all of_ the problems that people have. programming languages also are very flawed, because they're designed after natural languages, and because they're based on many bad models or other designs! to be precise, we can only say whether a person & language work well together; that is known, and that is useful information, and i love that stefik is promoting the idea of scientifically testing programming languages (or other tools), because that knowledge is potentially useful. however, to say that the language is at fault instead of the person is incorrect! furthermore, to say that either is bad is also incorrect! the underlying assumptions about modelling data may be ineffective. there are many aspects to consider, and they must be considered altogether, like optimizing a multiparameter function.

goal: to push the ability for humans to compute while retaining their humanity. sitting at a desk in a fixed posture indoors, staring at a screen, away from sunlight, without fresh air, even with glockhale's & egoscue's methods, which make computer use (somewhat) comfortable and obviate the perceived need for ergonomic equipment, computing sucks—especially coding. all thoughtwork is computation, btw. computation is generally merely systematic information transfer/relation that eventually arrives at a state that matches a particular pattern.

these concerns are observed as follows: humans think for various reasons. this corporation desires to maximize the degree to which thinking is focused, perceptive, creative, efficient, easy, rewarding; free from distraction, fallacy; and minimizing stress or worsening of physical or psychological health due to thinking. naturally, of course man-made computers are helpful aids to computation done by the brain.

the crippling <thing>, which is fallaciously assumed [taken for granted], is that all (good) reasoning or thinking is done abstractly, entirely logically. we acknowledge that there's a common method of learning, and actually most people assume that this is how all learning occurs, and maybe even that that's how it _must_ occur—with mistakes and personal refinement to "make the knowledge one's own". this is not true, however; my mathematics enables everyone to understand things exactly. few people discuss learning in terms of information theory, but it should be done. that being said, though logic is certainly powerful, it's only when it's blended with associative reasoning that humans witness their potential for reasoning! the body must be engaged! the amount of information in the body is far more than the information content of whatever is ever in the mind, understood abstractly, precisely! this is analagous (and somewhat similar to) how analog computers are more powerful than digital ones. see <https://www.youtube.com/watch?v=agOdP2Bmieg>. there's extreme, untapped potential to reasoning by exploiting the various varieties of understanding. we should map between data and senses (varieties of perception or physical encoding).

computers have been useful, but, as is extremely common across many techniques, their current designs are far inferior to their potential due to retainment of historical designs that were designed for constraints that were present then but no longer. here's a history of mainstream computing interface technology:

. in the mind entirely
. spoken language and visual symbols e.g. counting on fingers or an abacus. these were memory aids; their state was easily derived by looking at the physical object itself. computation was still exclusively the responsibility of the user.
. non-programmable computers e.g. babbage's tabulating machine.
. programmable computers. their programming configuration determined by flipping switches. programs' state was observed by "blinkenlights."
. programmable typewriters (as seen in that 1974 apl demo video). the user types a program statement, then, i guses, a camera reads it, evaluates it, and prints any output.
. the keyboard & text monitor came next. it was based on typewriters, despite being electronic, whereas typewriters mechanical and designed for paper. the staggered & QUERTY layouts were designed for typewriters' physical arms that struck the paper. characters newline and carriage return were characters also present, mimicing the typewriter's physical mechanism. the shift key was retained, but this is good design for either device.
. the mouse supplemented the keyboard. it was the _de facto_ device for working with graphical, not just textual, representations of data, or graphics.
. styluses were popular for touch-enabled portable devices. portable devices were not very capable nor worthwhile, though; they were either limited in capability such as the palm pilot, or limited in purpose, such as the nintendo ds.
. touchpads & tablets, then touchscreens came around, and supported gestures such as swiping, pinching, rotating. they had special support for multiple simultaneous pointers (namely fingers); this new physical design enabled organic, natural ways of manipulating data, virtually the familiar physical manipulations of reality.
. the leap motion controller, and vr controllers exist, but are not easily acquired, nor easily used. they are intentionally closed or restricted such that only qualified users may access the proprietary knowledge or tools needed to make the devices achieve their potential.

there hasn't been anything since the monitor, keyboard, mouse or trackpad, and touchpad. the keyboard is so needed that the touchpads emulate keyboards. granted, the ability to have various keyboards such as how the iphone allows switching among the japanese kana keyboard, the usual querty, and hardwritten character recognition. very impressive and uses a capability exclusive to the touchscreen. still, for the most part, computing, data, information, is textual. despite the hardware advances and displays of their usage, their potential is mostly squandered, relegated to some few particular uses.

so i say that it's past due time for us to design computing that fits a natural lifestyle, fully enabling people to do human/animal things like walking, being outside, stretching, breathing, looking around. indeed, not only are these more comfortable and healthier, but they make us better at thoughtwork, too, since good thoughtwork requires creativity, which is accessible to us only if we're sufficiently comfortable. we must breathe to be fully awake, and we need a great degree of varied stimulus, and exercise, in order to get blood flowing, oxygen to the brain, and for our brains to work at their best! we're more energetic, aware, and motivated when we move & push our bodies. inspiration (physical) increases degree of inspiration (abstract). adrenaline entices us to work, and excitedly so! many take to coffee because it can be sipped at a computer whereas one cannot code while doing pushups. we have become used to, but not comfortable with nor adapted to, the constraints of current computer interaction! we have become used to a lesser living condition! it has become our new norm. there is a great deal of all-around better computing & living yet right outside this condition!

the best common tech that we have yet is the pinephone, a fully-permissive, conveniently portable computer with a touchscreen bright enough to be seen well even on sunny days, and wifi, cellular radio, bluetooth. i'm happy to know that such permissive hardware is available. then again: 1. hardware limitation is usually not much a limitation when countered with a clever workaround, and 2. we must still change the design of software, the very notion of how we represent and manipulate data. text is not suited to our powerful visual cortex. it does not stimulate the eyes well—indeed, quite the opposite: staring at text, at a flat screen, causes the eyes to strain and does not allow them the continual exercise that they need to be healthy (coordinated at the least): darting around, and changing focus.

''''

.aside: the need for permissive (free/libre) hardware
the amazon kindle and apple iphone are impressive hardware. their software is shit and the devices impose such undue degrees of restriction on their users that it's downright disrespectful, but that's no matter. they don't owe us good products, just as we don't owe them the purchasing of their products. the fact that they have made these produces demonstrates that it's possible, which is exciting! so let us strive to create at least such good hardware! and of course the software will be a configurable, extendable one, that does not force software updates upon users, nor annoy them with undesired popups about signing-in to the cloud, or try to sabotauge the user from making the device actually do what they want. i'd think that such devices are only leased to users, not sold.

''''

(1) being said, actually decent speech recognition is something that people have struggled to attain, and especially with machine learning, we have it, and that may require special hardware (namely tpus) to achieve efficiently on a portable device. speech is an excellent way to interact with a computer. we use language to interact with people, and people are obviously capable. hearing is passive. it can be done anywhere that's not too noisy, and can be done in any posture, anywhere, without need for monitors, keyboard, mouse. the only interface that it needs can be found in wireless earbuds: speakers and a microphone. we will never strain our ears by listening too much. <cursorless.org> demonstrates good code editing via voice only. still, we can do better. there's no reason to assume that code should be done in text instead of shapes or sounds that have no standard textual representation. overlaying visuals is not as easy as overlaying audio sounds. also silence is the default for audio, as opposed to the default visual condition: whatever's left on the screen. one is more condusive to focused thinking than the other.

keyboards are better than mice because they put very many buttons, some of which are combinational (modifier keys), at the fingers for immediate use. the mouse is suited for continuous space, whereas the keyboard is good for a discrete space. audio is good for both, though working with hands is good, too. but it should be _working with_ hands; it should be tactile, organic, continuous. the leap motion controller is a good example of this; it can be used for manipulating 3D spaces but its physical form is not manipulated. instead it reads the manipulations of the fingers. this is good because the hands are not constrained to holding an object, and such a manipulable object would be impractical to design, anyway.

a reason that non-free software fails is that it can't be adapted experimentally as the need arises. to fulfill its potential, software must be alive. it must be actively manipulable, like something that you can poke around while it lives, change while it's running, morph in realtime like smart-putty. computers are electronic devices; it's only natural that we should leverage the very power that makes computers programmable, to reprogram programs as they run! also, the incentive of open, collaborative development is different from the capitalist incentive, and leads to a vastly different fate not only for each company that chooses between collaborative vs capitalist, link:https://www.youtube.com/watch?v=wVYG1mu8Lg8[but for _all_ of them]. the capitalist goal is money, not product quality, and these two aren't necessarily co-dependent; therefore the effort put into a capitalist company is risky for their customers. capitalist companies compete against each other, not working for the customer/product.

== the plan to make computing humane

we must develop both the hardware and ux, and they should be designed for each other. the system is altogether designed for humans. the hardware & software should be designed together, and them together designed for the human user.

=== hw needs

try just placing your laptop on your bed so that its angle is _slightly_ acute, with the back of the display on the bed. does looking at it immediately _feel_ different, like it's a "normal object" that you can move around and use freely? it feels that way for me. my body—particularly my eyes—is immediately more relaxed. being being freeform instead of in _any_ kind of fixed position is already an immediate & considerable benefit!

=== no longer keyboard-centric

the mouse really sucks compared to the keyboard. it's still not ergonomic, and being precise with it is an unnatural movement. thus i decided that at least with the keyboard, we have access to very many keys which we can press quickly in sequences and chords, thus enabling millions of input combinations immediately, all with much less effort required to be precise; because keys are tactile (whereas moving the cursor over a ui element is not) and discrete rather than continuous, it's easy to freely strike keys without making typos very often, given the input speed, whereas ensuring that you move the mouse correctly requires much more attention to movement precision. initially i was using windows, then mac, so i used accelerators & mnemonics as often as possible. then when i got to linux, i discovered that most tasks could be done even better in a text user interface (namely curses). it's time to take the next step in this direction: to use touch—the most natural method—and perhaps voice, too, to interact. what controls can we imagine beyond virtual buttons? what ux can we do better than sequenced (incl. hierarchical) submenus? *above all, we want kakoune-style ux*: everything is shown to the user as it's happening.

== tests

* code shouldn't be partitioned; it should be searchable, and perhaps indexed, but not separate. for example, the fact that i'm putting this note in this _file_ rather than into a _database_, is bad, since this note may belong here or in other, related files. having to manage these files is bad since i must identify the file by _name_. searching is much easier. thus the test: that i can quickly find an index at which i can insert arbitrary information, in such a way that the database can be _queried_—not _read_ as is the case of a file.
* that information is always considered relationally, especially across computers; e.g. i should effortlessly be able to open a list of she-ra video files of episodes featuring razz; the code should effectively (and should just as simply be): `"list of she-ra episodes" wikipedia "razz" find episode# "she-ra/" match mpv` except that i should type "wiki" then tab-complete wikipedia, and have wikipedia's completion let me fuzzy match "she-ra episo" to the full title, then give, if not a csv, an html page which i can there interactively scrape into a csv, then filter for `'razz'∈`. filtering itself will not be me typing "filter", but instead me pressing `^f` which causes a filter ui element to show. the fact of being interactive means no text, fewer typos, fewer keystrokes, immediate feedback, no redundant typing. _all_ coding should be like this. finally, notice that the code description that i gave is factor-like: left to right, following the progression of information as i find it then operate on it, interactively & iteratively shaping the information environment. other notes: `episode#` is not a predefined word; it's a value identified by the user as they interact with the data view; like k, nouns are implicitly verbs: indexing into other nouns; here "episode#" is a key used to access into the structure. given that html is hierarchical, the indexes would be those matched, and i'd select a subset of them that interested me, then select their tables—like kakoune, multiple selections, and like k, the indexing-noun `episode#` will implicitly apply to all of the multiple selections.
  ** summary: messy (by default; the user adds structure as & where they care to, by way of pattern matching or manual selection), exact (because the exact data is seen immediately as the user works through it), fast (overall, since humans work best this way), immediate interaction

=== inspiration

video games: interactive, fun, colorful, imaginative.
