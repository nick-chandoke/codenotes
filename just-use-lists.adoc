== types? structs? enums? just use lists!

TODO: use `de/aljs` as a premier example, and compare against `json-struct`.

this article criticizes typing as it's currently used in programming, and suggests using lists instead (favoring _shapes_ instead of [nominal] types). this does not pertain to using computational type-algebraic systems or formal type theory for exploration & study of mathematical structures which i wholeheartedly condone.

TODO: this article suggests that sexps are ideal for all code. this is incorrect; sexps are perfect only for general code. for specific systems, where meaning can be inferred from syntax that exploits particularities of the language, then sexps are inefficient. sexps are perfect only when there are no constraints on the code, making sexps basically a better version of xml, json, &c. revise this article to reflect this. in fact, homoiconicity is not even necessarily ideal; other metaprogramming is often more efficient (again, mp requires mere manipulation & evaluation of data.) furthermore stack & functional paradigms are never more efficient than stateful ones; in fact, they're equally efficient only for composition of unary fns.

TODO: write examples in apl, POW!ass, or other general form that most exploits symmetry and makes a/symmetry clear. this notation would exploit common information, would be mutable and likely use arrays^*^, would use dynamic or global binding, sensible shortcuts like pil's `@` (which requires dynamic binding). also give solution in prolog. the haskell solution is first, since that's whose faults this article seeks to demonstrate; then the ideal (most reduced notation that exploits common info) is given; then other solutions are given to see how they compare. ^*^also explore & discuss arrays as the natural structure where each array axis corresponds to one of symmetry.
TODO: when writing factor's version, use factor oop; cf lists & ADTs. in fact, very much attention should be given to lisp vs factor; they're very similar, but factor has easier syntax and is actually totally flexible, which lisp purports to be, but really isn't (except pil.)

.preface

i used haskell from 2017 through oct 2019, and it was the first real programming language that i'd learned since java, and it totally blew my mind. i still say that it (haskell2010) is an excellent language, yet lisp obviates it. i began learning scheme in feb 2020, and started using typed racket shortly thereafter, until feb 2022, when i decided that types were hardly beneficial overly-restrictive cruft. i'd always wondered how anyone ever got _anything_ done in untyped languages—and i still do, but only as its currently commonly done in practice; in principle, however, using untyped code is much better, when the code [encoding scheme] is simple. furthermore we can exploit common information in a system without types; we're free to work with information instead of trying to partition everything by types, which is hopeless; the types will either be so specific that they're a pain to use, or they'll be simple but lose the granularity to express particular facts. here i discuss types, when they're good, why they usually aren't, what's better, and how this better untyped system is different from common untyped systems like javascript or python. for almost all of my life i've been using typed programming and relying on the typechecker heavily, and using it (in the case of haskell) for considering the mathematical structure of my code, and using type calculus to redesign my code and correct design logical inconsistencies. i say all this so that you understand that i have valued formal math even before discovering haskell (and in fact i found haskell because it was the closest extant language to a theoretical purely-declarative language that i'd drafted over a few years)—that i'm not just someone who thought that it was "easier to get started" with untyped languages such as python just because they make it easier/faster to get code to run correctly in common use cases. to put it lightly, i'm *not* a fan of non-reproducability, unmanaged/emergent state, oop, or not being told the shape/type of the data with which i'm working.

''''

consider the following typed racket code that simply describes placing orders to trade stocks. pretend that `struct` accepts default values per field like `let` does; writing such a macro is easy anyway.

[source,scm]
----
;; sum types
(define-type LinkType (U 'value 'percent 'tick))
(define-type LinkBasis (U 'last 'bid 'ask 'mark))

;; product types
(struct ([price : Positive-Float] [link-type : (Option LinkType) #f] [link-basis : (Option LinkBasis) #f]) #:type-name Limit)
(struct ([value : Positive-Float] [trailing? : Boolean] [link-type : (Option LinkType) #f] [link-basis : (Option LinkBasis) #f]) #:type-name Stop)

;; equivalent to the type These Limit Stop where These a b := This a | That b | These a b
;; typed racket does not support ADTs
(struct order-cond ([stop : (Option Stop)] [limit : (Option Limit)])
  #:guard (λ (s l n) (cond [(and s l (stop-trailing? s)) (raise-arguments-error 'order "trailing stop limits are unsupported" "stop" s "limit" l)]
                           [(or s l) (values s l)]
                           [else (raise-arguments-error 'order "every price condition must have a stop, limit, or both." "stop" s "limit" l)]))
  #:type-name OrderCond)

;; structs -> hash tables -> json objects, which will be
;; later passed in an http request body to an online trading api.
(define (order-cond->hash pc)
  (let ([s (order-cond-stop pc)] [l (order-cond-limit pc)])
    (hash-set (hash-union (if s
                              (let ([v (stop-value s)])
                                ;; 4 decimal digits allowed if price is below $1; else 2.
                                (hash (if (stop-trailing? s) 'stopPriceOffset 'stopPrice) (round/num-digits (if (>= v 1) 2 4) v)
                                      'stopPriceLinkBasis                                 (kabob-case->UPPER_SNAKE_CASE (stop-link-basis s))
                                      'stopPriceLinkType                                  (kabob-case->UPPER_SNAKE_CASE (stop-link-type s))))
                              (hash))
                          (if l
                              (let ([v (limit-price l)])
                                (hash 'price          (round/num-digits (if (>= v 1) 2 4) v)
                                      'priceLinkBasis (kabob-case->UPPER_SNAKE_CASE (limit-link-basis l))))
                              (hash)))
      'orderType (cond [(and s l) "STOP_LIMIT"]
                       [s (if (stop-trailing? s) "TRAILING_STOP" "STOP")]
                       [l "LIMIT"]))))
----

22 lines. pretty straightforward structure.

we'll pretend that type inference in typed racket is as good as haskell. this article is criticizing typing, not how languages implement typing.

.good

* type checker can optimize `+` to `fl+`, which is specialized to floats.
* safe: checked automatically, so less burden on the programmer to check for typos or mistaking one symbol or type for another (e.g. arg is `LinkBasis` but `LinkType` was provided)

.bad

* type checker does not recognize the similarity of the `Limit` & `Stop` types; thus i need to write similar code for each despite their similarity. i also need to use `stop-link-type` & `limit-link-type` instead of just `link-type`, &c. a solution is to make a type class `HasLinkType` and have both `Stop` & `Limit` instance it,...but not really, since that quickly becomes cumbersome, requiring code in amounts proportional to the number of attributes shared by various structures. at least in lisp we can hide that extra code by writing a macro that expands to it,...but inelegance is inelegance even if hidden, and it tells us that we can do better.

now consider this alternative which uses lists of particular shape instead of a variety of types each having particular accessor methods:

[source,scm]
----
(define-syntax (cond-let stx)
  (syntax-parse stx
    [(_) #'(void)]
    [(_ [(~literal else) e ...+] . _) #'(begin e ...)]
    [(_ [g (~literal =>) (x ...) p e ...+] . rst) #'(let-values ([(x ...) g]) (if p (begin e ...) (cond-let . rst)))]
    [(_ [g (~literal =>) x e ...+] . rst) #'(let ([x g]) (if x (begin e ...) (cond-let . rst)))]
    [(_ [p e ...+] . rst) #'(if p (begin e ...) (cond-let . rst))]));; combo of assoc & member. also doesn't enforce racket's needlessly restrictive contract on assoc.

;; returns first list element matching a predicate or tail of first pair whose car matches a predicate.
;; this works on a mix of alist & list, which is a more useful structure than flat lists or alists.
;; as this is a mix of alists & lists, i'll call them "a/lists."
;; massoc with a/lists is a common. more generally, though, you'd loop over a list [stack], taking n elements where
;; n is related to the top of the stack.
(define (massoc k s)
  (let ([k (if (procedure? k) k (curry equal? k))])
    (let lp ([s s])
      (and (pair? s)
           (let ([c (car s)])
             (or (and (pair? c) (let ([y (k (car c))]) (and y (cdr c))))
                      (and (k c) c)
                      (lp (cdr s))))))))

;; example order conditions:
;; '((limit 42.04) (trailing 1%) mark) ; mark is applied to both trailing stop & limit
;; '((limit 42.04 mark) (stop 40 bid)) ; mark is applied to limit, and bid to stop
;; '(limit -2%)                        ; limit is 0.98 × link basis
(define TS '((trailing "TRAILING_STOP" stopPriceLinkBasis stopPriceLinkType stopPriceOffset)
             (stop     "STOP"          stopPriceLinkBasis stopPriceLinkType stopPrice)
             (limit    "LIMIT"         priceLinkBasis     priceLinkType     price)))
(define (order-cond->hash s)
  (let*-values ([(glb s) (partition symbol? s)]
                [(glb) (if (null? glb) #f (kabob-case->UPPER_SNAKE_CASE (car glb)))]
                [(type) (map car s)])
    (hash-union (for/fold ([h (hash)]) ([i (if (pair? (car s)) s `(,s))])
                  (cond-let [(massoc (car i) TS) => T
                             (match T [(list _ lb lt p) (for/fold ([h h]) ([v (cdr i)])
                                                          (cond [(member v '(last bid ask mark)) (hash-set h lb (kabob-case->UPPER_SNAKE_CASE v))]
                                                                [(symbol? v) (hash-set* h lt "PERCENT" p (let ([x (symbol->string v)])
                                                                                                           (string->number (substring x 0 (sub1 (string-length x))))))]
                                                                [(number? v) (hash-set* h lt "VALUE" p (round/num-digits (if (>= v 1) 2 4) v))]
                                                                [else (raise-argument-error 'order-cond->hash "link basis, number, or percent symobl" v)]))])]
                            [else h])) ; ignore now; catch invalid types below
                (cond-let [(subset? '(limit stop) type) (hash-set (if glb (hash 'stopPriceLinkBasis glb 'priceLinkBasis glb) (hash)) 'orderType "STOP_LIMIT")]
                          [(subset? '(limit trailing) type) (raise-argument-error 'order-cond->hash "trailing stop limits are unsupported" s)]
                          [(massoc (car type) TS) => x (hash-set (if glb (hash (cadr x) glb) (hash)) 'orderType (car x))]
                          [else (raise-argument-error 'order-cond->hash "stop, limit, stop & limit, or trailing" s)]))))

(order-cond->hash '(limit 42.04))
(order-cond->hash '(mark (limit 42.04) (stop 1%)))
----

20 lines, not counting `cond-let`'s & `massoc`'s definitions, since those are standard for this style of programming, and would be always included implicitly. so at only 2 lines terser, what does this style offer?

* this one has more code to handle more flexible order description; order literals are represented simply by quasiquoted lists.
* not only is the order description more flexible, but the order structure is more flexible, too; this code generalizes much more elegantly than the struct-based method.
* *[EDIT]* in retrospect, it was stupid to allow any order for value and link basis; it's always going to be price then basis. this reminds me of a truth i'd forgotten: parsers (with backtracking) are an elegant basis for all programs. they should be used to accept function args; function args should be either evaluated before or not a la picolisp; and the parser should be applied to the list of args a la `syntax-parse`. while a parser would not have made this code shorter nor easier to read, it would stay about the same size while ensuring that, e.g. neither price nor basis is specified more than once. the parser here would be `((U 'limit 'stop 'trailing) (-> (? price) parse-price) (-> (? 'last 'bid 'ask 'mark) kabob-case->UPPER_SNAKE_CASE))`. i should explore this more, especially comparing them with a/lists.
  ** parsers would make base cases vs recursive cases easier, too: we can try matching against either case (or the more specific of either case). of course, once part of the match fails the next parser is tried.

the code was made by following a few design rules:

* store all information in lists
  ** factor-out common list shapes
* if a list's value changes dependent on some later data, then parameterize the list by wrapping it into a lambda that accepts that later data
  ** this associates the conditionality with the data that is affects, making for easier refactoring than using branching forms, all of which are special syntax

.good

* more flexible
  ** order of arguments is irrelevant. by contrast, `These a b` is not equal to `These b a`. (though `(U Stop Limit)`)
  ** `These a b` does not automatically generalize; we'd need to create a new type for each arity, even though the real structure that we want to encode is, given a set `A`, we want some B ⊆ A : p(B) for some predicate p. however, the above logic generalizes easily and is commutative.
  ** sexps are inherently as extensible as xml; we can add, remove, or modify the lists. we can't do that with structs.
  ** lists implicitly describe row types, which allows us types like `{t1, t2, ... | r}` [purescript]; types specify a minimal description rather than a total one. this, especially combined with delaying shape/type checking until each particular place in which a list is used, enables very easy, flexible ad-hoc polymorphism: we can have a shape `(a 3 b 4 c 6)` used in functions `f` & `g` because `f` requires that the list have attributes `a` & `b`. `g` requires them, too, but also optionally supports attribute `c`, which `f` ignores. this is very natural; in reality things are complex, and we allow them to be whatever they are so long as they specify a small whitelist of constraints. in other words, we do not omit things because they satisfy properties that we didn't specify! types not supporting `| r` are effectively like saying "i want a cube," and when you try to give a blue cube, the type checker rejects it, because "blue" wasn't in the type spec; the solution to this in a typed system is to create a new product type of blue and cube—an ad-hoc join that prevents us from using elegant traversals and *structural polymorphism*.
* rather than using constructors, we use symbols. we can use `limit`, `stop`, and `trailing` without worrying about scope or shadowing. in other words, it's like a lisp-standard simpler alternative to prefab structs in racket.
  ** fields have context-sensitive meaning because they're bound to identifiers at each match rather than once at definition. this is useful because it reflects the truth that data are data, and we then interpret them, but some can permit multiple interpretations.
* much simpler structure
  ** easier to refactor
  ** faster to read (namely `TS`, which nicely describes ad-hoc groupings)
  ** uses `kabob-case->UPPER_SNAKE_CASE` only twice: once for when global link basis is set; and once for when local link bases are set. notice that the 2nd case is plural, yet we use `kabob-case->UPPER_SNAKE_CASE` only once for that case. this sees `kabob-case->UPPER_SNAKE_CASE` as being used ad-hoc in two different cases: singleton in the first case, and the 2nd case is a set of cases over which `kabob-case->UPPER_SNAKE_CASE` is symmetric.
  ** exploits mutual exclusivity of link & basis types, allowing them to be expressed in any order.
  ** both link & basis are simply sum types, so they can be expressed simply as lisp symbols. same with stop's or limit's ability ta accept percents or numbers.
  ** the expectation that everything is lists encourages developers to describe the shapes of their data, like how is done for macro syntaxes. if the syntax needs tl;dr description, authors are likely to use math terms or reference similar shapes. this is much nicer than giving a name, forcing me to jump around documentation from name to name (since types are often composed of other types) just to see what kind of data i'm dealing with!
  ** uses list to simultaneously express optionality and plurality; `[Either a b]` therefore replaces and generalizes `Maybe (These a b)`. in this case, though, we're even more general: a list of a sum of an arbitrary number of types (cf `Either` which is a sum of only exactly 2 types.) this is why this model works better than product types.
* natural
  ** permits factoring common properties. e.g. `[(String, [Order], [Order])]` can correspond to shape `((name (open) (filled)))`. this shape is, among its isomorphisms, particularly nice because we can `assoc` to get all orders which are naturally partitioned into open and filled. if we want to perform an operation on all orders, then we simply recurse on the value returned by `assoc` (assuming non-falsy.)
  ** the types are data, so:
    *** we can use `map`, `member`, &c to transform the "types," and interned symbols can easily be converted to strings, which makes conversion to json simple.
    *** permits using folds over structures. for example, with a product type of numbers `p`, i can `(> (apply min p) n)` for some `n`. this is sensible if `p` represents points on an interval, and we want to see if the whole interval is beyond a boundary.
  ** auto-optimizing: does not require us to be specific e.g. we may start with `A := B C | D E F`, then find that it should be refactored into `X := B C, Y := D E F, A := X | Y`. with lists, because the checking is done only when necessary, we're free to change structures' shapes without needing to refactor.

a/lists can be expressed better without extra delimitation, e.g. `'(a 1 b 2)` instead of `'((a . 1) (b . 2))` or `'((a 1) (b 2))`; or `'(a 1 b (2 3) c 4)`, which is alternative to `'(a 1 (b 2 3) (c 4))`. the only difference among all these is whether we use `cdr` or `cadr`, and which varieties a given lisp's `assoc` supports. not only is the simpler encoding of alists terser, but it sees "alist" an an _interpretation_ of flat lists, encoding the shape in the traversal rather than in the list itself. this is more efficient than building up a list, and it keeps the list simpler, thus allowing it to be used in more contexts, thus retaining higher flexiblity. also consolidate all discussions of encoding form in shape vs traversal.

.different

* if you want safety (like what types provide) then you need to implement your own mechanisms
* dynamic, so checks or other computations are at runtime rather than before.

.techniques that i want to later take time to explore

* devise a whole list algebra: a formalization of the modeling & transformation techniques that i used here, such as parameterizing lists or identifying the need to have a list of functions rather than a function that composes with itself-on-other-iterations. see <https://doisinkidney.com/posts/2019-05-08-list-manipulation-tricks.html>.
  ** lists & list [function] application provide a common notation for expressing all code.
* compare list building and function composition, and list iteration and function evaluation. also consider `(or (assoc k s) _)`/`(case k s [else _])` isomorphism
  ** `cond` is merely `case` but whereas `case` takes parameters key and alist from key to value, generalize the key comparison function `equal?` to a given predicate, then rather than distributing that predicate over the key and the alist's keys, just have the alists' keys be nullary predicates which are then evaluated.
* picolisp level of exploiting state

.things to consider

alists are relations natural with `assoc`. really any list can be considered as an alist, a la clojure's `let` syntax. `(massoc 'b '(a b c d))` should return `'(b c d)` (which would be done if i'd defined massoc in a lisp not scheme, wherein the falsy value is the null list rather than `#f`, which is symmetrical with `member` and `assoc`.) in this way all lists can implicitly be alists, here with `a` mapping to `(b c d)`, and `b` mapping to `(c d)` &c. if i want to associate a value with `b` and have `c` map to `(d)` then i just insert it: `(massoc 'b '(a b (3 4) c d))` returns `(b (3 4))` and i can insert `cadr` to connote this expectation that the list is of form `(k1 v1 k2 v2 ...)`, thus getting `b`'s associated value, `(3 4)`. this is still literally is an optimally efficient traversal (for unsorted data; otherwise we'd traverse in a heap-like way.)

in §bad, "a/lists are slower" is not present. while technically their lookup is slower than vectors', the difference is inconsiderable for a/lists of struct size; you'd never use a struct with enough fields for this difference to be appreciable. still, it suggests a good consideration: better rather than alists are splay trees; these are usually preferable over lists that represent sets, i.e. lists whose ordering is irrelevant. like in arc lisp, such lists' (a tree is just a list of a particular shape) elements should be mutable with O(1) update.

structs, alists, splay trees, and hash maps are mostly equivalent: all support lookup and default values, and are isomorphic. the only general difference is that alist lookup (via the `assoc` function) returns different values depending on whether the value was missing or whether it was found, but the found value was falsy i.e. `assoc : Alist a b -> Maybe b` where b may contain a falsy value e.g. `(assoc '((1 . hi)) 0)` returns `()` (not in the list) whereas `(assoc 0 '((0 . ())))` returns `(0)` (in the list, and associated value is `()`.) also, as that example shows, `assoc` returns the key, and the associated value may be a single value or a list of values; to assoc it's all the same since `'((0 . ()))` equals `((0))`; a more appropriate name for `assoc` is `find-car`.

racket is one of few languages that includes _contracts_: basically type checking that occurs at runtime, acts on runtime values, and uses general predicates to effectively do dependent type checking. contracts are nice, but writing contracts that represent the shapes of such organically-shaped lists is anything from a hassle to infeasable.

=== when types are appropriate

types are appropriate when data's shape has little variability and specific (and usually simple, depending on the capability of the type system) constraints. type systems are typically cumbersome, at least for not supporting anonymous types (except typed racket and roc.) more to the point, beyond type systems, structs & enumerations, which may be not typed, but still obviously correspond to product & sum types; when they should be used is determined by precisely the same rules as when their corresponding types should be used.

the alternative is lists. lists are universal because they're the simplest structure defined of [binary] relation & recursion. by the magic of math/order, such a fundamental structure must natural describe all other types. therefore we should ask ourselves, for any type, how that type is described by lists. every type can be described by a set list of particular shape(s). when dealing with structure as simple as lists, we can ask the usual properties—associativity, commutativity, invertability, &c—which we cannot so freely do with types, because types (or enums or structs) cannot be computed, unlike lists. of course, this is not the fault of type systems; it's the fault of how type systems are used/implemented in programming languages. if we're talking about type theory in as a subdiscipline of pure mathematics, then we're afforded all the wonderous algebraic freedom that we're used to in math. type theory and its notation creates a very different experience in math vs cs. still, type theory is no more beautiful than anything else in math; we can simply describe it programmatically by lists instead of "types" [cs], and we can either use formal methods or tests (or check via preprocessors such as macros) as a more capable (and much simpler) alternative to today's type systems.

=== with all things now considered, what exactly i'm proposing

==== naturality, shapes

lists are considered simply as data and can describe any type/structure, including programs. we as coders have complete freedom with them, whereas type systems currently lack such flexibility; e.g. type systems don't support an analogue of `assoc`.

==== computable programs

ideally we'd have super-fast, small code, that would be ungodly unsafe if written by hand, but the beauty of it is that it's generated automatically by a system assumed to be correct. suppose that a type checker refines code into C union types, combines multiple numbers into a single 64-bit register by using bitwise operations, and allocates a chunk of memory some of which contains numbers, strings, floats, &c; performs bitwise ops on floats, and the code rewrites itself during execution—all the most dangerous optimizations—then it's all welcome as long as there's no chance that it'll case the program to crash or otherwise behave outside of spec.

basically: type checkers guard programs against programmer flaws. there can be two solutions to this: check what the programmer's produced; or have a program produce code instead of a programmer. humans, like a.i., are better suited for complex yet approximate thinking rather than exact reasoning. of course, ideally we'd just provide the computer with a spec, and the computer would check our design for logical consistency and would question us to resolve any ambiguity in our expression of our design, then it'd produce an optimized implementation of our design. but that's not yet possible. still, in the meantime, we should reduce the amount of code written by humans! it's better for code to be "unsafe" but flexible and readable, then have that code checked as appropriate at or before runtime.

this could be solved by using a macro. however, that's potentially inconvenient or impossible, and we can do better anyway. let's say that we're using picolisp, which has not macros, and does not compile; it's interpreted only. this is fine, but we want to be able to check the code for correct structure & sensible definition before running it, and we want that check to be provable. fortunately it's a lisp, which is easy to parse, so we can make a preprocessor that parses certain metadata sexps, uses them to check the program, then removes them so that the program can be executed. adding a preprocessor is much better (orthogonality, for starters) than introducing a language extension that supports this ever-evolving correctness-checking system.

even better is the program being written in terms of simple structures with strong/capable algebraic properties such as matrices.

''''

NOTE: _apply_ means _evaluate on some args_; _evaluate_ by itself is shorthand for _evaluate on no args_.

TODO: consider all functions being unary and accepting quasiquoted lists. you may suggest that we just use arguments like normal and use `apply` as necessary, but that assumes that the arguments are in a list as opposed to an a/list or more complex shape. compare to factor and link to any relevant articles.

==== good for description, too

EDN has used sexps (though that spec is too complex if you ask me.) the beauty of a/lists is that they encode everything, so you don't need to think about which format to use; you can always just use a/lists! easily parsed, as simple as possible, and same format as executable code. this avoids issues like e.g. nushell has, which uses a toml file for its static config, but also allows sourcing source code files to execute sateful programmatic operations, this:

. creates confusion for newcomers
. requires multiple files for the single idea of configuration
. makes one need to learn the toml format (though at least in this case toml is short)

compare this with nxyt's config, which is a lisp source file. lisp code is easy to read, extensible, and executable. sexp heads are descriptive. and as always, sexps are easier to refactor than any other general-purpose syntax. not only that, but it has macros, so that particular complex patterns can be expressed simply.

let's rag on the toml file, too. sexps are simple and don't try anything clever. they're simple & stupid. in this particular example, i'd like to focus on how their delimitation is obvious, whereas toml's sections are not (yes, despite the name "tom's _obvious_ minimal language.")

[source,toml]
----
[env]
EDITOR = "kak"
VISUAL = "kitty kak"
KAKOUNE_CONFIG_DIR = "$HOME/.config/kak/"

# [textview]
# bools: grid header line_numbers true_color
# theme : String

# TOP LEVEL OPTIONS
# disable_table_indexes = true
# path = [ ...]
prompt = "echo (pwd) ' ║ '" # command whose output is used for the prompt
table_mode = "rounded" # "light" "none"

startup = [ "source ~/.config/nu/aliases.nu"
          ]
----

the `source` command in `startup` seemed to have no effect. i didn't understand; what could be going wrong when it's so simple? of course, i did all the things that any decent hacker would do before asking about it on discord:

. re-read the manual
. search the discord
. check that the commands' equivalents work correctly when executed in the shell repl rather than specifying them in the config file

and i got to that point where i wonder, "...could it be...no, surely they wouldn't..." and then try it, and of course it is. as the link:https://toml.io/en/v1.0.0#table[toml documentation] says, sections continue entil the next section or end of file.

thus the solution was to move before any sections:

[source,toml]
----
# TOP LEVEL OPTIONS. PUT BEFORE ANY TABLES (SECTIONS).
# disable_table_indexes = true
# path = [ ...]
prompt = "echo (pwd) ' ║ '" # command whose output is used for the prompt
table_mode = "rounded" # "light" "none"

startup = [ "source ~/.config/nu/aliases.nu"
          ]

[env]
EDITOR = "kak"
VISUAL = "kitty kak"
KAKOUNE_CONFIG_DIR = "$HOME/.config/kak/"

# [textview]
# bools: grid header line_numbers true_color
# theme : String
----

and then i reflexively thought to myself yet again, as so commonly developers do, "...r u fucking serious with this shit." devs should understand why the ending punctuation is a period. gee, for the whole point of a config file to be static, stateless specification of options, order sure shouldn't matter, should it? and there's no mechanism to end a section? really?

i got no warnings, no errors. why? because unsupported options are allowed and ignored. if they were arguments passed to a function, it's far less likely that invalid options would be silently ignored. another reason to eval sexps as simultaneous data & code.

and if you're thinking, rtfm, then i'll say "ok, but you need to remove 'obvious' from the spec name. also why are you using a format that requires a manual when you could simply use one simple enough to not require one?"

.lisp

what i want to be understood about lisp is that it is not a "special" thing; it is not "advanced," nor "esoteric," or anything other than "simple." i wholeheartedly reject describing lisp as anything even remotely similar to "alien technology" (as it's surprisingly often called;) it's a lie and a grand dis-service to lisp; to the contrary, the very thing that makes lisp good is that it is nothing more than fundamental! homoiconicity is not some quirky, useless gimick! here's what homoiconicity is: "what if...we just wrote what the fuck is going on, instead of putting it in code?" whoh, what a concept! i mean, homoiconicity also allows (again, most simply so) self-modifying programs and/or programs that generate other programs. what about sexps? some gimmick? *no!* it's like, "we have nouns & verbs: data & functions. functions have an ordered list of arguments and a name. so that's expressed by the duple (name, args). well what's a list? it's recursion on relation. relation is expressed as a duple, called in lisp a _cons cell_. add recursion, and we get lisp lists. given that duple/relation (a,b) is expressed as `'(a . b)` in lisp (by definition,) and adding recursion we get lists which are then `(list a b c)` = `'(a . (b . (c . ())))`; therefore (name, args) = `(name . args)` = `(list name arg1 ... argN)`—an sexp. again, mere simplicity—again, commonly increasingly desired due to growing intolerance for needless complexity: a natural consequence of exposute to needless complexity, since humans (along with everything else in the natural world) are averse to _inefficiency_—a term meaning _needless complexity_.

lisp demonstrates a _lack_ of syntax, a lack of design patterns, lack of constraints. it appears to be used by programmers who can't be bothered to follow any linguistic particularities. it is the final refuge for those who've seen (in languages & tools) syntax after syntax, model after model, each specializing in their own featured features while handling poorly anything outside the intended use case. lisp is the language for programmers who just want to write programs as they want, completely free to do as they please by both being unconstrained and empowered by lisp's perfect flexibliity. after some point we just want to work with data and code—very much like C except more elegant, terser, simpler, and without syntax.

what's more, lisp has demonstrated that it's an excellent language! so stop trying to do extra shit! just use lisp! just use lists. keep computing as simple as it needs to be; there's no sacrifice in doing so; in fact, it's the nicest experience. in a discipline as complex as computer programming, we can use all the elegance (simplicity & regularity) that we can get!

let's look again at nushell. currently in their discord they're discussing which syntaxes to use. they want something shell-like for familiarity (mostly for users new to nushell who already know posix shells,) yet with more capability than posix shells. aaahhh, which syntax to use?! such a conundrum. they have the same issue for features; which features to include? should they allow enabling or disabling them in a config file?

you know what comes next: "of course, these aren't problems in lisp." we already know the answer to the syntax problem. what about features? the commonality of features & syntax is that they're both builtin—_special_, particular. want a feature in lisp? write a function. want to toggle whether that feature's enabled? either import the function or don't. what about toggling parameters of already defined functions? that's an actually good question. dynamic binds is an arguably good or poor solution. emacs lisp has dynamic binding by default. racket has _parameters_. other schemes have `fluid-let`. i don't know what common lisp's solution is, though they almost certainly have one.

i want lisp to be used for everything—to be the standard for describing data & so programs. lisp should not be called "lisp" though; if i say that "i want lisp to be standard" then it sounds no different from "i want <my favorite language> to be standard" but that's wrong; lisp is plain, not special. it's the natural notation for expressing data, as must be true considering that it's just primitive literals and or delimited sequences/sets thereof. in other words, lisp is to programming what set notation is for math, and it's no mistake that sets are a foundation of math. similarly, it's no mistake that plaintext files are used in *nix systems to configure everything. lisp is what plaintext is trying to be; in the abscence of lisp, we have many plaintext formats (ini,toml,json,yaml,xml,...) each of which is either inflexible enough to need extensions, or too stylized so that people can't agree on which style they want, or the syntax is regular and completely flexible yet too verbose (talkin' 'bout xml, here.) edn is just what xml should've been. if you don't know, edn is a particular format of sexp. now, for the record, edn is too specific; rather than being a mere sexp, it's a format specifically made for use in clojure, and so it includes keywords, nil, maps (which uses _commas_—the poster child for needless syntax) and at this point suffice it to say that it's too specific to be used for general computing. it remains, fine as any imperfect format is, for clojure.

json is practically equivalent to edn, but for js instead of clojure. considered as a general data notation, its imperfections are, again:

* language-specific
* needless use of delimiters
  ** json doesn't have symbols, so we need to use strings, which are delimited by single- or double-quotes to express what would be unquoted in sexps e.g. `{"k1":4,"k2":0}` vs `(k1 4 k2 0)`. note that some lispers would use use alists e.g. `((k1 . 4) (k2 . 0))`. this is hardly better than json, and no better han the plainer sexp. another arbitrarily-delimited form is `((k1 4) (k2 0))`
    *** readability is a reasonabe argument. you can obviously juse tabs and newlines to improve readability, but i can see how sometimes some people would want a sexp parser to ignore a character without syntactic value, used only for delimitation as seen by humans
  ** colons when none is needed (see prior bullet's example)

if you complain about the parenthesis, think again: they're necessary. as the above examples show, though, only few parentheses are needed. consider scheme's vs other lisps' `let` forms' binding clauses: `(let ([k1 4] [k2 0]) (print (+ k1 k2)) (exit 0))` vs `(let (k1 4 k2 0) (print (+ k1 k2)) (exit 0))`. the latter is shorter, and in fact is almost the shortest that this idea can be expressed in code in general, given that each the number of binds and the number of forms inside the let block's body are arbitrary.

[NOTE]
this optimization is possible only because the arity of each bind clause is fixed at 2 elements; in `(let A ... | B ...)` if each a in A were of arbitrary arity, then we'd need to do `(let (a ...) ... | B ...)`. recall that `(a (b c))` is isomorphic to `(a . (b c))` which is equal to `(a b c)`; i.e. each key or function paired with values or arguments is more plainly expressed as a list whose head is the key/function.

.can we beat lisp?

i said that it's _almost_ the shortest; it's not much of an optimization, but we can optimize `(a . (b ...) . c)` to `a b ... | c` where the pipe represents any character arbitrarily chosen to delimit: `let k1 4 k2 0 | (print | + k1 k2) | exit 0`. such a syntax may be proven to be unambiguous, but even then it forces upon the programmer the mental overhead to check that they're delimiting properly; by contrast, lisp's delimitation model is totally stupid. for all languages (e.g. both applicative and concatenative and/or stack-based,) delimiters are needed once a dataflow becomes significantly complex. each kind of language has its own unique form of expression complex enough to necessitate delimiters. for fun, let's further optimize by imposing a stack model similar to but a bit different from the factor language: `| k1 4 k2 0 set | k1 k2 + print 0 exit reset`.

. a delimiter, k1, 4, k2, and 0 are pushed to the stack. the delimiter is needed for `set` to know over which elements it's supposed to act (as opposed to acting on the whole stack which is generally unknown whenever `set` is called.)
. like `set`, `print` is variadic; we must tell it when to stop taking arguments from the stack.
. `exit` is unary, so it knows to accept only the head of the stack, `0`
. `reset` is nullary. it sets `k1` & `k2` to whatever values they'd had before being bound by the prior `set` statement.

NOTE: complex sexps directly relate to complex dataflows (i.e. nestings of function calls)

* `let` can be thought of as syntactic sugar for binding then returning binding to any previously held value. therefore i use `set` instead. there's generally no _need_ to `reset`, though obviously it's good practice so that we don't just build state throughout our program's execution without tracking it.
* `reset` could be defined to accept a list of symbols to reset, e.g. `| k2 reset`. if passed an empty list (`| reset`) then it'd reset all symbols bound at last `set`.
* unless our evalutation model is non-strict, our syntax must be able to represent both functions-as-data and substituting a function (with optional args) for its return value. remember that this can be simplified by saying that each function with args is a list.
* removing delimiters makes selecting less easy. for example, in the kakoune text editor the `m` and `<alt-p>)` command(s)/key(s) selects code within parenthesis, which makes refactoring quick. in some cases it's also is easier to work with programmatically, though technically slightly less efficient.

shortest possible vs sexp:

----
| k1 4 k2 0 set | k1 k2 + print 0 exit reset
(let (k1 4 k2 0) (print (+ k1 k2)) (exit 0))
----

...literally the same length, huh? interesting. honestly i didn't expect that; i thought the "shortest" version would be at least _a character_ shorter! ok, ok, to be _totally_ fair, they don't use the same symbols! `reset` is many characters long. with them having the same symbols:

----
| k1 4 k2 0 set | k1 k2 + print 0 exit R
(let (k1 4 k2 0) (print (+ k1 k2)) (exit 0))
----

4 characters shorter. unless you're in a fierce code golf competition, just use lisp!

if you _do_ (for whatever reason) still want the terser notation, know that this terse list notation might not generalize well. i suppose expressing `(a ((b c) . d) e ((f) . (g)))` by it would be less readable, but then again, are such complex forms necessary in general? given the semantics & syntax of this stack language, can they be elegantly expressed differently? for starters, it seems like we wouldn't need `null` to terminate lists. under this new lang, it seems equivalent to `a | (b c) d | e | (f) (g)`. if this data is applied to functions, then we might be able to rearrange the data/functions to make it work nicely. however, if the data is in a config file, or is otherwise not bound to one particular purpose, then this is not an option.

we should still use `(+ a b c d)` instead of `a + b + c + d`, since the latter obviously is more syntax, and so more annoying to refactor, is less symmetric, and, in case it's found to still be useful, does not support `apply`...but this suggests that we factor-out the pipe delimiter into `(| a (b c) d e (f) (g))`! but if were going that far, then the pipe delimiter at the beginning is redundant! so we remove it, arriving at a sexp again!

i conclude that this deserves more research, but that isn't pertinent; if we can beat lisp, it's likely that we can hardly do better. personally, i'm thankful for having done this exercise, but i estimate that further study of it won't be worth my time, or at least i'll consider it when i'm learning picolisp atnd factor. still, it'd be nice to have a proof of what the tersest general *useful* syntax is. again, we don't need to support complex syntax if an equivalent set of simpler syntaxes can be used.

*you can measure a syntax's elegance by the number of conditional statements needed by a parser of the syntax.* a syntax's usability for computers (parsing) does not conflict with usability for humans (reading, writing, refactoring.) elegance is a property of information theory; it's intrinsic to the syntax itself, unrelated to anything relative to / using the syntax. stop debating, start calculating. use facts, not opinions. do not delude yourself into thinking that lisp/sexps this is a question of style. it is factually & plainly optimal & symmetric—the exact definition of elegance.

and again, if you _do_ use particular patterns, and find sexps too verbose, then just write a macro.

=== how this differs from python, js, &c

as i said in the preface, i can't even with such langs. why not? they're untyped. so how did i go from poorly, statically typed java for 8 years, to strongly, statically typed haskell for 3 years, to typed racket for 2 years, absolutely hating using untyped languages all this time, to preferring picolisp within a month? ya know, picolisp: a language with dynamic bindings, that prefers stateful updates and not recursion? picolisp: a language that segfaults as easily as c, and gives no error message, no stack trace—just "Segmentation fault (core dumped)."

well, in jan 2022 i realized some great things, detailed in link:codenotes.adoc[codenotes]. basically, of a system, extreme hackability is an asset if the system is simple enough. i see simplicity in the form of a language using only one structure that has strong algebraic properties:

[options="header"]
|================================
| lang       | model   | alg prop
| factor     | stack   | monoidal
| apl or j   | tensors | many
| picolisp   | lists   | any
|================================

this strongly contrasts with oop, where each class is its own particular structure, usually entirely defined ad-hoc without _any_ algebraic properties; for example, these systems can't test whether any two arbitrary structures are isomorphic. to make matters worse, these classes are complected by inheritance. still, even without oop, such things as featuring all of lists, generators, and tuples is horrible; just use one type! of course, what makes these effectively different is that each has its own set of methods (or where they share generic methods, they may differ in how they implement these methods,) and often we need to convert among these types; it's not done implicitly for us. so what's the point of being untyped if we still have types and need to respect their differences?! ah, yes, here transpires that _untyped_ is a lie, and that _latently typed_ is the truth!

the solution is to have as few types as necessary. note that picolisp, c, and j do not have boolean types; mere numbers are used. in picolisp, "number" specifically means "integer;" picolisp does not support floating point numbers. even better. the above langs each have only one structure. contrast this with most languages, which have not only both vectors and [linked] lists, but a whole mess of other structures, inelegantly wired together through a jungle of abstract classes, inheritance, polymorphism, conversion and instantiation functions, available at varying levels of abstraction or implementation. this design is supposedly good: it allows us to express various levels of abstraction, thus achieving polymorphism and composability, keeping things ordered.

did you see that last part? _keeping things ordered._ that's the problem. it's _all_ defined ad-hoc. it's all arbitrary, specified manually. none of that structure is found by natural consequence of the mathematical properties of some primitive structure(s) that form the canonical basis for the space of classes. it needs to be managed, properties specified and enforced. not only that, but it produces a ridiculous glut of method names, many of which have overlapping behavior, but many of which are particular. what effort and complexity! by contrast, in e.g. j, we do not need to specify _behaviors_ of tensors; merely defining them is enough to implicitly benefit from all linear algebra operations, and automatically guarantee the axioms of a vector space, etc. the reason that such structures necessarily are enough to elegantly express all programs is that they're exactly the most basic structure properties: relation and recursion i.e. a catamorphism from `(a, a)` (where `a` is typeless) to a collection of relations of arbitrary size, which guarantees symmetry, and therefore elegance: beauty, or more practically, simplicity of expression and ease of maintainability.

so long story short: extreme hackability is excellent for the simplest languages modeled on single structures that by their mere definition exhibit strong algebraic properties. ad-hoc relation of structures is inherently doomed to be an unmanageable mess.

also btw, important note: structures are defined as sets that obey predicates or shapes; therefore structures' equality is equality of their obeyed axioms and number of degrees of freedom.

=== how simplicity benefits

==== no need for syntax [wip]

the best way to avoid syntaxes limitations is to use lists instead of syntax. for example, i defined `cond-let` to handle what `cond` could not. writing macros is dumb. `let` for scoped binds? how about an alist: that's `let-values` whose scope is the alist itself; `assoc` can't refer to something outside the list, just as an identifier cannot refer to a bind that's not in scope as determined by `let`. btw, remember that `let` is just syntax for `lambda`, so the same argument is made for lambdas, too.

granted, you obviously don't _need_ syntax, as evidenced by lisp having only a dozen or so builtin functions/forms. i mean to emphasize that new syntaxes should not be defined; instead just use lists, and iterate over them. use combinators and folds over lists, and use lambdas for the only occasionally-needed (as demonstrated by factor [lang]) binds. use whichever of stateful iteration / goto or recursion / callable continuations is optimizied by the runtime that you're using. if you're using an array language, then use multiple arrays each with non-array elements, if that's faster. like, you don't need _lists_, exactly; you just need anything isomorphic to lists, interned symbols, and lambdas.

what can or can't we do by a/lists?

* a/list elements cannot reference each other, except via a common bind in the same scope as the a/list. this is directly related to circular buffers being impossible to define using lisp style lists (though possible with linked lists in C).
* TODO: what do a/lists enable us beyond the basics of a turing-complete language: bind (add to current a/list of binds,) goto/funcall/eval, _?
  ** not a suitable alternative to binds in lexically scoped langs b/c each list's element has none of the other elements in scope. still, alists are a fine representation of binds, and can be passed around, and are naturally scoped (as connoted by the delimiting parens)
  ** a/lists describe all complex structures, including implemenations of the basic features like binds

==== no need to think

.code

whenever i wonder how to start implementing some idea, my mind can be blank. it's nice to know that i have few options, and they're all orthogonal; it makes identifying the right choice easy; i just need to look through them for the first suitable one, and i'll know that it's the only option because, by the orthogonality, the other options cannot satisfy the need satisfied by the found option. my options are:

* control flow: `cond`
* process input: loop (named `let` [scheme] or `do` or `for` [lisps without continuations])
  ** `car` for current element, `cdr` for the rest
* what data do i use? my only choices are lists/pairs, primitives, and lambdas.
* produce output: i can either compose functions via `lambda`, or i can compose relations via `cons`

and that covers all of the builtin lisp functions (except `quote`, `def`, and `setq`) that aren't macros i.e. syntactic sugar. who needs a standard library?

.data

what about data structures? lists. that's it. want to group things? put 'em in a list. any time that you need to identify what a thing is:

. dentify its attributes, throwing them into a list without regard to order
. after you think that you've identified all its attributes, factor-out commonalities. generally, reduce redundancies. examples:
  .. if coding a stock trader, you might start with an order as `(quantity type)` where `type` is `'short`, `'buy`, or `'exit` and `quantity` is a positive float. this reduces to `(quantity)` where a negative quantity means `short`, positive means `buy`, and 0 means `exit`
  .. `()` factors into `()`
. identify relations/constraints among attributes; these will suggest ordering & grouping (consing) attributes so that traversals over the lists are natural. examples:
  .. a circle can be described by `(x y r)`, but `(r x y)` allows us to `car` to get `r` and `cdr` returns `(x y)`, which we can pass as a point to functions that take points, rather than needing to extract `x` & `y` individually then combine them.
    ... values that may be used multiple times can be defined then put in multiple positions, e.g. `(let (x (make-big-struct)) `(,x 0 1 (2 . ,x) 3))` which practically adds nothing to computation since we're merely putting a pointer to `x` in the list.

feel free to work with lists as organically as you please; lists impose no constraints. you can group as many things in as many ways as you want, e.g. pass ``(,f ,g)` somewhere where they're both used, and pass `'(,f ,g ,h)` somewhere where those three are used. no need to worry about types like `(struct _mandatory-name ([f : f]) ([g : g]) ([h : (Option h)]))`. it's amazing that there was a time when i wasn't vehemently opposed to such things.

using lists instead of structs is like using lambdas instead of needing to define functions; lists are the anonymous complex (cf primitive) data type.

==== oopy/groupy

.preface: oop is a lie

oop is said to be made of _composition/encapsulation_, _polymorphism_, and _inheritance_. these are such simple, common features that they can't be said to form a design/paradigm.

i can get ad-hoc polymorphism just by defining a variable then shadowing it in various subscopes. parametric polymorphism (which isn't even available in common oop languages) can be implemented by...(are you ready?)...a function that takes a parameter, where the parameter affects the function's output but not its control flow (it'd be ad-hoc poly if the parameter affected the function's control flow.)

composition or encapsulation is just putting things into data or logical structures whose data are scoped only to that structure.

inheritance is just composition with overridability rules (`public` or `private` to enable or disable overridability) for certain variables. also inheritance is a variety of ad-hoc polymorphism: multiple subclasses having various functions all referred to by the same name is ad-hoc polymorphism. 

oop refers to throwing around 

here's a typical oop example:

[source,java]
----
interface Eats() { abstract void eat(); }
class Animal {
  public int age;
  public Animal(age) {
    this.age = age;
  }
}

class Person extends Animal, implements Eats {
  public void eat() { System.out.println("eatin' like a human"); }
  public void walk() { System.out.println("walk like a human"); }
}

class Egyptian extends Person {
  public void walk() { System.out.println("all the cops in the donut shops say,..."); }
}

class Dolphin extends Animal, implements Eats {
  public void jumpThroughHoop(Hoop h) { System.out.println("jumpin' through hoop " + h.serialNumber); }
  public void eat() { System.out.println("how and what do dolphins eat"); }
}

Person tom = new Person(10);
Dolphin carrie = new Dolphin(4);
tom.age + carrie.age; //14
----

it features (ad-hoc) polymorphism via an interface, inheritance by subclasses, and composition by one class containing another. like typing, inflexibility is part of the design, to prevent programmers from accidentally writing & running inappropriate code. as with typing, i reject any restrictions, instead favoring simplicity as a way to avoid such mistakes. as with many restriction systems, it's unable to handle common things well (or at all), such as multiple inheritance.

plain version, which doesn't use particular syntax to identify oopy stuff:

[source,scm]
----
;; in a good lisp like picolisp, if `member` were to return `NIL`,
;; then `get` would return `NIL`, rather than the program crahsing,
;; so this would be a perfectly sensible definition.
(define (get p s) (cadr (member s p)))

(define (animal age) `(age ,age))
(define (human age) (list* 'walk (λ () (println "walk like a human"))
                           'eat (λ () (println "eatin' like a human")) (animal age)))
(define (egyptian age) (list* 'walk (λ () (println "all the cops in the donut shops say,...")) (human age)))
(define (dolphin age) (list* eat (λ () (println "how and what do dolphins eat")) (animal age)))
(define tom (human 10))
(define carrie (dolphin 4))
(+ (get 'age tom) (get 'age carrie))
(get 'walk (egyptian))
----

i could've used closures instead of alists. that discussion is in the following section, _main discussion_. also, to implement inheritance, we could use bind shadowing, or, as was done by `egyptian`, augment structures that affect lookup/resolution, such as consing onto a list, which makes `get` return at the earlier egyptian-specific function rather than the later human-specific one. this method, keeping all implemenations of given functions, would allow you do define another version of `get` that allows you to type cast, e.g. `(get 'walk (cast (egyptian 10) 'human))` to walk like a human, though this would require either building a class hierarchy when subclassing, or storing class info in the object lists.

i throw away `public` & `private`, types, the `Eats` interface (since i can just check whether `assoc` returns falsy or a function associated with `eat`). as usual the tradeoff is simplicity & flexibility for lack of safety by constraint enforcement, though again that can be accomplished by contracts or program processors or macros.

depending on your dataflow, some things may seem oopy or not. don't presume whether it is or not, though; start with the necessary functionality, then identify which data you need, then identify dataflow; let _each program's facts_ lead your design.

.main discussion

_grouping_ is the constraint or suggestion that some things should be used together, that they should not be mixed with other lists. as i explore in §_no refactoring_ below, alists can encode type classes; but more simply, alists whose cdrs are functions makes a good & simple way to bundle functionality together into a sort of on-the-fly class. this with closures makes simple oop style classes. arguably we can improve this by defining a meta-function that's `let` except accepts an identifier whose value is an alist, rather than an alist literal. in languages (like racket) where this isn't possible, we have a decent alternative: returning a function:

[source,scm]
----
;; convert an alist of functions into a function that selects & applies therefrom
(define (alist->fn m) (λ (f . args) (apply (cdr (assoc f m)) args)))
(define fns (alist->fn `((f . ,(λ (a) (+ 4 a)))
                      (g . ,(λ (a b) (/ (+ a b) 2))))))
`(,(fns 'f 1) ,(fns 'g 4 6)) ; (5 5)
----

you may recognize that this is prototype-style oop: functions that return maps from symbols to functions or data. this is what javascript used before it was given builtin oop classes in ECMAScript 2015. in such old js this would've been:

[source,js]
----
fns = { "f" : function(a)   {return 4 + a;}
      , "g" : function(a,b) {return (a + b) / 2;}
      };
[fns.f(4), fns.g(4)] //[8, 16]
----

we cannot do `(define (fns f . args) (case f [(f) (apply + args)] ...))` because, in languages with strict/eager evaluation, that would evaluate all cases each time you call any one function, which, aside from being wasteful, could be harmful if any of the functions were impure.

this oop has a significant limitation under lexical scoping: each of the a/lists's values' definitions have a common scope, but that scope does not include other of the a/list's elements! thus `f` cannot reference `g` nor vice versa. this is not a practical concern of lisps (see below for workaround) but rather highlights *a noteworthy limitation of the functional singly-linked list construct: they cannot express cyclic graphs, thus cannot support loops, and are thus insufficient for encoding general programs.*

again, in languages with dynamic binding/scoping this isn't a problem. oopy langs solve this by having the `this` keyword or other builtin oop primitives. in lisp we can simply define functions in terms of each other inside a closure that returns them in a map:

[source,scm]
----
(define fns
  ;; co-recursive f & g. terminating dummy definitions.
  (letrec ([f (λ (a) (if (> a 0) a (g a 16)))]
           [g (λ (a b) (- (/ (+ a b) 2) (f b)))])
    (alist->fn `((f . ,f) (g . ,g)))))
`(,(fns 'f 1) ,(fns 'g 4 6)) ; (1 -1)
----

==== designing programs: no need to conceptualize

typical design sees people enumerating various mechanisms, then putting them in a dataflow graph. for example, when designing a vacuum we suppose that there's an inlet, a debris storage chamber, some latch to open said chamber, etc. this object-oriented design is more sensible for physical design, but not so much for programming or math, where we work with _data_. data can be easily transmuted and perform multiple roles. while this is true of physical devices, too, and exploiting particularities can beget some clever designs, it's much easier to exploit such things in code, to arrive at minimal, optimal, clever designs.

NOTE: if a design is so clever as to need explanation, then explanation should be given! please do not be that slick fool who designs genius yet obtuse & undocumented code! for programmers, obscurity bears neither nobility nor glory. still, clever techniques are clever, and so good, and so should be adopted, even if they seem initially obtuse to newcomers.

to identify such clever designs, forget any initial ideas of what puzzle pieces you suppose must be fit together; instead, think in terms of primitive data and relations, since that's exactly what everything must be.

things are defined by parts of speech:

* nouns (items in scope): attributes
* verbs (lambdas or mutations)
  ** transitive actions: things that they interact with  and how they interact with them
  ** intransitive actions: a thing modifying its own form/states
* constraints (predicates defined of one or more things)

for example, say i'm designing a financial trading order system. certainly we must know what a stock order system is: what does it do, upon what does it act? what's common vs necessary?

let's start with the obvious: we need orders. what's an order? well, it's not an action. it's shape/type is given in the above example. notice that these are all primitives or relations thereof; there are no "opaque" types here—types not known in terms of primitives. such types are vague nonsense; they're assumptions about what should exist, but without having identified why they should exist. in other words, it's usually foolish to make a placeholder for a type with the expectation that its definition will be identified later; if you haven't identified its definition, then you don't know that it needs to exist, so there's no reason to suppose that it will. in fact, supposing that it should exist will only bias your design based on your arbitrary assumptions rather than observed truth.

anyway, with knowledge of what orders are, what's an order system? it's defined (insofar as i'm concerned, anyway) as "thing that enables us to change the amount of money invested in given financial instruments." ok, so it's obviously a relation between between investable funds and investment per instrument. the relation is not a fact, but instead about state change (of both related objects); it's is therefore a transitive verb and not a predicate.

an order is an instruction of how to change our investments. investments are called _positions_, btw, and are easily represented by an alist from stock symbol to money. orders may be functions of candles (or "data" if you're unfamiliar with trading). to be compatible with broker order systems, handling candles will be part of the order system rather than each order.

so the order system is `candles, orders -> positions` which is not strictly a function, but instead a list of inputs and associated outputs, where the only constraint on the inputs is that they're in scope, and the outputs are any representation of updated program state.

now, technically not all orders immediately beget new positions; some orders are _open_ until they're _filled_ by some condition being met. so the mutation is really `candles, orders -> open, positions`. now, as it turns-out, for historical computation, we need to keep track of all filled orders rather than combining them all into a single positions object; therefore we'll change the mutation to `candles, orders -> open, filled`, and `position` will be a unary function of `filled`—a view or aggregation, if you prefer those terms.

the actual order system is a bit more complicated:

----
instrument selector -> ((instrument (candle)))
funds := float {- positive -}
pseudo-order := (float {- on (0,1] -} instrument type . deps) {- where type is 'moc, 'mkt, or a list of conditions -}
filled := (open-time open-price)
positions := ((instrument money))
filled -> positions
apportioner := funds, positions, portion determiners -> ((instrument float))
apportioner, (strategy(positions, funds, (candle))) -> pseudo-orders, portion determiners {- e.g. expected profit -}
loop (pesudo-orders, portion determiners -> pseudo-orders) over (instrument) -> pseudo-orders
pseudo-orders, (candle) -> open, filled, funds
----

notation:

i use the usual lisp notation (e.g. `(a b . xs)` has usual meaning) except that (a) (singleton in parens) means homogenous list of a; actual singleton lists are no more useful than their single element alone, unless they're used to distinguish a thing from a thing with a particular condition. thus `(a)` is `(a . ())` i.e. `a` but with a flag. the cdr may as well be anything then; in fact, a boolean (2-⨿), or generally symbol (n-⨿), would be more informative. regardless, this notation represents singleton lists as `(a . ())`. for further example, a homogenous list of singleton lists is `((a . ()))`.

aside from readability, the the statements' order is meaningless. in the last statement, `filled` being on the RHS implies that there's been a change to `positions`, too, as implied by the earlier-mentioned statement `filled -> positions`.

design/technique:

the most general loop form is `loop <arrow> until <cond>`. non-terminating loops are denoted by `loop <arrow>`. if a loop is over a variable (a list or datum used in `<cond>`), then that variable must be present in the loop arrow's LHS. we do not need to specify loops over lists if it's implied. for example, if i were to have an arrow `candle -> potato`, and `candle` appears only as `(candle)`, and only on the LHS of any arrows, then the program must contain looping [mapping] over `(candle)`. looping would not be implied if there were also an arrow `(candle) -> candle`.

rather than assuming that there are objects with attributes, we do the opposite: we know that there are data, which we identify then group [relate] for simplification. there's no concept of "X has a Y;" instead, it's just "there're related things; how can we simplify description of their relations?"

notice that `((instrument candles))` can be joined [sql] with `positions` on `instrument`. an in-memory relational db e.g. sqlite would be a nice alternative to alists or map structures defined for whatever particular proglang you're using

notice that arrows are not usually named; we do not care about how in code the states are transmuted. the reason that `apportioner` is a named arrow is because that's a feature that i wanted, not because the data suggested that it was a necessary implementation detail. we presume _nothing_ about which functions, subroutines, language features, or other programmatic devices shall be used to implement the arrow diagram. we name data (or groups thereof) but not arrows. we also do not care about the data's purity nor whether they're nullary functions that return values or values stared in memory directly. likewise, there's no consideration of whether a strategy "contains" or "produces" determiners, i.e. whether a morphism is of a coproduct or not; insofar as i'm concerned, they're equal.

step-by-step:

. `instrument selector` is an abstract variable; it's defined as whatever produces a list of lists of shape `(instrument (candle))`. any single expression wrapped in parens means a list of that expression.
  .. the candle shape is `(float float float float float)`, which is common knowledge among traders.
. there's a datum called `funds` of primitive type `float` with condition that it be positive
. `:=` has higher precedence than `->`; `apportioner` refers to the arrow itself
. `strategy` is a function of `positions`, `funds`, and `(candle)`
. strategy is parameterized by / is a function of candles, filled orders, and candles, and each strategy produces a list of pseudo-orders. thus each strategy is associated with both a list of pseudo-orders and a financial instrument
. loop over `(instrument)`. within each iteration of the loop, an instrument is in scope; `instrument` is implicitly in the LHS of the loop arrow given its relation to `pseudo-order`. loops collect effects / outputs; in arrow diagrams, a loop that isn't on the LHS of an `-> <RHS>` means that the loop does not, by its end, have a net change on the program's state.
. 

given that i've concrete definitions for each item in the schema, i can replace names by their shapes, and i'm left with a program. i can then algebraically simplify for efficiency's sake. no wondering which builtin constructs to use for whatever programming language i'm using, nor which design patterns to use. just scope, state, relation, and grouping. this schema directly translates to any turing complete languages, which must support loops/goto and functions or mutation.

unfortunately most langs don't support alists. fine; using sequences/generators and hash tables or tree maps is still simple, if not as elegant. hopefully the language you're using doesn't offer an abundance of features that others use, so that when you use others' libraries you're constrained by those features' rules (like if someone defines a structure as a GADT—an extremely limiting variety of asymmetric constraint that should be used only by extremely wise designers, and then probably only for describing grammars.)


an alternative notation would be based on implicit specification of control flow by defining predicates, à la prolog. e.g. i may define a state `s0` by the condition `a < 10`, rather than defining `s0` by relation to other states; upon `s0` `b <- a + b; a <- a + 1`. there's no explicit control flow statement saying to return to the beginning of any loop; it's implied by the `s0` rule and `a`'s value.

.summary of design method

* describe entire or any fragment of program by state change / input/output arrows: `state0 -> state1`. refers to mutation and/or functions ambiguously. this notation generalizes function notation `A × B × ... -> C` to functions or mutations, and: instead of sets we use particular objects; instead of × we use comma; and multiple "codomains" are written without being enclosed in a tuple.
  ** `a -> a` means that `a` changes. things on RHS are what changed
  ** `a -> b` means that `a` produces b, regardless of whether `a` is deallocated
  ** all objects not present in the expression are assumed to have not changed
  ** if the arrow needs to be done for each of many data, then `map` it. if the data that you're mapping over are related, then use a loop mechanism—fold, named let or other recursion—or `while` (the most general iterator; called `do` in scheme.) all looping is just systematic / symmetric state change.
* identify things as (in)transitive verbs, nouns, or conditions
* don't suppose data; starting from your necessary thing(s), trace your design's implied inputs or outputs
* don't suppose functions; functions are a way of partitioning/grouping code, but we can't well predict in advance which groupings are best. just write code, then group/simplify/factor it as you write it, but not too soon, since, at any time, you may have yet to code code that uses forms outside the factorizations that you currently observe
* express all data in terms of primitives or relations; no opaque placeholders, since those are vague nonsense, so naturally you'll find them difficult to consider
  ** groups [grouping, oop-like patterns] naturally arise as you factor/simplify shapes as described in state change arrows. you may often find that multiple roles are satisfied by a single group, or that some roles are redundant.

===== analagous to ADTs [wip]

as we've already considered that lists are just recursive relation, it may seem a paradox that `List` is a product type. well, actually it's a general ADT `List a = CAR a | CDR (List a)`. all recursive ADTs in a strict-eval lang must entail coproducts to code base case(s). non-strict eval langs like haskell support unbounded ADTs like link:https://hackage.haskell.org/package/hinze-streams-1.0/docs/Data-Stream-Hinze-Stream.html[corecursive/coinductive types].

* coproducts are like `cond` (ad-hoc, mutually-exclusive). in fp we use `case` to branch on these. like in lisp's `case`, `case` is a specific version of `cond`.
* products...correspond to `assoc`? naw; assoc corresponds to selecting one of many ADT constructors....

==== no refactoring

===== type classes [wip]

type classes have an inherent flaw: people use them. this means that code depends on them. thus to change the type class, dependent code needs to be refactored. what if someone uses it in a way that you don't like? then you can use `newtype` [haskell], which isn't terrible, though it seems like a retrospective hack. and there will _always_ be another type class. perfect example: first haskell had `Monad`. then they added `Functor`, then `Applicative`, and then `Selective` (which is between a monad and applicative.) lists are naturally continuous.

instead, lists are a necessity; they'll always be used, and each occasion wherein a list is used, it must be of a particular shape. the shape restriction is relative only to where it's needed. this is perfect, natural modularity.

type classes are obviously encoded via lists: they're just alists from symbol (or other datum that supports a predicate) to alist of type class implemenations, e.g. some `Monad` instances:

[source,scm]
----
#| alist of abstract definitions (type class methods,) called "G" for "generic."

   there's no need to have separate type classes: no two type classes
   can have methods of the same name anyway, so the map from method to
   type class is unambiguous. to resolve the map from method to instance,
   we use predicates instead of nominal types. (if you want nominal lookup,
   you can tag data with symbols; then the predicate is just
   matching the implemenation name with the tag.

   G is an alist from predicate to an alist of method implemenations.
   to lookup implementations, we use a variant of assoc that generalizes eq? to
   predicates. predicate overlap is not a concern if you assume that, like haskell,
   no types overlap. if we choose to support type specificity, we can match against
   the most specific matching type, or raise an error if no predicate matched.
   a strict definition of specificity would use a set of predicates rather than a
   single predicate; then specificity is the size of that set and lookup would be
   in a max heap on specificity. 

   however, to keep this example simple, we'll just cons onto G, and lookup will
   match the first matched predicate. this is a heuristic for specificity: it
   assumes that more general types (and their implemenations) will be defined
   before more specific types (since more-specific types are usually defined in
   terms of their generalizations.)

   G in initialized to default definitions—here just return = pure. (const #t) is
   analagous to "any type."
|#
(define G `((,(const #t) (return . pure))))

(define-syntax-rule (instance x) (set! G (cons x G)))

;; pair implemenation
;; if mempty isn't found in G then that's effectively the same as trying to
;; instance a non-Monoid, still giving an error at lookup time.
(instance `(,pair? (fmap . ,(λ (f p) `(,(car p) . ,(f (cdr p)))))
                   (pure . ,(λ (x) `(,(tc mempty x) . ,x)))))

;; list implemenation. note that list is a subtype of pair, so we instance
;; it after instancing pair.
(instance '(,list? (>>=  . ,(λ (xs k) (apply append (map k xs))))
                   (pure . ,(λ (x) `(,x)))))

;; in lisp everything's implicitly maybe; lists are used as an n-ary generalization of maybe,
;; just like list->maybe & maybe->list are used in haskell.
;; in scheme everything can be #f or anything else—again, effectively maybe.
;; and again we see (const #t) being "any/every type."
(instance `(,(const #t) (>>=  . ,(λ (m k) (and m (k m))))
                        (pure . ,identity)))

;; TODO: define when i've the time.
(define-syntax (tc stx)
  (syntax-parse ()
    ((_ f e) #'(assoc c f))))

(tc >>= '(1 2 3) range)
----

regarding `list?` & `pair?`, i know that you probably want to make `instance` append onto instances already given rather than just consing onto `G`. noted, though like making G an alist instead of a heap, i'm keeping this example simple. and yes, `(eq? list? list?)` is `#t`, so we would be able to lookup by predicate then merge associated instances.

as `pair?`'s instance demonstrates, the use of type class functions in method definitions implicitly defines type class hierarchy & constraints.

`tc` is a simple implemenation. a more-advanced macro would not require one to specify tc; type class methods would be defined as macros.

also, the way that `tc` expands, lookup in `G` is done at runtime rather than before runtime. this is a design choice to make this example simpler; i'm using racket scheme, which uses different namespaces for macros vs ordinary code, so ideally i'd define `G` in the macro namespace; this would support type class lookup before runtime#, thus supporting "typecheck-time" errors. however, that would complicate this example, and is a consequence of racket, not lisp in general.

===== scope

consider, in a lexically scoped language, two modules that need to access a common data type. if defining the type via `struct` then we'd need to define a new module just to store the type (or, if we're lucky, then it'd be appropriate for one of the modules to require tho other) so that both modules can have access to the same type. however, because shapes are merely non-programmatically-specified constraints of lists, each of the two modules can simply use alists.

=== runtime efficiency [wip]

i promote a/lists as a universal structure for _describing_ things, among which are programs. how & when does this differ from literally using linked lists? an implementation would prefer SIMD (for supported architectures) or else continuous, static memory (arrays/vectors) if they allocate faster than linked lists, else uncontiguous, dynamic memory (linked lists, trees, skip lists, &c.)
