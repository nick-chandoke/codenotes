.practical wisdom

all of coding/programming is just 3 things, and they aren't even complicated: grouping data, arithmetic, and sequencing io. in other words, one of the simplest example programs is one that partitions words by whether their first letter is lowercase or uppercase, then printing all the lowercase ones as uppercase before all the uppercase ones as lowercase. case conversion is either a bitwise operation (for ascii), addition/subtraction (possibly for other codepoints), or transposition via a lookup table. a lookup is nothing more than a sequence of sequences—more specifically, a sequence of arbitrary length each of whose elements is an ordered pair. we see it just as well in turing's famous abstract machine: data related to each other by sequence position or selected by programmatic rules, then reads or writes performed in a particular order. the arithmetic is in deciding which value to set at a given index. most people consider `if` to be non-arithmetic, but it can just as well be described arithmetically. by the way, i think that sql programs are nice analogues to turing machines; they're the same idea, but obviously practical rather than underwhelming. to be fair, turing's machine is useful for theory, not use, but its simplicity is beautiful, and i'm sad to see students learn about a thing so dissimilar to programming languages that the student can hardly relate them, and mostly forgets turing's machine as some abstract, impractical, theoretical nonsense that their professors may care about, but what they do not, thus beginning their descent into the nonsense madness that is arbitrary language features, designs, and idioms, quickly losing sight of the actual _computation_: the storage and manipulation of data, all stored in one simple, given, universal, efficient structure.

NOTE: one may suggest that reading & writing are separate, important operations that i didn't mention as fundamental. however, they are exactly equivalent to arithmetic: firstly, all values are representable as numbers; secondly, one must load this number, and the number is only useful if we first do something with it—i.e. produce some transformed value from it—then do something with that result: either piping to an io resource or storing the data (setting it somewhere). more simply: "reading & writing" is a mere rephrasing of "input & output".

.universally applicable, philosophical wisdom

_computing_ is just interaction. what are commonly known as "computers" are no more particularly computers compared to any other interaction. they are notable, however, in that they're _programmable_ i.e. that manipulating them to do arbitrary desired tasks is easy.

all meaning is relation. therefore all computation is relation. all relation is symmetric or asymmetric. computation over asymmetric relations entails branching i.e. a computation per branch, and each branch is an asymmetry. asymmetries are partitions of symmetries i.e. some thing is by default symmetric, but when it's subsetted so that each subset can be related to a different computation/consideration, then it's been "broken into asymmetries." the once-symmetric structure is now _considered_ asymmetrically i.e. we've decided to *interpret* it so that its *meaning* is given by asymmetries. remember that data are matter, and computations are form i.e. relation i.e. meaning. we may call it "data", "code", or "information", but it's all the same thing. it's the "stuff", the matter, and matter itself can form other matter, and so of course even rectangles are parameterized by width & height, and can be generalized to other forms such as closed figures, which may have any number of sides or constraints, and can be generalized to curves simply by introducing more, special point parameters, such as bezier curves, and thus we have mutation of structure itself just as well as the mutation of the structure's constituent data.

value is value. values can be distinct (symbols) i.e. support equivalence, and i haven't explored that too much yet, but additional structure—order—is what gives us "numbers" and by "numbers" i mean "any totally-ordered value" e.g. naturals, reals, or sequences thereof, such as strings, which are just sequences of codepoints or otherwise of values supporting an alphabetical ordering. complex numbers may be totally ordered, but we decide for them not to be; only each of their components is ordered. these particular rules (algebra) that we enforce of complex numbers (i.e. that which makes an ordered pair a complex number) is fine, but note that it is a limation of computation. constraint can afford us efficiency, guarantees/provability, and hone our focus, but it constrains. always be careful the degree to which you constrain i.e. resist mutation. it can be useful, but it can limit you from discovery.

."low-level"?

what "levels"? there're just the 3 ops, and we can specify how the computer does them e.g. does it store as float vs int, and does it use the registers vs floating point stack to perform arithmetic. so the machine doesn't get in the way of the grouping nor arithmetic, and enables us reading & writing at any physical addresses e.g. sockets, file paths or handles, locations within a file, etc. we decide how to physically represent information in hardware such that the hardware can efficiently perform grouping & arithmetic! how could we not always consider the hardware? well...

* "levels" may refer to abstraction layers i.e. multiple dispatch i.e. selection—`case`, `cond`, or ad-hoc polymorphism—just a map from value to computation.
* somehow people tend to design systems of constraint upon each other, which they call "levels" and what i call "a mess"—exemplified by link:https://www.youtube.com/watch?v=aSEQfqNYNAc[hickey's `HttpServletRequest` rant].
