== sql

"a wrt b" is sql `select a from x join y on x.a like y.b`. cartesian product is like join w/union; left join is like join w/intersection. "a wrt b" means "the subset of a that's related to b, and return a & b."

[TODO]
* what're windows? cf local binds?
* how to organize/encode data: relations are basically their headers. thus make a table for each shape. whatever data have a common shape belong in the same table.
* cf j/k db
* partition this module into relalg, working with data, common exprs (e.g. exists, case), useful compound patterns, and techniques for deriving solutions to given problems
* does sql have any short-circuiting mechanisms? `case` has certain evaluation order but is not short-circuiting; if one branch is taken, there's no guarantee that the other branches' predicates aren't evaluated.

sql is an array-based proglang whose sole data structure—the table—generalizes maps and arrays to matrices any subset of whose columns can be indexed. like APL, a table's cell's value can itself be a table. 1D arrays in sql are represented by single columns. objects [structs] are represented by single rows, and arrays of objects are represented by tables. features:

* slicing
* concatenation
* sorting
* partitioning
* folds
  ** map
  ** filter
* generators
* set-theoretic operations—a very unique feature for arrays!
* extremely efficient; much easier to reason about performance than using linked lists, hash maps, arrays, etc
* link:https://www.sqlite.org/json1.html[native JSON support] (introduced in the SQL 2016 standard)
* null propogation (e.g. `select 45/null` returns `(null)`)
* declarative
  ** query-based
  ** reactive programming via triggers
* clear separation of ad-hoc and symmetric relations: symmetric across rows (with implicit map over rows, and explicit map over tables by `group by` and aggregates) and ad-hoc relation of attributes of each given row.

but lacks metaprogrammability! for queries to not be manipulable nor generated then executed is a big lack! that'd be like if factor didn't support composing quoted programs then ``call``ing them!

NOTE: sql is case-insensitive!

* though tables' attribute sets are mostly fixed (though they can be updated by `alter table`), creating new tables on the fly is easy: just use `select` to get a subset of attributes or `join` to adjoin!
* it's appropriate that sql forces everything to be in tables; that's symmetric treatment of singletons & collections. however, a table that implicitly stores global variables like lua's `_G` would be nice.

=== (no) lambdas

TODO: revise this section remembering that views are effectively functions, except on a fixed rather than variable table. also discuss fns as parameterized data.

firstly let's identify what a _lambda_ is. a λ is a relation of inputs and outputs. picolisp's λ syntax makes this obvious: `((arg ...) (body ...))`. it's not even special syntax, really; the pil interpreter knows to process an sexpr as a λ if its first element is a list. the best λ syntax (without regard to function composition viz apl trains &c) is clojurescript's shorthand: e.g. `(%1 + %2)/%3`, which generalizes apl's dfns syntax to multiple parameters; these achieve brevity by identifying arguments by ordinals instead of names. a _function_ is exactly a λ, though commonly in programming (namely in languages not supporting λ's), it refers to a special linguistic construct that is _effectively_ just a λ bound to an identifier. as such, in such languages, _functions_ [programming] are just poor implementations of functions [math] i.e. λ's, where the poorness is by senseless asymmetry of how data are treated viz treating functions differently from other data.

fns' advantage is their variability: the locations of both their args and output vary with each invocation, and may even be unnamed (in the cases of an input being a literal or the output of another fn.) thus fns achieve composability. purely tacit langs like joy are purely function based, and thus every arbitrary substring of these programs is a valid function (cf concatenative langs, generally for which any substring of a program is a valid program, but not necessarily a function.) fns can be also seen as a scoping mechanism: `f(a,b)` is seen as variables `a` & `b`, whose meaning is relative to each invocation of `f`. this can be encoded in sql as a table `f(a,b,e)` where each invocation of `f` is a row, and `e` is the location where the output should go. `a`, `b`, and `e` may each be literal values or addresses. this has the side effect of making computation lazy by implicit memoization (namely when `e` is not an address, and it's just stored right there in the table.)

usually λ is an asymmetric linguistic primitive; once a λ is created, it cannot be modified, and no operations exist on it aside from β-reduction by application of provided inputs (which is always at invocation time in strict-eval langs.) aside: apl is one of few languages that actually supports truly first-class functions [verbs] which support modification [by adverbs]. this is needless restriction. we _just_ said that λ's are merely relations of inputs & outputs. so why not express them as such?

* picolisp does this; once a λ is defined, it does not become a black box; it remains exactly the list of 2 lists that it ever was. see _fexprs_ in pil docs.
* some functional langs use currying as an isomorphism of a list: (a -> b -> c) -> (a,b) -> c, which corresponds to (a b c) = (a . (b . (c . ())))

we've now restructured λ's as some `eval`-type fn on a relation [of data]. so let's talk about `eval`.

`eval` takes syntax as an argument, whether that syntax be encoded as an sexpr (viz lisp) or a string (e.g. bash). instead, let's generalize `eval` from a single function that evaluates syntax to a set of functions that each evaluates some specific form of data. oh, wait, look at that: those are just ordinary functions! so `eval` should really not be considered unique at all. yet `eval` isn't user-definable! why?

`eval` being not user-definable expresses a serious flaw in the design of languages: asymmetry in the form of functions being treated differently from data! yes, even functional languages, in which λ's are "first-class", are really no better than a non-functional language like C! they're only more _convenient_. if λ's were really data, then we'd be able to modify them—ya know, like _any other data_. instead, functions are *not* first-class; instead the language merely supports a _still-special syntax_ for _inline functions_. to summarize, _first-class_ is used to mean:

. declared/declarable inline by some special syntax
. can be bound to multiple identifiers
  .. passable to other functions (thus binding the λ to one of that function's parameters)

thus enabling one to express the full λ-calculus, which basically means function _composition_. it does not entail function _modification_!

now try to imagine _first-class_ data: you can declare the data, and you can pass them to functions—no mutation—that's purely-functional programming. so how is it that even non-pure functional programming retains the same non-mutation limitation, but only for functions instead of other data?

this restriction is not necessary. we can have functions and mutation. again, functions are only relations; let's express them as such, and not in some special linguistic, asymmetric way. the only way to do that is for literally _everything_ to be mere relations. in the usual dictonomy of "data and code", this means all data and no code.

i know: "no code?! that implies no evaluation, and thus no program!" so there must be _some_ code. the question is: what? anything will do, so let's choose the least-constrained solution. thus the solution is β-reduction/evaluation of a relation—basically, effectively a lambda, only now we aren't thinking of _lambdas_ as their own concept, really; now we see just the evaluation of particularly-structured relations. now the necessary linguistic construct is not the `lambda` builtin, but instead `eval`, which now means "evaluate this relation" instead of "evaluate this syntax."

we have now achieved a language model where all data except `eval`, which is the only code, and is _not_ a datum, since that'd be redundant.

so how does this relate to sql? sql is made for relations. a fold can be expressed by a fn called `fold` that parses a [probably single-element] table of header `(init,s,op)` where `init` is the initial value, `s` is a relation (probably `select * from t` for some `t`) and `op` is a relation of inputs and outputs, expressed by `select body(arg,...) from t` where `t` contains all the args, where each column name practically is a locally-bound identifier of a function parameter. the only problem is that sql does not support variable `t`! `t` must be an absolute identifier! this is sad. however, at least it identifies the exact fault that makes sql non-composable—the single lack of variability that it'd need to be as capable as any language could be. such functionality can be easily implemented in a small language that merely converts a simple syntax into sql statements. i suggest making a dsl in factor [lang]. the lang would generate nonce identifiers to satisfy sql's requirement that every of its lingustic elements must be named.

.sql metaprogramming example

TODO: given that we can define fns as views, if we permit paramerizing the query, then metaprogramming = non-meta-programming?

pseudocode `def f(a,b,c) := (a+b)/c; f(1,2,3)` is sql `create view f as select (a+b)/c from params; insert into params values(1,2,3); select f`.

there are an infinite number of alternative expressions for defining & invoking functions, but ultimately we must group (fn id, param vec, output expr), and be able to invoke the function. to make regular programming into metaprogramming, simply make the function identifier an ordinary datum, e.g. a string; that way function names (and table and column names too) can be computed from any data. in fact, for full metaprogrammability _every_ _semantic_ (not necessarily _syntactic_) element must be computable. in lisp this is decently easy via `eval` or macros, and in scheme it's more difficult because of they hygeine constraint. hygeine is a concern only because of scope. because all scopes in sql are tables, which all must be named, accidental shadowing is impossible, so hygeine isn't a concern. furthermore, functions may be identifiable by more than a mere string; they may be stored in a table with arbitrary attributes, and we may filter as per usual (e.g. `select * from fns where name = ? and other_attr = ?`) to disambiguate.

NOTE: scoping is a concern in sql queries, e.g. how a table alias (by `as`) is usable in a `where` clause. also, subqueries have access to outer queries identifiers.

as an ending aside, note that a side-effect of data-only programming is that all computation is delayed, since all computations are only data until explicitly passed to `eval`.

.λ's aren't needed for aggregates in sql

haskell `map (\x -> x+4) xs` is `select x+4 from xs`, which returns a fresh, unnamed table; `select` is basically lambda. haskell `foldl + 0 xs` is sql `select sum(x) from xs`. indeed, lambdas would benefit folds in sql, though their benefit would be only be a bit of efficiency or syntactic elegance; general folds can be expressed in sql by using `with recursive ... select`, using the tables locally bound by `with` to store fold state. at the end of the fold, all that remains is the output of the fold; the temporary tables are garbage-collected.

.where λ's would be nice in sql

update clauses: _update_ (cf _put_ or _set_) connotes modifying a value already present, which is a fn of the value to be updated. λ's would be especially useful in triggers that update per row! altirnatively it'd be very nice to be able to automatically select a table of rows that caused the trigger!

furthermore, though tangential, association or lack thereof is encoded in such phrases as `x,y` where, if `x` is a single value, then it's associated with all in `y`, akin to `let x = 4 in map (\y -> f x y) y`. the (lack of) association between memebers of different sets (viz {x} and y) implicitly tells how they must be scoped & sequenced. in this case, `x` must be in `y`'s scope when we compute `f` over all in `y`, but `x` is only associated with `y`, not there being a unique association between some `x` with every element of `y`.

.when `join` cannot alternatively express `where` (maybe. this is a tough-to-identify thought that needs investigation)

commonly `select a from t where a=(select b from u)` can be re-expressed as `select a,b from t join u on a=b where p`. but if there's nothing to join on, because one of the queries returns empty,...well then we can still use coalesce or exists maybe?

=== what is sql

sql is a bit mysterious:

* there's an open standard, but you must pay to access it
* despite the standard existing, no sql database totally conforms to the standand—both lacking standard features and including extra non-standard features
* sql began as merely a relational database system in 1974, but updates to the standard from SQL-99 onward have introduced much more functionality

=== sql basics

a table A may have a primary key (uniquely identifying set of attributes), and may have a set of attributes that, in another table B, is a primary key; then: this attribute set is called a _foreign key_, B is called the _child_ table, and A is called the _referenced_ or _parent_ table. foreign key is its own concept (as opposed to a column that we can `join` on) because it can be used as a constraint in a table's schema, which enforces only proper now insertions & updates.

[NOTE]
primary keys are strange; they enforce uniqueness of each row. however, a row, like any single thing, generalizes to a group of things, which could be encoded as multiple rows sharing a common key.

the beauty of sql is that you don't need to care how you store data; all relations are equally flexible and easy to use. your queries are easy and practically the same regardless of whether you store `x` as an attribute in table `y`, or `y` as an attribute in table `x`.

foreign keys' sole use is in rejecting inserts that would violate the pk/fk relationship [constraint], called maintaining _referential integrity_. they add neither functionality nor efficiency, though, at least in sqlite, they conveniently make some dependent operations automatic; see `foreign-key-clause` in `CREATE TABLE`'s spec. aside from that convenience, though, it's a verify-only constraint.

a _virtual table_ acts like a table but is not actually _stored_ as a sql table, e.g. json virtual tables.

.foreign key example

[source,sql]
----
pragma foreign_keys = on; -- needed in sqlite; else foreign key clauses are not syntax errors, but foreign key constraints are ignored
create table t(id integer primary key autoincrement,
               x,
               dep integer,
               foreign key (dep) references t(id));
create index tdep on t(dep); -- make the upcoming join efficient
insert into t values(null,20,null); -- null is given to autoincrement columns, to use the autoincrement feature
insert into t values(null,40,3); -- fails b/c there's no record in x whose id is 3
insert into t values(null,40,1); -- succeeds b/c we've successfully inserted one row already
select x.x,y.id from x join x as y on x.dep = y.id; -- returns one row: {x=40,x=20}
----

this example creates a table with a foreign key constraint on itself. `dep`, which may be null, since the `not null` constraint was not given, is an optional value to consider after we've considered `x`.

TODO: how to efficiently & elegantly select rows that are or are not referenced by a foreign key, e.g. here, selecting only rows that are not dependencies i.e. rows whse ``id``s are not in any other rows' `deps`? decent solutions: 1. have a boolean attribute flag this; 2. store un/flagged ones in their own table, this making the "foreign" in _foreign key_ appropriate; however, this would be horrible attribute duplication! the 2nd table would have all the same columns as the original! so really only (1) is a decent solution so far.

.foreign keys as lattice of relations on subset of attributes

x := (a b c)
y := (x z)

thus:

* a, b, c ∈ x (i.e. {a, b, c} ⊂ x)
* x, z ∈ y

[source,sql]
----
pragma foreign_keys = on;
create table x(id integer primary key autoincrement, -- always good to have an auto inc integral pk column in
                                                     -- every table in case of need to join or use as foreign key.
               a, b, c);
create table y(id integer primary key autoincrement, x, z, foreign key (x) references x(id));
insert into x values(null, 1, 2, 3);
insert into y values(null, 1, 20);
select a,b,c,z from y join x on y.x = x.id; -- (1,2,3,20)
----

rather than explicitly join `x` with `y` on each `select`, it's more sensible to create a view that represents the relation x ⊂ y:

[source,sql]
----
create view y_full(a,b,c,z) as select a,b,c,z from y join x on y.x = x.id
select * from y_full; -- (1,2,3,20)
----

you may name the view 'y' & the underlying table _y, or you may name the view e.g. y_full & the underlying one 'y'. consider that you cannot delete, insert, nor update a view; those must be done to the actual table.

=== design

match data's logic's invariants with sql invariants e.g. attribute sets, sql column or table constraints. all symmetric data belong in rows, sql's only symmetric aspect.

=== [anti]patterns

==== encoding schemes (relational algebra)

* if you want to store a one-to-some map, e.g. parent -> {child1,child2,...}, then you can (but should not) use a `dependent` attribute. the attribute has multiple values, which may be encoded by multiple rows, e.g. `insert into t(...,dependent) values(...,1),(...,2),...`, but that's quite redundant. a more efficient encoding is to use `parent` instead of `dependent`: `insert into t(...,parent) values(...,1),(...,1),...`. this method inserts each of the parent and all its dependents only once, and all of the dependents' `parent` attributes are the same. in the `dependent` version, all of the parent's attributes except for `dependent` must be redundantly specified per each dependent!
  ** this doesn't generalize to multiple "parents" (tables referencing the "child" table), as that'd mean adding to the referenced table a column per referencing table.
  ** consider `s(id)` & `t(id,s references s(id))`. this is redundant; we can leverage the fact that `t` already has an `id`. this is the parent pattern again; if we were to describe this as JSON, then type `t` would contain subobject of type `s`. in sql it's better to have subset `s` reference superset `t`: `t(id)` & `s(t references t(id))`, which uses only one `id`. i use `references` (foreign keys) here when the referenced attribute is a primary key. if it isn't a primary key or even isn't unique, then we can still `join` on it and use triggers instead of trigger-like foreign key constraints such as `on delete [...]`.
    *** one fewer attribute upon which we'd join means one fewer index, too.
    *** this makes insertion order a bit more intuitive: rather than needing to insert the subsets firstly so that the superset can reference them, we insert the superset firstly, then the subsets secondly.
    *** this scheme is not possible if the superset may have a value other than a foreign key, e.g. `t(s)` where `s<0` is just a number, but `s>0` is a foreign key. the closest way to use the subset-references-superset encoding with this schema is for the subset to have an attribute for the superset's value, e.g. `t:{s:<int|{a:int,b:string}>}` (adt `T = S Int | AB Int String`) as `t(s)` & `s(id,s integer,a integer,b string)` constrained to `s is null or (a is null and b is null)`. the former version would require joining on a `case` clause, which would not use indexes, whereas the latter would join on `id` which, if indexed, would make for a much more efficient [left] join; the `case` would be deferred to after the join, performed on the joined table.
* using `like` is dubious. using `regex` is almost cetainly bad; you probably want a db designed specifically for text searching.
* the semantic meaning of an attribute can depend on other attributes e.g. in `person(age integer,alive boolean)`, if `alive` then `age` means number of days alive; else it means number of days since death. furthermore, any of a row's attributes may be used or not depending on its other attributes' values.
* compress information as much as possible e.g. Y-M-D as just days since some arbitrary start date; that means that dates require only one column. the type `A or B` where A & B are both natural numbers can be encoded as a single integer whose sign determines whether A or B.
* do not move from one table `a` to another `b` by `insert into a ... where p; delete from b where p`; instead, store all in one table `t`, and have an attribute that designates whether a row would belong to `a` or `b`; then filter on that to effectively get virtual subtables `a` & `b` from `t`.
* consider encoding schemes' supported partitioning schemes e.g. integer primary key can be generalized to indexed reals. reals can be partitioned by floor.
* unless uniqueness is required by some algebraic properties of your data, then feel free to see rows in a table as elements of a [multi]set. elements can be grouped [partitioned] by attributes (general prodicate, not just equality), which generalizes "thing at index" to "things with a given property", and set-theoretic operations can be performed for all predicates, and all predicates can be defined of multiple attributes [columns]
* to delete w/cascade a la foreign key w/o the relation technically being implemented as a foreign key, which would be sensible if the parent table referenced a table whose keys were non-unique, hence all of the referenced table's rows of a common predicate would be deleted:
  ** solution 1: `after delete` trigger
  ** solution 2: in sqlite (and maybe other sqls) by using `returning` (non-standand sql), though the returned value is not available as a sql expression; it's usable only by a client program e.g. `(let (rid (sql "delete from parent where id=? returning fk" pid)) (sql "delete from referenced where id=?" rid))`
  ** `on delete cascade` cascades when the *parent* (the referenced table, the one with [that must have] the primary key) row is deleted, not the child! e.g. with `create table a(a primary key, v); create table b(a references a(a) on delete cascade)` means that deleting one of `a`'s rows will implicitly delete one of `b`'s, but not _vice versa_; for the inverted case, you'll need a trigger; however, if you're considering that, you may want to reconsider how you're structuring your data; you should be able to use foreign key cascades. particularly, remember that it's better to have a `parent` attribute rather than `children`. using this design will help you better decide whether either of your tables should have a primary key. remember that foreign keys are one-to-many relationships; many in `b` may have common foreign keys; deleting their corresponding row in `a` will delete all those corresponding in `b`.

===== TODO's

* how can we encode logical constraints as sql constraints or relations? common constraints are types, lengths, [recursive] predicates

==== sensible querying

sensible means elegant, which implies efficient.

* prefer join over subqueries e.g. `select a,(select b from t2 where a=b)) from t1`, or subqueries in a `case` clause; and prefer `in` over `=`, as these support multiple values
  ** the subquery-to-join refactoring pattern is `select (select x from t2 where p) from t1` becomes `select x from t1 join t2 where p`. if `x` & `y` don't have common predicate `p`, e.g. there's a unique `y` identified by `p`, but no `x` satisfies `p`, then use a left join and append `or x is null` to `p`; this new predicate will see the _rows_ for which it holds be returned, then from those rows either `x` or `y` will be chosen, and both will be available; `y` is always available, but `x` may be `null`. either way, the important thing is that the _row_ is in the result set.

.example: use join rather than subqueries
[source,sql]
----
select * from x;
┌───┬───┐
│ a │ b │
├───┼───┤
│ 1 │ 2 │
│ 3 │ 4 │
│ 5 │ 6 │
│ 7 │ 5 │
└───┴───┘
select * from y;
┌───┬────┐
│ b │ c  │
├───┼────┤
│ 2 │ 20 │
│ 5 │ 50 │
└───┴────┘
select a,b,case when c is not null then c else 20 end as 'c or 20' from x join y using (b);
┌───┬───┬─────────┐
│ a │ b │ c or 20 │
├───┼───┼─────────┤
│ 1 │ 2 │ 20      │
│ 7 │ 5 │ 50      │
└───┴───┴─────────┘
----

then use `where` to select a particular row. another possible condition is, instead of `c is not null`, `c>0` where `c<0` denotes an element of a sum type but `c>0` denotes that `c` is a product type, which in sql is encoded as a datum upon which we can join with a table of named tuples.

''''

* suppose that table `t(x)` has one row and table `s(y)` has many rows. if you want to x+sum(y), do `select min(x)+sum(y) from t join s` (or use `max` instead of `min`); `x` will be `count(y)` duplicate rows, but to avoid bare columns, we select one of `x`'s rows, and only `min` and `max` select one row without regard to other rows.
* using a `distinct` query whose result attribute set contains an attribute having a primary key is redundant
* `distinct` means inefficiency in the form of pruning a query; we've asked for data, then discarded some of it—so why did we ask for it, then?! good schema & query design sees that `distinct` should not be used often.
* `having` should be used for aggregates only
* refactor nested queries, _(top-level,nested)_, into a flat one with join.
  ** this is ostensibly possible generally when _nested_ is:
    *** `from` one table (i.e. _correlated_)
    *** used in an `any`, `all`, or `exists` predicate
  ** if the query planner can determine that uncorrelated subquery returns at least one row, then the query planner should flatten.
  ** example: refactor `select x from t1 where x = (select y from t2 where p)` into `select x from t1 join t2 where x = y and p`
* if multiplicity is inconsiderable, then use `union all` instead of `union` because it's faster
* use indexes in `where` &al clauses. e.g. if `a` is indexed, `where f(a)=b` will not use the index! you'd need to have indexed `f(a)`. predicates like `between`, comparison operators, and `like` use indexes. some functions like `min` & `max` should use indexes, too.
* aliasing all tables and using qualified attributes is safer than not; it ensures that you don't accidentally use a wrong attribute that happens to be in scope from another table; if you were to use a qualified attribute name, then you'd get an error saying that that table does not have said attribute.
* `where` is evaluated after joins; if your join lacks results, consider moving your `where` predicate into the join clause
* `[not] in` is fine if you're using literals, but if its arg is a subquery, that's an antipattern; use `except` or left join with `where is [not] null` instead.

==== semantics (sql)

* booleans should always be encoded as a `boolean` type, if that's unfortunately what your sql engine uses; else 0 or 1. never use `null` or `not null` to encode booleans; it's simply incorrect no matter how you measure it.
* prefer fixed precision (often called `numeric` or `decimal sql`) instead of `float` or `real`. if your engine doesn't support that, then you can emulate it by a table with `num` and `den` columns; or just use fixed-point numerals.
  ** at least in sqlite, `floor` retains a real if any real was part of the return expression; if the expression was composed entirely of integers then `floor` is redundant and returns an integer.
* ``select``ing a mix of grouped or aggregate with non-grouped/aggregate data is handled differently by each sql engine. it's best to not mix; refactor queries into all-aggregate/grouped or all-not.
  ** sqlite, perhaps among others, calls non-aggregate columns among aggregates _bare_ columns
  ** in sqlite at least, bare columns' values are deterministic if only one of `max` or `min` aggregate functions is selected
  ** see §2.[4,5] of sqlite docs for `select`
* because sql table identifiers are not first-class (i.e. we cannot, in sql, programmatically generate a table name then reference it i.e. table names must be literal syntax rather than expressions), the only way to keep lisp-grade flexibility [dynamicism] is to use the lisp encoding or something that does not require creation, modification, or reference of a dynamic identifier.
* `x not in (select a from t)` may return ∅ if the `select` returns a set containing `null`; the whole query would translate to `x not in (b,...,null)`, which is equivalent to `not(x=b or x=... or x=null)`. in 3-valued logic, which sql uses, `x=null` is an expression involving `null`, so the whole expression evaluates to `null`. the solution is to use `exists`, which uses 2-valued logic. other solutions are `except`, `where x is not null`, or, if your sql engine supports it, `left outer join`
* predicate evaluation order is nondeterministic e.g. in `isint(a) and a > 0` may fail with "can't apply > 0 to string" since that may be evaluated before `isint`. cte's are not a solution; they suffer from the same non-deterministic evaluation order. `case` is a solution because it has definite evaluation order.

.grouping & bare columns examples

in the following query, `a` is not a bare column because it is in the `group by` clause, so `a`'s value is properly determined in the result set:

[source,sql]
----
create table x(a,b);
insert into x values("x",1),("x",2),("y",34),("y",65);
select a,sum(b) from x group by a;
┌───┬────────┐
│ a │ sum(b) │
├───┼────────┤
│ x │ 3      │
│ y │ 99     │
└───┴────────┘
----

according to sqlite v3.39's `select` docs, §2.4, `group by` associates each row with a group. `select a,f(b) from t group by e` where `e` is an expression that uses [only?] `a`, should be a common idiom. idk how `select` behaves if `e` uses multiple column ids. 

`select a,1.0/count(x) from x` returns only one arbitrary column. `a` is bare here. fix: `select a,cnt from x join (select 1.0/count(*) as cnt from x)`.

==== using sql engines efficiently

* query attributes' order should match a compound index's. not sure if this applies to ordering only in `order by` or if it's important in the selection attributes, or elsewhere,...? or for which engines this is a concern. furthermore, i saw an example whose order was opposite the index, so what's that about?
* except in `count(*)`, the asterisk form is inefficient and its abstraction can cause problems when schemata are modified
* as tables become large, `exists` becomes faster than `distinct`. refactor `select distinct * from t1,t2 where t1.x=t2.y` into `select * from t1 where exists (select 0 from t2 where x=y)`. 0 is a dummy value; we use `exists` to determine whether its argument query is empty, and we _must_ `select` _something_, so we choose a dummy value.
* `having` forces the query planner to not use indexes. refactor `select x,y from t group by z having w` into `select x,y from t where w group by z`
* `in` is more efficient (b/c it uses indexes) than `or` *when the `in` list contains only constants*. e.g. `x=1 or x=2` is better as `x in (1,2)`
* columns that you'll join should be indexed

==== recepies / particular use cases

* a ⊂ b (i.e. all a are in b) is said as `a in b`
* x ∈ a ∧ x ∈ b (i.e. at least one of a's elements is in b) is rephrased into x ∈ a ∩ b, which is `x in a intersect b` in sql
* use views to act as recursively-defined tables by defining the view in terms of `with recursive ... select ...`
* rather than store filepaths, store their contents as blobs; this way deleting an item from the db actually deletes the file, as one would probably expect.

===== TODO's

* how to (especially efficiently) produce a shuffle of a table?
* suppose i've a table `t(a,b,c,...)`, and i want to effectively `with x(x) as (select * from t where p) select sum(a)/x,sum(b)/x,sum(c)/x,... from t`; how to do this for arbitrary number of `a,b,c`, and how to effectively do `(apply / '(sums union all x))`?
  ** we'd need to store a,b,c,... in rows....

==== attributes with multiple values (wip)

NOTE: developing this example is halted until i thoroughly study relational algebra, and take a course in sql from a seasoned professional. also consider the dependents/parent fact stated above.

not all tables are rectangular. sometimes we want to store tables within other tables i.e. have attributes each with multiple values. to effectively do this, we store, in each row, a _pointer_ to another table's row, which will contain multiple data for that attribute. for this example, we'll consider the song _Gold Digger_, which two artists—Kanye West and Jamie Foxx—which sits in a table `songs(title,artist,album)`

one non-solution is storing artist as a string e.g. `"Kanye West feat. Jamie Foxx"` or `"Kanye West, Jamie Foxx"`, then searching on `artist like "Kanye West" and artist like "Jamie Foxx"`. this fails because `like` may match an inappropriate substring, e.g. i search for "James" (the artist who sang the 1990's hit, _Laid_) but also get songs by James Blunt, since `"James" like "James Blunt"`. the solution would be to use `=`, but that obviously fails.

we need a solution that properly stores multiple data as multiple data—namely rows. thus `artist` would be a foreign key to an `artsts` table and there'd be, for every song, one row per artist, e.g. `insert into songs(title,artist,album) ("Gold Digger",1,1),("Gold Digger",2,1)` which reference `(1,"Kanye West"),(2,"Jamie Foxx")` in `artists`. the full code follows:

[source,sql]
----
create table songs(id integer primary key, title);
create table albums(id integer primary key, album);
create table artists(id integer primary key, artist);
create table lib(title integer references songs(id), artist integer references artists(id), album references albums(id));
insert into artists values(1,"Kanye West")       , (2,"Jamie Foxx"), (3,"James"),        (4,"James Blunt");
insert into albums  values(1,"Late Registration"), (2,"The 90's")  , (3,"Back to Bedlam");
insert into songs   values(1,"Gold Digger")      , (2,"Laid")      , (3,"Billy");
insert into lib(title,artist,album) values(1,1,1),(1,2,1),(2,3,2),(3,4,3);
select songs.title,artists.artist,albums.album from lib join songs on lib.title=songs.id
                                                        join artists on lib.artist=artists.id
                                                        join albums on lib.album=albums.id;
┌─────────────┬─────────────┬───────────────────┐
│    title    │    name     │       title       │
├─────────────┼─────────────┼───────────────────┤
│ Gold Digger │ Kanye West  │ Late Registration │
│ Gold Digger │ Jamie Foxx  │ Late Registration │
│ Laid        │ James       │ The 90's          │
│ Billy       │ James Blunt │ Back to Bedlam    │
└─────────────┴─────────────┴───────────────────┘
select songs.title,artists.artist,albums.album from lib join songs on lib.title=songs.id
                                                        join artists on lib.artist=artists.id
                                                        join albums on lib.album=albums.id
                                               where artists.artist="Kanye West" or artists.artist="Jamie Foxx";
-- NEXT: vary the recursive query to produce #(("Gold Digger", "Kanye West, Jamie Foxx", "Late Registration"))
with recursive
  x(id,ps) as (select id+1,value from c where id=1),
  acc(id,ps) as (select * from x
                 union all
                 select id+1,printf("%s %s",ps,value)
                 from acc natural join (select * from c where id>1))
select ps from acc order by id desc limit 1;
----

. we need to store each song as having its own `id` because it's possible, though unlikely, that two artists that did a song together also each did two different songs of the same name on different albums. actually, even crazier: for a few (artist,album)s in my library, there are two different songs of the same name.

.no need to organize data

if you've having trouble organizing your table schemata, you can always use a simple but inefficient encoding in one table. considering the last example differently: say that you want a music db, and you first suppose that artists have albums, and albums have songs; thus your songs should foreign key ref an album, and likewise an album should ref an artist. simple. oh, wait, though; some songs have no albums, and some albums (or songs) have multiple artists. rather than worry about how to "solve this problem," just `create table songs(name string, artist string, album string)` without worrying about foreign keys. any song can now support multiple artists by using multiple rows e.g. `insert into songs values("Gold Digger","Kanye West","Late Registration"),("Gold Digger","Jamie Foxx","Late Registration")`. this encoding is less efficient, but simple, and works; it's therefore useful for encoding data while you're sill developing your database. obviously we can make this more efficient just by making `album` an `integer` which is a foreign key to a table `albums(id,name string)`.

.alternative: lisp encoding

[source,sql]
----
-- general lisp encoding tables: lists & atoms
create table lists(id integer primary key, parent integer, foreign key (parent) references lists(id));
create table atoms(value,                  parent integer, foreign key (parent) references lists(id));

-- song-specific stuff. by lisp alists, this would be (songs . ((name album)))
create table songs(name string, artist string, album string, foreign key (album) references albums(name));
insert into lists values(1,null),(2,1);
insert into atoms(a,1),(b,1),(c,2),(d,1);
---- 

NOTE: lisp encoding cannot accomodate multiple indexes.

=== language design problems (inelegance & inability)

consider `select aapl.c,goog.c from aapl join goog using d`. note how verbose this would become if i were to consider an arbitrary number of tables, despite that being a simple idea. the problem is that columns are not row types; they're less flexible. furthermore, that sql cannot transpose is a serious limitation! indeed, this lang-specific asymmetry limits the metaprogrammability of sql. this certainly is what makes sql bound to being poor, while the relational db model is good.

* columns in a select statement must be hardcoded. i cannot, for example, say `select (cond col1="x" => col2,col3; col1="y" => col3; ...; else *) from t`.
* there's neither support for naked variables (e.g. `x := 3` not explicitly of a table) nor eponymous tables (or views) e.g. `create table x(x)` (to my knowledge yet.)

=== relational algebra

.terminology

[options="header"]
|===================================================
| relational algebra | common name or implementation
| tuple              | row
| attribute          | column (w/type if applicable)
| relation/selection | table
|===================================================

* _constraints_ on a table or column [attribute], e.g. `UNIQUE`, `NOT NULL`, `FOREIGN KEY`, `PRIMARY KEY`. they're verify-only constraints, not adding functionality, and so should be avoided (except indexes, should those be considered constraints)
* tuples are unordered, instead being expressed as attribute-tagged unions
* a tuple's set of attributes is called its _heading_, _domain identifying list_, or when as an argument to projection (see below,) a _projection list_. the heading is a list of indexes, whether ordinal or nominal.
* a set of tuples sharing a common heading is called a _body_
* a relation can thus be partitioned into a heading and body

degree:: number of attributes
schema:: heading with constraints (all needed to produce a selection)

.primitive operations

TODO: continue from ~/Downloads/pacific75-eval.pdf

union-compatible:: having the same attribute (column) sets

* link:https://en.wikipedia.org/wiki/Selection_(relational_algebra)[selection (aka _restriction_)] (σ_pred(R)): filter by predicate
* link:https://en.wikipedia.org/wiki/Projection_(relational_algebra)[projection] (π) of a heading onto a table, π_L(R) := {r[L]: r ∈ R} is just a subset of R found by restricting to attributes L, which must be a subset of R's original attributes; ior a projection may be a map over R's values, e.g. `select a+2 from R` maps `(+2)` over a ∈ R. only the column space is concerned; the number of rows is unaffected.
* link:https://en.wikipedia.org/wiki/Rename_(relational_algebra)[rename ρ]: rename an attribute
* [flattened cartesian] product (×). TODO: test: in sql lhs & rhs tables must have mutually exclusive attribute sets.
* set difference (aka _relative complement_) (\). requires union-compatiblity and may be defined in terms of union: given relations R & S of equal degree _n_, R \ S = (σ_(r[1] ≠ s[1] ∨ ... ∨ r[n] ≠ s[n])(S)).
* union (∪). union-compatible.
* join
  * natural (⋈): defined when lhs & rhs share exactly one attribute. attribute set is the union of lhs' & rhs' attribute sets. (e.g. join a,b,c and b,c,d = a,b,b,c,c,d)
  * inner (intersection in relation algebra): natural but without repeated columns [WRONG] (e.g. join a,b,c and b,c,d = a,b,c,d). union-compatible? not in sql! or perhaps this could be said to be a succession of projection then union.
  * outer: flattened cartesian product
  * left or right
* division: for relations R & S of headings A & B (without repitition) of degrees m & n respectively, the division R[A÷B]S is a subset of π_A'(R), viz {r[A']: r ∈ R ∧ ∀s ∈ S ∃r' ∈ R : r[A'] = r'[A'] ∧ r'[A] = s[B]}. definitions vary when S is null.

the _theta join_ is a non-primitive operation: x θ y = σ_pred(x ⋈ y), expressed in sql as `select attrs from x natural join y where pred;`

the relational algebra is closed under all these operations.

NOTE: *for the love of god, use `BEGIN TRANSACTION` &al*

=== the language

==== semantics

* as per sqlite's graphical grammar description for `expr`, `column-value` is a valid `expr`.
* sqlite stores table schemata as strings rather than as tables (despite the style of `pragma table_info(t)`'s output); this is a design oversight that must be dealt with in a hacky way (see the `alter table` docs)
* both `0` is falsy in sqlite. anything other than null is truthy. null is neither truthy nor falsy; `select x from t where x` will select truthy `x`; `... not x` will select where `x=0`. in neither case will null x's be returned.
* when a sqlite db can be opened read-only, we can still create and modify temporary tables
* everything is a table (multiset of tuples whose positions may be bound to, in a given conext, a name) viz the results of statements, which can be enclosed in parens, e.g. `select * from (select * from mytbl) t`
  * such statements are called _derived tables_
  * thus tables can be locally bound. this allows passing multiple data, e.g. `select * from (values(1),(2),(3)) t` to mean scheme `(values 1 2 3)`
    * this is apparently equivalent to `select * from (select 1 as a from dual union all; select 2 as a from dual union all; select 3 as a from dual) t`
  * _rows_ have no special meaning; they're just singleton tables. all operations are over tables.
    * generally all operations are on the entire table
* if both args to `/` are integers, then `/` is integer division. `cast(expr as real)/cast(expr as real)` to ensure floating point division. however, it's best to use rational arithmetic (`numeric` or `decimal sql` types, if supported) or fixed point arithmetic, instead of floating point.

[options="header"]
|==============================================================================
| sql                 | java 8, math, or scheme
| table               | list of vectors
| `where` & `having`  | filter
| `group by`          | concatMap (useful for aggregates only)
| `except`            | \
| `order by`          | sort
| `union all`         | concat
| `union`             | distinct concat
| `with`              | `letrec`
| `check`             | guards
| `join`              | flatmap [TODO: how?]
| `collate`           | specifies sort fn to be used by `order by`. may be specified in column spec or `expr` grammar
| `escape`            | TODO
| `exists`            | whether argument select query returns non-empty
| `frame-spec` grammar  | TODO
|==============================================================================

TODO: consider (in `expr` grammar): 

===== joins

all joins are refinements of cartesian product. `join` (or comma) is cartesian product. `join on <pred>` filters cartesian product to those matching `pred`. `join using attrs ...` is shorthand for `join on t1.attr=t2.attr ...`. `natural join` is shorthand for `join using X` where `X` is the intersection of tables' attributes.

* `inner` & `cross` are redundant; just say `join`. however, as a non-standard sqlite feature, `cross` prevents query optimizer from reordering input tables.
  ** `cross` join means "cross product" as in cartesian product
* `outer` applies only to `left`, `full`, and `right` joins. idk what `outer` is.
  ** `inner` is inapplicable to `left`, `full`, and `right` joins. 
* `left` join is just `join` unless an `on` or `using` clause is provided.
* `full` & `right` are currently unsupported in sqlite; at least `right` is redundant: `x right join y <join-clause>` = `y left join x <join-clause>`

.examples
[source,sql]
----
-- kinda odd that we can't just do create tablet(a1,...) as (values...)
create table x as with x(a,b) as (values(1,2),("x","y")) select * from x;
create table y as with x(o,b) as (values(6,"y"),(100,2),(101,"B")) select * from x;
-- it's honestly probably nicer to instead use separate create table & insert statements
select * from x left join y using (b);
┌───┬───┬─────┐
│ a │ b │  o  │
├───┼───┼─────┤
│ 1 │ 2 │ 100 │
│ x │ y │ 6   │
└───┴───┴─────┘
select * from y left join x using (b);
┌─────┬───┬───┐
│  o  │ b │ a │
├─────┼───┼───┤
│ 6   │ y │ x │
│ 100 │ 2 │ 1 │
│ 101 │ B │   │ -- (101,B,NIL)
└─────┴───┴───┘
----

in `a left join b`, all of `a`'s rows are present, but some of their corresponding `b` attributes may be null, namely when there _are no_ corresponding `b` attributes.

==== syntax

comments: `-- ... ` for single line, `/* ... */` for multiline

`table.attr` disambiguates when `attr` is shared by multiple tables; otherwise attr is resolved against the table of the `from` clause.

.basic operators
|======================================================================
| &          | bitwise and
| \|         | bitwise or
| ^          | bitwise xor
| += &al, %= | assignment can be used for variables bound in a funcbody
| &=         | bitwise and assignment
| ^-=        | bitwise or assignment
| \|*=       | bitwise xor assignment
| \|\|       | strcat (casts both args to strings if needed)
|======================================================================

===== `create table`

* `create table as` still inserts a table into a database. it's used to init a table at declaration time, for convenience.
* `temp` tables are accessible in the remaining sql script, but are not persistent; it isn't inserted into the database, and so doesn't exist after the sql script that created it finishes execution.

===== user-defined functions (not supported by sqlite)

[source,sql]
----
-- define
create procedure foo @param1 nvarchar(30), @param2 nvarchar(10) as
select * from customers where p2 = @param1 and p2 = @param2
go;

-- invoke
exec foo @param1 = 42, @param2 = "stuff";
----

===== columns

====== `case`

determines a column's value. syntax: `case [when <cond> then <value>]+ [else <value>] end`.

.examples

[source,sql]
----
select customername, city, country from customers
order by case when city is null then country else city end

-- or
select case when city is null then country else city end from customers
----

====== `exists`

should be called `any`, but oh, well. `exists <select-stmt>` checks whether the selection is (not) empty. when used in `case`, one can effectively do `<|>`/`asum`.

====== null

* ifnull(<col>,<val>)
* isnull(<col>) -- returns bool. called nvl on oracle.
* coalesce(<col>) -- 1st non-null value in a list. generalizes `ifnull` to accept multiple values each of which may be null (though it'd be expected that at least one isn't)
  ** it's common to supply a default literal value as the last arg. this guarantees prevention of null propogation

====== constraints

all constraints can be added or dropped via `alter table` or can be added in `create table`

* primary and foreign keys
* `check`, which guards inserts
* default
* indexes
* auto increment

===== filters

* `having` is simply `where` that is a boolean of aggregates instead of per row, e.g. `having count(x) > 5`. using count
* `where` clause accepts things that eval to bools
  ** <, = &al common equivalence relations and boolean conjunctions
  ** between <lb> and <ub>
  ** in <set>
  ** like <pat> (useful only for strings)
    *** `%` is regex `/.*/`
    *** `_` is regex `/./` 
    *** regex-style character classes
  ** exists
  ** <attr> <bin_comp_op> <`any` | `all`> <single_col_tbl> -- `any` is called `some` in some sql implementations

===== result set modifiers

* order by
* limit (or `select top <number> [percent]` in MSSQL; or `fetch first <number> rows only` in oracle 12+) 
* group by

==== table set operations

===== union

union tables' rows. valid only for tables of equal column sets. `union` returns sets; `union all` returns multisets and preserves order like ++.

==== `with` & recursion (common table expression (CTE) subquery refactoring)

this is how we do local binds.

TODO: cf normal aliases

* supports recursion
* exists temporarily: discarded after the statement that uses its binds
* considered a cleaner alternative to temp tables
* alternative to views (prob like `let*` in alt to `define` in funcbods)
* repeated aggregations, e.g. avg of maxes
* "overcome constraints such as what `select` has, e.g. non-deterministic `group by`"

[source,sql]
----
with
  t1(v1, v2) as (select 1, 2),
  t2(w1, w2) as (select v1 * 2, v2 * 2 from t1)
select *
from t1, t2
----

produces

[options="header"]
|==================
| v1 | v2 | w1 | w2
| 1  | 2  | 2  | 4
|==================

could use `values` instead of `select`; `values` is just `select` but more efficient and without a limit on number of supported rows.

.generator example

[source,sql]
----
with recursive t(v) as ( values(1) union all select v+1 from t ) select v from t limit 5;
----

TODO: why does this not produce (1)++(2),(3)++(3),(4),(5)++...? i'd think that `select` would return the whole table (since there's no `where`) on each iteration.

produces a column `v` with five rows of values 1 through 5, effectively equal to haskell `take 5 (Data.List.NonEmpty.unfoldr (\n -> (n, Just $ n + 1)) 1)`. the definition of `t` is unbounded; the bound is in `limit 5`; therefore locally bound tables (at least when bound with `recursive`) are not stricted evaluated before the body of the `select` statement.

.example: trace predecessors/ancestors

this works for a tree, or more generally a dag.

[source,sql]
----
create table x(id integer, prev integer, val integer);
insert into x values(1,null,20),(2,1,40),(3,2,50),(4,2,100),(5,4,200),(6,3,400),(6,4,300),(7,6,1000);
select * from x;
┌────┬──────┬──────┐
│ id │ prev │ val  │
├────┼──────┼──────┤
│ 1  │      │ 20   │
│ 2  │ 1    │ 40   │
│ 3  │ 2    │ 50   │
│ 4  │ 2    │ 100  │
│ 5  │ 4    │ 200  │
│ 6  │ 3    │ 400  │
│ 6  │ 4    │ 300  │
│ 7  │ 6    │ 1000 │
└────┴──────┴──────┘
with recursive y(id,prev,val) as (select * from x where id=4
                                  union -- union all produces some redundancies, since the graph is a dag
                                        -- rather than a mere tree
                                  select x.id,x.prev,x.val from y join x on y.prev=x.id)
select * from y;
┌────┬──────┬─────┐
│ id │ prev │ val │
├────┼──────┼─────┤
│ 4  │ 2    │ 100 │
│ 2  │ 1    │ 40  │
│ 1  │      │ 20  │
└────┴──────┴─────┘
----

maybe unexpectedly, we select from `x`, not `y`! `[...] select y.id,y.prev,y.val from [..]` is unbounded recursion.

.example: trace successors/descendants

this works for a tree, or more generally a dag.

for descendants instead of ancestors, simply swap `y.prev=x.id` with `x.prev=y.id`:

[source,sql]
----
with recursive y(id,prev,val) as (select * from x where id=4
                                  union
                                  select x.id,x.prev,x.val from y join x on y.id=x.prev)
select * from y;
┌────┬──────┬──────┐
│ id │ prev │ val  │
├────┼──────┼──────┤
│ 4  │ 2    │ 100  │
│ 5  │ 4    │ 200  │
│ 6  │ 4    │ 300  │
│ 7  │ 6    │ 1000 │
└────┴──────┴──────┘
----

===== insert

* `select <cols> into <new_tbl_name> [in <external_db>] from ...` is equivalent to a sequence of `create table` and `insert` statements (not available in sqlite)
  ** remember that you can use `as` to rename the columns. they'll retain their column attributes.
  ** `select * into <newtable> from <oldtable> where 1 = 0;` creates a new empty table with the same schema
* `insert into <dest> select <cols> from <src> ...;` is the same but for a table that already exists. both tables must be of the same schema.

===== table ops

* `alter table` changes schema
* <create | drop> db
* <create | drop> table

==== compound or miscellaneous/general examples

.units

[source,sql]
----
create table to_mg(oz,g);
insert into to_mg values(28349.5,1000);
select 3*g,12*oz from to_mg;
----

this sees using a table as a simple ad-hoc relation. obviously the symmetry constraint here restricts the table from holding expressions beyond mere literal values. such functionality would require first-class λ's or `eval` (see sql metaprogrammability section.)

.merge with default value

[source,sql]
----
create table x(id,y);
create table y(id,z);
insert into x values(0,1),(1,20);
insert into y values(1,10),(20,40);
insert into x values(10,100);
select y,case when z is null then 2000 else z end as z from x left join y on x.y=y.id;
┌─────┬──────┐
│  y  │  z   │
├─────┼──────┤
│ 1   │ 10   │
│ 20  │ 40   │
│ 100 │ 2000 │
└─────┴──────┘
----

.get successive integer 

we get the greatest integer in the table, or if the table is empty, then start with 10.

[source,sql]
----
create table x(id integer);
select case when count(id) > 0 then max(id)+1 else 10 end from x; -- 10
insert into x values(100);
select case when count(id) > 0 then max(id)+1 else 10 end from x; -- 101
----

.tic tac toe

this example demonstrates many things about how to reason about relations. to start, the 3×3 grid will not be a table with 3 rows and 3 columns. think about how you'll check for a winner: you'll want to check each of the rows, and each of the columns (and each of the diagonals, too.) to check all of the columns, you'll want to use the same logic for each column, just a different column number. ah, there's one hint: we want column _numbers_; sql does not number columns. columns are fixed and must be addressed by name. rows, on the other hand, are arbitrary in number and are all treated the same. furthermore, we want code that generalizes non-verbosely to higher dimensions, say for _connect four_. x & y should be treated the same; thus we'll use `(x,y)` indices. x's & o's will be stored as -1 and 1 respectively; an empty cell is 0. this makes checking for winners easy: if the absolute value of the sum _s_ of a row, col, or diag is 3, then the winner is `sign(s)`.

[source,sql]
----
-- make the grid
create table grid(x integer, y integer, v integer default 0, primary key (x,y));
insert into grid(x,y) select * from generate_series(1,3) as x join generate_series(1,3) as y; -- generate_series(x,y) is interval [x,y]
-- assume that player just moved, which updates grid. now check for winner:
select sum(v) from grid where x=y;   -- one diagonal
select sum(v) from grid where x=4-y; -- the other diagonal
select sum(v) from grid where x=1;
select sum(v) from grid where x=2;
select sum(v) from grid where x=3;
select sum(v) from grid where y=1;
select sum(v) from grid where y=2;
select sum(v) from grid where y=3;
----

ugly as sin, eh? clearly we're considering the cartesian product {x,y}×[1,3], so our code should reflect that. `where x=n` is here actually a poor way of referring to the set {(x,y)|x=n}! that set is described properly as a cartesian product in sql:

[source,sql]
----
with t(x,y) as (select * from (values(1)) join (select * from generate_series(1,3))) select * from t;
┌───┬───┐
│ x │ y │
├───┼───┤
│ 1 │ 1 │
│ 1 │ 2 │
│ 1 │ 3 │
└───┴───┘
----

we could `natural join` that table with grid on `(x,y)`. (btw, expressions like `where (x,y)=(1,2)` are valid!) however, this is a perfect use case for `group by` & the `sum` aggregate. the finished code is:

[source,sql]
----
create table grid(x integer, y integer, v integer default 0, primary key (x,y));
insert into grid(x,y) select * from generate_series(1,3) as x join generate_series(1,3) as y;
-- check diagonals
select sum(v) from grid where x=y;
select sum(v) from grid where x=4-y;
-- check rows & columns
select * from grid group by x having abs(sum(v))=3;
select * from grid group by y having abs(sum(v))=3;
----

so there you go: checking for winners in tic tac toe simply by 4 queries. maybe it can be syntactically shorter, but this is a good encoding of the game's rules: you win if you cross any row, column, or diagonal.

we see that `group by` partitions by equality, which is analagous to the set of (sets each one of whose axes' value is fixed.)

.select by day

[source,sql]
----
select * from tbl where strftime("%Y-%m-%d",date) = "2022-07-01";
----

`date` may be a datetime or date string.

.resample 1m candles into day candles (single day)

[source,sql]
----
with x(start,end,high,low,open,vol)
  as (select strftime("%Y-%m-%d",min(datetime)), max(datetime), max(high), min(low), open, sum(vol)
  from AAPL where datetime between datetime("2010-01-04 09:30") and datetime("2010-01-04 16:00"))
select start,high,low,open,vol,close from x join (select close from AAPL where datetime = (select end from x));
----

in a common proglang this would be like:

----
let t = {AAPL | datetime ∈ ("2010-01-04 09:30", "2010-01-04 16:00")}
    end = max(t.datetime)
    close = t[end].datetime
 in (start,high,low,open,vol,close)
----

the `join` is not done as a cartesian product, but instead should be interpreted as putting the `close` at `end` into the `select` clause's scope. `x` is a local binding. if i'm using sql from another proglang, then alternatively i could have stored `x` as its own table (a non-local binding) then done `select start,...vol from x` in one query and `select close from AAPL where datetime = (select end from x)` in another.

`open` needs neither aggregate nor other special calculation because for any data selected among aggregates, the first encountered value is used in practice, though according to sqlite's documentation (§2.4 of the `SELECT` docs), "each non-aggregate expression in the result-set is evaluated once for an arbitrarily selected row." if this turned-out to be a problem in practice, then we'd need to endow it with similar logic as we used for `close`.

NOTE: the datetime format requires leading zeroes for all values, e.g. day, hour, &al.

.resample 1m candles into day candles (multiple days)

[source,sql]
----
with x(start,end,high,low,vol) as (
  select min(datetime), max(datetime), max(high), min(low), sum(vol)
  from x_AAPL
  where datetime between datetime("2010-01-01") and datetime("2010-02-01")
    and time(datetime) between time("09:30") and time("15:59")
  group by strftime("%d",datetime)
)
select strftime("%Y-%m-%d",start),high,low,open,close,vol
from x join (select datetime as cdt, close from x_AAPL) on end = cdt
       join (select datetime as odt, open  from x_AAPL) on start = odt;
----

returns

----
2010-01-04  30.64  30.34  30.48  30.59  116694802
2010-01-05  30.79  30.46  30.64  30.62  136014592
2010-01-06  30.74  30.10  30.62  30.13  133300727
2010-01-07  30.28  29.86  30.25  30.08  113809059
2010-01-08  30.28  29.86  30.04  30.27  104221936
2010-01-11  30.42  29.77  30.41  30.01  111353487
2010-01-12  29.96  29.48  29.88  29.67  129700571
2010-01-13  30.13  29.15  29.69  30.05  145122992
2010-01-14  30.06  29.86  30.01  29.91  98356076
2010-01-15  30.22  29.41  30.13  29.41  130680837
2010-01-19  30.74  29.60  29.76  30.72  161574329
2010-01-20  30.79  29.92  30.69  30.26  148014426
2010-01-21  30.47  29.60  30.29  29.74  145818463
2010-01-22  29.64  28.16  29.54  28.25  205441418
2010-01-25  29.24  28.59  28.93  28.92  216214306
2010-01-26  30.53  28.94  29.39  29.41  425729542
2010-01-27  30.08  28.50  29.54  29.71  417601177
2010-01-28  29.35  28.38  29.27  28.47  281731401
2010-01-29  28.88  27.17  28.72  27.44  300374774
----

=== implementation-specific

TODO: this document should be stored as database table with indexes on both topic and sql implementation. furthermore, searching sql (with regex) is better than ripgrep.

==== output

.sqlite output modes

`.mode <mode>` changes output.

* pretty:
  ** `box` uses unicode box drawing characters
  ** `column`: clean
  ** `table`: boxes drawn with plus, hyphen, and pipe
* easily parsed:
  ** `list` (default)
  ** `json`
  ** `csv`
* special output:
  ** `html`
  ** `insert`: sql insert statements; good for copying from one table to another, but not for duplicating table schema. for that you'll likely want `.clone` or using a system shell to copy the db then use sql to modify the copy.

all except `list`, `csv`, `insert`, `html` force headers to be displayed. other modes aren't good.

==== performance

* gather multiple successive statements into transactions (see your db's docs for the `TRANSACTION` keyword)
  ** at least in sqlite, all actions occur in a transaction, and creating & destroying transaction is non-trivial like creating & destroying pthreads.
* sqlite (and perhaps others?): prepare statements that will be executed multiple times. TODO: ipossile only in sqlite (which defines a bytecode) when invoking it from other langs (i.e. preparation isn't possible in sqlite's repl)?
  ** e.g. with connection `d` to db containing table `x(a,b,c,d)`, `(define st (prepare "insert into x values(?,?,?,?)")) (call-with-transaction d (λ _ (query-exec d st 1 2 3 4) (query-exec d st "A" "B" "C" "D")))`. note that the prepared statement can be free in its parameters' values.
* sqlite `PRAGMA synchronous=OFF` disables the usual waiting for data to be safely on disk, thus making writes faster but making corrupton possible.

[TODO]
* sqlite: can i prepare a transaction statement? i should be able to, if transaction is symmetric. otherwise i'll use transactions all of whose statements are prepared.

.exceptions

* akavache is designed to be efficient without the user trying
* sqlite in-memory dbs are probably fast no matter what

==== mutiple databases

[source,sql]
----
create table table1(x integer);
attach database "db2.db" as db2;
create table db2.table1(y integer primary key autoincrement);
insert into main.table1 values(56);
insert into main.table1 values(90);
insert into db2.table1 select * from main.table1 limit 1; -- table1 of file "db2.db" now contains 56.
----

.common

* `insert into t1 (a, b, c) select a, b, c from t2;`
* `all` (cf `distinct`) is often not supported. this is fine because it's the default anyway.

.sqlite3-specific execution

* to open a db as read-only, specify its location as a URI, then append a query: `file://<path>?mode=ro`

.quoting

|===
| single quotes | string literal
| double quotes | identifier (used to, e.g. use a keyword as a symbol
| brackets      | (non-standard) identifier, same as double quotes. used by MS-SQL server and sqlite
| backticks     | (non-standard) identifier. used by MySQL and sqlite
|===

see link:https://www.sqlite.org/lang_keywords.html[sqlite's documentation] on parsing quoted strings.

.csv to sqlite

NOTE: sqlite has a csv virtual table plugin

prefer using link:https://github.com/harelba/q[q] (not in nixpkgs,) which allows running sql on multiple csv files or sqlite databases.

use package `csvs-to-sqlite`. you'll probably want to use options `pk`, `d` or `dt`, `i` whose arguments are the column names as in first row of csv file. if you use these options, then you'll need to run the command for each table that you want to add, unless the tables share common columns for which the options apply.

it's likely in your best interest to add csvs as tables into a db, then use sql to create a new table, rather than doing this all at once programatically.

.list all tables

|===
| sqlite | .table
|===

.describe a table

|===
| sqlite | `pragma table_info(tableName);` //NOTE: no effect if tableName ∉ db
| mysql  | `describe tableName`
|===

=== reldb programming

using (generally reldbs, currently practically sqlite) as a proglang.

* model: declarative, array-based
* bools are 0 & 1
* each shape gets a table
* `with` locally binds 
* virtual tables, table-valued fns & extensions e.g. https://www.sqlite.org/src/artifact?ci=trunk&filename=ext/misc/series.c
  ** TODO: explore
* control flow:
  ** recursion in `with`
  ** `case` 
* folds are called _aggregate functions_
* like a properly set-theoretic language, everything is sets. this is like apl and unlike lisp; in lisp `1` ≠ `'(1)`; if one were considering a datum that may be either a thing or a thing attached to some properties (e.g. `'(1 to 6)`), one would need to break symmetry: `(cond x [(number? x) ...] [(list? x) ...])`, which is just stupid. it's much better to store everything in sets, even if forced to name attributes—sql `with t(x) as (values(1)) select x from t`—which maintains symmetry and does not change form when generalized e.g. adding an attribute to `t`. plurality is a common generalization of singularity, and is thus a more appropriate form than supporting both singularity & plurality. this being said, the requirement for everything to be named does not imply that things must be named _in syntax_; any syntax that unambiguously translates to a product type is acceptable, and its brevity is welcome. for example, sql does this when saying `insert into t values(...)`: you do not need to specify column names, because sql infers this from values' ordinal positions. another brief form is `insert into t(x,y) values(...)` where t may contain many more attributes than `x` & `y`.

==== json

sqlite is an excellent json extractor and manipulator. it considers json as a set of flat tables implicitly nested by (`id`,`parent`) relations rather than recursively nested objects (which introduces scoping), thus making arbitrary traversal easy.

* `.mode json` outputs json to stdout
  ** `.once <file path>` writes next query's output to file (so can write table as json to file)
* if using sqlite as a library in another proglang, then conversion from rows to json is trivial
* json is stored as ordinary strings, except return value of `json`
* json is stored in table cells or string literals

.fns

json:: id fn but cod is string pseudo-typed as json.
json_valid:: 0 or 1 whether a value is a (valid) json string.
json_array(e,...):: constructor
json_object(k,v,...):: constructor
json_array_length:: obvious. useful in query predicates.
json_extract:: select elements from json tree. if one path arg given and selected value does not refer to json array, then returns single value as sql atom; else returns json array string.
json_insert, json_replace, json_set:: put: 1. unless exists; 2. when exists; 3. either; respectively.
json_remove:: duh
json_patch:: put (or remove if put to null) values in json object at keys. treats arrays as atoms.
json_each, json_tree:: json tree as sql tables, top set of children only, or children on all levels
json_group_array, json_group_object:: aggregate fn. return selection as json array or object (see example below). take 1 & 2 args respectively.

.operators

both introduced in sqlite v3.38.0 (2022-02-22). they're `json_extract` but:

->:: always returns json string.
->>:: always returns sql table.

.examples
[source,sql]
----
create table d as with x(k,v) as (values("j",'{"a":3,"b":[1,2,3,4],"c":{"d":"hi"}}')) select * from x;
select key,value,type,atom,id,parent,fullkey,path from json_each(v) join d where k="j";
┌─────┬──────────────────────────────────────┬─────────┬──────┬────┬────────┬─────────┬──────┐
│ key │   value                              │  type   │ atom │ id │ parent │ fullkey │ path │
├─────┼──────────────────────────────────────┼─────────┼──────┼────┼────────┼─────────┼──────┤
│ a   │ 3                                    │ integer │ 3    │ 2  │        │ $.a     │ $    │
│ b   │ [1,2,3,4]                            │ array   │      │ 4  │        │ $.b     │ $    │
│ c   │ {"d":"hi"}                           │ object  │      │ 10 │        │ $.c     │ $    │
└─────┴──────────────────────────────────────┴─────────┴──────┴────┴────────┴─────────┴──────┘

select key,value,type,atom,id,parent,fullkey,path from json_tree(v) join d where k="j";
┌─────┬──────────────────────────────────────┬─────────┬──────┬────┬────────┬─────────┬──────┐
│ key │                value                 │  type   │ atom │ id │ parent │ fullkey │ path │
├─────┼──────────────────────────────────────┼─────────┼──────┼────┼────────┼─────────┼──────┤
│     │ {"a":3,"b":[1,2,3,4],"c":{"d":"hi"}} │ object  │      │ 0  │        │ $       │ $    │
│ a   │ 3                                    │ integer │ 3    │ 2  │ 0      │ $.a     │ $    │
│ b   │ [1,2,3,4]                            │ array   │      │ 4  │ 0      │ $.b     │ $    │
│ 0   │ 1                                    │ integer │ 1    │ 5  │ 4      │ $.b[0]  │ $.b  │
│ 1   │ 2                                    │ integer │ 2    │ 6  │ 4      │ $.b[1]  │ $.b  │
│ 2   │ 3                                    │ integer │ 3    │ 7  │ 4      │ $.b[2]  │ $.b  │
│ 3   │ 4                                    │ integer │ 4    │ 8  │ 4      │ $.b[3]  │ $.b  │
│ c   │ {"d":"hi"}                           │ object  │      │ 10 │ 0      │ $.c     │ $    │
│ d   │ hi                                   │ text    │ hi   │ 12 │ 10     │ $.c.d   │ $.c  │
└─────┴──────────────────────────────────────┴─────────┴──────┴────┴────────┴─────────┴──────┘

select json_group_array(key) from json_each(v), d where k="j"; -- ["a","b","c"]
select json_group_object(key,fullkey) from json_each(j), d where k="j"; -- {"a":"$.a","b":"$.b","c":"$.c"}
----

* `path` is the path to the object that contains a given element
* `fullkey` is the path to the given element
* `atom` is not more useful than value, but should be considered a boolean (i.e. null or not) which is useful for query filters
* `v` is in `json_each`'s scope, implying that, in a join, attributes are unioned before virtual tables are computed.

=== triggers

triggers are very powerful. they enable reactive programming aka _hooks_. the excellence of this design pattern is freedom to not concern scope, and so nor code structure.

[source,sql]
----
create table x as with x(a) as (values(0)) select * from x; -- counter a := 0
create table y(b); -- just some table
create trigger tr after insert on y for each row begin update x set a = (select 1+a from x); end;
select a from x; -- 0
insert into y values(10);
select a from x; -- 1
insert into y values(10),(30);
select a from x; -- 3. if not FOR EACH ROW, would be 2. however, as of sqlite 3.39.2 only FOR EACH ROW is supported, so it's implicit.
----

as you can see, `tr` is a hook that increments counter `a` for each row inserted into `y`.

==== common programming patterns expressed in sql

never assume that a common pattern should be used; instead, *listen to the data*, *follow the implications of design specs*, and then see if the suggested system's (sub)structure(s) happens to exhibit a pattern naturally like a prior-known pattern.

.folds

a fold is a stateful traversal. in reldbs, state is obviously stored, as is everything, in relations. a recursive `with` may be more efficient, however. even more efficient is a fold written as a runtime-loadable extension written in c, loaded by sqlite from a shared library.

`foldl (\a b -> a ++ b) xs`:

[source,sql]
----
create table c(id integer primary key autoincrement, value string);
insert into c(value) values("hello"),("there"),("my"),("good"),("friend");

-- with trim, to remove the leading space character
with recursive acc(id,ps) as (values(1,"") -- initial value (base case)
                              union all
                              select id+1,printf("%s %s",ps,value) from acc natural join c) -- recursive case
select trim(ps) from acc
order by id desc limit 1; -- acc is a scan; get the last element to be effectively a fold

-- proper general solution for folds whose initial object must be the input lists' 1st element
with recursive
  x(id,ps) as (select id+1,value from c where id=1),
  acc(id,ps) as (select * from x
                 union all
                 select id+1,printf("%s %s",ps,value)
                 from acc natural join (select * from c where id>1))
select ps from acc order by id desc limit 1;
----

* we really do use functional style here. we can't use one `with` clause over both an `update` and a `select` statement. rather than use `update` (a stateful, non-functional style), we can use recursion and nested ``select``s. each row is defined in terms of its predecessor.
* `acc` is the named tuple of the fold. `printf` (`format` in other sql engines) is used for string concatenation since sqlite has no separate such function.
* the proper solution binds `x` b/c `select * from c limit 1 union all ...` is invalid syntax; we can't use `limit` there, though `where` is fine there
* i'ven't yet ``explain``ed this query to see its efficiency
* we can't use aggregate functions in predicates; therefore `where id=max(id)` is not a valid alternative to `order by id desc limit 1`

of course, this fold is more easily done by the aggregate `group_concat`, but this example serves generally, when an aggregate may not be already written for it.

.functions

views (especially defined by cte) can represent fns. `create view f(f) as select sin(x + y) from t` is the sql version of `f x y = map (\[x,y] -> sin x y) sql(conn,"select x,y from t")` haskell-like pseudo-code. yes, `f` is the name of the view and the name of its single column. if you've ever defining a fn in code that's using a sql connection, think about how easily you could express that fn as a sql view. views are a sort of variety of prepared statement, except that they're standard sql and are stored by the sql engine internally.

pointwise-with-aggregate array programming example:

[source,sql]
----
create table things(name string, value real);
insert into things values("a",40),("b",16),("c",5),("d",4);
-- equal weight to all things
with weight(weight) as (select 1.0/count(*) from things) select name, weight, weight*value as adjusted from weight, things;
┌──────┬────────┬──────────┐
│ name │ weight │ adjusted │
├──────┼────────┼──────────┤
│ a    │ 0.25   │ 10.0     │
│ b    │ 0.25   │ 4.0      │
│ c    │ 0.25   │ 1.25     │
│ d    │ 0.25   │ 1.0      │
└──────┴────────┴──────────┘
----

notice that the ordinary join (cartesian product) of a single value with a row of values is effectively equivalent to scalar expansion (or w/e it's called) in apl `0.25 × values`.

.local binds

[source,haskell]
----
a = 9      -- dummy value
let a = 20 -- shadow a
 in a + 4  -- returns 24
----

[source,sql]
----
create table scope(a);        -- unlike haskell, we must define a in a table. its dummy value is implicitly [].
with scope(a) as (values(20)) -- local scope(a) shadows global one for duration of this select statement
  select a + 4 from scope;
----

* by naming tables `scope` i mean that tables are scoping mechanisms
* `with` is not properly its own clause; it's a clause of the `insert` statement, as well as `select`, `delete`, & `update`

sql binds cannot be , e.g. in a `create trigger` statement's final clause where it takes a sequence of statements, each statement may have each its own local binds, but local binds over all statements are not supported. instead, you'll need to create a (global) table then have the body statements use it, then drop or reset it as the last body statement, if appropriate. the table may be created before the trigger (being just a global table used only in the trigger) or may be created as the first statement of the trigger's body.

the ability to choose either demonstrates that local binds, like all scoping mechanisms, are not necessary, but instead exist only as a namespace management tool, namely to allow multiple homonomic data across different contexts. sql is unique in that all data must exist in tables, and tables are scoped, so namespacing is more of a constraint than an option. in contexts with homonomic data, sql gives us `as` clauses to disambiguate.
