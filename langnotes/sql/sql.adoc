== sql

this document discusses sql as understood by sqlite, and its underlying model.

[TODO]
* see `optoverview.html` to see what sql's primitives
* how to organize/encode data: relations are basically their headers. thus make a table for each shape. whatever data have a common shape belong in the same table.
* is relalg easily expressed by linalg? can today's gpus already compute relalg programs easily?

=== sql technique

==== quickref

this section is for those who've already read the rest of this document.

* types: scalars, row values, or relations of null (which i'll call ⊥ to reduce confusion about null's varied meanings in other contexts), integer, text, real, or blob (which i'll call bytes, short for bytestring, which obviously is similar to text). i define type str := bytes | text, and num := ℤ | ℝ.
  ** `0.0/0.0` gives `null`, not infinity. for positive & negative infinity, use the string `"+infinity"` or `"-infinity"` for an attribute typed as `real`. for literals: `cast("-infinity" as real)`.
    *** because strings are ordinally superior to numbers, both `x < "infinity"` & `x < "-infinity"` are true for all `x`. `select substr(cast("-infinity" as real),0,6)` returns `0.0` whereas `select substr("-infinity",0,6)` returns `-infi`.
    *** real literals vary with external language e.g. in racket scheme `(require db) (define c (sqlite3-connect #:database 'memory)) (values (query-value c "select 4<?" -inf.0) (query-value c "select 4<?" +inf.0))` gives `(values 0 1)`.
  ** see `floatingpoint.html`
* colmns/attributes/axes, rows/attributes/points
  ** the axes & points interpretation is best
  ** values are symmetric along axes. each point is an ad-hoc relation.
* select | expr. everything else is only for persistent data, not calculation.
  ** triggers and views are available only on non-transient tables
* `join`/cartesian product: adjoin table to query scope. `join` is a primitive; else you'd need ∈, thus not only using sets, but using elements, too, thus affecting the ring algebra. `join` breaks sets into elements then relates elements.
* `with`: fold/scan/loop
* relations represent spaces. columns represent axes. rows represent points. don't worry about schemata much; `join` (couple) & `select` (decouple) are powerful enough that no difference in schema affects a significant difference in ease of querying. this being said, there are 2 simple rules that you must follow in order to have sensible schemata:
  ** that the space is the cartesian product of the axes, that rows have sensible values for each of their elements. 
  ** that one-to-many relations are stored in separate relations e.g. if i've a table `drug(prodname, chemname text primary key, dosage)` then it can't have a property `sideeffects`, since each drug has multiple. i must have a separate table `sideeffects(drug references drug(chemname), effect)`.
* *exploit*: row values; numbers & strings; [builtin] [non-]aggregate ("core") [window] fns; `join`, `select`.
  ** `coalesce` is short-circuiting `or` like in lisp. like `iif` it's just a more convenient representation of `case when`.
    *** `coalesce` returns the first non-null, or if all null, then returns `null`. it works with aggregates e.g. `select coalesce(sum(x),0) from t` returns scalar `0` for empty `t`.
  ** you can encode strings especially to exploit helpful string functions (see below), even creating indexes on these
* use `upsert` instead of `insert`, `update`, `replace`
* recursive queries generalize literal relations just as horn clauses with symbols generalize horn clauses of only value literals. recursive queries are equivalent to horn clauses or functions. this power is of relating symbolic expressions. a recursive `select`'s expressions (of its many clauses) are related to the result of applying those expressions on the prior iteration; as in other reductionist langs, the expressions are transitively related by evaluating expressions on prior-output values.
* sqlite can replace record editors such as awk. sqlite cannot be used for general text; for that, prefer `kak -f`
* sql solves the control flow problem (forcing the programmer to identify correct nesting for loops) by join & array paradigm: just a single loop over pointwise-related data, a subset of the cartesian product of relations; and each datum in a relation is associated with other data in its row.

helpful string functions:

NOTE: `substr` & `instr` are 1-indexed, not 0-indexed

* substring: `substr`, `ltrim`, `rtrim`, `trim`
* index: `instr`
* `length` (number of characters for strings, or number of bytes for blobs)
* `min` & `max`
  ** unary is aggregate; else it's per row
* `lower` & `upper` (load icu extension for non-ascii)
* `replace`
* the concatenation operator, `||`

using sql with another language:

* control flow must be handled in some non-sql lang just to accomodate per-statement triggers and/or simple loops that feature i/o. for these two uses, a language other than sql is acceptable.
  ** all sql mutations are idempotent except:
    *** ``update``s requiring `new.` & `old.`
    *** ``insert``s with `replace`, `rollback`, or `fail`
    *** any mutation whose `where` predicate features an aggregate
* *never* get rows from a db then use them in another db query; that should always be done in sql alone!
* put all supported data into sql relations. for example, say i've a relation `vals(name text, value integer)`, and that i want to, for each `name`, pass `value` to a corresponding complex unary function defined in some other language. then i'll need to effectively join `vals` with a hash table of names to functions; or, more efficiently, store functions in an array where the order of indices matches the order of names: `zipWith (\value f -> f value) (query dbconn "select value from vals order by name") fns` or `map (\[name value] -> (hashref fns name) value) (query dbconn "select name, value from vals")`.

the need for both `from` & `join` (syntax, not concept) is asymmetric. it can be framed as `from` accepting one table and `join: Table -> Table -> Table`, but it can also be framed as `join : Query -> Table -> Query`, acting on `from table`. 

firstly, there are _statements_ & _expressions_. the `select` statement is the only real thing in sql; the other statements are just stateful `selects` e.g. `create table` corresponds to expressions that return (transient) tables, `delete` corresponds to `except`, `insert` corresponds to `union`. non-select statements are just for mutative/persistent & named, instead of pure & anonymous, manipulations. therefore i'll consider only `select` and `expr` forms. one should use pure (non-mutative) sql for general computation; one should save data persistently only if it's to be used later, stored for the sake of keeping data, not for the mere sake of facilitating computation!

the pure version of `update t set x = y where p` is `with r as (select * from t where p) select * from t except r union (select ...,y,... from r)`. surprisingly, `select * from (values(1,2)) join (values(1,3)) using (column1)` produces relation (column1,column2,column2)=values(1,2,3).

first, though we must discuss how sqlite handles data. it uses _scalars_ and _row values_. scalars are relations with a single attribute. row values are relations with 2+ attributes. _relations_ are sets of row values. i haven't seen a reason to distinguish between scalars and row values; why not just say that row values are 1+ values?

==== relations

*relation* is useful only when eventually interpreted as an edge in a traversal; an edge may be given one of two ways: two literal data, or two symbolic expressions sharing at least one symbol. sql "relations" [tables] only encode literal data, not symbolic expressions (since even expressions stored as string literals can't be executed, because sql doesn't have `eval`.) as prolog demonstrates, symbolic expressions generalize literal data: horn clauses may accept constants or symbols. sql, being non-metaprogrammable (i.e. lacking `eval`), requires that symbolic relations be expressed as code, not data. so how do we most elegantly express horn clauses despite this asymmetry?

symbols must be in statements or expressions, not in relations. all variables' values are stored in relations. this makes sql like most common programming languages: reductionist, non-reflective. we can exploit the hack where all vars are stored in dictionaries by storing variables in a single relation e.g. the canonical encodings are `var x = 4; var y = [1,2,3]` in js or `create table x(v) with values(4); create table y(v) with values(1),(2),(3)` in sql are instead represented as json `{"x":4,"y":[1,2,3]}` or `crate table _g(k,v) as values("x",4),("y",1),("y",2),("y",3)`, which gives expected output for functions on `y` e.g. `select sum(v) from _g where k="y"`.

by this hack we can store sql's `select` statement as `select(clause string, value)`. suppose sample `values("from", "x join y on p")`. wait. that's already an error since the value is not a value at all; it's an expression, which isn't helpful because we don't have `eval`! we could `insert into "select" select "from",* from x join y on p`. moving on, we'd insert `insert into "select" select * "where",x<y from x join y on p`, etc; we use the usual `(x,[ys])` as `[(x,y)]` encoding where `x` is here a clause name and `ys` is the clause's value for each row. this is obviously not helpful for storing a `select` statement as a relation, but the same principle applies to other data, and it actually _is_ useful in general.

without the hack, `select(attribute string, src_relation, where, group_by, having,...)` is insensible because these attributes' values are not related to each other; for each row there's no relation between a `src_relation` and a `where` clause. *relations relate points' axes*, analagous to choosing which data to include together in a tuple/struct/object/class/type. relations should be interpreted as the cartesian product of sets, or the corresponding unioning of axes which beget a space of points. different relations correspond to different spaces. transforms among spaces may exist. as per prolog's predicate system, relations may also be interpreted as functions. `f(g(x,y),z)` simultaneously represents:

[options="header"]
|================================================================================================================
| `f` & `g`  | `x` &al     | action      | output                               | output form
| functions  | arguments   | reduction   | literal value(s)                     | `a,b...`
| predicates | constraints | unification | predicate (satisfied by some values) | <predicate of `x` &al>
| relations  | attributes  | query       | set of related literal values        | `rel(a,b,...), values(_,_),...`
|================================================================================================================

all under the same notation^*^, and with equal arities regardless of interpretation. the only information essentially encoded in the string `f(g(x,y),z)` is the order of application and arities. in all cases the evaluation of expression identifies some values. in the relational model, `f(g(x,y),z)` is to suppose `f(a1,a2)` and `g(b1,b2)` then `select from f where (a1,a2)=(select v from g where (b1,b2)=(x,y), z)`. symbolic relations e.g. peano values are calculated by querying with any of the loop/fold forms identified in the following table of primitives. *functions are generally defined in the relational model as recursive queries.*

^*^actually the predicate and relation forms would include the output value alongside the inputs: `f(a1,a2,u)` & `g(b1,b2,v)`. i glossed-over this in `select v from g ...`; `v` was actually not in `g` as i'd declared `g`! again, as always, `u`/`v` may be specified as datum literal or expression that evaluates to a literal, per row.

to conclude i answer the recently posed question: to most elegantly express horn clauses despite the asymmetry, we use recursive queries, which generalize literal relations just as horn clauses with symbols generalize horn clauses of only value literals. the symbolic expressions are in the recursive `select`'s attribute list, `where`, `group by`, &al clauses.

==== primitives

|============================================================================
| types            | ⊥, str, num
| loop/fold        | attributes as sets^*^, `with`, aggregates, triggers^**^
| prog/fn          | query
| short-circuiting | ⊥
| (0,+,×,-)        | (∅,∪,∩,\) [rel]; (⊥,expr) [expr]
| atoms            | attr [rel], datum [expr]
| choice           | `case when` [primitive]; `iif`, `coalesce` [convenience]
| extract          | `substr` &c, query/`select`/expr; `where`, `having` 
| element relation | `join`
|============================================================================

^*^ i.e. sql is an array language; ordinary (non-aggregate) expressions are specified in terms of attributes, but each attribute represents a set of values. `map` is implicit in sql, or `filter-map` for queries featuring a `where` clause.
^**^ triggers that trigger themselves until their condition no longer holds are equivalent to `while` loops. looping by triggers is declarative; the control flow's literal path is implied by implicitly sequenced rules—trigger conditions. by contrast, `with`'s control flow is literally given as a sequence of `select` forms, always ultimately accumulating a relation.

==== null

* null propogates; `is` can stop propogation
* `case when` & `iif`, and `where` consider `null` a failure just like `0`
  ** you can exploit this for elegance & brevity e.g. what in most langs would be `(if (or (null? x) (> x 0)) y z)` is in sql `iif(x>0,y,z)`, since a null `x` will cause `x>0` to evaluate to `null`, causing the `iif`'s failure result to be returned. other example: this code is silly: `case when x is null then x when count>0 then min(x,y) else max(x,z) end`. the non-redundant version is `case when count>0 then min(x,y) else max(x,z) end`, since `min` & `max` return `null` if any of their args is `null`.
* `count` counts non-nulls
* `x not in (select a from t)` may return ∅ if the `select` returns a set containing `null`; the whole query would translate to `x not in (b,...,null)`, which is equivalent to `not(x=b or x=... or x=null)`. in 3-valued logic, which sql uses, `x=null` is an expression involving `null`, so the whole expression evaluates to `null`. the solution is to use `exists`, which uses 2-valued logic. other solutions are `except`, `where x is not null`, or, if your sql engine supports it, `left outer join`
* there are no illegal values in sql; what would usually be an illegal value (and so a runtime exception) in other langs is `null` in sql
* aggregate functions may handle `null` asymmetrically; e.g. `sum` treats nulls as 0 rather than making the whole sum `null`.

==== row values

* row values plainly represent the concept of grouping. this is separate from relations, which are array variables instead of single data; the difference is that each of a row of values is particularly, certainly identified, whereas those in an array var are non-particular, anonymous. this is quite similar to how tuples vs lists are handled in haskell. indeed, lists/relations may be empty, but row values may not be empty!
* a row value's _size_ is the count of its attributes
* row values may be syntactically expressed as `(v,...)`. note the lack of `values` which denotes _relation_ literals, not row value literals.
* any binary operations on row values require row values of the same size
*  `<`, `<=`, `>`, `>=`, `=`, `<>`, `is`, `is not`, `in`, `not in`, `between`, and `case` with a base expression (e.g. `case (select * from (values(1,2),(4,3)) where column1 > column2) when (1,2) then 3 else 4`) are such binary operators that accept 2 same-size row values (or 2 scalars)
  ** these evaluate pointwise on row values from left to right, stopping on one of 3 conditions:
    *** all row values are evaluated; retval is as expected
    *** value is encountered that determines retval regardless of remainder of row values e.g. `(1,null) < (2,null)` is known to be true after evaluating `1<2`, so the nulls aren't even considered
    *** a `null` is encountered; then retval is `null`
  ** it's not always so simple. e.g. `(1,2,3)=(1,null,4)` returns `0` because 3≠4 regardless of the other values. yet `(1,2,3)=(1,null,3)` returns `null` because the retval 0 or 1 depends on what the `null` would be. remember that sqlite considers `null` as a lack of information. like sigfigs, a lack of information must propogate.
  ** row values are only for convenience; you can't use other binops e.g. `+` with them.
* `update` (not talking about `update from` here) expects a row value on the rhs
  ** `update t set a=x,(b,c)=(select ...) where ...` works
* `<rowv> in <rel>` tests whether a row value is a member of a relation. `<scalar> in <rowv>` works, too.
  ** given that other binops work on row values, `in` should be parameterized by a binop; then we'd have `any` instead of `in`
* wrt an `order by` clause, when a relation is not a multiset, then rows can be indices; use them instead of `offset` because that's more efficient
* use row values in a predicate clause instead of `and` or `or` e.g. `where (x,y) between (0,10) and (3,100)`
* table names are often directly syntactically usable instead of `select * from t`
* select statements that return a single row eval to row values, so `(select a,b from t1) > (select x,y from t2)` is valid & sensible.

NOTE: `is` & `is not` are `=` & `<>`/`!=` but produce 0 or 1 instead of `null` if either of their args is `null`.

* to test whether x is a subset of y: `y x \ ∅ =`, or `x y ∩ x =`
 ** this generalizes x∈y. that generalization is good because it makes everything sets (no "naked" elements)
* to test whether any of x is in y: `x y ∩ ∅ =`

==== expr

_expression_ means _relation_. singleton values as relations have one row & one attribute. some expression forms evaluate to a singleton relation, and others an empty relation. the context in which the expr is used may mandate constraints on the expr e.g. the number of attributes or rows that it has.

obvious ones like datum literals or operators/functions thereon aren't here enumerated.

* symbols bound in query's scope
* absolute symbol reference ([schema.]rel.attr)
* row values
* like (`%` for regex `/.*/`, `_` for `/./` ) or glob (`*` for `/.*/`, `?` for `/./`). (`match` & `regexp` aren't usefully defined; they exist for the user to define those functions, so they're useless.)
  ** `like`/`glob` requires single-attribute relations. if left arg has multiple rows, only its first is used; `(values(x),(y),...) like (values(a),(b),...)` = `x like a`.
* `is [not] [distinct from]`
  ** `is` & `is not` are sqlite-specific terser forms of sql standard `is not distinct from` & `is distinct from`. holy golly, man.
  ** `isnull`, `notnull`, both equivalent to `is null` & `is not null`. no idea why these especially specific forms exist. `isnull` is a binary function in ms sql server, and has different behavior in other sql engines, so given that it's not standand, i can't imagine why it's included in sqlite.
    *** prefer `is [not] null` for compatibility with other sql engines
* `[not] in`.
  ** `e1 in e2` requires that `e1` & `e2` have the same number of attributes e.g. `[...] where (x,y) in s` works if `s` has two columns. it then uses `=` on row exprs. use subquery for use with single attributes e.g. `[...] where x in (select a from s)`
  ** see note below.
* `[not] exists` is conceptually equal to `having count(*)=0`. but the two have different uses; `exists` is used for subqueries, not an aggregate of the current query. also hopefully, being a special syntax, `exists` optimizes queries like `exists x intersect y` to not actually compute the full intersection, but return when any of `x` is found to be in `y`.
* `case when` is short-circuiting / lazy eval, unlike `iif`. see _§first-class functions and conditionals_ for thorough discussion.
  ** `case` is an expression, not a table. `select case when 1 then (values(3),(4)) end;` is correct; there's no `from` clause.
  ** in `case when p then y end`, both `p` & `y` must be single-attribute relations, and only the relations' 1st values are used. 
    ** if the chosen relation is empty then `case` returns `null` as a scalar
* builtin numeric functions include trig, ceil, &c; see `pragma function_list` for complete set
* builtin bitwise functions are syntax: `&`, `|`, `>>`, `<<`, `~`. idk if xor is supported. `^` isn't working.

see best-paradigms-lang.adoc. `where` is implicit in predicate logic; it's the same as predicate unification/evaluation. the `expression` grammar's `[not] in` subgrammar is set membership/intersection, which is is equivalent to testing against a predicate. therefore `a in b` = `exists a where b` = `exists a intersect b` except that `a where b` is incorrect in sql since therein `b` must be a relation which sql considers distinct from a predicate; however, theoretically, by predicate-set correspondence, the three are equivalent. in sql we'd need to unify sets & predicates by saying `exists a intersect select * from b where b`. the equivalence can be seen by `a [not] in b` (or `a like b` &al) being set membership if `a` is a row value and `b` a relation, or set intersection if both `a` & `b` are relations. *however, `[not] in` has one characteristic: it also accepts a scalar lhs with a row value rhs. this is a blatant asymmetry in how sql considers groups of data.* the symmetric solution would be if scalar were equal to a row value with a single datum and a row value equal to a relation with one row. indeed, this would imply that a scalar equal a singleton, single-attribute relation, which _is_ true almost always, but not in `[not] in` and perhaps in some few other contexts.

sql's (relalg's) primitives are the (+,×,-,0)=(∪,∩,\,∅) ring, expressed in predicate logic as (∨,∧,¬,⊥). one of sql's troubles is that it is not symmetric; it considers predicates distinctly from sets, and sometimes considers elements distinctly from sets. also, though this model seems appropriate, one must be careful to distinguish between expressions that act per row vs aggregates, which act per relation, for logical reasons, even though they're of the same grammar, both accepting expressions as inputs and returning a relation/expression as an output. also, and again usefully so, `null` is the empty row value whereas an empty relation is an empty set. `null` has short-circuiting/null-propogation semantics whereas an empty set is the identity for union but a short-circuiting operator for `intersect`. these semantics can be a bit confusing, but they are elegant!

* `where`/`between` (these use indexes)
* `join` (or `where` [filter] & `union`) (general filter)
* `intersect` (common elements), `except` (asymmetric difference). symmetric difference isn't a sql primitive; you must do `select * from x except select * from y union all select * from y except select * from x`. yikes. if `select * from` were assumed if omitted, and symbols were supported instead of english words, then the statement would be expressable as `(x\y)∪(y\x)`. note the need for parentheses here, which are not needed in the verbose syntax. the symmetry is obvious in the symbolic version! in factor this is `[ diff ] [ swap diff ] 2bi union`, again showing the symmetry.
  ** `select from x,y where x not in y and y not in x` (or anything involving join) is incorrect because we aren't concerned with cartesian products, elementwise-pairing, nor combining column sets.
* `exists` predicates on a relation's emptiness; `where exists ...` makes one relation's emptiness imply this relation's emptiness.

==== select

* `from` merely binds symbols for the query
* `where` & `having` are both the same concept—"such that"—but one is applied to row values and the other to groups of row values. if sql were more symmetric, then `having` should apply to the whole query (the single group) just like aggregates do. however, most people would consider that more like a guard [list monad].
  ** if the expression supplied to `having` is an aggregate, then it's run over the group's rows. if it's non-aggregate, then it's applied to an arbitrary row of the group.
* `where` is scoped to all expressions following `select` e.g. `select 4 as x where x%2=0;` is valid
* `where`'s expression cannot use aggregates. this leads to the unfortunate workaround of duplicating majority of queries in a `with` clause e.g. `with ml(ml) as (select max(length(body)) from docs where title="trailing stops") select * from docs,ml where title="trailing stops" and length(body)=ml;`. for _selection_ we can `order by length(body) limit 1`, but that does not work if we want to update or delete a row with a most extreme attribute value. i wonder if there's a better solution, perhaps using `group by` & `having`?
* `group by`, `having`, and `window` support [window] aggregate functions, what would be expressed in an ml-style lang as `map (foldl1 f) . filter p2 . partition p1`
  ** `group by <expr>` evaluates `<expr>` for each row; the number of groups is the number of unique values of `<expr>`
  ** `group by x` is the dual of distributing `(x,)` over `[y]`
* `order by` & `limit [offset]` enable [sub]sequencing. along with `union all` this is the only way to guarantee ordering.

.common algebraic patterns

* a ⊂ b (i.e. all a are in b) is said as `a in b`
* x ∈ a ∧ x ∈ b (i.e. at least one of a's elements is in b) is rephrased into x ∈ a ∩ b, which is `x in a intersect b` in sql
* use views to act as recursively-defined tables by defining the view in terms of `with recursive ... select ...`
  ** see sqlite docs' `lang_with.html` page, §3 for exact details
* rather than store filepaths, store their contents as blobs; this way deleting an item from the db actually deletes the file, as one would probably expect.

[TODO]
* how to (especially efficiently) produce a shuffle of a table?
* suppose i've a table `t(a,b,c,...)`, and i want to effectively `with x(x) as (select * from t where p) select sum(a)/x,sum(b)/x,sum(c)/x,... from t`; how to do this for arbitrary number of `a,b,c`, and how to effectively do `(apply / '(sums union all x))`?
  ** we'd need to store a,b,c,... in rows....

===== window functions

see `windowfunctions.html` for both a description of the window function design/mechanism, *and a list of the bulitin window functions.*

a _window [frame]_ is a subset of a relation. the only thing that distinguishes it from a `select` expression is that each subset is associated with a row. this is much more powerful than otherwise: join, which associates each row being with exactly one other; or aggregate functions, which evaluate to a scalar, albeit one per group when `group by` is used. therefore window functions are extremely powerful but more particular versions of aggregates on `group by` that give aggregate outputs per row rather than just per group of rows.

* aggregate window functions' window frame is determined by a predicate given to the `order` clause. also the window's contents may be ordered.
* aggregate window functions don't present the bare columns problem.
* even non-aggregate window functions effectively implement stateful loops (folds). aggregate winfns implement stateful loops that would be a pain to implement with a fold because their state isn't easily expressed by a single accumulator value, as is the case for aggregate functions on a rolling window e.g. selecting a value and the sum of its immediate neighbors. this is easily defined by a for loop in c, but not by `reduce` in factor, or especially by a fold in racket, haskell, python, &c.

all binary functions are foldable i.e. usable as aggregates. only in typed langs is this not true, for they often use types like `a -> b -> c` or `a -> b -> b -> a` or `a -> b -> b`, instead of `a -> a -> a` or `a -> (b -> a) -> a` &c. all unary fns are mappable & applicable, and all binary fns are applicable and foldable. sql has only numbers (integers & floats) and strings (strings & blobs.) all other "typing" is done by relations & row values. thus sql meets my personal requirement for using only relations of types primitive to a physical cpu. therefore, excepting lack of metaprogrammability and the arguably bad & arbitrary constraint of needing to group all columns as tables, *the ability to define aggregate window functions should be all that's needed to make sql a perfect language*, even if its syntax is verbose.

NOTE: because winfns' results are not per-row, window functions cannot be aliased then used in `where` clauses.

====== examples

.difference of adjacent elements

this example's essence is `log`.

[source,sql]
----
create table x(x);
insert into x values(1),(10),(100),(2);
with t(x) as (select x-lag(x) over (order by x) as d from x) select * from t where x is not null;
----

returns a column `x` of `1 8 90`.

''''

NOTE: sql is case-insensitive!

* though tables' attribute sets are mostly fixed (though they can be updated by `alter table`), creating new tables on the fly is easy: just use `select` to get a subset of attributes or `join` to adjoin!
* it's appropriate that sql forces everything to be in tables; that's symmetric treatment of singletons & collections. however, a table that implicitly stores global variables like lua's `_G` would be nice.

==== encoding in relations

you can choose expressions associated with each of x>0, x<0, x=0 by `select <exprs> from r where sgn(x)=choice_id` where `r(choice_id,e,...)` is populated by `select 1,a,... from t union all select 0,b,... from t union all select -1,c,... from t`.

sql cannot accomodate storing operators in tables. however, you can store integers or other data in tables, and map them to operators by `case when` or `union` (see _§conditionality_ below.) all that matters is that you have an isomorphism one of whose versions is encodable in sql (as a relation.) for example, an alternative encoding for {x<0, x>0, x=0} is (x,{0,1,2}), having factored-out `x` then using the relation {(0,<),(1,>),(2,=)}, encodable in sql as `select case eq_id when 0 x<0 when 1 x>0 when 2 x=0 end from table_that_has_x`. generally any expression is comprised of a sequence of operators/functions, generally relations. each relation can be assigned an arbitrary uid of any type, and the arrangement of operators/functions, if it's simple enough, can feasably be expressed by a bitstring, text string, or set of attributes, all of which can be stored in a relation. in fact, you can even exploit symmetric encodings for a better alternative to symbolic function identifiers. for example, the equivalence relations <,>,= can be expressed as 0,1,2, but if you want to include >= & <=, you can simply use the fist two bits of a bitstring to be of 0,1,2, and let the 3rd bit represent "or equal to", of 0 or 1, representing nothing, or "or equal to"; thus > is represented by 0b10 and >= is represented by 0b10|0b100=0b110=6. an equivalent encoding would be the strings ">=", "=", &c, and determine whether to check "or equal to" by whether the last character of the string is `"="`. 

NOTE: symmetries exhibited by (bit)strings & integers allow more elegant expressions of code than most languages allow.

in summary: as always you can exploit natural structure of integers and (bit)strings, but this is especially important in sql because they may be the only encodings supported by sql! sql has only types nulll, number, & string, so any other concepts must be encoded by these types. remember that any ast, stack, or other structure can be encoded as a relation, and that any traversal of that structure can be expressed in sql, with the most general traversals being expressed by `with [recursive]` or a trigger that, when run, triggers itself until its condition is not satisfied.

==== select & expr together (common compound tech)

===== conditionality

TODO: having identified `group by` as dual to distribution of tupling, can't i express these *columns* by a distribution? what about rows? (a,[b]) in sql is stored as two tables: a list of b per element a.

i would prefer the term _choice_ instead of _conditional_ or _branch_ because _branch_ implies control flow, which is an unnatural interpretation in a declarative paradigm. _conditional_ is technically correct but needlessly technical; _choice_ is the common term. however, even _choice_ is perhaps not ideal; we're dealing with sets and sql, here. choice is choosing from a set of choices; we may choose one or multiple things. this is the same as _selecting_ one or multiple rows or columns, which is just done in a common `select` statement.

`where` chooses rows; `case when` chooses columns. in sql we need `case when` because column specification is syntax instead of data; if we could calculate the column set then pass it to a `select` form then there'd be no need for `case when`; the desired columns would be calculated by common relational algebraic expressions. for example we'd be able to specify the column set conditionally as a single-attribute relation by `(select col from columnset1 where p) union all (select col from columnset2 where not p)`. `join` is not a good alternative, since it requires us to uniquely identify alternative columns by names, even though we know in advance that we'll use only one! `join` is used only to bring multiple tables into scope, while `case when` actually chooses a column: `select case when p then c1.col else c2.col end from columnset1 as c1 join columnset2 as c2`. we can't even prediacte `join` (into `join ... on`) because we aren't taking a subset of rows! we want to retain all the original rows but choose output values.

if `case when` were to support returning multiple-attribute relations then we could plainly & elegantly express multiple values being conditional per choice. this would look like `select case p when 1 then (x+y,y+z) when 2 then (0,z/y) end from x`. then `case when` would return row values *and* `select` [syntax, not statement grammar] would accept a row value of attributes, which would mean that columns would be specified as calculated values rather than as expressions-as-literal-syntax. that's bordering on metaprogramming—which _would_ enable many methods of expressing conditionality (among other things) e.g. having a relation of choice number and view/table name, then being able to select `from`'s argument from that relation. that sql accepts syntax instead of calculated parameters for statements limits sql greatly. unfortunately are current solutions are either:

. `with choice(choice_id) as <expr> select case choice_id when 1 then a when 2 then b ... end, case choice_id when 1 then x when 2 then y ... end, ... from x join choice`. this is inelegant insofar as needing to repeat `choice_id` multiple times. it's inefficient because it computes the `cose when` per row despite actually needing to compute it only once.
. `with selected_choice(choice_id) as <expr> select *,a,... from x join selected_choice where choice_id=1 union all select b,... from x join selected_choice where p=2 union all ...` where each `a`,`b`,... represents a list of attributes associated with a choice e.g:

[source,sql]
----
-- with x(x,y) as
┌────┬────┐
│ x  │ y  │
├────┼────┤
│ 20 │ 40 │
│ 6  │ 7  │
│ 3  │ 5  │
└────┴────┘
-- cid=1
with c(cid) as (values(1)) select *, x+y as fxy, y/x as gxy from x join c where cid=1 union all select *, x-y, x/y from x join c where cid=2;
┌────┬────┬─────┬─────┐
│ x  │ y  │ fxy │ gxy │
├────┼────┼─────┼─────┤
│ 20 │ 40 │ 60  │ 2   │
│ 6  │ 7  │ 13  │ 1   │
│ 3  │ 5  │ 8   │ 1   │
└────┴────┴─────┴─────┘
-- now change cid to 2
with c(cid) as (values(2)) select *, x+y as fxy, y/x as gxy from x join c where cid=1 union all select *, x-y, x/y from x join c where cid=2;
┌────┬────┬─────┬─────┐
│ x  │ y  │ fxy │ gxy │
├────┼────┼─────┼─────┤
│ 20 │ 40 │ -20 │ 0   │
│ 6  │ 7  │ -1  │ 0   │
│ 3  │ 5  │ -2  │ 0   │
└────┴────┴─────┴─────┘
----

like the `case when` technique, this is inefficient because it computes a predicate for each row despite logically needing to do it only per relation being unioned.

this effectively uses a relation `conditional_exprs(choice_id,e1,e2)` of `values(1,x+y,y/x),(2,x-y,x/y)`. this exact table cannot be defined exactly so, though, because it requires the `x` & `y` attributes of relation `x`; it must be defined in terms of `x`, as i've done above by joining `x` with the result of the union of alternatives.

* this can be refactored so that the `select` statements are views.
* any solution that joins alternatives *instead of unioning* e.g. `r(choice_id,x,y,fxy1,fxy2,gxy1,gxy2)` will ultimately still need to use a `case when` to select the correct choices e.g. `select x, y, case p when 1 then fxy1 when 2 fxy2 end, case p when 1 then gxy1 when 2 gxy2 end from r`, and is therefore a redundant version of `case when` that does not feature joining.
  ** an equivalent re-expression of the union solution is to define a relation `choices(choice_id,alt1,alt2,...)` as the union of alternates as done above (except probably defined as a view of `x`) then `select * from (select choice_id,* from x) join choices using (choice_id)`. this solution keeps separate `x` from associated functions of `x`, in case such a division were useful.
* there are many equivalent alternative expressions of the union pattern e.g. selecting the above attributes plus a choice number then unioning all, then having one `where cid=n` clause over the whole union: `select 1 as c,x+y as fxy ... union all select 2,x-y,... where cid=c`. that's actually a bit terser and more obvious. i wonder how its efficiency as computed by sqlite compares to the above version.
* the only way for a relation to have columns defined of multiple expressions (so that for any column some of its rows are computed of one expression, but other rows are computed of other expressions) is to `select` expressions then union [all] with other ``select``s that select different expressions.

just to be totally clear: join _relates_ points by relations of their attributes, but the attributes always remain separate! only `union [all]`, `intersect`, or `except` can combine, and by `where` choose from, alternative defining expressions for any given columns!

NOTE: short-circuiting is not a problem in sql because all computations are valid, whereas some are invalid, e.g. in lisp `(cond ((atom x) 0) ((= 5 (car x)) 1))` importantly evaluates the 2nd predicate only if the 1st fails; swapping these predicates' order would result in a program crash if `x` were an atom. no operations fail in sql; even dividing by 0 produces `null`. sql has many wonderful advantages over other languages, but this is probably sql's best feature, along with null propogation and the ability to choose `=` vs `is` to handle invalid or unknown values appropriately.

NOTE: if you need related conditions, such as are available in factor [lang] by using its `cond`, then you can use `with [recursive]` to pass state across calculations of predicates.

NOTE: that `case when` is short-circuiting is a bit concerning; it makes me doubt that it or `iif`'s values are computed in parallel, which could have some performance penalty.

.derivation of these techniques for expressing choice in sql

recall that general branchless form is Σ[(p,x)]p×x, where `p` are [expressions that evaluate to] logical values (0 or 1) and `x` are associated values. re-expressed by ring isomorphism, that's ⋃ [(p,x)∈R] (p∩x). again, though, this is not quite appropriate; the logical version, ⋁ p∧x, is, but the set version isn't because p∩x where `p` is a logical value is actually `select x from R where p`. generally ⋃ [x∈S] p(x) is equivalent to {x∈S|p(x)}, which is always a subset of S. X ∩ Y also always produces a subset of both X and Y, so this is how intersecting a predicate with a set p∩X is a re-expression of {x∈X|p(y)}∩X, which is always a subset of X. in summary, `select x from R where p` is equivalent to `cond` [lisp] that returns multiple values where `R` is an alist. to add an else value `y`, use `select x from R union all values(y) where p limit 1`. like in factor, sql can return multiple values by returning a relation of multiple attributes. this shows `limit` as a sub__sequence__ operator, unusual in a _set_-based language.

==== state machines & mutation

* `changes()` tells the number of rows modified by the last `delete`, `update`, or `insert` statement.
* `total_changes()` tells the sum of `changes()` since a db connection was opened
  ** you can track the sum of changes since an event by keeping a singleton relation, say `c(c)`, `update c set c=total_changes()` upon an event (probably set within a trigger), then `select total_changes()-c from c` to see the number of changes since the last event.

=== λ's

tl;dr: sql's functions/lambdas are queries—relation endomorphisms. queries may be composed, but only pointedly. the points are attributes, not data; or the points are arrays of data. mutate one of the input tables to change the function's input. sql does not support higher-order functions. one difference between sql's functions and lambdas is that sql identifies arguments only by name, not positional order.

a λ is a relation of inputs to outputs. the relational algebra uses relations generally, not partitioning attributes into inputs & outputs. sql is still reductionist, however. queries are sql's functions. their inputs are expressions following the syntax `select`. `from` does not concern inputs _per se_; it only scopes inputs. obviously queries' outputs are relations. queries thus relate relations. a query's inputs can be adjusted by mutating tables of that query's `from` clause e.g. pseudocode `def f(a,b,c) := (a+b)/c; f(1,2,3)` as sql `create view f as select (a+b)/c from params; insert into params values(1,2,3); select f`. sql does not support storing functions/queries/subprograms in tables. see _§first-class functions and conditionals_ for how to code without higher order functions.

in sql, queries are functions are subprograms. queries are evaluated by default, since that's all that a sql engine does. if a subprogram were stored as a relation or string or by any other encoding then we'd need `eval` to evaluate it; the use of `eval` sees queries as subprograms. if queries were encoded as relations, then queries (subprograms) would be modifiable by other queries and sql would thus then be metaprogrammable.

the _factor_ language does not have lambdas; instead it has only quoted subprograms and eval (called `call`.) naturally these quoted programs can be modified; the quoted program is just a list of other subprograms. this is similar to a variety of sql that would use relations to encode programs. the point is that evaluable subprograms are superior to lambdas/functions. in this model the only separation of "data vs code" is that `eval` is the only code, and is not data (since that'd be redundant,) and everything else is data.

the ability to modify subprograms then evaluate them is an ability that lambdas lack! lambdas are mere reductions, not true functions, anyway; we can't discuss the inverse of a λ as freely as we discuss a function's inverse in math. this is because mathematical functions may be implicitly defined by characteristic constraints whereas λ's are necessarily definite, explict relations of inputs & outputs. a λ's input symbols are mere stand-ins for literal data, not a predicate-quantified set of possible inputs. the fact that λ's definitions cannot be examined (except in picolisp) exemplifies their reductionist nature; functions can only be applied and composed; no other operations with them are supported. therefore lambdas do not enable anything new; they're worth is their convenience: they're function literals. they relieve one of the need to use crufty syntax to define a function, which is ironic because applicative languages still require data to be named unless it's used exactly once, in which case it can be inlined.

fns can be interpreted as a scoping mechanism: `f(a,b)` is seen as variables `a` & `b`, whose meaning is relative to each invocation of `f`. this can be encoded in sql as a table `f(a,b,e)` where each invocation of `f` is a row, and `e` is the location where the output should go. `a`, `b`, and `e` may each be literal values or _addresses_—values supporting join with another table.

within function definitions local data are stored on the stack (for stack langs) or in a function-local namespace (for applicative langs); in sql local data can be stored as array variables as attributes of a locally-scoped relation bound by a `with` clause. or you can just leave the local data in the query's output; the using query can use it if it needs; if not, there's no extra cost.

NOTE: scoping is a concern in sql queries, e.g. how a table alias (by `as`) is usable in a `where` clause. also, subqueries have access to outer queries identifiers.

as an ending aside, note that a side-effect of data-only programming is that all computation is delayed, since all computations are only data until explicitly passed to `eval`.

NOTE: the need for lambdas in `update` clauses is covered by old.attr & new.attr. see the sqlite docs for `update`.

=== functions

λ's have their own calculus. _function_ here refers to an expression written in terms of other data (inputs) but without the expectation that functions can be composed freely nor that inputs can be freely specified; a function may have constant inputs, which is useful when the input is an expression in terms of attributes, which may have multiple or variable (due to mutation) values. thus _function_ here ultimately refers to a referenced (named) expression. the following implement functions:

* generated columns (see sqlite's `gencol.html` doc), cached or not
* views

=== primary & foreign keys

see `rowidtable.html` and `withoutrowid.html`.

. a table A may have a primary key (uniquely identifying set of attributes), and may have a set of attributes that, in another table B, is a primary key; then: this attribute set is called a _foreign key_, B is called the _child_ table, and A is called the _referenced_ or _parent_ table. foreign key is its own concept (as opposed to a column that we can `join` on) because it can be used as a constraint in a table's schema, which enforces only proper now insertions & updates.
. primary keys are strange; they enforce uniqueness of each row. however, a row, like any single thing, generalizes to a group of things, which could be encoded as multiple rows sharing a common key. therefore i discourage primary keys but encourage indexes.
. foreign keys reject inserts that would violate the pk/fk relationship [constraint], called maintaining _referential integrity_. they add neither functionality nor efficiency; one can use `check` (table constraint) and triggers instead.

.foreign key example

[source,sql]
----
pragma foreign_keys = on; -- needed in sqlite; else foreign key clauses are not syntax errors, but foreign key constraints are ignored
create table t(id integer primary key autoincrement,
               x,
               dep integer,
               foreign key (dep) references t(id));
create index tdep on t(dep);                        -- make the upcoming join efficient
-- null `id` uses autoincrement
insert into t values(null,20,null);                 -- reference checking isn't done for null foreign keys
insert into t values(null,40,3);                    -- fails b/c there's no record in x whose id is 3
insert into t values(null,40,1);                    -- succeeds b/c we've successfully inserted one row already
select x.x,y.id from x join x as y on x.dep = y.id; -- returns one row: {x=40,x=20}
----

this example creates a table with a foreign key constraint on itself. `dep`, which may be null, since the `not null` constraint was not given, is an optional value to consider after we've considered `x`.

TODO: how to efficiently & elegantly select rows that are or are not referenced by a foreign key, e.g. here, selecting only rows that are not dependencies i.e. rows whse ``id``s are not in any other rows' `deps`? decent solutions: 1. have a boolean attribute flag this; 2. store un/flagged ones in their own table, this making the "foreign" in _foreign key_ appropriate; however, this would be horrible attribute duplication! the 2nd table would have all the same columns as the original! so really only (1) is a decent solution so far.

.foreign keys as lattice of relations on subset of attributes

x := (a b c)
y := (x z)

thus:

* a, b, c ∈ x (i.e. {a, b, c} ⊂ x)
* x, z ∈ y

[source,sql]
----
pragma foreign_keys = on;
create table x(id integer primary key autoincrement, -- always good to have an auto inc integral pk column in
                                                     -- every table in case of need to join or use as foreign key.
               a, b, c);
create table y(id integer primary key autoincrement, x, z, foreign key (x) references x(id));
insert into x values(null, 1, 2, 3);
insert into y values(null, 1, 20);
select a,b,c,z from y join x on y.x = x.id; -- (1,2,3,20)
----

rather than explicitly join `x` with `y` on each `select`, it's more sensible to create a view that represents the relation x ⊂ y:

[source,sql]
----
create view y_full(a,b,c,z) as select a,b,c,z from y join x on y.x = x.id
select * from y_full; -- (1,2,3,20)
----

you may name the view 'y' & the underlying table _y, or you may name the view e.g. y_full & the underlying one 'y'. consider that you cannot delete, insert, nor update a view; those must be done to the actual table.

=== pointwise `update`

TODO: carefully read sqlite's docs, then revise with wiser tech if appropriate.

sql does not support updating multiple rows by a map. instead we must set a set of values by another set of values; thus instead of `(map! f x)` we do `(set! x (f x))` but must associate each `x` with a corresponding `f(x)`; of course we do this by join:

[source,sql]
----
create table t(x);
insert into t values(1),(2),(3);
select x,x*10 from t;
┌───┬──────┐
│ x │ x*10 │
├───┼──────┤
│ 1 │ 10   │
│ 2 │ 20   │
│ 3 │ 30   │
└───┴──────┘
update t set x =         (select          x*10 as fx from t);            -- wrong: sets all in x to 10
update t set x = fx from (select x as id, x*10 as fx from t) where x=id; -- correctly sets each x to f(x)
----

the 1st form would be correct were sql to see `x` as a free symbol. unfortunately sql is limited to using literal data sets only.

in `update from`, pointwise relation is done by `x=id`. we could've given `fx` as a literal, without naming it: `update t set x = x*10 from (select x as id from t) where x=id;`. `update t set x = x*10 from t;` fails because `x` is ambiguous.

`update from` is a non-standard form yet commonly supported by sql engines. plain `update` can assign only one value to many rows. `update from` selects many rows then pointwise matches them to rows to be updated by the predicate given to `where`, effectively setting `t` to `t join (select ...)`.

were our sql engine (sqlite) not support `update from`, we'd need to execute an `update` statement for each row in a table e.g.

[source,factor]
----
"select x from t" query-rows
[ [ f ] [ ] bi "update t set x=? where x=?" query-exec ]
each
----

`where x=?` is the pointwise association of `x` with `f(x)` and `each` represents `∀x`. ideally, for efficiency, we'd collect all queries into a list then run them together in a single transaction.

=== zipping/joining of non-indexed relations is impossible in relational algebra

there appears to be no way to zip [n..] with an arbitrary relation. zipping is possible only by `join on`. `join` cannot work becasue that's cartesian product, which is not pointwise association. however, for `join on` to work, there must be a common attribute upon which to join, but no such attribute exists unless the relation is already indexed by [n..]!

neither recursive `select` nor `update` helps, either; to associate an index with a value would still require the value to already be indexed.

relalg is based on sets, not sequences; indexing rows would be a primitive. indeed, is sql it's accounted for by special attribute modifier `autoincrement`! thus we never need to zip; we can effectively implicitly make all sets sequences with order by nth insert. with the set being a sequence, it fulfills the requirement that allows it to be joined by index. it can thus be effectively zipped. really, though, it never makes sense, in general, to systematically order a set by arbitrary indices!

the `autoincrement` value is set to the nth insert; you may prefer `insert into t(id,x) values((select max(id)+1 from t),x)`. if a row is removed, then you will be left with a sequence with a missing element. how to handle that is your choice. for example, you may mark the removal by not actually removing the row, but by setting its value to `null`; or you may truly remove the row then update all of the indices greater than it to be each one their lesser.

however, we can use the window function `row_number`:

[source,sql]
----
select row_number() over (order by rowid rows current row) as uid,* from mylist;
----

will give unique row numbers to each item in `mylist`. suppose that mylist is a relation of variables, and you select a function of those variables, and you want to unit test whether each output is correct for the given inputs. then you'd need to join a table of expected outputs with the actual values; you must join on a common value, and you're not about to specify all of the function inputs as the join value. you can assign each row a uid, then see which uids correspond to which inputs & outputs, then manually specify a table to join on, of schema (uid,expected). i'm unsure whether `order by rowid` is redundant, but i know that specifying it guarantees deterministic uid/row association. note, however, that `with expected(uid,e) as (values...) select row_number() over (order by rowid rows current row) as uid,*,fn(...),e from mylist join expected using (uid);` is incorrect; that'll produce a row number for all in the selected table—the join of `mylist` & `expected`! we want `row_number()` to run only for rows of `mylist`; therefore `with expected(uid,e) as (values...) select * from (select row_number() over (order by rowid rows current row) as uid,*,fn(...) from test) join expected using (uid);` is correct.

=== [anti]patterns

==== encoding schemes (relational algebra)

* if you want to store a one-to-some map, e.g. parent -> {child1,child2,...}, then you can (but should not) use a "dependent" attribute. the attribute has multiple values, which may be encoded by multiple rows, e.g. `insert into t(...,dependent) values(...,1),(...,2),...`, but that's quite redundant. a more efficient encoding is to use `parent` instead of `dependent`: `insert into t(...,parent) values(...,1),(...,1),...`. this method inserts each of the parent and all its dependents only once, and all of the dependents' `parent` attributes are the same. in the `dependent` version, all of the parent's attributes except for `dependent` must be redundantly specified per each dependent!
  ** this doesn't generalize to multiple "parents" (tables referencing the "child" table), as that'd mean adding to the referenced table a column per referencing table.
  ** consider `s(id)` & `t(id,s references s(id))`. this is redundant; we can leverage the fact that `t` already has an `id`. this is the parent pattern again; if we were to describe this as JSON, then type `t` would contain subobject of type `s`. in sql it's better to have subset `s` reference superset `t`: `t(id)` & `s(t references t(id))`, which uses only one `id`. i use `references` (foreign keys) here when the referenced attribute is a primary key. if it isn't a primary key or even isn't unique, then we can still `join` on it and use triggers instead of trigger-like foreign key constraints such as `on delete [...]`.
    *** one fewer attribute upon which we'd join means one fewer index, too.
    *** this makes insertion order a bit more intuitive: rather than needing to insert the subsets firstly so that the superset can reference them, we insert the superset firstly, then the subsets secondly.
    *** this scheme is not possible if the superset may have a value other than a foreign key, e.g. `t(s)` where `s<0` is just a number, but `s>0` is a foreign key. the closest way to use the subset-references-superset encoding with this schema is for the subset to have an attribute for the superset's value, e.g. `t:{s:<int|{a:int,b:string}>}` (adt `T = S Int | AB Int String`) as `t(s)` & `s(id,s integer,a integer,b string)` constrained to `s is null or (a is null and b is null)`. the former version would require joining on a `case` clause, which would not use indexes, whereas the latter would join on `id` which, if indexed, would make for a much more efficient [left] join; the `case` would be deferred to after the join, performed on the joined table.
* using `like` is dubious. using `regex` is almost cetainly bad; you probably want a db designed specifically for text searching. string pattern matching does not use indexes and is thus does not make efficient queries.
* the semantic meaning of an attribute can depend on other attributes e.g. in `person(age integer,alive boolean)`, if `alive` then `age` means number of days alive; else it means number of days since death. furthermore, any of a row's attributes may be used or not depending on its other attributes' values.
* compress information as much as possible e.g. Y-M-D as just days since some arbitrary start date; that means that dates require only one column. the type `A or B` where A & B are both natural numbers can be encoded as a single integer whose sign determines whether A or B.
  ** you can, at least in sqlite, exploit `cast` for booleans; to interpret anything as a boolean sqlite ``cast``s it to an integer then checks equality with 0. thus you can make a string's first character /[1-9]/ to mark it as true; any other character will interpret the string as false.
* do not move from one table `a` to another `b` by `insert into a ... where p; delete from b where p`; instead, store all in one table `t`, and have an attribute that designates whether a row would belong to `a` or `b`; then filter on that to effectively get virtual subtables `a` & `b` from `t`.
* consider encoding schemes' supported partitioning schemes e.g. integer primary key can be generalized to indexed reals. reals can be partitioned by floor.
* unless uniqueness is required by some algebraic properties of your data, then feel free to see rows in a table as elements of a [multi]set. elements can be grouped [partitioned] by attributes (general prodicate, not just equality), which generalizes "thing at index" to "things with a given property", and set-theoretic operations can be performed for all predicates, and all predicates can be defined of multiple attributes [columns]
* to delete w/cascade a la foreign key w/o the relation technically being implemented as a foreign key, which would be sensible if the parent table referenced a table whose keys were non-unique, hence all of the referenced table's rows of a common predicate would be deleted:
  ** solution 1: `after delete` trigger
  ** solution 2: in sqlite (and maybe other sqls) by using `returning` (non-standand sql), though the returned value is not available as a sql expression; it's usable only by a client program e.g. `(let (rid (sql "delete from parent where id=? returning fk" pid)) (sql "delete from referenced where id=?" rid))`
  ** `on delete cascade` cascades when the *parent* (the referenced table, the one with [that must have] the primary key) row is deleted, not the child! e.g. with `create table a(a primary key, v); create table b(a references a(a) on delete cascade)` means that deleting one of `a`'s rows will implicitly delete one of `b`'s, but not _vice versa_; for the inverted case, you'll need a trigger; however, if you're considering that, you may want to reconsider how you're structuring your data; you should be able to use foreign key cascades. particularly, remember that it's better to have a `parent` attribute rather than `children`. using this design will help you better decide whether either of your tables should have a primary key. remember that foreign keys are one-to-many relationships; many in `b` may have common foreign keys; deleting their corresponding row in `a` will delete all those corresponding in `b`.

[TODO]
* how can we encode logical constraints as sql constraints or relations? common constraints are types, lengths, [recursive] predicates

==== sensible querying

sensible means elegant, which implies efficient.

* prefer join over subqueries e.g. `select a,(select b from t2 where a=b)) from t1`, or subqueries in a `case` clause; and prefer `in` over `=`, as these support multiple values
  ** the subquery-to-join refactoring pattern is `select (select x from t2 where p) from t1` becomes `select x from t1 join t2 where p`. if `x` & `y` don't have common predicate `p`, e.g. there's a unique `y` identified by `p`, but no `x` satisfies `p`, then use a left join and append `or x is null` to `p`; this new predicate will see the _rows_ for which it holds be returned, then from those rows either `x` or `y` will be chosen, and both will be available; `y` is always available, but `x` may be `null`. either way, the important thing is that the _row_ is in the result set.

.example: use join rather than subqueries
[source,sql]
----
select * from x;
┌───┬───┐
│ a │ b │
├───┼───┤
│ 1 │ 2 │
│ 3 │ 4 │
│ 5 │ 6 │
│ 7 │ 5 │
└───┴───┘
select * from y;
┌───┬────┐
│ b │ c  │
├───┼────┤
│ 2 │ 20 │
│ 5 │ 50 │
└───┴────┘
select a,b,case when c is not null then c else 20 end as 'c or 20' from x join y using (b);
┌───┬───┬─────────┐
│ a │ b │ c or 20 │
├───┼───┼─────────┤
│ 1 │ 2 │ 20      │
│ 7 │ 5 │ 50      │
└───┴───┴─────────┘
----

then use `where` to select a particular row. another possible condition is, instead of `c is not null`, `c>0` where `c<0` denotes an element of a sum type but `c>0` denotes that `c` is a product type, which in sql is encoded as a datum upon which we can join with a table of named tuples.

NOTE: `case <expr> when ...` uses a _base expression_; in this case, rather than predicates being tested against 0 or 1, they're tested against the base expression's result. `case x when y then r1 when z r2` is better than `case when x=y then r1 when x=z then r2` because it's terser and guarantees that `x` will be evaluated only once. the base expression form is to `case` [scheme] as the non-base-expr form is to `cond`.

''''

* suppose that table `t(x)` has one row and table `s(y)` has many rows. if you want to x+sum(y), do `select min(x)+sum(y) from t join s` (or use `max` instead of `min`); `x` will be `count(y)` duplicate rows, but to avoid bare columns, we select one of `x`'s rows, and only `min` and `max` select one row without regard to other rows.
* using a `distinct` query whose result attribute set contains an attribute having a primary key is redundant
* `distinct` means inefficiency in the form of pruning a query; we've asked for data, then discarded some of it—so why did we ask for it, then?! good schema & query design sees that `distinct` should not be used often.
* `having` is a predicate applied to groups produced by `group by` or aggregates which may implicitly be over one group of the whole set
* refactor nested queries, _(top-level,nested)_, into a flat one with join.
  ** this is ostensibly possible generally when _nested_ is:
    *** `from` one table (i.e. _correlated_)
    *** used in an `any`, `all`, or `exists` predicate
  ** if the query planner can determine that uncorrelated subquery returns at least one row, then the query planner should flatten.
  ** example: refactor `select x from t1 where x = (select y from t2 where p)` into `select x from t1 join t2 where x = y and p`
* if multiplicity is inconsiderable, then use `union all` instead of `union` because it's faster
* use indexes in `where` &al clauses. e.g. if `a` is indexed, `where f(a)=b` will not use the index! you'd need to have indexed `f(a)`. predicates like `between`, comparison operators, and `like` use indexes. some functions like `min` & `max` should use indexes, too.
* aliasing all tables and using qualified attributes is safer than not; it ensures that you don't accidentally use a wrong attribute that happens to be in scope from another table; if you were to use a qualified attribute name, then you'd get an error saying that that table does not have said attribute.
* `where` is evaluated after joins; if your join lacks results, consider moving your `where` predicate into the join clause
* `[not] in` is fine if you're using literals, but if its arg is a subquery, that's an antipattern; use `except` or left join with `where is [not] null` instead.

==== semantics

* booleans should always be encoded as a `boolean` type, if that's unfortunately what your sql engine uses; else 0 or 1. never use `null` or `not null` to encode booleans; it's simply incorrect no matter how you measure it.
* prefer fixed precision (often called `numeric` or `decimal sql`) instead of `float` or `real`. if your engine doesn't support that, then you can emulate it by a table with `num` and `den` columns; or just use fixed-point numerals.
  ** at least in sqlite, `floor` retains a real if any real was part of the return expression; if the expression was composed entirely of integers then `floor` is redundant and returns an integer.
* ``select``ing a mix of grouped or aggregate with non-grouped/aggregate data is handled differently by each sql engine. it's best to not mix; refactor queries into all-aggregate/grouped or all-not.
  ** sqlite, perhaps among others, calls non-aggregate columns among aggregates _bare_ columns
  ** in sqlite at least, bare columns' values are deterministic if only one of `max` or `min` aggregate functions is selected
  ** see §2.[4,5] of sqlite docs for `select`
* because sql table identifiers are not first-class (i.e. we cannot, in sql, programmatically generate a table name then reference it i.e. table names must be literal syntax rather than expressions), the only way to keep lisp-grade flexibility [dynamicism] is to use the lisp encoding or something that does not require creation, modification, or reference of a dynamic identifier.
* predicate evaluation order is nondeterministic e.g. in `isint(a) and a > 0` may fail with "can't apply > 0 to string" since that may be evaluated before `isint`. cte's are not a solution; they suffer from the same non-deterministic evaluation order. `case` is a solution because it has definite evaluation order.
* `update t set x = f(x) returning g(x)` returns `g(f(x))`
* `returning e` in a `delete` statement returns `e` for all deleted rows

.grouping & bare columns examples

in the following query, `a` is not a bare column because it is in the `group by` clause, so `a`'s value is properly determined in the result set:

[source,sql]
----
create table x(a,b);
insert into x values("x",1),("x",2),("y",34),("y",65);
select a,sum(b) from x group by a;
┌───┬────────┐
│ a │ sum(b) │
├───┼────────┤
│ x │ 3      │
│ y │ 99     │
└───┴────────┘
----

according to sqlite v3.39's `select` docs, §2.4, `group by` associates each row with a group. `select a,f(b) from t group by e` where `e` is an expression that uses [only?] `a`, should be a common idiom. idk how `select` behaves if `e` uses multiple column ids. 

`select a,1.0/count(x) from x` returns only one arbitrary column. `a` is bare here. fix: `select a,cnt from x join (select 1.0/count(*) as cnt from x)`.

==== using sql engines efficiently

* query attributes' order should match a compound index's. not sure if this applies to ordering only in `order by` or if it's important in the selection attributes, or elsewhere,...? or for which engines this is a concern. furthermore, i saw an example whose order was opposite the index, so what's that about?
* except in `count(*)`, the asterisk form is inefficient and its abstraction can cause problems when schemata are modified
* as tables become large, `exists` becomes faster than `distinct`. refactor `select distinct * from t1,t2 where t1.x=t2.y` into `select * from t1 where exists (select 0 from t2 where x=y)`. 0 is a dummy value; we use `exists` to determine whether its argument query is empty, and we _must_ `select` _something_, so we choose a dummy value.
* `having` forces the query planner to not use indexes. refactor `select x,y from t group by z having w` into `select x,y from t where w group by z`
* `in` is more efficient (b/c it uses indexes) than `or` *when the `in` list contains only constants*. e.g. `x=1 or x=2` is better as `x in (1,2)`
* columns that you'll join should be indexed

=== attributes with multiple values (wip)

NOTE: developing this example is halted until i thoroughly study relational algebra, and take a course in sql from a seasoned professional. also consider the dependents/parent fact stated above.

not all tables are rectangular. sometimes we want to store tables within other tables i.e. have attributes each with multiple values. to effectively do this, we store, in each row, a _pointer_ to another table's row, which will contain multiple data for that attribute. for this example, we'll consider the song _Gold Digger_, which two artists—Kanye West and Jamie Foxx—which sits in a table `songs(title,artist,album)`

one non-solution is storing artist as a string e.g. `"Kanye West feat. Jamie Foxx"` or `"Kanye West, Jamie Foxx"`, then searching on `artist like "Kanye West" and artist like "Jamie Foxx"`. this fails because `like` may match an inappropriate substring, e.g. i search for "James" (the artist who sang the 1990's hit, _Laid_) but also get songs by James Blunt, since `"James" like "James Blunt"`. the solution would be to use `=`, but that obviously fails.

we need a solution that properly stores multiple data as multiple data—namely rows. thus `artist` would be a foreign key to an `artsts` table and there'd be, for every song, one row per artist, e.g. `insert into songs(title,artist,album) ("Gold Digger",1,1),("Gold Digger",2,1)` which reference `(1,"Kanye West"),(2,"Jamie Foxx")` in `artists`. the full code follows:

[source,sql]
----
create table songs(id integer primary key, title);
create table albums(id integer primary key, album);
create table artists(id integer primary key, artist);
create table lib(title integer references songs(id), artist integer references artists(id), album references albums(id));
insert into artists values(1,"Kanye West")       , (2,"Jamie Foxx"), (3,"James"),        (4,"James Blunt");
insert into albums  values(1,"Late Registration"), (2,"The 90's")  , (3,"Back to Bedlam");
insert into songs   values(1,"Gold Digger")      , (2,"Laid")      , (3,"Billy");
insert into lib(title,artist,album) values(1,1,1),(1,2,1),(2,3,2),(3,4,3);
select songs.title,artists.artist,albums.album from lib join songs on lib.title=songs.id
                                                        join artists on lib.artist=artists.id
                                                        join albums on lib.album=albums.id;
┌─────────────┬─────────────┬───────────────────┐
│    title    │    name     │       title       │
├─────────────┼─────────────┼───────────────────┤
│ Gold Digger │ Kanye West  │ Late Registration │
│ Gold Digger │ Jamie Foxx  │ Late Registration │
│ Laid        │ James       │ The 90's          │
│ Billy       │ James Blunt │ Back to Bedlam    │
└─────────────┴─────────────┴───────────────────┘
select songs.title,artists.artist,albums.album from lib join songs on lib.title=songs.id
                                                        join artists on lib.artist=artists.id
                                                        join albums on lib.album=albums.id
                                               where artists.artist="Kanye West" or artists.artist="Jamie Foxx";
-- NEXT: vary the recursive query to produce #(("Gold Digger", "Kanye West, Jamie Foxx", "Late Registration"))
with recursive
  x(id,ps) as (select id+1,value from c where id=1),
  acc(id,ps) as (select * from x
                 union all
                 select id+1,printf("%s %s",ps,value)
                 from acc natural join (select * from c where id>1))
select ps from acc order by id desc limit 1;
----

. we need to store each song as having its own `id` because it's possible, though unlikely, that two artists that did a song together also each did two different songs of the same name on different albums. actually, even crazier: for a few (artist,album)s in my library, there are two different songs of the same name.

.no need to organize data

if you've having trouble organizing your table schemata, you can always use a simple but inefficient encoding in one table. considering the last example differently: say that you want a music db, and you first suppose that artists have albums, and albums have songs; thus your songs should foreign key ref an album, and likewise an album should ref an artist. simple. oh, wait, though; some songs have no albums, and some albums (or songs) have multiple artists. rather than worry about how to "solve this problem," just `create table songs(name string, artist string, album string)` without worrying about foreign keys. any song can now support multiple artists by using multiple rows e.g. `insert into songs values("Gold Digger","Kanye West","Late Registration"),("Gold Digger","Jamie Foxx","Late Registration")`. this encoding is less efficient, but simple, and works; it's therefore useful for encoding data while you're sill developing your database. obviously we can make this more efficient just by making `album` an `integer` which is a foreign key to a table `albums(id,name string)`.

.alternative: lisp encoding

[source,sql]
----
-- general lisp encoding tables: lists & atoms
create table lists(id integer primary key, parent integer, foreign key (parent) references lists(id));
create table atoms(value,                  parent integer, foreign key (parent) references lists(id));

-- song-specific stuff. by lisp alists, this would be (songs . ((name album)))
create table songs(name string, artist string, album string, foreign key (album) references albums(name));
insert into lists values(1,null),(2,1);
insert into atoms(a,1),(b,1),(c,2),(d,1);
---- 

NOTE: lisp encoding cannot accomodate multiple indexes.

=== relational algebra

.terminology

[options="header"]
|===================================================
| relational algebra | common name or implementation
| tuple              | row
| attribute          | column (w/type if applicable)
| relation/selection | table
|===================================================

* _constraints_ on a table or column [attribute], e.g. `unique`, `not null`, `foreign key`, `primary key`. they're verify-only constraints, not adding functionality, and so should be avoided (except indexes, should those be considered constraints)
* tuples are unordered, instead being expressed as attribute-tagged unions
* a tuple's set of attributes is called its _heading_, _domain identifying list_, or when as an argument to projection (see below,) a _projection list_. the heading is a list of indexes, whether ordinal or nominal.
* a set of tuples sharing a common heading is called a _body_
* a relation can thus be partitioned into a heading and body

degree:: number of attributes
schema:: heading with constraints (all needed to produce a selection)

.primitive operations

TODO: continue from ~/Downloads/pacific75-eval.pdf

union-compatible:: having the same attribute (column) sets

* link:https://en.wikipedia.org/wiki/Selection_(relational_algebra)[selection (aka _restriction_)] (σ_pred(R)): filter by predicate
* link:https://en.wikipedia.org/wiki/Projection_(relational_algebra)[projection] (π) of a heading onto a table, π_L(R) := {r[L]: r ∈ R} is just a subset of R found by restricting to attributes L, which must be a subset of R's original attributes; ior a projection may be a map over R's values, e.g. `select a+2 from R` maps `(+2)` over a ∈ R. only the column space is concerned; the number of rows is unaffected.
* link:https://en.wikipedia.org/wiki/Rename_(relational_algebra)[rename ρ]: rename an attribute
* [flattened cartesian] product (×). TODO: test: in sql lhs & rhs tables must have mutually exclusive attribute sets.
* set difference (aka _relative complement_) (\). requires union-compatiblity and may be defined in terms of union: given relations R & S of equal degree _n_, R \ S = (σ_(r[1] ≠ s[1] ∨ ... ∨ r[n] ≠ s[n])(S)).
* union (∪). union-compatible.
* join
  * natural (⋈): defined when lhs & rhs share exactly one attribute. attribute set is the union of lhs' & rhs' attribute sets. (e.g. join a,b,c and b,c,d = a,b,b,c,c,d)
  * inner (intersection in relation algebra): natural but without repeated columns [WRONG] (e.g. join a,b,c and b,c,d = a,b,c,d). union-compatible? not in sql! or perhaps this could be said to be a succession of projection then union.
  * outer: flattened cartesian product
  * left or right
* division: for relations R & S of headings A & B (without repitition) of degrees m & n respectively, the division R[A÷B]S is a subset of π_A'(R), viz {r[A']: r ∈ R ∧ ∀s ∈ S ∃r' ∈ R : r[A'] = r'[A'] ∧ r'[A] = s[B]}. definitions vary when S is null.

the _theta join_ is a non-primitive operation: x θ y = σ_pred(x ⋈ y), expressed in sql as `select attrs from x natural join y where pred;`

the relational algebra is closed under all these operations.

NOTE: *for the love of god, use `BEGIN TRANSACTION` &al*

=== common semantics

* sqlite stores table schemata as strings rather than as tables (despite the style of `pragma_table_info(t)`'s output); this is a design oversight that must be dealt with in a hacky way (see the `alter table` docs)
* `0` is falsy in sqlite. a value's truthiness is determined by whether its coercion to an integer produces `0` or not. `null` is null, a value that represnts a lack of sensible information; it's neither truthy nor falsy. `select x from t where x` will select truthy `x`; `... not x` will select where `x=0`. in neither case will any `x` with `null` values be returned.
* when a sqlite db can be opened read-only, we can still create and modify temporary tables
* everything is a table (multiset of tuples whose positions may be bound to, in a given conext, a name) viz the results of statements, which can be enclosed in parens, e.g. `select * from (select * from mytbl) t`
  * such statements are called _derived tables_
  * thus tables can be locally bound. this allows passing multiple data, e.g. `select * from (values(1),(2),(3)) t` to mean scheme `(values 1 2 3)`
    * this is apparently equivalent to `select * from (select 1 as a from dual union all; select 2 as a from dual union all; select 3 as a from dual) t`
  * _rows_ have no special meaning; they're just singleton tables. all operations are over tables.
    * generally all operations are on the entire table
* if both args to `/` are integers, then `/` is integer division. `cast(expr as real)/cast(expr as real)` to ensure floating point division. however, it's best to use rational arithmetic (`numeric` or `decimal sql` types, if supported) or fixed point arithmetic, instead of floating point.

=== joins

all joins are refinements of cartesian product. `join` (or comma) is cartesian product. `join on <pred>` filters cartesian product to those matching `pred`. `join using attrs ...` is shorthand for `join on t1.attr=t2.attr ...`. `natural join` is shorthand for `join using X` where `X` is the intersection of tables' attributes.

* `inner` & `cross` are redundant; just say `join`. however, as a non-standard sqlite feature, `cross` prevents query optimizer from reordering input tables.
  ** `cross` join means "cross product" as in cartesian product
* `outer` applies only to `left`, `full`, and `right` joins. idk what `outer` is.
  ** `inner` is inapplicable to `left`, `full`, and `right` joins. 
* `left` join is just `join` unless an `on` or `using` clause is provided.
* `full` & `right` are currently unsupported in sqlite; at least `right` is redundant: `x right join y <join-clause>` = `y left join x <join-clause>`

.examples
[source,sql]
----
-- kinda odd that we can't just do create tablet(a1,...) as (values...)
create table x as with x(a,b) as (values(1,2),("x","y")) select * from x;
create table y as with x(o,b) as (values(6,"y"),(100,2),(101,"B")) select * from x;
-- it's honestly probably nicer to instead use separate create table & insert statements
select * from x left join y using (b);
┌───┬───┬─────┐
│ a │ b │  o  │
├───┼───┼─────┤
│ 1 │ 2 │ 100 │
│ x │ y │ 6   │
└───┴───┴─────┘
select * from y left join x using (b);
┌─────┬───┬───┐
│  o  │ b │ a │
├─────┼───┼───┤
│ 6   │ y │ x │
│ 100 │ 2 │ 1 │
│ 101 │ B │   │ -- (101,B,NIL)
└─────┴───┴───┘
----

in `a left join b`, all of `a`'s rows are present, but some of their corresponding `b` attributes may be null, namely when there _are no_ corresponding `b` attributes.

=== syntax

* comments: `-- ... ` for single line, `/* ... */` for multiline
* single quotes for string literals
* double quotes for identifiers that may contain spaces or be reserved words
* neither standard nor sqlite-specific, but specifically of the `sqlite3(1)` repl, are _dot-commands_. see `cli.html#dotcmd` and _§sqlite everywhere_.
* blob strings can be specified as hex by a leading `x`. they must be byte-sized; `x'a'` is an unrecognized token; you must do `x'0a'`.
* escape sequences e.g. `\n` are unsupported; `'hello\njello'` is literal. a single quote repeated, `''`, represents a single quote in a string literal. however, you can say `'hello' || x'0a' || 'jello'`.
* integers may be specified in hex by leading `0x`
* float literals may be specified in engineer's notation

`table.attr` disambiguates when `attr` is shared by multiple tables; otherwise attr is resolved against the table of the `from` clause.

.basic operators
|======================================================================
| &          | bitwise and
| \|         | bitwise or
| ^          | bitwise xor
| += &al, %= | assignment can be used for variables bound in a funcbody
| &=         | bitwise and assignment
| ^-=        | bitwise or assignment
| \|*=       | bitwise xor assignment
| \|\|       | strcat (casts both args to strings if needed)
|======================================================================

=== `with` & recursion (common table expression (CTE) subquery refactoring)

see §3 of the sqlite docs' `lang_with.html` page for exact description of recursion structure & evaluation.

this is how we do local binds.

* supports recursion
* exists temporarily: discarded after the statement that uses its binds
* considered a cleaner alternative to temp tables
* alternative to views (prob like `let*` in alt to `define` in funcbods)
* repeated aggregations, e.g. avg of maxes
* "overcome constraints such as what `select` has, e.g. non-deterministic `group by`"

.`let*`
[source,sql]
----
with
  t1(v1, v2) as (select 1, 2),
  t2(w1, w2) as (select v1 * 2, v2 * 2 from t1)
select *
from t1, t2
----

produces

[options="header"]
|==================
| v1 | v2 | w1 | w2
| 1  | 2  | 2  | 4
|==================

could use `values` instead of `select`; `values` is just `select` but more efficient and without a limit on number of supported rows.

.`letrec` generator example
[source,sql]
----
with recursive t(v) as (values(1) union all select v+1 from t where v < 5) select v from t;
----

NOTE: despite the SQL99 standard spec, sqlite appropriately does not require `recursive` in order for a cte to be recursive.

this does not produce (1)++(2),(3)++(3),(4),(5)++.... `select` does not return the whole table on each iteration; as described in `with§3`, one item is taken from a queue (step 2a); `select` is a misnomer in recursive queries.

produces a column `v` with five rows of values 1 through 5, effectively equal to haskell `take 5 (Data.List.NonEmpty.unfoldr (\n -> (n, Just $ n + 1)) 1)`. the definition of `t` is unbounded; the bound is in `limit 5`; therefore locally bound tables (at least when bound with `recursive`) are not stricted evaluated before the body of the `select` statement.

.example: trace predecessors/ancestors

this works for a tree, or more generally a dag.

[source,sql]
----
create table x(id integer, prev integer, val integer);
insert into x values(1,null,20),(2,1,40),(3,2,50),(4,2,100),(5,4,200),(6,3,400),(6,4,300),(7,6,1000);
select * from x;
┌────┬──────┬──────┐
│ id │ prev │ val  │
├────┼──────┼──────┤
│ 1  │      │ 20   │
│ 2  │ 1    │ 40   │
│ 3  │ 2    │ 50   │
│ 4  │ 2    │ 100  │
│ 5  │ 4    │ 200  │
│ 6  │ 3    │ 400  │
│ 6  │ 4    │ 300  │
│ 7  │ 6    │ 1000 │
└────┴──────┴──────┘
with recursive y(id,prev,val) as (select * from x where id=4
                                  union -- union all produces some redundancies, since the graph is a dag
                                        -- rather than a mere tree
                                  select x.id,x.prev,x.val from y join x on y.prev=x.id)
select * from y;
┌────┬──────┬─────┐
│ id │ prev │ val │
├────┼──────┼─────┤
│ 4  │ 2    │ 100 │
│ 2  │ 1    │ 40  │
│ 1  │      │ 20  │
└────┴──────┴─────┘
----

maybe unexpectedly, we select from `x`, not `y`! `[...] select y.id,y.prev,y.val from [..]` is unbounded recursion.

.example: trace successors/descendants

this works for a tree, or more generally a dag.

for descendants instead of ancestors, simply swap `y.prev=x.id` with `x.prev=y.id`:

[source,sql]
----
with recursive y(id,prev,val) as (select * from x where id=4
                                  union
                                  select x.id,x.prev,x.val from y join x on y.id=x.prev)
select * from y;
┌────┬──────┬──────┐
│ id │ prev │ val  │
├────┼──────┼──────┤
│ 4  │ 2    │ 100  │
│ 5  │ 4    │ 200  │
│ 6  │ 4    │ 300  │
│ 7  │ 6    │ 1000 │
└────┴──────┴──────┘
----

see §3.[3-5] for very useful graph/traversal considerations.

=== miscellaneous sql examples

never assume design patterns; instead, only consider the axes and whether they're related e.g. with data `x`, y`, & `z`, is the idea of a `(x,y,z)` point sensible?

==== folds

a fold is a stateful traversal. in reldbs, state is obviously stored, as is everything, in relations. a recursive `with` may be more efficient, however. even more efficient is a fold written as a runtime-loadable extension written in c, loaded by sqlite from a shared library.

`foldl (\a b -> a ++ b) xs`:

[source,sql]
----
create table c(id integer primary key autoincrement, value string);
insert into c(value) values("hello"),("there"),("my"),("good"),("friend");

-- with trim, to remove the leading space character
with recursive acc(id,ps) as (values(1,"") -- initial value (base case)
                              union all
                              select id+1,printf("%s %s",ps,value) from acc natural join c) -- recursive case
select trim(ps) from acc
order by id desc limit 1; -- acc is a scan; get the last element to be effectively a fold

-- proper general solution for folds whose initial object must be the input lists' 1st element
with recursive
  x(id,ps) as (select id+1,value from c where id=1),
  acc(id,ps) as (select * from x
                 union all
                 select id+1,printf("%s %s",ps,value)
                 from acc natural join (select * from c where id>1))
select ps from acc order by id desc limit 1;
----

* we really do use functional style here. we can't use one `with` clause over both an `update` and a `select` statement. rather than use `update` (a stateful, non-functional style), we can use recursion and nested ``select``s. each row is defined in terms of its predecessor.
* `acc` is the named tuple of the fold. `printf` (`format` in other sql engines) is used for string concatenation since sqlite has no separate such function.
* the proper solution binds `x` b/c `select * from c limit 1 union all ...` is invalid syntax; we can't use `limit` there, though `where` is fine there
* i'ven't yet ``explain``ed this query to see its efficiency
* we can't use aggregate functions in predicates; therefore `where id=max(id)` is not a valid alternative to `order by id desc limit 1`

of course, _this_ fold is more easily done by the aggregate `group_concat`, but this example serves generally, when an aggregate may not be already written for it.

==== functions

views (especially defined by cte) can represent fns. `create view f(f) as select sin(x + y) from t` is the sql version of `f x y = map (\[x,y] -> sin x y) sql(conn,"select x,y from t")` haskell-like pseudo-code. yes, `f` is the name of the view and the name of its single column. if you've ever defining a fn in code that's using a sql connection, think about how easily you could express that fn as a sql view. views are a sort of variety of prepared statement, except that they're standard sql and are stored by the sql engine internally.

pointwise-with-aggregate array programming example:

[source,sql]
----
create table things(name string, value real);
insert into things values("a",40),("b",16),("c",5),("d",4);
-- equal weight to all things
with weight(weight) as (select 1.0/count(*) from things) select name, weight, weight*value as adjusted from weight, things;
┌──────┬────────┬──────────┐
│ name │ weight │ adjusted │
├──────┼────────┼──────────┤
│ a    │ 0.25   │ 10.0     │
│ b    │ 0.25   │ 4.0      │
│ c    │ 0.25   │ 1.25     │
│ d    │ 0.25   │ 1.0      │
└──────┴────────┴──────────┘
----

notice that the ordinary join (cartesian product) of a single value with a row of values is effectively equivalent to scalar expansion (or w/e it's called) in apl `0.25 × values`.

==== local binds

[source,haskell]
----
a = 9      -- dummy value
let a = 20 -- shadow a
 in a + 4  -- returns 24
----

[source,sql]
----
create table scope(a);        -- unlike haskell, we must define a in a table. its dummy value is implicitly [].
with scope(a) as (values(20)) -- local scope(a) shadows global one for duration of this select statement
  select a + 4 from scope;
----

* by naming tables `scope` i mean that tables are scoping mechanisms
* `with` is not properly its own clause; it's a clause of the `insert` statement, as well as `select`, `delete`, & `update`

sql binds cannot be <what?>, e.g. in a `create trigger` statement's final clause where it takes a sequence of statements, each statement may have each its own local binds, but local binds over all statements are not supported. instead, you'll need to create a (global) table then have the body statements use it, then drop or reset it as the last body statement, if appropriate. the table may be created before the trigger (being just a global table used only in the trigger) or may be created as the first statement of the trigger's body.

the ability to choose either demonstrates that local binds, like all scoping mechanisms, are not necessary, but instead exist only as a namespace management tool, namely to allow multiple homonomic data across different contexts. sql is unique in that all data must exist in tables, and tables are scoped, so namespacing is more of a constraint than an option. in contexts with homonomic data, sql gives us `as` clauses to disambiguate.

==== cond/case

alists are obviously encoded in sql by schema `alist(k,v)`. then `select v from alist where k=?`

==== find

`find p xs` = `first? (filter p xs)`. in sql: `select x from t where p order by i limit 1`

==== one-to-many relations

to associate e.g. each song with many tags, `[(song,[tags])]`, use sql schemata `song(songid integer primary key, songname string)` & `tags(songid integer, tag string)`: `select song,group_concat(tag,",") from song join tags using (songid) group by song`. in sql `(k,[v])` is encoded as `[(k,v)]`. `group by` and/or aggregate [window] functions work well with 1:n relations; in fact, `group by k` is the dual of distribution of `(k,)` over `[v]`.

remember that we cannot use `rowid` as a foreign key because `rowid` is not a primary key.

remember to state everything in the singular; this will help you remember that everything is flat/array in sql.

NOTE: metadata may not need to be exact e.g. though we can tag songs with multiple tags and certainly have correct results, we may tolerate `tags` as a string of delimited tags and `select song where tags like ?`. this isn't exact, but if the user is going to manually look through the results of a query and modify or curate it, then being exact isn't really beneficial. as another example, if instead of songs we've a database of titled text documents, `docs(title string, body string)`, then although we may have tags (like is usefully done in factor's docs), there's hardly any sense in tagging an article with tags that're already present in its title or body; if you're searching through docs, you'll probably search through the title, body, and tags altogether, ordering by some match strength measure. in fact, there may be only miscellaneous facts that don't belong to any article; in this acse `title` may be null and they may have only tags! tags are good for searching, and titles for displaying! if your db is huge, though, then you can't well index on tags as delimited strings, so still be sensible. you also can't add tags just by ``insert``ing; you'll need to use `update` & `||`.

anyway, *don't waste time adding redundant information to your database. schema are hard to change or work with, but queries are very flexible & simple to construct & modify! however poor your schemata, thorough understanding of queries will make schemata elegance inconsiderable.* this applies also to the efficiency statement at the end of the prior paragraph: if your db grows large, just create a new table with appropriate schema or add an index and populate the table with an `update` statement. there's really no such thing as sunk cost in sql, so don't worry.

==== plurality

a common interpretation of a thing is that the thing is single yet composed of multiple things, e.g. an xml element may have many children. in sql we not say that the element contains children, but instead that the "children" are just a set of things that support a predicate that groups them. an obvious predicate is `id=?` where all in the set share a common value for the `id` attribute. in sql this cannot be done by a primary key, since each row must have a unique primary key value. we must therefore use what's conceptually a primary key as technically not a primary key. this is fine, since foreign keys & primary keys don't add any ability; they only check constraints and enable cascading mutations, but `unique`, indexes, and triggers are perfect alternatives.

i suggest the relation naming convention that plurality corresponds to each row. for example, a relation named `widget` should be a relation each of whose row is a widget. a relation named `widgets` should have each row represent some aggregate expression of the `widget` relation.

==== first-class functions

there are no first-class functions in sql; _all_ function use is pointed. the only way to "compose functions" is `f(g(a))`; all functions are tied to their literal arguments. therefore whereas in functional code you may find a function to an identifier then use it in various places on various args (shape (f,[args])), in sql you must enumerate all function & arg pairs (shape [(f,[args])]). notice that this is the same pattern that sql uses to encode lists: what is most langs is (f,[a]) is expressed in sql as that but distributed: [(f,a)].

==== conditionality/branching/choice

technique for expressing conditionality is covered in _§sql technique_; this section expresses some real-world scheme code (which also uses some sql) as sql so that you can compare the two.

the scheme code (racket, to be specific) to translate:

[source,scheme]
----
;; cc[opp]ext is current candle's extreme value. ext is historical.
(let-values ([(at-least-as-attractive? more-attractive ccext ccoppext) (if (> count 0)
                                                                       (values <= min low high)
                                                                       (values >= max high low))])
  (and (or (sql-null? stop)
           (if (sql-null? ext)
               (unless (at-least-as-attractive? stop ccoppext)
                 (query-exec D "update orders set stop = null where oid = ?" oid))
               (let-values ([(newext test-limit?) (if (>= (abs (- ext ccoppext)) (abs stop))
                                                      (values sql-null #t)
                                                      (values (more-attractive ext ccext) #f))])
                 (query-exec D "update orders set ext = ? where oid = ?" newext oid)
                 test-limit?)))
       (let ([most-attractive (more-attractive open limit)])
         (and (at-least-as-attractive? ccext limit)
              `(,most-attractive . ,o)))))
----

we'll translate it into both the `case when` form (using `iif` since there're only two cases here) and a `union` form. the code operates on one order, selected from the `orders` relation. it then uses this order to update `orders`. this is very bad design; one should never extract orders from a sql db then perform logic on it then use it to update the db! this code as expressed in sql would implicitly do it for all orders, moving these predicates into a `where` clause. there also will be no need for `oid = ?` because the orders will be known already.

the variable functions are `<=`, `>=`, `min`, & `max`. the following attributes are of the `orders` relation: `low`, `high`, `count`, `stop`, `limit`, `ext`, & `oid`. where they're used in the scheme code, they've been extracted from a sql query then bound to identifiers in scheme homonymous with their sql representations.

scheme uses control flow forms to choose when to execute action `query-exec`. in sql, optional actions (mutations) are possible exclusively via triggers. we'll flatten the nested statements and partition the code into binds & computed values (subsets of a relation), and optional actions (triggers). to convert the function binds, first identify the arguments of each of the locally bound functions `at-least-as-attractive?` & `more-attractive`:

* at-least-as-attractive?: (stop,ccoppext),(ccext,limit)
* more-attractive:         (ext ,ccext)   ,(open ,limit)

clearly the scheme `values` special form corresponds to sql `choices(poscnt,alat,ma,ccext,ccoppext)` with `values(1,<=,min,low,high),(0,>=,max,high,low))`. but of course we can't store fns in relations, so we combine the args and functions:

[options="header"]
|=================================================================================
| poscnt | alat_stop_ccoppext | alat_ccext_limit | ma_ext_ccext  | ma_open_limit
|      1 | stop <= high       | low  <= limit    | min(ext,low)  | min(open,limit)
|      0 | stop >= low        | high >= limit    | max(ext,high) | max(open,limit)
|=================================================================================

because `alat_stop_ccoppext` is used only before `unless`, i.e. `when . not`, it should be negated and renamed, yielding

[options="header"]
|=================================================================================
| poscnt | stop_lat_ccoppext | ccext_alata_limit | ma_ext_ccext  | ma_open_limit
|      1 | stop > high       | low  <= limit     | min(ext,low)  | min(open,limit)
|      0 | stop < low        | high >= limit     | max(ext,high) | max(open,limit)
|=================================================================================

with _lat_ abbreviating _less attractive than_ and _alata_ abbreviating _at least as attractive as_.

it's more coupled, which i, before i translated the scheme code to sql, thought would be bad, but i actually prefer this because i can see _all_ of the places in which `at-least-as-attractive?` `more-attractive` `ccext`, & `ccoppext` are used at once rather than needing to read through nested code! this is appropriate because these data _are_ coupled! it's not like i'm defining separate functions in a library. i'm binding particular data & functions for a particular purpose, for their copuled use in a small block of code. also the tabular formatting is clean.

i'm actually questioning whether abstraction is a good idea. it certainly, practically by definition, reduces redundant information by factoring [algebra] code. however, by so doing, it introduces complexity when reading the code. with too many concepts, the whole of a program can be hard to see. i guess that abstraction is good for writing libraries, but bad for writing programs (effectful sequences). i suggest that bit twiddling methods (see codenotes/bit-twiddling-and-encoding.adoc) offer compression and elegance better than linguistic abstraction.

anyway, that table is, in sql:

[source,sql]
----
create view v1(A,...,          stop_lat_ccoppext, ccext_alata_limit, ma_ext_ccext,  ma_open_limit) as (
  select * from
  (select A,...,1 as poscount, stop>high,         low<=limit,        min(ext,low),  min(open,limit) from orders
   union all
   select A,...,0            , stop<low,          high>=limit,       max(ext,high), max(open,limit) from orders)
  where count>0=poscnt
)
----

where `A,...` is any of `order`'s attributes that i may want to retain (in scope) verbatim wherever this query is used. were `A,...` be `*` then this view would only add information to `orders`. this query is equivalently expressed as:

[source,sql]
----
create view v1(A,...,stop_lat_ccoppext,ccext_alata_limit,ma_ext_ccext,ma_open_limit) as (
  select * from
         iif(count>0, stop>high      , stop<low)        as a,
         iif(count>0, low<=limit     , high>=limit)     as b,
         iif(count>0, min(ext,low)   , max(ext,high))   as c,
         iif(count>0, min(open,limit), max(open,limit)) as d
  from orders
)
----

* the union one has 1 `union all`, clearly representing 2 choices, whereas the `iif` one generally selects 4 attributes each of which can have a different predicate.
* the `iif` and `union` versions are basically matrix transposes of each other.
* i can't use `with` because `count>0` must be evaluated per row. well, i _could_ use `with`, but i'd need to join it with orders to associate each row with its `count>0` value. that's kinda ugly, probably inefficient, and not generally possible, as it'd require each of `orders`'s rows to have some unique value.

next we finish the translation by translating the conditional `query-exec`'s by using triggers. the triggers will use this view.

[source,sql]
----
create trigger update_stop after insert on orders
begin update orders set (stop,ext) = (iif(stop notnull and ext is null
                                                       and iif(count>0, stop>high, stop<low) -- stop less attractive than ccoppext?
                                         ,null  -- set stop to null
                                         ,stop) -- don't change stop (literally set stop to itself)
                                     ,iif(abs(ext - ccoppext) >= abs(stop), null, iif(count>0, min(ext,low), max(ext,high)))); -- whichever of ext or ccext is more attractive
      -- <some action> about *,iif(count>0, min(open,limit), max(open,limit)) from orders where stop is null or ext is null or abs(ext - ccoppext) >= abs(stop)
                                                                                                and iif(count>0, low<=limit, high>=limit); -- ccext is at least as attractive as limit
      -- see comment below about scheme returning values
end
----

ok, looks like the triggers was actually one trigger (see 1st note below) and it didn't use the view! instead it inlined the view's expressions and identified them by comments!

NOTE: triggers have scope, and their scope is limited to the table that triggers the trigger. however, trigger's only clause that uses scope is `when`; triggers' bodies' statements are all top-level and unrelated to the table that triggers the trigger. this usually isn't a concern because a `when` condition can be put inside the `where` clause of `delete`, `insert`, or `update` anyway.

. i combined the two `query-exec`'s, which was possible because they're both ``update``s of the same table. i exploited that `update set x=y where p` is equivalent to `update set x=iif(p,y,x)`.
  .. for `update` we can use `iif` instead of `where` to effectively have a `where` per attribute.
. i inlined `stop_lat_ccoppext` anyway! may as well inline it rather tahn define it in a view, arbitrarily coupled with other data.
. `were oid = oid` is redundant! it was needed in the scheme version only b/c the order was pulled from the db, then used in scheme code, then i needed to identify it in the db again; here, since all logic is done in sql exclusively, each order is already identified uniquely by ordinary implicit row-by-row traversal through a relation.
. sqlite supports only `for each row` (it's implied if not syntactically specified); `when`'s (not featured in this example) predicate is evaluated per row *that is modified in the triggering table. this is generally unrelated to which rows we're modifying in any of the statements in the trigger's body, and we must specify predicates for both `when` in the trigger and `where` in each of the trigger's body's statements!*
  .. `when` was not featured in this example; that's because the trigger evaluates per update of `orders` (per statement, actually; see next note). however, `update`'s predicate is evaluated for each of `orders`'s rows _after_ the insert into `orders`.
  .. because these should be per-statement triggers, i may use another language (scheme in this case) to control flow. the program is semantically equivalent regardless of whether the trigger is run per row or per trigger because the `update`'s new value is not in terms of that same attribute's old value.
. you can't use relations bound by `with` across multiple statements within a trigger, but you can create a table to hold that value, then update the table across a trigger's statements.

the scheme code returns a value (namely false or a pair.) values are only useful when passed as arguments to other functions, which must ultimately be passed to some effectful (i/o) function. sql triggers are actions; they occur when a (named) table is modified and always have the _effect_ of modifying another table. as it turns-out, this code's return value is passed to a function that passes it to a sql query; thus the trigger actually invokes another trigger, but that's outside the scope of this example.

sql, being declarative and symmetric over data (i.e. it's an array language) greatly frees the developer from needing to care about control flow; the only control flow in sql is achieved by the two looping constructs: triggers and recursive `with`. loops are `if` with jump. `if` is done in sql by `where` (filter, i.e. pure `if` mapped over a set) and `case when`/`iif`. looping over a set is implied by sql being an array language.

*any of sql's ugliness is well worth sql's model's simplicity & elegance, because it makes easier both reasoning (designing the program) and specifying the logic as code, and its implicit control flow removes a great class of common coding (again, both reasoning and specifying as code) errors.*

==== `filter (any p) xs`

given a table `t`:

┌──────┬────┐
│  x   │ y  │
├──────┼────┤
│ cat1 │ 40 │
│ cat1 │ 20 │
│ cat2 │ 81 │
│ cat2 │ 40 │
│ cat2 │ 3  │
│ cat3 │ 5  │
└──────┴────┘

* to select categories containing any odd numbers: `select x from t group by x having count(iif(y%2<>0,1,null))>0`
* we can delet from that set, too: `delete from t where x in (select x from t group by x having count(iif(y%2<>0,1,null))>0)`
* to select the number of adds per category: `select x,count(iif(y%2<>0,1,null)) as c from t group by x having c>0;`
  ** remember that `count` counts non-nulls; the `1` may as well have been any non-null value, though i prefer `1` because it's the standard notation for "unit."

===== `concatMap head`

since we're dealing with sets, we're really selecting a element, which may be arbitrary or particular. of course, in sql `concatMap` is done by `group by`.

`group by`: `select x,y from t group by x` works. remember that `x` is not a bare column, because it is the column used for grouping. `y` is a bare column. you can use any aggregate function to make it proper sql, since selection of `y` is already arbitrary. a fine default is selecting the `min` of each group. if you want an aggregate function over a set of data whose elements have a 1:1 map with a group, then you can use nested ``select``s e.g. `with e(e) as (select min(y) from t group by x) select sum(e) from e`,...or maybe a window function? it'd be nice to be able to do it with one `select`.

====== selecting multiple attributes of a common row per group

TODO: why does `with t(name,age) as (values('tom',9),('terry',60),('betty',6),('brett',16),('brandon',20)) select name,max(age) from t group by substr(name,1,1);` give correct results? i thought that it'd return ('betty',20),('tom',60) because those're, per group, the first rows in the relation, each associated with the aggregate over the groups. yet somehow it's associated `name` with `substr(name,1,1)`.

if you're selecting multiple items, then the above technique will not work. consider a variant of the above table:

┌──────┬────┬───┐
│  x   │ y  │ z │
├──────┼────┼───┤
│ cat1 │ 40 │ 6 │
│ cat1 │ 20 │ 8 │
│ cat2 │ 81 │ 4 │
│ cat2 │ 40 │ 5 │
│ cat2 │ 3  │ 2 │
└──────┴────┴───┘

then how do we select an arbitrary row per distinct x? well...we can `select * from t group by x;`, though a correct result isn't guaranteed by sql semantics, even though sqlite does the sensible thing of just selecting the first rows for each unique `x`:

┌──────┬────┬───┐
│  x   │ y  │ z │
├──────┼────┼───┤
│ cat1 │ 40 │ 6 │
│ cat2 │ 81 │ 4 │
└──────┴────┴───┘

sensible as that is, it's probably good to identify a method that's guaranteed to give a correct answer as per sql's semantics. we can't `select min(x),max(y),max(z) from t group by x;`, since that gives

┌────────┬────────┬────────┐
│ min(x) │ max(y) │ max(z) │
├────────┼────────┼────────┤
│ cat1   │ 40     │ 8      │
│ cat2   │ 81     │ 5      │
└────────┴────────┴────────┘

notice that 40 & 8 are returned in a common row! i want an arbitrary row from the original table! no row in `t` has y=40 & z=8! i suppose that the most elegant solution is to select distinct primary key then join that with the original table e.g, assuming that `t` as an autoincrement integer primary key called `id`: `select x,y,z from t join (select max(id) as id from t group by x) using (id)`:

┌──────┬────┬───┐
│  x   │ y  │ z │
├──────┼────┼───┤
│ cat1 │ 20 │ 8 │
│ cat2 │ 3  │ 2 │
└──────┴────┴───┘

* notice that it gives correct results regardless of whether we use `min` or `max` in the nested query.
* as always, if you'ven't a primary key, let's say because you're using a transient table, then use window function `row_number()` to assign unique row ids.

otherwise we can recurse through `select distinct x from t`, unioning with `select from t where x= limit 1`. ideally we want a `group by` or `join` with an aggregate predicate e.g. that count(*)<=1 so that each thing joined or grouped has an upper size limit.

==== thinking in sql/relalg

i was writing a factor program. i wanted to branch on whether the command line array contained any of a given list of arguments. being in factor-mind, my first thought was to use `any?`. i wondered about using `member?` vs `any?`; they seemed to connote the same idea! indeed, `member?` is defined as `[ = ] with any?`. then though `[ { "-l" "-clear" "-ui" } member? ] any?` seems correct, it also seems oddly redundant to use a function nested inside a function of that function. then i realized that "any of x is member of y" is x∩y=∅, which is obviously symmetric (a proprety that i hadn't even considered when coding now), and is also flat, so i don't need to consider how to traverse, namely here being nesting order. its naïve runtime is constantly equal to its worst-case sensible runtime, both O(mn). the sensible version would stop once it's found a common element, and would know to halt there because of `∅ =`. the naïve version computes the intersection then compares it against ∅, which in sql is marked by the special symbol `exists`, so i should be able to safely assume that this optimization is done by any decent sql engine.

anyway, intersect is fine here in factor, and i can use it here. factor and other non-relational languages don't have support for `join` nor query optimization, but at least they've set-theoretic operations. for few data they should be fine to freely use; for large data we can use a sqlite binding, relying on its query planner.

it's cool that we can compute x∩y once then compare it against `x` or ∅ to determine whether any or all of x is in y. generally a function that partitions x & y into (x\y,x∩y,y\x) would be most useful because it's computed in O(n) (if x & y are commonly ordered) and expresses all needed relations of x & y.

==== constraint solving

a constraint solver is possible in sql just as in a logic language. however, sql works on literal values, not predicates which may represent infinities of values. to account for this, we must use ranges of values in sql. for all sets of ranges {(a,b)} ∃ min(a) & max(b). thus predicates like x≥4 can be expressed in a limited but useful capacity as [4,max(b)]. intervals may be considered as 1D boundaries. [a,b] generalizes to a subset of [a,b]×[c,d], etc. these are subsets of cartesian products—the very same definition as joins. thus the n-dimensional geometry model obviously corresponds to sql joins. of course we can take geometric intersections, which is the same as set intersection, which is available plainly in sql as `intersect`. aside from intersection obviously there's the shortest distance between two geometries.

a really cool property of intersection & union is that they tend toward convergence or divergence—a nice interpretation of their dualism!

==== statistics

TODO

==== literature

writing a nonfiction book, commonly a reference.

==== encrypted password database

NOTE: the official AES sqlite encryption extension is not free, licensed only. 

===== the easy way

to write to the db:

. open & write a plaintext sqlite db in volatile memory by e.g. in linux, opening a file in `/dev/shm/creds.db`
. open a text file in volatile memory (assumedly named `/dev/shm/inserts.sql`); put `insert into creds(service,user,pass,comments)` with plaintext values there. do not write the file yet, since if you do, then your sensitive creds will be readable by any running process!
. in one line of script (for whichever script you use):
  .. write `/dev/shm/inserts.sql`
  .. `sqlite3 /dev/shm/creds.db '.read inserts'`. we use `.read` because sql statements entered interactively in the repl are stored in `~/.sqlite3_history`, and there's no way to prevent this (see _§the hard way_ below)
  .. `gpg -er <your name> --output ~/creds.db /dev/shm/creds.db`
  .. `rm /dev/shm/inserts.sql /dev/shm/creds.db`

to read from the db, just decrypt to volatile memory then read from it as per the usual repl stuff (except that you can't use rlwrap because it disrupts gpg's tui password reader) then delete the decrypted file when you're done with it. it's actually most appropriate to do this all in a script instead of interactively so that the decrypted file doesn't exist any longer than it has to.

this method's disadvantage is that plaintext data is ever stored in a manner where it's readable by other processes. that's insecure by principle, though that may not matter if you aren't a target and you're the sole user of your system.

===== the hard way

this achieves the ideal result—encrypting & decrypting only the concerned information—even if we need to pull teeth to achive it.

====== the preferable solution

use sqlite as a library, not the sqlite3(1) program; code a program that:

. accepts sensitive plaintext from stdin
. parses a text buffer of sensitive information, encrypts it via gpg, then inserts into a sqlite db by the sqlite library
. accepts a simple syntax to select creds then decrypt them to stdout

to create a secure credential db with a mix of plaintext and encrypted attributes:

. decrypt current credential data into a text editor buffer
. select each attribute to encrypt, then pass each to an invocation of `gpg -e`, and replace the input plaintext by the output ciphertext. this is done in kakoune simply by using multiple selection with `|gpg -e`.
. write this file somewhere or otherwise pass the buffer content to a program that you'll write which parses the buffer into sql `insert` statements

now you've a db! to insert or update any single credential (you'll assumedly do only one at a time), write a simple program that reads input data from stdin, then passes those data to `gpg -e`, then passes the ciphertext to sqlite via a wrapper api.

printing the credentials to stdout is as simple as: `sqlite3 creds.db 'select edit(user,"factor -run=gpghere -d"),edit(pass,"factor -run=gpghere -d") from creds where service="whamazon"'`, for which you can make a shell alias or function, or your own small program to effectively execute this kind of statement.

====== unsuccessful methods

only read this section, not executing its code nor writing the `gpghere` program; it'll ultimately not be used anyway.

to arrive at a working solution, i went through many failing solutions, only eventually identifying a satisfactory one. the "lesson" here is that software often features unnecessary evil, but there's little that kakoune, sqlite, & factor together can't handle.

.sqlite3_history

the only way around it is non-interactive statement execution by `.read`, but even that does not work because it still requires a file or process that outputs bytes. the file option is unsatisfactory since other procs would be able to read that file. using with a process that produces insert statements turns-out to be unnecessary or redundant.

* removing write permissions for `~/.sqlite3_history` causes sqlite3 to crash upon launch.
* replacing `~.sqlite3_history` by a soft link to a location is in volatile memory fails: sqlite3 overwrites the link with a regular file, storing history there as usual.

.scripts

to prevent `ps` scraping attacks (or accidental `~.bash_history` &c attacks), we should avoid specifying sensitive data in command line strings.

.sqlite, gpg, files, and data streams

when `gpg` encrypts or decrypts, it always does so with output to a different file than its input, and `sqlite3` always reads from the same temp file that `edit` creates. aside from temporary files being insecure, they're a logistical pain, too, since sqlite3 & gpg don't compromise and have incompatible methods of reading & writing data; sqlite3 requires overwriting a file, which gpg doesn't support.

`gpg -er <name> path > path` always leaves `path` an empty file, so that's no good.

something like this is on the right track:

[source,shell]
----
#!/bin/sh
case "$1" in -e) out="$(gpg -er nic --output - "$2")" ;; -d) out="$(gpg -d "$2")" ;; esac
# TODO: write "$out" to "$2" somehow
----

the only problem is how to write `out` to a file such that gpg decrypts it properly. i tried `echo -n "$out" > "$2"` but gpg gave an error when trying to decrypt. `printf` instead of `echo -n` gave a different error. both `cat > "$2" <<< "$out"` and `cat <<< "$out" > "$2"` apparently aren't even syntactically correct.

so the solution is to use an actual programming language (like factor) instead of bash, or to use `--armor` so that encrypted data is text instead of a bitstring. i like bitstrings, since they generalize text and support more flexible, much terser encodings, so i'll try using factor instead of `--armor`. i'll call the factor program `gpghere` it's the same logic as the bash above but it actually overwrites the original file properly. we then can insert:

[source,sql]
----
create table creds(service text primary key, user blob, pass blob);
insert into creds values("whamazon"
                        ,edit("secret username","factor -run=gpghere -e")
                        ,edit("secret password","factor -run=gpghere -e"));
----

and retrieve: `select edit(user,"factor -run=gpghere -d"),edit(pass,"factor -run=gpghere -d") from creds where service='whamazon'`.

NOTE: rlwrap makes gpg's cli password input ui fail

though again, we want to avoid typing `insert` statements in the interactive repl, since that'd log them in the history file. if you already have a secure passwords file, then you'll need to decrypt it into a buffer, then individually encrypt each username, password, &c, inside the text editor buffer, by passing text to `gpg`, e.g. in kakoune, select text to encrypt, then enter `|gpg -er nic -o -` followed by `return`. you can make your text file of the form

----
service
user
pass
note
...

service ...

...
----

then write a simple factor program to parse it as bytes rather than text, and use factor's sqlite wrapper api to send insert statements directly in a loop. or you can do something else; this is the point where you can be creative and do whatever works. there are many options.

lastly, as far as i can imagine, you'll need to enter multiple passwords only once; after you've your secure creds db, you'll either insert or update only one credential row at a time, though you may select multiple at a time, which is fine & easy. for example, i pay my credit cards to 4 banks at once, so i select the creds to all of them. i can simply add a relation attribute called `tag` then give the bank rows `tag='bank'`, then `select edit(user,"factor -run=gpghere -d"),edit(pass,"factor -run=gpghere -d") from creds where tag='bank';`. fortunately, like `sudo`, `gpg` asks us for a password only after some period of inactivity, so despite multiple attributes to decrypt, only the first will trigger the passphrase entry dialog. though selecting is easy and can be safely done in sqlite3 [non-]interactively, the inserts & updates must still be done in a text editor or some interactive program that accepts sensitive input in such a way that it's not output to a file nor part of the command line. reading from stdin is a simple solution! `gpg -e -` does not work well; however, it'd be easy to write a program that reads a from stdin until a line whose sole content is `EOF`, then pass that text to `gpg -e`, and then pass that to sqlite via a wrapper. thus we've arrived at a solution, but in the end, it's easiest to consolidate these methods into the single program that is the aforementioned, preferred solution.

==== units

[source,sql]
----
create table to_mg(oz,g);
insert into to_mg values(28349.5,1000);
select 3*g,12*oz from to_mg;
----

this sees using a table as a simple ad-hoc relation. obviously the symmetry constraint here restricts the table from holding expressions beyond mere literal values. such functionality would require first-class λ's or `eval` (see sql metaprogrammability section.)

==== merge with default value

[source,sql]
----
create table x(id,y);
create table y(id,z);
insert into x values(0,1),(1,20);
insert into y values(1,10),(20,40);
insert into x values(10,100);
select y,case when z is null then 2000 else z end as z from x left join y on x.y=y.id;
┌─────┬──────┐
│  y  │  z   │
├─────┼──────┤
│ 1   │ 10   │
│ 20  │ 40   │
│ 100 │ 2000 │
└─────┴──────┘
----

==== get successive integer 

we get the greatest integer in the table, or if the table is empty, then start with 10.

[source,sql]
----
create table x(id integer);
select case when count(id) > 0 then max(id)+1 else 10 end from x; -- 10
insert into x values(100);
select case when count(id) > 0 then max(id)+1 else 10 end from x; -- 101
----

==== tic tac toe

this example demonstrates many things about how to reason about relations. to start, the 3×3 grid will not be a table with 3 rows and 3 columns. think about how you'll check for a winner: you'll want to check each of the rows, and each of the columns (and each of the diagonals, too.) to check all of the columns, you'll want to use the same logic for each column, just a different column number. ah, there's one hint: we want column _numbers_; sql does not number columns. columns are fixed and must be addressed by name. rows, on the other hand, are arbitrary in number and are all treated the same. furthermore, we want code that generalizes non-verbosely to higher dimensions, say for _connect four_. x & y should be treated the same; thus we'll use `(x,y)` indices. x's & o's will be stored as -1 and 1 respectively; an empty cell is 0. this makes checking for winners easy: if the absolute value of the sum _s_ of a row, col, or diag is 3, then the winner is `sign(s)`.

[source,sql]
----
-- make the grid
create table grid(x integer, y integer, v integer default 0, primary key (x,y));
insert into grid(x,y) select * from generate_series(1,3) as x join generate_series(1,3) as y; -- generate_series(x,y) is interval [x,y]
-- assume that player just moved, which updates grid. now check for winner:
select sum(v) from grid where x=y;   -- one diagonal
select sum(v) from grid where x=4-y; -- the other diagonal
select sum(v) from grid where x=1;
select sum(v) from grid where x=2;
select sum(v) from grid where x=3;
select sum(v) from grid where y=1;
select sum(v) from grid where y=2;
select sum(v) from grid where y=3;
----

ugly as sin, eh? clearly we're considering the cartesian product {x,y}×[1,3], so our code should reflect that. `where x=n` is here actually a poor way of referring to the set {(x,y)|x=n}! that set is described properly as a cartesian product in sql:

[source,sql]
----
with t(x,y) as (select * from (values(1)) join (select * from generate_series(1,3))) select * from t;
┌───┬───┐
│ x │ y │
├───┼───┤
│ 1 │ 1 │
│ 1 │ 2 │
│ 1 │ 3 │
└───┴───┘
----

we could `natural join` that table with grid on `(x,y)`. (btw, expressions like `where (x,y)=(1,2)` are valid!) however, this is a perfect use case for `group by` & the `sum` aggregate. the finished code is:

[source,sql]
----
create table grid(x integer, y integer, v integer default 0, primary key (x,y));
insert into grid(x,y) select * from generate_series(1,3) as x join generate_series(1,3) as y;
-- check diagonals
select sum(v) from grid where x=y;
select sum(v) from grid where x=4-y;
-- check rows & columns
select * from grid group by x having abs(sum(v))=3;
select * from grid group by y having abs(sum(v))=3;
----

so there you go: checking for winners in tic tac toe simply by 4 queries. maybe it can be syntactically shorter, but this is a good encoding of the game's rules: you win if you cross any row, column, or diagonal.

we see that `group by` partitions by equality, which is analagous to the set of (sets each one of whose axes' value is fixed.)

==== select by day

[source,sql]
----
select * from tbl where strftime("%Y-%m-%d",date) = "2022-07-01";
----

`date` may be a datetime or date string.

==== resample 1m candles into day candles (single day)

[source,sql]
----
with x(start,end,high,low,open,vol)
  as (select strftime("%Y-%m-%d",min(datetime)), max(datetime), max(high), min(low), open, sum(vol)
  from AAPL where datetime between datetime("2010-01-04 09:30") and datetime("2010-01-04 16:00"))
select start,high,low,open,vol,close from x join (select close from AAPL where datetime = (select end from x));
----

in a common proglang this would be like:

----
let t = {AAPL | datetime ∈ ("2010-01-04 09:30", "2010-01-04 16:00")}
    end = max(t.datetime)
    close = t[end].datetime
 in (start,high,low,open,vol,close)
----

the `join` is not done as a cartesian product, but instead should be interpreted as putting the `close` at `end` into the `select` clause's scope. `x` is a local binding. if i'm using sql from another proglang, then alternatively i could have stored `x` as its own table (a non-local binding) then done `select start,...vol from x` in one query and `select close from AAPL where datetime = (select end from x)` in another.

`open` needs neither aggregate nor other special calculation because for any data selected among aggregates, the first encountered value is used in practice, though according to sqlite's documentation (§2.4 of the `SELECT` docs), "each non-aggregate expression in the result-set is evaluated once for an arbitrarily selected row." if this turned-out to be a problem in practice, then we'd need to endow it with similar logic as we used for `close`.

NOTE: the datetime format requires leading zeroes for all values, e.g. day, hour, &al.

==== resample 1m candles into day candles (multiple days)

[source,sql]
----
with x(start,end,high,low,vol) as (
  select min(datetime), max(datetime), max(high), min(low), sum(vol)
  from x_AAPL
  where datetime between datetime("2010-01-01") and datetime("2010-02-01")
    and time(datetime) between time("09:30") and time("15:59")
  group by strftime("%d",datetime)
)
select strftime("%Y-%m-%d",start),high,low,open,close,vol
from x join (select datetime as cdt, close from x_AAPL) on end = cdt
       join (select datetime as odt, open  from x_AAPL) on start = odt;
----

returns

----
2010-01-04  30.64  30.34  30.48  30.59  116694802
2010-01-05  30.79  30.46  30.64  30.62  136014592
2010-01-06  30.74  30.10  30.62  30.13  133300727
2010-01-07  30.28  29.86  30.25  30.08  113809059
2010-01-08  30.28  29.86  30.04  30.27  104221936
2010-01-11  30.42  29.77  30.41  30.01  111353487
2010-01-12  29.96  29.48  29.88  29.67  129700571
2010-01-13  30.13  29.15  29.69  30.05  145122992
2010-01-14  30.06  29.86  30.01  29.91  98356076
2010-01-15  30.22  29.41  30.13  29.41  130680837
2010-01-19  30.74  29.60  29.76  30.72  161574329
2010-01-20  30.79  29.92  30.69  30.26  148014426
2010-01-21  30.47  29.60  30.29  29.74  145818463
2010-01-22  29.64  28.16  29.54  28.25  205441418
2010-01-25  29.24  28.59  28.93  28.92  216214306
2010-01-26  30.53  28.94  29.39  29.41  425729542
2010-01-27  30.08  28.50  29.54  29.71  417601177
2010-01-28  29.35  28.38  29.27  28.47  281731401
2010-01-29  28.88  27.17  28.72  27.44  300374774
----

=== output

.sqlite output modes

`.mode <mode>` changes output.

* pretty:
  ** `box` uses unicode box drawing characters
  ** `column`: clean
  ** `table`: boxes drawn with plus, hyphen, and pipe
* easily parsed:
  ** `list` (default)
  ** `json`
  ** `html`
  ** `csv`
  ** `line` (best for reading long outputs in terminal)

=== performance

* gather multiple successive statements into transactions (see your db's docs for the `TRANSACTION` keyword)
  ** at least in sqlite, all actions occur in a transaction, and creating & destroying transaction is non-trivial like creating & destroying pthreads.
* sqlite (and perhaps others?): prepare statements that will be executed multiple times. TODO: ipossile only in sqlite (which defines a bytecode) when invoking it from other langs (i.e. preparation isn't possible in sqlite's repl)?
  ** e.g. with connection `d` to db containing table `x(a,b,c,d)`, `(define st (prepare "insert into x values(?,?,?,?)")) (call-with-transaction d (λ _ (query-exec d st 1 2 3 4) (query-exec d st "A" "B" "C" "D")))`. note that the prepared statement can be free in its parameters' values.
* sqlite `PRAGMA synchronous=OFF` disables the usual waiting for data to be safely on disk, thus making writes faster but making corrupton possible.

[TODO]
* sqlite: can i prepare a transaction statement? i should be able to, if transaction is symmetric. otherwise i'll use transactions all of whose statements are prepared.

.exceptions

* akavache is designed to be efficient without the user trying
* sqlite in-memory dbs are probably fast no matter what

=== mutiple databases

[source,sql]
----
create table table1(x integer);
attach database "db2.db" as db2;
create table db2.table1(y integer primary key autoincrement);
insert into main.table1 values(56);
insert into main.table1 values(90);
insert into db2.table1 select * from main.table1 limit 1; -- table1 of file "db2.db" now contains 56.
----

.common

* `insert into t1 (a, b, c) select a, b, c from t2;`
* `all` (cf `distinct`) is often not supported. this is fine because it's the default anyway.

.sqlite3-specific execution

* to open a db as read-only, specify its location as a URI, then append a query: `file://<path>?mode=ro`

.quoting

|===================================================================================================
| single quotes | string literal
| double quotes | identifier (used to, e.g. use a keyword as a symbol
| brackets      | (non-standard) identifier, same as double quotes. used by MS-SQL server and sqlite
| backticks     | (non-standard) identifier. used by MySQL and sqlite
|===================================================================================================

see link:https://www.sqlite.org/lang_keywords.html[sqlite's documentation] on parsing quoted strings.

=== pragmas useful for implementing metaprogramming 

usage notes:

* all pragmas may be more usefully used as relations e.g. instead of `pragma table_info(t)`, use `select * from pragma_table_info('t')`.
  ** the pragma version does not require table to be quoted
  ** any typo in a pragma will silently do nothing (e.g. `pragma table_infos("t")`); however, the virtual table form will fail appropriately if there's a typo *in the pragma name* e.g. `select * from pragma_table_infos("t")` will say "no such table pragma_table_infos." however, as the table name is just a string literal, if you give a table name that does not exist, then the empty relation will be returned.

pragmas:

* pragma `table_list` gives more info than `.tables` and can be used in `sqlite3_exec` instead of only in a repl
* describe a table: `table_info("n")`. no effect or empty relation if n ∉ db.
  ** `table_xinfo` is the same but also shows hidden columns
* `function_list`: all functions (and their types) available on current db connection!
  ** `s` means per row; `w` means aggregate.
  ** narg<0 denotes variadic fn
  ** nullary functions must have trailing parens e.g. `random()`, `pi()`. the trailing parens distinguishes them from column names.
    *** some nullary functions are usefuly only as window functions, e.g. `percent_rank()` or `cume_dist()`

for functions to accept args instead of relations is foolish design.

=== json

sqlite is an excellent json extractor and manipulator. it considers json as a set of flat tables implicitly nested by (`id`,`parent`) relations rather than recursively nested objects (which introduces scoping), thus making arbitrary traversal easy.

* `.mode json` outputs json to stdout
  ** `.once <file path>` writes next query's output to file (so can write table as json to file)
* if using sqlite as a library in another proglang, then conversion from rows to json is trivial
* json is stored as ordinary strings, except return value of `json`
* json is stored in table cells or string literals

.fns

json:: id fn but cod is string pseudo-typed as json.
json_valid:: 0 or 1 whether a value is a (valid) json string.
json_array(e,...):: constructor
json_object(k,v,...):: constructor
json_array_length:: obvious. useful in query predicates.
json_extract:: select elements from json tree. if one path arg given and selected value does not refer to json array, then returns single value as sql atom; else returns json array string.
json_insert, json_replace, json_set:: put: 1. unless exists; 2. when exists; 3. either; respectively.
json_remove:: duh
json_patch:: put (or remove if put to null) values in json object at keys. treats arrays as atoms.
json_each, json_tree:: json tree as sql tables, top set of children only, or children on all levels
json_group_array, json_group_object:: aggregate fn. return selection as json array or object (see example below). take 1 & 2 args respectively.

.operators

both introduced in sqlite v3.38.0 (2022-02-22). they're `json_extract` but:

->:: always returns json string.
->>:: always returns sql table.

.examples
[source,sql]
----
create table d as with x(k,v) as (values("j",'{"a":3,"b":[1,2,3,4],"c":{"d":"hi"}}')) select * from x;
select key,value,type,atom,id,parent,fullkey,path from json_each(v) join d where k="j";
┌─────┬──────────────────────────────────────┬─────────┬──────┬────┬────────┬─────────┬──────┐
│ key │   value                              │  type   │ atom │ id │ parent │ fullkey │ path │
├─────┼──────────────────────────────────────┼─────────┼──────┼────┼────────┼─────────┼──────┤
│ a   │ 3                                    │ integer │ 3    │ 2  │        │ $.a     │ $    │
│ b   │ [1,2,3,4]                            │ array   │      │ 4  │        │ $.b     │ $    │
│ c   │ {"d":"hi"}                           │ object  │      │ 10 │        │ $.c     │ $    │
└─────┴──────────────────────────────────────┴─────────┴──────┴────┴────────┴─────────┴──────┘

select key,value,type,atom,id,parent,fullkey,path from json_tree(v) join d where k="j";
┌─────┬──────────────────────────────────────┬─────────┬──────┬────┬────────┬─────────┬──────┐
│ key │                value                 │  type   │ atom │ id │ parent │ fullkey │ path │
├─────┼──────────────────────────────────────┼─────────┼──────┼────┼────────┼─────────┼──────┤
│     │ {"a":3,"b":[1,2,3,4],"c":{"d":"hi"}} │ object  │      │ 0  │        │ $       │ $    │
│ a   │ 3                                    │ integer │ 3    │ 2  │ 0      │ $.a     │ $    │
│ b   │ [1,2,3,4]                            │ array   │      │ 4  │ 0      │ $.b     │ $    │
│ 0   │ 1                                    │ integer │ 1    │ 5  │ 4      │ $.b[0]  │ $.b  │
│ 1   │ 2                                    │ integer │ 2    │ 6  │ 4      │ $.b[1]  │ $.b  │
│ 2   │ 3                                    │ integer │ 3    │ 7  │ 4      │ $.b[2]  │ $.b  │
│ 3   │ 4                                    │ integer │ 4    │ 8  │ 4      │ $.b[3]  │ $.b  │
│ c   │ {"d":"hi"}                           │ object  │      │ 10 │ 0      │ $.c     │ $    │
│ d   │ hi                                   │ text    │ hi   │ 12 │ 10     │ $.c.d   │ $.c  │
└─────┴──────────────────────────────────────┴─────────┴──────┴────┴────────┴─────────┴──────┘

select json_group_array(key) from json_each(v), d where k="j"; -- ["a","b","c"]
select json_group_object(key,fullkey) from json_each(j), d where k="j"; -- {"a":"$.a","b":"$.b","c":"$.c"}
----

* `path` is the path to the object that contains a given element
* `fullkey` is the path to the given element
* `atom` is not more useful than value, but should be considered a boolean (i.e. null or not) which is useful for query filters
* `v` is in `json_each`'s scope, implying that, in a join, attributes are unioned before virtual tables are computed.

=== triggers

triggers are very powerful. they enable reactive programming aka _hooks_.

[source,sql]
----
create table x as with x(a) as (values(0)) select * from x; -- counter a := 0
create table y(b); -- just some table
create trigger tr after insert on y for each row begin update x set a = (select 1+a from x); end;
select a from x; -- 0
insert into y values(10);
select a from x; -- 1
insert into y values(10),(30);
select a from x; -- 3. if not FOR EACH ROW, would be 2. however, as of sqlite 3.39.2 only FOR EACH ROW is supported, so it's implicit.
----

as you can see, `tr` is a hook that increments counter `a` for each row inserted into `y`.

=== efficiency

so we compute a whole relation then select a (usually singleton) subset from it by `where`. most langs use a complex tree of nested `cond`'s, wherein predicates are evaluated until one succeeds; some predicates may not be calculated, saving cpu cycles. in sql, as in apl, all predicates and their associated values are calculated and then matching value(s) are selected. perhaps theoretically the `cond` tree is more efficient since it eschews irrelevant computations, but jumping (e.g. `jle`) greatly slows a program because the program loaded must try to anticipate which code of the binary's _executable_ section must be loaded, and i/o is hella slow. the sql version computes more but:

. they may be small computations
. there's no jumping but traversing a relation takes time
. invalid branches may contain `null`, which makes any computation O(1) by null propogation
. logic is obvious: all branches are flatly enumerated
  .. the program is simpler to reason about or study formally
  .. the flatness implies ability to parallelize computation. indeed, the relational algebra can be easily handled by a specialized processor, which is the ultimate speedup.

the solution (2) is to use bit vectors in cpu registers to avoid jumps and heap access.

=== interfacing with sqlite

firstly, i imagine that every desired extension to sqlite can be accomplished via its c api. yet c is compiled and a poor language. sqlite is extensible and can dynamically load functionality, but coding that functionality is still crufty! our sqlite interface options:

* `sqlite3(1)`, which includes a sql repl and many useful specialty functions
* direct c funcalls
* wrapped funcalls (for non-c langs like lua, lisp, &c)
* command line

* all support multiple statements (you can use heredocs for multiple statements on the command line—even pragmas or those with leading dots.)
* all except the command line keep a connection open after executing a sequence of sql statements.
* only the c interface can be used to extend sqlite's functionality e.g. defining new [window] functions or virtual tables
  ** is this really true? why can't <the c things needed to implement these> be wrapped for another lang, then implement the wrapper in that lang? rather, that _is_ possible, but what would it take? the output must ultimately be a shared object file, if we want dynamic loading, wihch is ideal.

i've yet to find or implement a non-interactive socket interface with an open sqlite connection, which would enable remote control from other programs without needlessly opening & closing db files. basically the solution is to use factor as the computer interface language, but use its connection to a sqlite db to leverage most computation. this being said, we're afforded pretty decent interaction simply by:

* `.output`
* `edit`
* `.read`
* `readfile`
* outputting as `csv` or `list`
* csv virtual tables

you can pretty print single-expression selections by `.mode list`, where your text is formatted using hex blob strings for formatting e.g. with a table `articles`,

┌─────────┬─────────────────────────────┐
│  item   │            point            │
├─────────┼─────────────────────────────┤
│ topic 1 │ my first point              │
│ topic 1 │ another enlightening point! │
│ topic 2 │ just one thing              │
└─────────┴─────────────────────────────┘, we query into a pretty string: 

[source,sql]
----
select item || x'3a0a' || group_concat(x'092a20' || point, x'0a') from articles group by item;
----

produces the literal text:

----
topic 1:
	* my first point
	* another enlightening point!
topic 2:
	* just one thing
----

i tried using `black circle, u+25cf`, but obviously that's 2 bytes, not one, so sqlite didn't print it properly. idk if there's an extension that can print non-ascii chracters.

for more general pretty print, csv output can easily be passed to a line editor e.g. `awk(1)`, or you can use `.mode list` then `select '' from (select edit("a string value","cat"))`. again, though, `sqlite3` always exactly opens a connection, execute a sequence of statements, then closes the connection.

=== sqlite everywhere

this section discusses using sqlite as an alternative to anything conceivable.

* sqlar: sql archive format, alternative to `zip` &c
* `sqlite3` works on zip archives, too, regardless of filename extension (works on e.g. odp files). see `cli.html#zipdb`.
* pass query results to `fzf --preview 'sqlite3 db 'select * from t where x like "{}"'`. again, though, this is preferably done without opening & closing each time that the cursor moves. this quoting is horrible, btw.
* store comic archives as sql dbs instead of rar, 7z, zip, or any other number of alternative formats
* use sql db instead of filesystem

==== `sqlite3(1)`

.useful dot commands
|======================================================================================================================================================
| dot-command                                           | description
| `.changes`                                            | sanity check mutative queries
| `.headers`, `.separator`, & `.mode`                   | modify output (namely `list` and `csv` output)
| `.nullvalue`                                          | for modes of csv,html,tabs, outputa string instead of ''. not used for `json` output.
| `.schema`                                             | show tables' & views' definitions
| `.trace`                                              | debug
| `.indexes`, `.databases` & `.tables` (includes views) | see what of the db is queryable
| `.recover`                                            |
| `.output` & `.once`                                   | write to file or pipe the repl's output (`.output`) or just the next query's output (`.once`)
| `.read`                                               | read a file (or a proccess's output, when prefixed with `|`) of sqlite3(1) commands
| `.import`                                             | create a relation by reading from supported formats^*^
| `.save`                                               | write in-memory db to a file
| `.dump`                                               | write to stdout a sql source code that generates the currently-loaded db
|======================================================================================================================================================

^*^see `.help import`. input format is specified by `.mode`, the output format, unless overridden by `--ascii` or `--csv`. *the latter allows importing csv files without `.load csv`*! this makes extremely easy the querying of a db, passing to an editor, modding only part of the output, then updating the db with new values.

the following in-repl extension functions are useful:

* `readfile`. useful for inserting blobs into relations. you'll probably want to use `.cd` with this
* `writefile` as an alternative to `.once`
* `edit(expr,prog)`. (or if `prog` omitted then the `VISUAL` envar.) writes selected value to a temporary file, then passes its filepath to the given program *as the sole command line argument*; it does not pass the expression's value via stdin!

NOTE: when i use `edit(expr,"kak")`, kakoune (running in `kitty(1)`, and with `sqlite3` launched from `rlwrap`) did not properly support keybinds involving the `alt` key after i'd exit insert mode via the `esc` key. before using `edit`, test your editor with it on innocuous data (e.g. `select edit("","your_editor")`) for any quirks! i think that, in my case, rlwrap was the only cause of erroneous editor behavior. also be aware if your editor always appends a newline to the file that it writes. this will make tabular output (e.g. `.mode box`) ugly. a simple fix is to write a small program that runs your editor on the file, then when the editor exits, the written file is trimmed.

read the docs; `sqlite3` is enabled by its ability to interface with other programs or the environment.

==== useful virtual tables

these ought be implemented if they'ven't yet been.

* env
* fs
