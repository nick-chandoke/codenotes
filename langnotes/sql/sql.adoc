== sql

this document discusses sql for sqlite. all other sql engines are not concerned.

sql is a bit of an interesting and probably redundant (and therefore inelegant) mix of predicates & sets. i've yet to study the relational algebra to see how it compares to set theory and predicate logic. consider `select x from t where p`. that means _get subset of `t` that obeys `p` then get only the `x` attribute of that set_. _relations_ are basically sets of tuples with named cells. firstly, were everything sets, then relations would just be sets of sets, considering tuples as sets (alists.) though we could (and probably should) consider not tables, but only columns—a column is just an array version of an identified datum (array variable instead of single variable.) so why arbitrarily group things into tables? because it associates/groups each datum with other data. this is not a worthwhile benefit; we can achieve the same thing without introducing simply by indexing all columns then associating columns by joining on indices. this seems asymmetric, though, since grouping things is done by table schemata and prolog predicates.

"a wrt b" is sql `select a from x join y on x.a like y.b`. cartesian product is like join w/union; left join is like join w/intersection. "a wrt b" means "the subset of a that's related to b, and return a & b."

[TODO]
* see link:file:///home/nic/codenotes/langnotes/sql/sqlite-doc-3390200/optoverview.html[optimizations page] to see what sql reduces to; this more describes sql's essence/primitives.
* see <file:///home/nic/codenotes/langnotes/sql/sqlite-doc-3390200/c3ref/create_function.html>
* how to organize/encode data: relations are basically their headers. thus make a table for each shape. whatever data have a common shape belong in the same table.
* cf j/k db
* partition this module into relalg, working with data, common exprs (e.g. `exists`, `case`), useful compound patterns, and techniques for deriving solutions to given problems
* is relalg isomorphic with linalg? can today's gpus already compute relalg programs easily?

=== sql technique

.quickref

* types: scalars, row values, or relations of null, integer, text, real, or blob.
* select | expr. everything else is only for persistent data, not calculation.
  ** triggers and views are available only on non-transient tables
* from, join: adjoin table to query scope
* `with`: fold
* don't worry about relation schemata much; they're pretty arbitrary. that's why we have `join` (couple) & `select` (decouple) operators. single-attribute relations are good because they're decoupled, but lack coupling. both coupling & decoupling are double-edged swords.
* *exploit*: row values; numbers & strings; [builtin] [non-]aggregate ("core") [window] fns; `join`, `select`.
  ** `coalesce` is short-circuiting `or` like in lisp
  ** you can encode strings especially to exploit `ltrim`, `rtrim`, `trim`, or `substr`, even creating indexes on these
* use `upsert` instead of `insert`, `update`, `replace`
* sqlite can replace record editors such as awk. sqlite cannot be used for general text; for that, prefer `kak -f`

the need for both `from` & `join` is asymmetric. it can be framed as `from` accepting one table and `join: Table -> Table -> Table`, but it can also be framed as `join : Query -> Table -> Query`, acting on `from table`. 

''''

the best way to use sql regardless of orthodoxy. this is to replace most sql notes. it's written in nov 2022.

firstly, there are _statements_ & _expressions_. the `select` statement is the only real thing in sql; the other statements are just stateful `selects` e.g. `create table` corresponds to expressions that return (transient) tables, `delete` corresponds to `except`, `insert` corresponds to `union`. non-select statements are just for mutative/persistent & named, instead of pure & anonymous, manipulations. therefore i'll consider only `select` and `expr` forms. one should use pure (non-mutative) sql for general computation; one should save data persistently only if it's to be used later, stored for the sake of keeping data, not for the mere sake of facilitating computation!

first, though we must discuss how sqlite handles data. it uses _scalars_ and _row values_. scalars are relations with a single attribute. row values are relations with 2+ attributes. _relations_ are sets of row values. i haven't seen a reason to distinguish between scalars and row values; why not just say that row values are 1+ values?

.row values

* row values plainly represent the concept of grouping. this is separate from relations, which are array variables instead of single data; the difference is that each of a row of values is particularly, certainly identified, whereas those in an array var are non-particular, anonymous. this is quite similar to how tuples vs lists are handled in haskell. indeed, lists/relations may be empty, but row values may not be empty!
* a row value's _size_ is the count of its attributes
* row values may be syntactically expressed as `(v,...)`. note the lack of `values` which denotes _relation_ literals, not row value literals.
* any binary operations on row values require row values of the same size
*  `<`, `<=`, `>`, `>=`, `=`, `<>`, `is`, `is not`, `in`, `not in`, `between`, and `case` are such binary operators that accept 2 same-size row values (or 2 scalars)
  ** these evaluate pointwise on row values from left to right, stopping on one of 3 conditions:
    *** all row values are evaluated; retval is as expected
    *** value is encountered that determines retval regardless of remainder of row values e.g. `(1,null) < (2,null)` is known to be true after evaluating `1<2`, so the nulls aren't even considered
    *** a `null` is encountered; then retval is `null`
  ** it's not always so simple. e.g. `(1,2,3)=(1,null,4)` returns `0` because 3≠4 regardless of the other values. yet `(1,2,3)=(1,null,3)` returns `null` because the retval 0 or 1 depends on what the `null` would be. remember that sqlite considers `null` as a lack of information. like sigfigs, a lack of information must propogate.
* `update` (not talking about `update from` here) expects a row value on the rhs
  ** `update t set a=x,(b,c)=(select ...) where ...` works
* `<rowv> in <rel>` tests whether a row value is a member of a relation. `<scalar> in <rowv>` works, too.
  ** given that other binops work on row values, `in` should be parameterized by a binop; then we'd have `any` instead of `in`
* wrt an `order by` clause, when a relation is not a multiset, then rows can be indices; use them instead of `offset` because that's more efficient
* use row values in a predicate clause instead of `and` or `or` e.g. `where (x,y) between (0,10) and (3,100)`
* table names are often directly syntactically usable instead of `select * from t`
* select statements that return a single row eval to row values, so `(select a,b from t1) > (select x,y from t2)` is valid & sensible.

NOTE: `is` & `is not` are `=` & `<>`/`!=` but produce 0 or 1 instead of `null` if either of their args is `null`.

* to test whether x is a subset of y: `x y \ ∅ =`
 ** this generalizes x∈y. that generalization is good because it makes everything sets (no "naked" elements)
* to test whether any of x is in y: `x y ∩ ∅ =`

[TODO]
* do these binops work on relations as well as row values?

==== expr

_expression_ means _relation_. singleton values as relations have one row & one attribute. some expression forms evaluate to a singleton relation, and others an empty relation. the context in which the expr is used may mandate constraints on the expr e.g. the number of attributes or rows that it has.

obvious ones like datum literals or operators/functions thereon aren't here enumerated.

* symbols bound in query's scope
* absolute symbol reference ([schema.]rel.attr)
* (expr,...)
* like/glob/regexp/match
* `is [not] [distinct from]`
  ** `is` & `is not` are sqlite-specific terser forms of sql standard `is not distinct from` & `is distinct from`. holy golly, man.
  ** `isnull`, `notnull`, both equivalent to `is null` & `is not null`. no idea why these especially specific forms exist.
* `[not] in` (see note below)
* `[not] exists` (emptiness test; considers whole relation so can be interpreted as an aggregate fn)
* `case when` is short-circuiting / lazy eval, unlike `iif`. see _§first-class functions and conditionals_ for thorough discussion.
  ** `case` is an expression, not a table. `select case when 1 then (values(3),(4)) end;` is correct; there's no `from` clause.
  ** in `case when p then y end`, both `p` & `y` must be single-attribute relations, and only the relations' 1st values are used. 
    ** if the chosen relation is empty then `case` returns `null` as a scalar

see best-paradigms-lang.adoc. `where` is implicit in predicate logic; it's the same as predicate unification/evaluation. the `expression` grammar's `[not] in` subgrammar is set membership/intersection, which is is equivalent to testing against a predicate. therefore `a in b` = `exists a where b` = `exists a intersect b` except that `a where b` is incorrect in sql since therein `b` must be a relation which sql considers distinct from a predicate; however, theoretically, by predicate-set correspondence, the three are equivalent. the equivalence can be seen by `a [not] in b` (or `a like b` &al) being set membership if `a` is a row value and `b` a relation, or set intersection if both `a` & `b` are relations. *however, `[not] in` has one characteristic: it also accepts a scalar lhs with a row value rhs. this is a blatant asymmetry in how sql considers groups of data.* the symmetric solution would be if scalar were equal to a row value with a single datum and a row value equal to a relation with one row. indeed, this would imply that a scalar equal a singleton, single-attribute relation, which _is_ true almost always, but not in `[not] in` and perhaps in some few other contexts.

sql's (relalg's) primitives are the (+,×,-,0)=(∪,∩,\,∅) ring, expressed in predicate logic as (∨,∧,¬,⊥). one of sql's troubles is that it is not symmetric; it considers predicates distinctly from sets, and sometimes considers elements distinctly from sets. also, though this model seems appropriate, one must be careful to distinguish between expressions that act per row vs aggregates, which act per relation. also, and again usefully so, `null` is the empty row value whereas an empty relation is an empty set. `null` has short-circuiting/null-propogation semantics whereas an empty set is the identity for union but a short-circuiting operator for `intersect`. these semantics can be a bit confusing, but they are elegant!

* where / between (uses index)
* join (or where/filter & union) (general filter)
* intersect, except
* `exists` predicates on a relation's emptiness; `where exists ...` makes one relation's emptiness imply this relation's emptiness.

==== select

* `from` merely binds symbols for the query
* `where` & `having` are both the same concept—"such that"—but one is applied to row values and the other to groups of row values. if sql were more symmetric, then `having` should apply to the whole query (the single group) just like aggregates do. however, most people would consider that more like a guard [list monad].
* we can use `like` with a single-column table e.g. `where x like (select y from t)` to ormap `like x` (curried) over `table`. in fact, we can use multiple values for both: `where x like y` tests the emptiness of `x join y on x like y` (with tables `x(x)` & `y(y)`).

===== window functions

see `windowfunctions.html`.

a _window [frame]_ is a subset of a relation. the only thing that distinguishes it from a `select` expression is that each subset is associated with a row. this is much more powerful than otherwise: join, which associates each row being with exactly one other; or aggregate functions, which evaluate to a scalar, albeit one per group when `group by` is used. therefore window functions are extremely powerful but more particular versions of aggregates on `group by` that give aggregate outputs per row rather than just per group of rows.

* aggregate window functions' window frame is determined by a predicate given to the `order` clause. also the window's contents may be ordered.
* aggregate window functions don't present the bare columns problem.
* even non-aggregate window functions effectively implement stateful loops (folds). aggregate winfns implement stateful loops that would be a pain to implement with a fold because their state isn't easily expressed by a single accumulator value, as is the case for aggregate functions on a rolling window e.g. selecting a value and the sum of its immediate neighbors. this is easily defined by a for loop in c, but not by `reduce` in factor, or especially by a fold in racket, haskell, python, &c.

all binary functions are foldable i.e. usable as aggregates. only in typed langs is this not true, for they often use types like `a -> b -> c` or `a -> b -> b -> a` or `a -> b -> b`, instead of `a -> a -> a` or `a -> (b -> a) -> a` &c. all unary fns are mappable & applicable, and all binary fns are applicable and foldable. sql has only numbers (integers & floats) and strings (strings & blobs.) all other "typing" is done by relations & row values. thus sql meets my personal requirement for using only relations of types primitive to a physical cpu. therefore, excepting lack of metaprogrammability and the arguably bad & arbitrary constraint of needing to group all columns as tables, *the ability to define aggregate window functions should be all that's needed to make sql a perfect language*, even if its syntax is verbose.

NOTE: because winfns' results are not per-row, window functions cannot be aliased then used in `where` clauses.

====== examples

.difference of adjacent elements

this example's essence is `log`.

[source,sql]
----
create table x(x);
insert into x values(1),(10),(100),(2);
with t(x) as (select x-lag(x) over (order by x) as d from x) select * from t where x is not null;
----

returns a column `x` of `1 8 90`.

''''

NOTE: sql is case-insensitive!

* though tables' attribute sets are mostly fixed (though they can be updated by `alter table`), creating new tables on the fly is easy: just use `select` to get a subset of attributes or `join` to adjoin!
* it's appropriate that sql forces everything to be in tables; that's symmetric treatment of singletons & collections. however, a table that implicitly stores global variables like lua's `_G` would be nice.

=== λ's

tl;dr: sql's functions/lambdas are queries—relation endomorphisms. queries may be composed, but only pointedly. the points are attributes, not data; or the points are arrays of data. mutate one of the input tables to change the function's input. sql does not support higher-order functions. one difference between sql's functions and lambdas is that sql identifies arguments only by name, not positional order.

a λ is a relation of inputs to outputs. the relational algebra uses relations generally, not partitioning attributes into inputs & outputs. sql is still reductionist, however. queries are sql's functions. their inputs are expressions following the syntax `select`. `from` does not concern inputs _per se_; it only scopes inputs. obviously queries' outputs are relations. queries thus relate relations. a query's inputs can be adjusted by mutating tables of that query's `from` clause e.g. pseudocode `def f(a,b,c) := (a+b)/c; f(1,2,3)` as sql `create view f as select (a+b)/c from params; insert into params values(1,2,3); select f`. sql does not support storing functions/queries/subprograms in tables. see _§first-class functions and conditionals_ for how to code without higher order functions.

in sql, queries are functions are subprograms. queries are evaluated by default, since that's all that a sql engine does. if a subprogram were stored as a relation or string or by any other encoding then we'd need `eval` to evaluate it; the use of `eval` sees queries as subprograms. if queries were encoded as relations, then queries (subprograms) would be modifiable by other queries and sql would thus then be metaprogrammable. the _factor_ language does not have lambdas; instead it has only quoted subprograms and eval (called `call`.) naturally these quoted programs can be modified; the quoted program is just a list of other subprograms. this is similar to a variety of sql that would use relations to encode programs. the point is that evaluable subprograms are superior to lambdas/functions. in this model the only separation of "data vs code" is that `eval` is the only code, and is not data (since that'd be redundant,) and everything else is data.

the ability to modify subprograms then evaluate them is an ability that lambdas lack! lambdas are mere reductions, not true functions, anyway; we can't discuss the inverse of a λ as freely as we discuss a function's inverse in math. this is because mathematical functions may be implicitly defined by characteristic constraints whereas λ's are necessarily definite, explict relations of inputs & outputs. a λ's input symbols are mere stand-ins for literal data, not a predicate-quantified set of possible inputs. the fact that λ's definitions cannot be examined (except in picolisp) exemplifies their reductionist nature; functions can only be applied and composed; no other operations with them are supported. therefore lambdas do not enable anything new; they're worth is their convenience: they're function literals. they relieve one of the need to use crufty syntax to define a function, which is ironic because applicative languages still require data to be named unless it's used exactly once, in which case it can be inlined.

fns can be interpreted as a scoping mechanism: `f(a,b)` is seen as variables `a` & `b`, whose meaning is relative to each invocation of `f`. this can be encoded in sql as a table `f(a,b,e)` where each invocation of `f` is a row, and `e` is the location where the output should go. `a`, `b`, and `e` may each be literal values or _addresses_—values supporting join with another table.

within function definitions local data are stored on the stack (for stack langs) or in a function-local namespace (for applicative langs); in sql local data can be stored as array variables as attributes of a locally-scoped relation bound by a `with` clause. or you can just leave the local data in the query's output; the using query can use it if it needs; if not, there's no extra cost.

NOTE: scoping is a concern in sql queries, e.g. how a table alias (by `as`) is usable in a `where` clause. also, subqueries have access to outer queries identifiers.

as an ending aside, note that a side-effect of data-only programming is that all computation is delayed, since all computations are only data until explicitly passed to `eval`.

NOTE: the need for lambdas in `update` clauses is covered by old.attr & new.attr. see the sqlite docs for `update`.

=== what is sql

sql is a bit mysterious:

* there's an open standard, but you must pay to access it
* despite the standard existing, no sql database totally conforms to the standand—both lacking standard features and including extra non-standard features
* sql began as merely a relational database system in 1974, but updates to the standard from SQL-99 onward have introduced much more functionality

=== sql basics

a table A may have a primary key (uniquely identifying set of attributes), and may have a set of attributes that, in another table B, is a primary key; then: this attribute set is called a _foreign key_, B is called the _child_ table, and A is called the _referenced_ or _parent_ table. foreign key is its own concept (as opposed to a column that we can `join` on) because it can be used as a constraint in a table's schema, which enforces only proper now insertions & updates.

[NOTE]
primary keys are strange; they enforce uniqueness of each row. however, a row, like any single thing, generalizes to a group of things, which could be encoded as multiple rows sharing a common key.

the beauty of sql is that you don't need to care how you store data; all relations are equally flexible and easy to use. your queries are easy and practically the same regardless of whether you store `x` as an attribute in table `y`, or `y` as an attribute in table `x`.

foreign keys' sole use is in rejecting inserts that would violate the pk/fk relationship [constraint], called maintaining _referential integrity_. they add neither functionality nor efficiency, though, at least in sqlite, they conveniently make some dependent operations automatic; see `foreign-key-clause` in `CREATE TABLE`'s spec. aside from that convenience, though, it's a verify-only constraint.

a _virtual table_ acts like a table but is not actually _stored_ as a sql table, e.g. json virtual tables.

.foreign key example

[source,sql]
----
pragma foreign_keys = on; -- needed in sqlite; else foreign key clauses are not syntax errors, but foreign key constraints are ignored
create table t(id integer primary key autoincrement,
               x,
               dep integer,
               foreign key (dep) references t(id));
create index tdep on t(dep); -- make the upcoming join efficient
insert into t values(null,20,null); -- null is given to autoincrement columns, to use the autoincrement feature
insert into t values(null,40,3); -- fails b/c there's no record in x whose id is 3
insert into t values(null,40,1); -- succeeds b/c we've successfully inserted one row already
select x.x,y.id from x join x as y on x.dep = y.id; -- returns one row: {x=40,x=20}
----

this example creates a table with a foreign key constraint on itself. `dep`, which may be null, since the `not null` constraint was not given, is an optional value to consider after we've considered `x`.

TODO: how to efficiently & elegantly select rows that are or are not referenced by a foreign key, e.g. here, selecting only rows that are not dependencies i.e. rows whse ``id``s are not in any other rows' `deps`? decent solutions: 1. have a boolean attribute flag this; 2. store un/flagged ones in their own table, this making the "foreign" in _foreign key_ appropriate; however, this would be horrible attribute duplication! the 2nd table would have all the same columns as the original! so really only (1) is a decent solution so far.

.foreign keys as lattice of relations on subset of attributes

x := (a b c)
y := (x z)

thus:

* a, b, c ∈ x (i.e. {a, b, c} ⊂ x)
* x, z ∈ y

[source,sql]
----
pragma foreign_keys = on;
create table x(id integer primary key autoincrement, -- always good to have an auto inc integral pk column in
                                                     -- every table in case of need to join or use as foreign key.
               a, b, c);
create table y(id integer primary key autoincrement, x, z, foreign key (x) references x(id));
insert into x values(null, 1, 2, 3);
insert into y values(null, 1, 20);
select a,b,c,z from y join x on y.x = x.id; -- (1,2,3,20)
----

rather than explicitly join `x` with `y` on each `select`, it's more sensible to create a view that represents the relation x ⊂ y:

[source,sql]
----
create view y_full(a,b,c,z) as select a,b,c,z from y join x on y.x = x.id
select * from y_full; -- (1,2,3,20)
----

you may name the view 'y' & the underlying table _y, or you may name the view e.g. y_full & the underlying one 'y'. consider that you cannot delete, insert, nor update a view; those must be done to the actual table.

==== pointwise `update`

TODO: carefully read sqlite's docs, then revise with wiser tech if appropriate.

sql does not support updating multiple rows by a map. instead we must set a set of values by another set of values; thus instead of `(map! f x)` we do `(set! x (f x))` but must associate each `x` with a corresponding `f(x)`; of course we do this by join:

[source,sql]
----
create table t(x);
insert into t values(1),(2),(3);
select x,x*10 from t;
┌───┬──────┐
│ x │ x*10 │
├───┼──────┤
│ 1 │ 10   │
│ 2 │ 20   │
│ 3 │ 30   │
└───┴──────┘
update t set x =         (select          x*10 as fx from t);            -- wrong: sets all in x to 10
update t set x = fx from (select x as id, x*10 as fx from t) where x=id; -- correctly sets each x to f(x)
----

the 1st form would be correct were sql to see `x` as a free symbol. unfortunately sql is limited to using literal data sets only.

`update from` is a non-standard form yet commonly supported by sql engines. plain `update` can assign only one value to many rows. `update from` selects many rows then pointwise matches them to rows to be updated by the predicate given to `where`, effectively setting `t` to `t join (select ...)`.

were our sql engine (sqlite) not support `update from`, we'd need to execute an `update` statement for each row in a table e.g.

[source,factor]
----
"select x from t" query-rows
[ [ f ] [ ] bi "update t set x=? where x=?" query-exec ]
each
----

`where x=?` is the pointwise association of `x` with `f(x)` and `each` represents `∀x`. ideally, for efficiency, we'd collect all queries into a list then run them together in a single transaction.

==== no `zip`, _per se_

there appears to be no way to zip [n..] with an arbitrary relation. zipping is possible only by `join on`. `join` cannot work becasue that's cartesian product, which is not pointwise association. however, for `join on` to work, there must be a common attribute upon which to join, but no such attribute exists unless the relation is already indexed by [n..]!

neither recursive `select` nor `update` helps, either; to associate an index with a value would still require the value to already be indexed.

relalg is based on sets, not sequences; indexing rows would be a primitive. indeed, is sql it's accounted for by special attribute modifier `autoincrement`! thus we never need to zip; we can effectively implicitly make all sets sequences with order by nth insert. with the set being a sequence, it fulfills the requirement that allows it to be joined by index. it can thus be effectively zipped. really, though, it never makes sense, in general, to systematically order a set by arbitrary indices!

the `autoincrement` value is set to the nth insert; you may prefer `insert into t(id,x) values((select max(id)+1 from t),x)`. if a row is removed, then you will be left with a sequence with a missing element. how to handle that is your choice. for example, you may mark the removal by not actually removing the row, but by setting its value to `null`; or you may truly remove the row then update all of the indices greater than it to be each one their lesser.

=== design

match data's logic's invariants with sql invariants e.g. attribute sets, sql column or table constraints. all symmetric data belong in rows, sql's only symmetric aspect.

=== [anti]patterns

==== encoding schemes (relational algebra)

* if you want to store a one-to-some map, e.g. parent -> {child1,child2,...}, then you can (but should not) use a "dependent" attribute. the attribute has multiple values, which may be encoded by multiple rows, e.g. `insert into t(...,dependent) values(...,1),(...,2),...`, but that's quite redundant. a more efficient encoding is to use `parent` instead of `dependent`: `insert into t(...,parent) values(...,1),(...,1),...`. this method inserts each of the parent and all its dependents only once, and all of the dependents' `parent` attributes are the same. in the `dependent` version, all of the parent's attributes except for `dependent` must be redundantly specified per each dependent!
  ** this doesn't generalize to multiple "parents" (tables referencing the "child" table), as that'd mean adding to the referenced table a column per referencing table.
  ** consider `s(id)` & `t(id,s references s(id))`. this is redundant; we can leverage the fact that `t` already has an `id`. this is the parent pattern again; if we were to describe this as JSON, then type `t` would contain subobject of type `s`. in sql it's better to have subset `s` reference superset `t`: `t(id)` & `s(t references t(id))`, which uses only one `id`. i use `references` (foreign keys) here when the referenced attribute is a primary key. if it isn't a primary key or even isn't unique, then we can still `join` on it and use triggers instead of trigger-like foreign key constraints such as `on delete [...]`.
    *** one fewer attribute upon which we'd join means one fewer index, too.
    *** this makes insertion order a bit more intuitive: rather than needing to insert the subsets firstly so that the superset can reference them, we insert the superset firstly, then the subsets secondly.
    *** this scheme is not possible if the superset may have a value other than a foreign key, e.g. `t(s)` where `s<0` is just a number, but `s>0` is a foreign key. the closest way to use the subset-references-superset encoding with this schema is for the subset to have an attribute for the superset's value, e.g. `t:{s:<int|{a:int,b:string}>}` (adt `T = S Int | AB Int String`) as `t(s)` & `s(id,s integer,a integer,b string)` constrained to `s is null or (a is null and b is null)`. the former version would require joining on a `case` clause, which would not use indexes, whereas the latter would join on `id` which, if indexed, would make for a much more efficient [left] join; the `case` would be deferred to after the join, performed on the joined table.
* using `like` is dubious. using `regex` is almost cetainly bad; you probably want a db designed specifically for text searching. string pattern matching does not use indexes and is thus does not make efficient queries.
* the semantic meaning of an attribute can depend on other attributes e.g. in `person(age integer,alive boolean)`, if `alive` then `age` means number of days alive; else it means number of days since death. furthermore, any of a row's attributes may be used or not depending on its other attributes' values.
* compress information as much as possible e.g. Y-M-D as just days since some arbitrary start date; that means that dates require only one column. the type `A or B` where A & B are both natural numbers can be encoded as a single integer whose sign determines whether A or B.
  ** you can, at least in sqlite, exploit `cast` for booleans; to interpret anything as a boolean sqlite ``cast``s it to an integer then checks equality with 0. thus you can make a string's first character /[1-9]/ to mark it as true; any other character will interpret the string as false.
* do not move from one table `a` to another `b` by `insert into a ... where p; delete from b where p`; instead, store all in one table `t`, and have an attribute that designates whether a row would belong to `a` or `b`; then filter on that to effectively get virtual subtables `a` & `b` from `t`.
* consider encoding schemes' supported partitioning schemes e.g. integer primary key can be generalized to indexed reals. reals can be partitioned by floor.
* unless uniqueness is required by some algebraic properties of your data, then feel free to see rows in a table as elements of a [multi]set. elements can be grouped [partitioned] by attributes (general prodicate, not just equality), which generalizes "thing at index" to "things with a given property", and set-theoretic operations can be performed for all predicates, and all predicates can be defined of multiple attributes [columns]
* to delete w/cascade a la foreign key w/o the relation technically being implemented as a foreign key, which would be sensible if the parent table referenced a table whose keys were non-unique, hence all of the referenced table's rows of a common predicate would be deleted:
  ** solution 1: `after delete` trigger
  ** solution 2: in sqlite (and maybe other sqls) by using `returning` (non-standand sql), though the returned value is not available as a sql expression; it's usable only by a client program e.g. `(let (rid (sql "delete from parent where id=? returning fk" pid)) (sql "delete from referenced where id=?" rid))`
  ** `on delete cascade` cascades when the *parent* (the referenced table, the one with [that must have] the primary key) row is deleted, not the child! e.g. with `create table a(a primary key, v); create table b(a references a(a) on delete cascade)` means that deleting one of `a`'s rows will implicitly delete one of `b`'s, but not _vice versa_; for the inverted case, you'll need a trigger; however, if you're considering that, you may want to reconsider how you're structuring your data; you should be able to use foreign key cascades. particularly, remember that it's better to have a `parent` attribute rather than `children`. using this design will help you better decide whether either of your tables should have a primary key. remember that foreign keys are one-to-many relationships; many in `b` may have common foreign keys; deleting their corresponding row in `a` will delete all those corresponding in `b`.

[TODO]
* how can we encode logical constraints as sql constraints or relations? common constraints are types, lengths, [recursive] predicates

==== sensible querying

sensible means elegant, which implies efficient.

* prefer join over subqueries e.g. `select a,(select b from t2 where a=b)) from t1`, or subqueries in a `case` clause; and prefer `in` over `=`, as these support multiple values
  ** the subquery-to-join refactoring pattern is `select (select x from t2 where p) from t1` becomes `select x from t1 join t2 where p`. if `x` & `y` don't have common predicate `p`, e.g. there's a unique `y` identified by `p`, but no `x` satisfies `p`, then use a left join and append `or x is null` to `p`; this new predicate will see the _rows_ for which it holds be returned, then from those rows either `x` or `y` will be chosen, and both will be available; `y` is always available, but `x` may be `null`. either way, the important thing is that the _row_ is in the result set.

.example: use join rather than subqueries
[source,sql]
----
select * from x;
┌───┬───┐
│ a │ b │
├───┼───┤
│ 1 │ 2 │
│ 3 │ 4 │
│ 5 │ 6 │
│ 7 │ 5 │
└───┴───┘
select * from y;
┌───┬────┐
│ b │ c  │
├───┼────┤
│ 2 │ 20 │
│ 5 │ 50 │
└───┴────┘
select a,b,case when c is not null then c else 20 end as 'c or 20' from x join y using (b);
┌───┬───┬─────────┐
│ a │ b │ c or 20 │
├───┼───┼─────────┤
│ 1 │ 2 │ 20      │
│ 7 │ 5 │ 50      │
└───┴───┴─────────┘
----

then use `where` to select a particular row. another possible condition is, instead of `c is not null`, `c>0` where `c<0` denotes an element of a sum type but `c>0` denotes that `c` is a product type, which in sql is encoded as a datum upon which we can join with a table of named tuples.

NOTE: `case <expr> when ...` uses a _base expression_; in this case, rather than predicates being tested against 0 or 1, they're tested against the base expression's result. `case x when y then r1 when z r2` is better than `case when x=y then r1 when x=z then r2` because it's terser and guarantees that `x` will be evaluated only once. the base expression form is to `case` [scheme] as the non-base-expr form is to `cond`.

''''

* suppose that table `t(x)` has one row and table `s(y)` has many rows. if you want to x+sum(y), do `select min(x)+sum(y) from t join s` (or use `max` instead of `min`); `x` will be `count(y)` duplicate rows, but to avoid bare columns, we select one of `x`'s rows, and only `min` and `max` select one row without regard to other rows.
* using a `distinct` query whose result attribute set contains an attribute having a primary key is redundant
* `distinct` means inefficiency in the form of pruning a query; we've asked for data, then discarded some of it—so why did we ask for it, then?! good schema & query design sees that `distinct` should not be used often.
* `having` is a predicate applied to groups produced by `group by` or aggregates which may implicitly be over one group of the whole set
* refactor nested queries, _(top-level,nested)_, into a flat one with join.
  ** this is ostensibly possible generally when _nested_ is:
    *** `from` one table (i.e. _correlated_)
    *** used in an `any`, `all`, or `exists` predicate
  ** if the query planner can determine that uncorrelated subquery returns at least one row, then the query planner should flatten.
  ** example: refactor `select x from t1 where x = (select y from t2 where p)` into `select x from t1 join t2 where x = y and p`
* if multiplicity is inconsiderable, then use `union all` instead of `union` because it's faster
* use indexes in `where` &al clauses. e.g. if `a` is indexed, `where f(a)=b` will not use the index! you'd need to have indexed `f(a)`. predicates like `between`, comparison operators, and `like` use indexes. some functions like `min` & `max` should use indexes, too.
* aliasing all tables and using qualified attributes is safer than not; it ensures that you don't accidentally use a wrong attribute that happens to be in scope from another table; if you were to use a qualified attribute name, then you'd get an error saying that that table does not have said attribute.
* `where` is evaluated after joins; if your join lacks results, consider moving your `where` predicate into the join clause
* `[not] in` is fine if you're using literals, but if its arg is a subquery, that's an antipattern; use `except` or left join with `where is [not] null` instead.

==== semantics (sql)

* booleans should always be encoded as a `boolean` type, if that's unfortunately what your sql engine uses; else 0 or 1. never use `null` or `not null` to encode booleans; it's simply incorrect no matter how you measure it.
* prefer fixed precision (often called `numeric` or `decimal sql`) instead of `float` or `real`. if your engine doesn't support that, then you can emulate it by a table with `num` and `den` columns; or just use fixed-point numerals.
  ** at least in sqlite, `floor` retains a real if any real was part of the return expression; if the expression was composed entirely of integers then `floor` is redundant and returns an integer.
* ``select``ing a mix of grouped or aggregate with non-grouped/aggregate data is handled differently by each sql engine. it's best to not mix; refactor queries into all-aggregate/grouped or all-not.
  ** sqlite, perhaps among others, calls non-aggregate columns among aggregates _bare_ columns
  ** in sqlite at least, bare columns' values are deterministic if only one of `max` or `min` aggregate functions is selected
  ** see §2.[4,5] of sqlite docs for `select`
* because sql table identifiers are not first-class (i.e. we cannot, in sql, programmatically generate a table name then reference it i.e. table names must be literal syntax rather than expressions), the only way to keep lisp-grade flexibility [dynamicism] is to use the lisp encoding or something that does not require creation, modification, or reference of a dynamic identifier.
* `x not in (select a from t)` may return ∅ if the `select` returns a set containing `null`; the whole query would translate to `x not in (b,...,null)`, which is equivalent to `not(x=b or x=... or x=null)`. in 3-valued logic, which sql uses, `x=null` is an expression involving `null`, so the whole expression evaluates to `null`. the solution is to use `exists`, which uses 2-valued logic. other solutions are `except`, `where x is not null`, or, if your sql engine supports it, `left outer join`
* predicate evaluation order is nondeterministic e.g. in `isint(a) and a > 0` may fail with "can't apply > 0 to string" since that may be evaluated before `isint`. cte's are not a solution; they suffer from the same non-deterministic evaluation order. `case` is a solution because it has definite evaluation order.

.grouping & bare columns examples

in the following query, `a` is not a bare column because it is in the `group by` clause, so `a`'s value is properly determined in the result set:

[source,sql]
----
create table x(a,b);
insert into x values("x",1),("x",2),("y",34),("y",65);
select a,sum(b) from x group by a;
┌───┬────────┐
│ a │ sum(b) │
├───┼────────┤
│ x │ 3      │
│ y │ 99     │
└───┴────────┘
----

according to sqlite v3.39's `select` docs, §2.4, `group by` associates each row with a group. `select a,f(b) from t group by e` where `e` is an expression that uses [only?] `a`, should be a common idiom. idk how `select` behaves if `e` uses multiple column ids. 

`select a,1.0/count(x) from x` returns only one arbitrary column. `a` is bare here. fix: `select a,cnt from x join (select 1.0/count(*) as cnt from x)`.

==== using sql engines efficiently

* query attributes' order should match a compound index's. not sure if this applies to ordering only in `order by` or if it's important in the selection attributes, or elsewhere,...? or for which engines this is a concern. furthermore, i saw an example whose order was opposite the index, so what's that about?
* except in `count(*)`, the asterisk form is inefficient and its abstraction can cause problems when schemata are modified
* as tables become large, `exists` becomes faster than `distinct`. refactor `select distinct * from t1,t2 where t1.x=t2.y` into `select * from t1 where exists (select 0 from t2 where x=y)`. 0 is a dummy value; we use `exists` to determine whether its argument query is empty, and we _must_ `select` _something_, so we choose a dummy value.
* `having` forces the query planner to not use indexes. refactor `select x,y from t group by z having w` into `select x,y from t where w group by z`
* `in` is more efficient (b/c it uses indexes) than `or` *when the `in` list contains only constants*. e.g. `x=1 or x=2` is better as `x in (1,2)`
* columns that you'll join should be indexed

==== recepies / particular use cases

* a ⊂ b (i.e. all a are in b) is said as `a in b`
* x ∈ a ∧ x ∈ b (i.e. at least one of a's elements is in b) is rephrased into x ∈ a ∩ b, which is `x in a intersect b` in sql
* use views to act as recursively-defined tables by defining the view in terms of `with recursive ... select ...`
  ** see sqlite docs' `lang_with.html` page, §3 for exact details
* rather than store filepaths, store their contents as blobs; this way deleting an item from the db actually deletes the file, as one would probably expect.

[TODO]
* how to (especially efficiently) produce a shuffle of a table?
* suppose i've a table `t(a,b,c,...)`, and i want to effectively `with x(x) as (select * from t where p) select sum(a)/x,sum(b)/x,sum(c)/x,... from t`; how to do this for arbitrary number of `a,b,c`, and how to effectively do `(apply / '(sums union all x))`?
  ** we'd need to store a,b,c,... in rows....

==== attributes with multiple values (wip)

NOTE: developing this example is halted until i thoroughly study relational algebra, and take a course in sql from a seasoned professional. also consider the dependents/parent fact stated above.

not all tables are rectangular. sometimes we want to store tables within other tables i.e. have attributes each with multiple values. to effectively do this, we store, in each row, a _pointer_ to another table's row, which will contain multiple data for that attribute. for this example, we'll consider the song _Gold Digger_, which two artists—Kanye West and Jamie Foxx—which sits in a table `songs(title,artist,album)`

one non-solution is storing artist as a string e.g. `"Kanye West feat. Jamie Foxx"` or `"Kanye West, Jamie Foxx"`, then searching on `artist like "Kanye West" and artist like "Jamie Foxx"`. this fails because `like` may match an inappropriate substring, e.g. i search for "James" (the artist who sang the 1990's hit, _Laid_) but also get songs by James Blunt, since `"James" like "James Blunt"`. the solution would be to use `=`, but that obviously fails.

we need a solution that properly stores multiple data as multiple data—namely rows. thus `artist` would be a foreign key to an `artsts` table and there'd be, for every song, one row per artist, e.g. `insert into songs(title,artist,album) ("Gold Digger",1,1),("Gold Digger",2,1)` which reference `(1,"Kanye West"),(2,"Jamie Foxx")` in `artists`. the full code follows:

[source,sql]
----
create table songs(id integer primary key, title);
create table albums(id integer primary key, album);
create table artists(id integer primary key, artist);
create table lib(title integer references songs(id), artist integer references artists(id), album references albums(id));
insert into artists values(1,"Kanye West")       , (2,"Jamie Foxx"), (3,"James"),        (4,"James Blunt");
insert into albums  values(1,"Late Registration"), (2,"The 90's")  , (3,"Back to Bedlam");
insert into songs   values(1,"Gold Digger")      , (2,"Laid")      , (3,"Billy");
insert into lib(title,artist,album) values(1,1,1),(1,2,1),(2,3,2),(3,4,3);
select songs.title,artists.artist,albums.album from lib join songs on lib.title=songs.id
                                                        join artists on lib.artist=artists.id
                                                        join albums on lib.album=albums.id;
┌─────────────┬─────────────┬───────────────────┐
│    title    │    name     │       title       │
├─────────────┼─────────────┼───────────────────┤
│ Gold Digger │ Kanye West  │ Late Registration │
│ Gold Digger │ Jamie Foxx  │ Late Registration │
│ Laid        │ James       │ The 90's          │
│ Billy       │ James Blunt │ Back to Bedlam    │
└─────────────┴─────────────┴───────────────────┘
select songs.title,artists.artist,albums.album from lib join songs on lib.title=songs.id
                                                        join artists on lib.artist=artists.id
                                                        join albums on lib.album=albums.id
                                               where artists.artist="Kanye West" or artists.artist="Jamie Foxx";
-- NEXT: vary the recursive query to produce #(("Gold Digger", "Kanye West, Jamie Foxx", "Late Registration"))
with recursive
  x(id,ps) as (select id+1,value from c where id=1),
  acc(id,ps) as (select * from x
                 union all
                 select id+1,printf("%s %s",ps,value)
                 from acc natural join (select * from c where id>1))
select ps from acc order by id desc limit 1;
----

. we need to store each song as having its own `id` because it's possible, though unlikely, that two artists that did a song together also each did two different songs of the same name on different albums. actually, even crazier: for a few (artist,album)s in my library, there are two different songs of the same name.

.no need to organize data

if you've having trouble organizing your table schemata, you can always use a simple but inefficient encoding in one table. considering the last example differently: say that you want a music db, and you first suppose that artists have albums, and albums have songs; thus your songs should foreign key ref an album, and likewise an album should ref an artist. simple. oh, wait, though; some songs have no albums, and some albums (or songs) have multiple artists. rather than worry about how to "solve this problem," just `create table songs(name string, artist string, album string)` without worrying about foreign keys. any song can now support multiple artists by using multiple rows e.g. `insert into songs values("Gold Digger","Kanye West","Late Registration"),("Gold Digger","Jamie Foxx","Late Registration")`. this encoding is less efficient, but simple, and works; it's therefore useful for encoding data while you're sill developing your database. obviously we can make this more efficient just by making `album` an `integer` which is a foreign key to a table `albums(id,name string)`.

.alternative: lisp encoding

[source,sql]
----
-- general lisp encoding tables: lists & atoms
create table lists(id integer primary key, parent integer, foreign key (parent) references lists(id));
create table atoms(value,                  parent integer, foreign key (parent) references lists(id));

-- song-specific stuff. by lisp alists, this would be (songs . ((name album)))
create table songs(name string, artist string, album string, foreign key (album) references albums(name));
insert into lists values(1,null),(2,1);
insert into atoms(a,1),(b,1),(c,2),(d,1);
---- 

NOTE: lisp encoding cannot accomodate multiple indexes.

=== language design problems (inelegance & inability)

consider `select aapl.c,goog.c from aapl join goog using d`. note how verbose this would become if i were to consider an arbitrary number of tables, despite that being a simple idea. the problem is that columns are not row types; they're less flexible. furthermore, that sql cannot transpose is a serious limitation! indeed, this lang-specific asymmetry limits the metaprogrammability of sql. this certainly is what makes sql bound to being poor, while the relational db model is good.

* columns in a select statement must be hardcoded. i cannot, for example, say `select (cond col1="x" => col2,col3; col1="y" => col3; ...; else *) from t`.
* there's neither support for naked variables (e.g. `x := 3` not explicitly of a table) nor eponymous tables (or views) e.g. `create table x(x)` (to my knowledge yet.)

=== relational algebra

.terminology

[options="header"]
|===================================================
| relational algebra | common name or implementation
| tuple              | row
| attribute          | column (w/type if applicable)
| relation/selection | table
|===================================================

* _constraints_ on a table or column [attribute], e.g. `UNIQUE`, `NOT NULL`, `FOREIGN KEY`, `PRIMARY KEY`. they're verify-only constraints, not adding functionality, and so should be avoided (except indexes, should those be considered constraints)
* tuples are unordered, instead being expressed as attribute-tagged unions
* a tuple's set of attributes is called its _heading_, _domain identifying list_, or when as an argument to projection (see below,) a _projection list_. the heading is a list of indexes, whether ordinal or nominal.
* a set of tuples sharing a common heading is called a _body_
* a relation can thus be partitioned into a heading and body

degree:: number of attributes
schema:: heading with constraints (all needed to produce a selection)

.primitive operations

TODO: continue from ~/Downloads/pacific75-eval.pdf

union-compatible:: having the same attribute (column) sets

* link:https://en.wikipedia.org/wiki/Selection_(relational_algebra)[selection (aka _restriction_)] (σ_pred(R)): filter by predicate
* link:https://en.wikipedia.org/wiki/Projection_(relational_algebra)[projection] (π) of a heading onto a table, π_L(R) := {r[L]: r ∈ R} is just a subset of R found by restricting to attributes L, which must be a subset of R's original attributes; ior a projection may be a map over R's values, e.g. `select a+2 from R` maps `(+2)` over a ∈ R. only the column space is concerned; the number of rows is unaffected.
* link:https://en.wikipedia.org/wiki/Rename_(relational_algebra)[rename ρ]: rename an attribute
* [flattened cartesian] product (×). TODO: test: in sql lhs & rhs tables must have mutually exclusive attribute sets.
* set difference (aka _relative complement_) (\). requires union-compatiblity and may be defined in terms of union: given relations R & S of equal degree _n_, R \ S = (σ_(r[1] ≠ s[1] ∨ ... ∨ r[n] ≠ s[n])(S)).
* union (∪). union-compatible.
* join
  * natural (⋈): defined when lhs & rhs share exactly one attribute. attribute set is the union of lhs' & rhs' attribute sets. (e.g. join a,b,c and b,c,d = a,b,b,c,c,d)
  * inner (intersection in relation algebra): natural but without repeated columns [WRONG] (e.g. join a,b,c and b,c,d = a,b,c,d). union-compatible? not in sql! or perhaps this could be said to be a succession of projection then union.
  * outer: flattened cartesian product
  * left or right
* division: for relations R & S of headings A & B (without repitition) of degrees m & n respectively, the division R[A÷B]S is a subset of π_A'(R), viz {r[A']: r ∈ R ∧ ∀s ∈ S ∃r' ∈ R : r[A'] = r'[A'] ∧ r'[A] = s[B]}. definitions vary when S is null.

the _theta join_ is a non-primitive operation: x θ y = σ_pred(x ⋈ y), expressed in sql as `select attrs from x natural join y where pred;`

the relational algebra is closed under all these operations.

NOTE: *for the love of god, use `BEGIN TRANSACTION` &al*

=== the language

==== semantics

* as per sqlite's graphical grammar description for `expr`, `column-value` is a valid `expr`.
* sqlite stores table schemata as strings rather than as tables (despite the style of `pragma table_info(t)`'s output); this is a design oversight that must be dealt with in a hacky way (see the `alter table` docs)
* both `0` is falsy in sqlite. anything other than null is truthy. null is neither truthy nor falsy; `select x from t where x` will select truthy `x`; `... not x` will select where `x=0`. in neither case will null x's be returned.
* when a sqlite db can be opened read-only, we can still create and modify temporary tables
* everything is a table (multiset of tuples whose positions may be bound to, in a given conext, a name) viz the results of statements, which can be enclosed in parens, e.g. `select * from (select * from mytbl) t`
  * such statements are called _derived tables_
  * thus tables can be locally bound. this allows passing multiple data, e.g. `select * from (values(1),(2),(3)) t` to mean scheme `(values 1 2 3)`
    * this is apparently equivalent to `select * from (select 1 as a from dual union all; select 2 as a from dual union all; select 3 as a from dual) t`
  * _rows_ have no special meaning; they're just singleton tables. all operations are over tables.
    * generally all operations are on the entire table
* if both args to `/` are integers, then `/` is integer division. `cast(expr as real)/cast(expr as real)` to ensure floating point division. however, it's best to use rational arithmetic (`numeric` or `decimal sql` types, if supported) or fixed point arithmetic, instead of floating point.

[options="header"]
|==============================================================================
| sql                 | java 8, math, or scheme
| table               | list of vectors
| `where` & `having`  | filter
| `group by`          | concatMap (useful for aggregates only)
| `except`            | \
| `order by`          | sort
| `union all`         | concat
| `union`             | distinct concat
| `with`              | `letrec`
| `check`             | guards
| `join`              | flatmap [TODO: how?]
| `collate`           | specifies sort fn to be used by `order by`. may be specified in column spec or `expr` grammar
| `escape`            | TODO
| `exists`            | whether argument select query returns non-empty
| `frame-spec` grammar  | TODO
|==============================================================================

TODO: consider (in `expr` grammar): 

===== joins

all joins are refinements of cartesian product. `join` (or comma) is cartesian product. `join on <pred>` filters cartesian product to those matching `pred`. `join using attrs ...` is shorthand for `join on t1.attr=t2.attr ...`. `natural join` is shorthand for `join using X` where `X` is the intersection of tables' attributes.

* `inner` & `cross` are redundant; just say `join`. however, as a non-standard sqlite feature, `cross` prevents query optimizer from reordering input tables.
  ** `cross` join means "cross product" as in cartesian product
* `outer` applies only to `left`, `full`, and `right` joins. idk what `outer` is.
  ** `inner` is inapplicable to `left`, `full`, and `right` joins. 
* `left` join is just `join` unless an `on` or `using` clause is provided.
* `full` & `right` are currently unsupported in sqlite; at least `right` is redundant: `x right join y <join-clause>` = `y left join x <join-clause>`

.examples
[source,sql]
----
-- kinda odd that we can't just do create tablet(a1,...) as (values...)
create table x as with x(a,b) as (values(1,2),("x","y")) select * from x;
create table y as with x(o,b) as (values(6,"y"),(100,2),(101,"B")) select * from x;
-- it's honestly probably nicer to instead use separate create table & insert statements
select * from x left join y using (b);
┌───┬───┬─────┐
│ a │ b │  o  │
├───┼───┼─────┤
│ 1 │ 2 │ 100 │
│ x │ y │ 6   │
└───┴───┴─────┘
select * from y left join x using (b);
┌─────┬───┬───┐
│  o  │ b │ a │
├─────┼───┼───┤
│ 6   │ y │ x │
│ 100 │ 2 │ 1 │
│ 101 │ B │   │ -- (101,B,NIL)
└─────┴───┴───┘
----

in `a left join b`, all of `a`'s rows are present, but some of their corresponding `b` attributes may be null, namely when there _are no_ corresponding `b` attributes.

==== syntax

comments: `-- ... ` for single line, `/* ... */` for multiline

`table.attr` disambiguates when `attr` is shared by multiple tables; otherwise attr is resolved against the table of the `from` clause.

.basic operators
|======================================================================
| &          | bitwise and
| \|         | bitwise or
| ^          | bitwise xor
| += &al, %= | assignment can be used for variables bound in a funcbody
| &=         | bitwise and assignment
| ^-=        | bitwise or assignment
| \|*=       | bitwise xor assignment
| \|\|       | strcat (casts both args to strings if needed)
|======================================================================

===== `create table`

* `create table as` still inserts a table into a database. it's used to init a table at declaration time, for convenience.
* `temp` tables are accessible in the remaining sql script, but are not persistent; it isn't inserted into the database, and so doesn't exist after the sql script that created it finishes execution.

===== user-defined functions (not supported by sqlite)

[source,sql]
----
-- define
create procedure foo @param1 nvarchar(30), @param2 nvarchar(10) as
select * from customers where p2 = @param1 and p2 = @param2
go;

-- invoke
exec foo @param1 = 42, @param2 = "stuff";
----

===== columns

====== `case`

determines a column's value. syntax: `case [when <cond> then <value>]+ [else <value>] end`.

.examples

[source,sql]
----
select customername, city, country from customers
order by case when city is null then country else city end

-- or
select case when city is null then country else city end from customers
----

====== `exists`

should be called `any`, but oh, well. `exists <select-stmt>` checks whether the selection is (not) empty. when used in `case`, one can effectively do `<|>`/`asum`.

====== null

* ifnull(<col>,<val>)
* isnull(<col>) -- returns bool. called nvl on oracle.
* coalesce(<col>) -- 1st non-null value in a list. generalizes `ifnull` to accept multiple values each of which may be null (though it'd be expected that at least one isn't)
  ** it's common to supply a default literal value as the last arg. this guarantees prevention of null propogation

====== constraints

all constraints can be added or dropped via `alter table` or can be added in `create table`

* primary and foreign keys
* `check`, which guards inserts
* default
* indexes
* auto increment

===== filters

* `having` is simply `where` that is a boolean of aggregates instead of per row, e.g. `having count(x) > 5`. using count
* `where` clause accepts things that eval to bools
  ** <, = &al common equivalence relations and boolean conjunctions
  ** between <lb> and <ub>
  ** in <set>
  ** like <pat> (useful only for strings)
    *** `%` is regex `/.*/`
    *** `_` is regex `/./` 
    *** regex-style character classes
  ** exists
  ** <attr> <bin_comp_op> <`any` | `all`> <single_col_tbl> -- `any` is called `some` in some sql implementations

===== result set modifiers

* order by
* limit (or `select top <number> [percent]` in MSSQL; or `fetch first <number> rows only` in oracle 12+) 
* group by

==== table set operations

===== union

union tables' rows. valid only for tables of equal column sets. `union` returns sets; `union all` returns multisets and preserves order like ++.

==== `with` & recursion (common table expression (CTE) subquery refactoring)

NOTE: see §3 of the sqlite docs' `lang_with.html` page for exact description of recursion structure & evaluation.

this is how we do local binds.

TODO: cf normal aliases

* supports recursion
* exists temporarily: discarded after the statement that uses its binds
* considered a cleaner alternative to temp tables
* alternative to views (prob like `let*` in alt to `define` in funcbods)
* repeated aggregations, e.g. avg of maxes
* "overcome constraints such as what `select` has, e.g. non-deterministic `group by`"

.`let*`
[source,sql]
----
with
  t1(v1, v2) as (select 1, 2),
  t2(w1, w2) as (select v1 * 2, v2 * 2 from t1)
select *
from t1, t2
----

produces

[options="header"]
|==================
| v1 | v2 | w1 | w2
| 1  | 2  | 2  | 4
|==================

could use `values` instead of `select`; `values` is just `select` but more efficient and without a limit on number of supported rows.

.`letrec` generator example
[source,sql]
----
with recursive t(v) as (values(1) union all select v+1 from t where v < 5) select v from t;
----

NOTE: despite the SQL99 standard spec, sqlite appropriately does not require `recursive` in order for a cte to be recursive.

this does not produce (1)++(2),(3)++(3),(4),(5)++.... `select` does not return the whole table on each iteration; as described in `with§3`, one item is taken from a queue (step 2a); `select` is a misnomer in recursive queries.

produces a column `v` with five rows of values 1 through 5, effectively equal to haskell `take 5 (Data.List.NonEmpty.unfoldr (\n -> (n, Just $ n + 1)) 1)`. the definition of `t` is unbounded; the bound is in `limit 5`; therefore locally bound tables (at least when bound with `recursive`) are not stricted evaluated before the body of the `select` statement.

.example: trace predecessors/ancestors

this works for a tree, or more generally a dag.

[source,sql]
----
create table x(id integer, prev integer, val integer);
insert into x values(1,null,20),(2,1,40),(3,2,50),(4,2,100),(5,4,200),(6,3,400),(6,4,300),(7,6,1000);
select * from x;
┌────┬──────┬──────┐
│ id │ prev │ val  │
├────┼──────┼──────┤
│ 1  │      │ 20   │
│ 2  │ 1    │ 40   │
│ 3  │ 2    │ 50   │
│ 4  │ 2    │ 100  │
│ 5  │ 4    │ 200  │
│ 6  │ 3    │ 400  │
│ 6  │ 4    │ 300  │
│ 7  │ 6    │ 1000 │
└────┴──────┴──────┘
with recursive y(id,prev,val) as (select * from x where id=4
                                  union -- union all produces some redundancies, since the graph is a dag
                                        -- rather than a mere tree
                                  select x.id,x.prev,x.val from y join x on y.prev=x.id)
select * from y;
┌────┬──────┬─────┐
│ id │ prev │ val │
├────┼──────┼─────┤
│ 4  │ 2    │ 100 │
│ 2  │ 1    │ 40  │
│ 1  │      │ 20  │
└────┴──────┴─────┘
----

maybe unexpectedly, we select from `x`, not `y`! `[...] select y.id,y.prev,y.val from [..]` is unbounded recursion.

.example: trace successors/descendants

this works for a tree, or more generally a dag.

for descendants instead of ancestors, simply swap `y.prev=x.id` with `x.prev=y.id`:

[source,sql]
----
with recursive y(id,prev,val) as (select * from x where id=4
                                  union
                                  select x.id,x.prev,x.val from y join x on y.id=x.prev)
select * from y;
┌────┬──────┬──────┐
│ id │ prev │ val  │
├────┼──────┼──────┤
│ 4  │ 2    │ 100  │
│ 5  │ 4    │ 200  │
│ 6  │ 4    │ 300  │
│ 7  │ 6    │ 1000 │
└────┴──────┴──────┘
----

see §3.[3-5] for very useful graph/traversal considerations.

===== insert

* `select <cols> into <new_tbl_name> [in <external_db>] from ...` is equivalent to a sequence of `create table` and `insert` statements (not available in sqlite)
  ** remember that you can use `as` to rename the columns. they'll retain their column attributes.
  ** `select * into <newtable> from <oldtable> where 1 = 0;` creates a new empty table with the same schema
* `insert into <dest> select <cols> from <src> ...;` is the same but for a table that already exists. both tables must be of the same schema.

===== table ops

* `alter table` changes schema
* <create | drop> db
* <create | drop> table

==== compound or miscellaneous/general examples

.units

[source,sql]
----
create table to_mg(oz,g);
insert into to_mg values(28349.5,1000);
select 3*g,12*oz from to_mg;
----

this sees using a table as a simple ad-hoc relation. obviously the symmetry constraint here restricts the table from holding expressions beyond mere literal values. such functionality would require first-class λ's or `eval` (see sql metaprogrammability section.)

.merge with default value

[source,sql]
----
create table x(id,y);
create table y(id,z);
insert into x values(0,1),(1,20);
insert into y values(1,10),(20,40);
insert into x values(10,100);
select y,case when z is null then 2000 else z end as z from x left join y on x.y=y.id;
┌─────┬──────┐
│  y  │  z   │
├─────┼──────┤
│ 1   │ 10   │
│ 20  │ 40   │
│ 100 │ 2000 │
└─────┴──────┘
----

.get successive integer 

we get the greatest integer in the table, or if the table is empty, then start with 10.

[source,sql]
----
create table x(id integer);
select case when count(id) > 0 then max(id)+1 else 10 end from x; -- 10
insert into x values(100);
select case when count(id) > 0 then max(id)+1 else 10 end from x; -- 101
----

.tic tac toe

this example demonstrates many things about how to reason about relations. to start, the 3×3 grid will not be a table with 3 rows and 3 columns. think about how you'll check for a winner: you'll want to check each of the rows, and each of the columns (and each of the diagonals, too.) to check all of the columns, you'll want to use the same logic for each column, just a different column number. ah, there's one hint: we want column _numbers_; sql does not number columns. columns are fixed and must be addressed by name. rows, on the other hand, are arbitrary in number and are all treated the same. furthermore, we want code that generalizes non-verbosely to higher dimensions, say for _connect four_. x & y should be treated the same; thus we'll use `(x,y)` indices. x's & o's will be stored as -1 and 1 respectively; an empty cell is 0. this makes checking for winners easy: if the absolute value of the sum _s_ of a row, col, or diag is 3, then the winner is `sign(s)`.

[source,sql]
----
-- make the grid
create table grid(x integer, y integer, v integer default 0, primary key (x,y));
insert into grid(x,y) select * from generate_series(1,3) as x join generate_series(1,3) as y; -- generate_series(x,y) is interval [x,y]
-- assume that player just moved, which updates grid. now check for winner:
select sum(v) from grid where x=y;   -- one diagonal
select sum(v) from grid where x=4-y; -- the other diagonal
select sum(v) from grid where x=1;
select sum(v) from grid where x=2;
select sum(v) from grid where x=3;
select sum(v) from grid where y=1;
select sum(v) from grid where y=2;
select sum(v) from grid where y=3;
----

ugly as sin, eh? clearly we're considering the cartesian product {x,y}×[1,3], so our code should reflect that. `where x=n` is here actually a poor way of referring to the set {(x,y)|x=n}! that set is described properly as a cartesian product in sql:

[source,sql]
----
with t(x,y) as (select * from (values(1)) join (select * from generate_series(1,3))) select * from t;
┌───┬───┐
│ x │ y │
├───┼───┤
│ 1 │ 1 │
│ 1 │ 2 │
│ 1 │ 3 │
└───┴───┘
----

we could `natural join` that table with grid on `(x,y)`. (btw, expressions like `where (x,y)=(1,2)` are valid!) however, this is a perfect use case for `group by` & the `sum` aggregate. the finished code is:

[source,sql]
----
create table grid(x integer, y integer, v integer default 0, primary key (x,y));
insert into grid(x,y) select * from generate_series(1,3) as x join generate_series(1,3) as y;
-- check diagonals
select sum(v) from grid where x=y;
select sum(v) from grid where x=4-y;
-- check rows & columns
select * from grid group by x having abs(sum(v))=3;
select * from grid group by y having abs(sum(v))=3;
----

so there you go: checking for winners in tic tac toe simply by 4 queries. maybe it can be syntactically shorter, but this is a good encoding of the game's rules: you win if you cross any row, column, or diagonal.

we see that `group by` partitions by equality, which is analagous to the set of (sets each one of whose axes' value is fixed.)

.select by day

[source,sql]
----
select * from tbl where strftime("%Y-%m-%d",date) = "2022-07-01";
----

`date` may be a datetime or date string.

.resample 1m candles into day candles (single day)

[source,sql]
----
with x(start,end,high,low,open,vol)
  as (select strftime("%Y-%m-%d",min(datetime)), max(datetime), max(high), min(low), open, sum(vol)
  from AAPL where datetime between datetime("2010-01-04 09:30") and datetime("2010-01-04 16:00"))
select start,high,low,open,vol,close from x join (select close from AAPL where datetime = (select end from x));
----

in a common proglang this would be like:

----
let t = {AAPL | datetime ∈ ("2010-01-04 09:30", "2010-01-04 16:00")}
    end = max(t.datetime)
    close = t[end].datetime
 in (start,high,low,open,vol,close)
----

the `join` is not done as a cartesian product, but instead should be interpreted as putting the `close` at `end` into the `select` clause's scope. `x` is a local binding. if i'm using sql from another proglang, then alternatively i could have stored `x` as its own table (a non-local binding) then done `select start,...vol from x` in one query and `select close from AAPL where datetime = (select end from x)` in another.

`open` needs neither aggregate nor other special calculation because for any data selected among aggregates, the first encountered value is used in practice, though according to sqlite's documentation (§2.4 of the `SELECT` docs), "each non-aggregate expression in the result-set is evaluated once for an arbitrarily selected row." if this turned-out to be a problem in practice, then we'd need to endow it with similar logic as we used for `close`.

NOTE: the datetime format requires leading zeroes for all values, e.g. day, hour, &al.

.resample 1m candles into day candles (multiple days)

[source,sql]
----
with x(start,end,high,low,vol) as (
  select min(datetime), max(datetime), max(high), min(low), sum(vol)
  from x_AAPL
  where datetime between datetime("2010-01-01") and datetime("2010-02-01")
    and time(datetime) between time("09:30") and time("15:59")
  group by strftime("%d",datetime)
)
select strftime("%Y-%m-%d",start),high,low,open,close,vol
from x join (select datetime as cdt, close from x_AAPL) on end = cdt
       join (select datetime as odt, open  from x_AAPL) on start = odt;
----

returns

----
2010-01-04  30.64  30.34  30.48  30.59  116694802
2010-01-05  30.79  30.46  30.64  30.62  136014592
2010-01-06  30.74  30.10  30.62  30.13  133300727
2010-01-07  30.28  29.86  30.25  30.08  113809059
2010-01-08  30.28  29.86  30.04  30.27  104221936
2010-01-11  30.42  29.77  30.41  30.01  111353487
2010-01-12  29.96  29.48  29.88  29.67  129700571
2010-01-13  30.13  29.15  29.69  30.05  145122992
2010-01-14  30.06  29.86  30.01  29.91  98356076
2010-01-15  30.22  29.41  30.13  29.41  130680837
2010-01-19  30.74  29.60  29.76  30.72  161574329
2010-01-20  30.79  29.92  30.69  30.26  148014426
2010-01-21  30.47  29.60  30.29  29.74  145818463
2010-01-22  29.64  28.16  29.54  28.25  205441418
2010-01-25  29.24  28.59  28.93  28.92  216214306
2010-01-26  30.53  28.94  29.39  29.41  425729542
2010-01-27  30.08  28.50  29.54  29.71  417601177
2010-01-28  29.35  28.38  29.27  28.47  281731401
2010-01-29  28.88  27.17  28.72  27.44  300374774
----

=== implementation-specific

TODO: this document should be stored as database table with indexes on both topic and sql implementation. furthermore, searching sql (with regex) is better than ripgrep.

==== output

.sqlite output modes

`.mode <mode>` changes output.

* pretty:
  ** `box` uses unicode box drawing characters
  ** `column`: clean
  ** `table`: boxes drawn with plus, hyphen, and pipe
* easily parsed:
  ** `list` (default)
  ** `json`
  ** `csv`
* special output:
  ** `html`
  ** `insert`: sql insert statements; good for copying from one table to another, but not for duplicating table schema. for that you'll likely want `.clone` or using a system shell to copy the db then use sql to modify the copy.

all except `list`, `csv`, `insert`, `html` force headers to be displayed. other modes aren't good.

==== performance

* gather multiple successive statements into transactions (see your db's docs for the `TRANSACTION` keyword)
  ** at least in sqlite, all actions occur in a transaction, and creating & destroying transaction is non-trivial like creating & destroying pthreads.
* sqlite (and perhaps others?): prepare statements that will be executed multiple times. TODO: ipossile only in sqlite (which defines a bytecode) when invoking it from other langs (i.e. preparation isn't possible in sqlite's repl)?
  ** e.g. with connection `d` to db containing table `x(a,b,c,d)`, `(define st (prepare "insert into x values(?,?,?,?)")) (call-with-transaction d (λ _ (query-exec d st 1 2 3 4) (query-exec d st "A" "B" "C" "D")))`. note that the prepared statement can be free in its parameters' values.
* sqlite `PRAGMA synchronous=OFF` disables the usual waiting for data to be safely on disk, thus making writes faster but making corrupton possible.

[TODO]
* sqlite: can i prepare a transaction statement? i should be able to, if transaction is symmetric. otherwise i'll use transactions all of whose statements are prepared.

.exceptions

* akavache is designed to be efficient without the user trying
* sqlite in-memory dbs are probably fast no matter what

==== mutiple databases

[source,sql]
----
create table table1(x integer);
attach database "db2.db" as db2;
create table db2.table1(y integer primary key autoincrement);
insert into main.table1 values(56);
insert into main.table1 values(90);
insert into db2.table1 select * from main.table1 limit 1; -- table1 of file "db2.db" now contains 56.
----

.common

* `insert into t1 (a, b, c) select a, b, c from t2;`
* `all` (cf `distinct`) is often not supported. this is fine because it's the default anyway.

.sqlite3-specific execution

* to open a db as read-only, specify its location as a URI, then append a query: `file://<path>?mode=ro`

.quoting

|===================================================================================================
| single quotes | string literal
| double quotes | identifier (used to, e.g. use a keyword as a symbol
| brackets      | (non-standard) identifier, same as double quotes. used by MS-SQL server and sqlite
| backticks     | (non-standard) identifier. used by MySQL and sqlite
|===================================================================================================

see link:https://www.sqlite.org/lang_keywords.html[sqlite's documentation] on parsing quoted strings.

.csv to sqlite

NOTE: sqlite has a csv virtual table plugin

prefer using link:https://github.com/harelba/q[q] (not in nixpkgs,) which allows running sql on multiple csv files or sqlite databases.

use package `csvs-to-sqlite`. you'll probably want to use options `pk`, `d` or `dt`, `i` whose arguments are the column names as in first row of csv file. if you use these options, then you'll need to run the command for each table that you want to add, unless the tables share common columns for which the options apply.

it's likely in your best interest to add csvs as tables into a db, then use sql to create a new table, rather than doing this all at once programatically.

.pragmas useful for implementing metaprogramming 

usage notes:

* all pragmas may be more usefully used as relations e.g. instead of `pragma table_info("t")`, use `select * from pragma_table_info("t")`.
  ** any typo in a pragma will silently do nothing (e.g. `pragma table_infos("t")`); however, the virtual table form will fail appropriately if there's a typo *in the pragma name* e.g. `select * from pragma_table_infos("t")` will say "no such table pragma_table_infos." however, as the table name is just a string literal, if you give a table name that does not exist, then the empty relation will be returned.

pragmas:

* pragma `table_list` gives more info than `.tables` and can be used in `sqlite3_exec` instead of only in a repl
* describe a table: `table_info("n")`. no effect or empty relation if n ∉ db.
  ** `table_xinfo` is the same but also shows hidden columns
* `function_list`: all functions (and their types) available on current db connection!
  ** `s` means per row; `w` means aggregate.
  ** narg<0 denotes variadic fn
  ** nullary functions must have trailing parens e.g. `random()`, `pi()`. the trailing parens distinguishes them from column names.
    *** some nullary functions are usefuly only as window functions, e.g. `percent_rank()` or `cume_dist()`

for functions to accept args instead of relations is foolish design.

=== reldb programming

using (generally reldbs, currently practically sqlite) as a proglang.

* model: declarative, array-based
* bools are 0 & 1
* each shape gets a table
* `with` locally binds 
* virtual tables, table-valued fns & extensions e.g. https://www.sqlite.org/src/artifact?ci=trunk&filename=ext/misc/series.c
  ** TODO: explore
* control flow:
  ** recursion in `with`
  ** `case` 
* folds are called _aggregate functions_
* like a properly set-theoretic language, everything is sets. this is like apl and unlike lisp; in lisp `1` ≠ `'(1)`; if one were considering a datum that may be either a thing or a thing attached to some properties (e.g. `'(1 to 6)`), one would need to break symmetry: `(cond x [(number? x) ...] [(list? x) ...])`, which is just stupid. it's much better to store everything in sets, even if forced to name attributes—sql `with t(x) as (values(1)) select x from t`—which maintains symmetry and does not change form when generalized e.g. adding an attribute to `t`. plurality is a common generalization of singularity, and is thus a more appropriate form than supporting both singularity & plurality. this being said, the requirement for everything to be named does not imply that things must be named _in syntax_; any syntax that unambiguously translates to a product type is acceptable, and its brevity is welcome. for example, sql does this when saying `insert into t values(...)`: you do not need to specify column names, because sql infers this from values' ordinal positions. another brief form is `insert into t(x,y) values(...)` where t may contain many more attributes than `x` & `y`.

==== json

sqlite is an excellent json extractor and manipulator. it considers json as a set of flat tables implicitly nested by (`id`,`parent`) relations rather than recursively nested objects (which introduces scoping), thus making arbitrary traversal easy.

* `.mode json` outputs json to stdout
  ** `.once <file path>` writes next query's output to file (so can write table as json to file)
* if using sqlite as a library in another proglang, then conversion from rows to json is trivial
* json is stored as ordinary strings, except return value of `json`
* json is stored in table cells or string literals

.fns

json:: id fn but cod is string pseudo-typed as json.
json_valid:: 0 or 1 whether a value is a (valid) json string.
json_array(e,...):: constructor
json_object(k,v,...):: constructor
json_array_length:: obvious. useful in query predicates.
json_extract:: select elements from json tree. if one path arg given and selected value does not refer to json array, then returns single value as sql atom; else returns json array string.
json_insert, json_replace, json_set:: put: 1. unless exists; 2. when exists; 3. either; respectively.
json_remove:: duh
json_patch:: put (or remove if put to null) values in json object at keys. treats arrays as atoms.
json_each, json_tree:: json tree as sql tables, top set of children only, or children on all levels
json_group_array, json_group_object:: aggregate fn. return selection as json array or object (see example below). take 1 & 2 args respectively.

.operators

both introduced in sqlite v3.38.0 (2022-02-22). they're `json_extract` but:

->:: always returns json string.
->>:: always returns sql table.

.examples
[source,sql]
----
create table d as with x(k,v) as (values("j",'{"a":3,"b":[1,2,3,4],"c":{"d":"hi"}}')) select * from x;
select key,value,type,atom,id,parent,fullkey,path from json_each(v) join d where k="j";
┌─────┬──────────────────────────────────────┬─────────┬──────┬────┬────────┬─────────┬──────┐
│ key │   value                              │  type   │ atom │ id │ parent │ fullkey │ path │
├─────┼──────────────────────────────────────┼─────────┼──────┼────┼────────┼─────────┼──────┤
│ a   │ 3                                    │ integer │ 3    │ 2  │        │ $.a     │ $    │
│ b   │ [1,2,3,4]                            │ array   │      │ 4  │        │ $.b     │ $    │
│ c   │ {"d":"hi"}                           │ object  │      │ 10 │        │ $.c     │ $    │
└─────┴──────────────────────────────────────┴─────────┴──────┴────┴────────┴─────────┴──────┘

select key,value,type,atom,id,parent,fullkey,path from json_tree(v) join d where k="j";
┌─────┬──────────────────────────────────────┬─────────┬──────┬────┬────────┬─────────┬──────┐
│ key │                value                 │  type   │ atom │ id │ parent │ fullkey │ path │
├─────┼──────────────────────────────────────┼─────────┼──────┼────┼────────┼─────────┼──────┤
│     │ {"a":3,"b":[1,2,3,4],"c":{"d":"hi"}} │ object  │      │ 0  │        │ $       │ $    │
│ a   │ 3                                    │ integer │ 3    │ 2  │ 0      │ $.a     │ $    │
│ b   │ [1,2,3,4]                            │ array   │      │ 4  │ 0      │ $.b     │ $    │
│ 0   │ 1                                    │ integer │ 1    │ 5  │ 4      │ $.b[0]  │ $.b  │
│ 1   │ 2                                    │ integer │ 2    │ 6  │ 4      │ $.b[1]  │ $.b  │
│ 2   │ 3                                    │ integer │ 3    │ 7  │ 4      │ $.b[2]  │ $.b  │
│ 3   │ 4                                    │ integer │ 4    │ 8  │ 4      │ $.b[3]  │ $.b  │
│ c   │ {"d":"hi"}                           │ object  │      │ 10 │ 0      │ $.c     │ $    │
│ d   │ hi                                   │ text    │ hi   │ 12 │ 10     │ $.c.d   │ $.c  │
└─────┴──────────────────────────────────────┴─────────┴──────┴────┴────────┴─────────┴──────┘

select json_group_array(key) from json_each(v), d where k="j"; -- ["a","b","c"]
select json_group_object(key,fullkey) from json_each(j), d where k="j"; -- {"a":"$.a","b":"$.b","c":"$.c"}
----

* `path` is the path to the object that contains a given element
* `fullkey` is the path to the given element
* `atom` is not more useful than value, but should be considered a boolean (i.e. null or not) which is useful for query filters
* `v` is in `json_each`'s scope, implying that, in a join, attributes are unioned before virtual tables are computed.

=== triggers

triggers are very powerful. they enable reactive programming aka _hooks_.

[source,sql]
----
create table x as with x(a) as (values(0)) select * from x; -- counter a := 0
create table y(b); -- just some table
create trigger tr after insert on y for each row begin update x set a = (select 1+a from x); end;
select a from x; -- 0
insert into y values(10);
select a from x; -- 1
insert into y values(10),(30);
select a from x; -- 3. if not FOR EACH ROW, would be 2. however, as of sqlite 3.39.2 only FOR EACH ROW is supported, so it's implicit.
----

as you can see, `tr` is a hook that increments counter `a` for each row inserted into `y`.

==== common programming patterns expressed in sql

never assume that a common pattern should be used; instead, *listen to the data*, *follow the implications of design specs*, and then see if the suggested system's (sub)structure(s) happens to exhibit a pattern naturally like a prior-known pattern.

.folds

a fold is a stateful traversal. in reldbs, state is obviously stored, as is everything, in relations. a recursive `with` may be more efficient, however. even more efficient is a fold written as a runtime-loadable extension written in c, loaded by sqlite from a shared library.

`foldl (\a b -> a ++ b) xs`:

[source,sql]
----
create table c(id integer primary key autoincrement, value string);
insert into c(value) values("hello"),("there"),("my"),("good"),("friend");

-- with trim, to remove the leading space character
with recursive acc(id,ps) as (values(1,"") -- initial value (base case)
                              union all
                              select id+1,printf("%s %s",ps,value) from acc natural join c) -- recursive case
select trim(ps) from acc
order by id desc limit 1; -- acc is a scan; get the last element to be effectively a fold

-- proper general solution for folds whose initial object must be the input lists' 1st element
with recursive
  x(id,ps) as (select id+1,value from c where id=1),
  acc(id,ps) as (select * from x
                 union all
                 select id+1,printf("%s %s",ps,value)
                 from acc natural join (select * from c where id>1))
select ps from acc order by id desc limit 1;
----

* we really do use functional style here. we can't use one `with` clause over both an `update` and a `select` statement. rather than use `update` (a stateful, non-functional style), we can use recursion and nested ``select``s. each row is defined in terms of its predecessor.
* `acc` is the named tuple of the fold. `printf` (`format` in other sql engines) is used for string concatenation since sqlite has no separate such function.
* the proper solution binds `x` b/c `select * from c limit 1 union all ...` is invalid syntax; we can't use `limit` there, though `where` is fine there
* i'ven't yet ``explain``ed this query to see its efficiency
* we can't use aggregate functions in predicates; therefore `where id=max(id)` is not a valid alternative to `order by id desc limit 1`

of course, _this_ fold is more easily done by the aggregate `group_concat`, but this example serves generally, when an aggregate may not be already written for it.

.functions

views (especially defined by cte) can represent fns. `create view f(f) as select sin(x + y) from t` is the sql version of `f x y = map (\[x,y] -> sin x y) sql(conn,"select x,y from t")` haskell-like pseudo-code. yes, `f` is the name of the view and the name of its single column. if you've ever defining a fn in code that's using a sql connection, think about how easily you could express that fn as a sql view. views are a sort of variety of prepared statement, except that they're standard sql and are stored by the sql engine internally.

pointwise-with-aggregate array programming example:

[source,sql]
----
create table things(name string, value real);
insert into things values("a",40),("b",16),("c",5),("d",4);
-- equal weight to all things
with weight(weight) as (select 1.0/count(*) from things) select name, weight, weight*value as adjusted from weight, things;
┌──────┬────────┬──────────┐
│ name │ weight │ adjusted │
├──────┼────────┼──────────┤
│ a    │ 0.25   │ 10.0     │
│ b    │ 0.25   │ 4.0      │
│ c    │ 0.25   │ 1.25     │
│ d    │ 0.25   │ 1.0      │
└──────┴────────┴──────────┘
----

notice that the ordinary join (cartesian product) of a single value with a row of values is effectively equivalent to scalar expansion (or w/e it's called) in apl `0.25 × values`.

.local binds

[source,haskell]
----
a = 9      -- dummy value
let a = 20 -- shadow a
 in a + 4  -- returns 24
----

[source,sql]
----
create table scope(a);        -- unlike haskell, we must define a in a table. its dummy value is implicitly [].
with scope(a) as (values(20)) -- local scope(a) shadows global one for duration of this select statement
  select a + 4 from scope;
----

* by naming tables `scope` i mean that tables are scoping mechanisms
* `with` is not properly its own clause; it's a clause of the `insert` statement, as well as `select`, `delete`, & `update`

sql binds cannot be <what?>, e.g. in a `create trigger` statement's final clause where it takes a sequence of statements, each statement may have each its own local binds, but local binds over all statements are not supported. instead, you'll need to create a (global) table then have the body statements use it, then drop or reset it as the last body statement, if appropriate. the table may be created before the trigger (being just a global table used only in the trigger) or may be created as the first statement of the trigger's body.

the ability to choose either demonstrates that local binds, like all scoping mechanisms, are not necessary, but instead exist only as a namespace management tool, namely to allow multiple homonomic data across different contexts. sql is unique in that all data must exist in tables, and tables are scoped, so namespacing is more of a constraint than an option. in contexts with homonomic data, sql gives us `as` clauses to disambiguate.

.cond/case

alists are obviously encoded in sql by schema `alist(k,v)`. then `select v from alist where k=?`

.find

`find p xs` = `first? (filter p xs)`. in sql: `select x from t where p order by i limit 1`

.one-to-many relations

to associate e.g. each song with many tags, `[(song,[tags])]`, use sql schemata `song(songid integer primary key, songname string)` & `tags(songid integer, tag string)`: `select song,group_concat(tag,",") from song join tags using (songid) group by song`. in sql `(k,[v])` is encoded as `[(k,v)]`. `group by` and/or aggregate [window] functions work well with 1:n relations.

remember that we cannot use `rowid` as a foreign key because `rowid` is not a primary key.

remember to state everything in the singular; this will help you remember that everything is flat/array in sql.

NOTE: metadata may not need to be exact e.g. though we can tag songs with multiple tags and certainly have correct results, we may tolerate `tags` as a string of delimited tags and `select song where tags like ?`. this isn't exact, but if the user is going to manually look through the results of a query and modify or curate it, then being exact isn't really beneficial. as another example, if instead of songs we've a database of titled text documents, `docs(title string, body string)`, then although we may have tags (like is usefully done in factor's docs), there's hardly any sense in tagging an article with tags that're already present in its title or body; if you're searching through docs, you'll probably search through the title, body, and tags altogether, ordering by some match strength measure. in fact, there may be only miscellaneous facts that don't belong to any article; in this acse `title` may be null and they may have only tags! tags are good for searching, and titles for displaying! if your db is huge, though, then you can't well index on tags as delimited strings, so still be sensible. you also can't add tags just by ``insert``ing; you'll need to use `update` & `||`.

anyway, *don't waste time adding redundant information to your database. schema are hard to change or work with, but queries are very flexible & simple to construct & modify! however poor your schemata, thorough understanding of queries will make schemata elegance inconsiderable.* this applies also to the efficiency statement at the end of the prior paragraph: if your db grows large, just create a new table with appropriate schema or add an index and populate the table with an `update` statement. there's really no such thing as sunk cost in sql, so don't worry.

.plurality

a common interpretation of a thing is that the thing is single yet composed of multiple things, e.g. an xml element may have many children. in sql we not say that the element contains children, but instead that the "children" are just a set of things that support a predicate that groups them. an obvious predicate is `id=?` where all in the set share a common value for the `id` attribute. in sql this cannot be done by a primary key, since each row must have a unique primary key value. we must therefore use what's conceptually a primary key as technically not a primary key. this is fine, since foreign keys & primary keys don't add any ability; they only check constraints and enable cascading mutations, but `unique`, indexes, and triggers are perfect alternatives.

.first-class functions and conditionals

TODO: summarize this section and its solutions, and compare salutions. identify algebraic ideals and sql's limitations.

our conditionality options in sql are `case when` and `where`. for conditional relations `where` assumedly produces a relation of 1 or 0 rows. `case when` is evaluated per row and does not affect the count of the resultant relation.

`if p t f` generalizes to `if p1 t1 p2 t2 ... [f]` (as is defined in arclisp) which is more regularly expressed as in factor: `{ { [ pi ] [ ...] } ... { [ t ] [ ... ] } } if`. the factor version clearly communicates a 2-ary relation: `if` as an assoc map, not a special form. the factor version is the most general: predicates are stack effects, which may affect later predicates; and the predicates may overlap. however, if predicates are not specified of related state and do not overlap, then we may evaluate them in any order, but treating the `else` branch especially, since its predicate is `const t` which overlaps with all predicates. thus the general pure conditional form in sql is of form `select t from ifrel where p union all values(f) limit 1`. we must use `union all` instead of `union`, just as we must use early-exit `or`. in prolog this pattern is seen as accepting the first true rule _else_ backtracking to the next. note that, unless predicates overlap, `and` does not need to be early-exit; it may be commutative. as always, if rows are related then use `with recursive`; for conditional branching this is equally powerful as factor's `cond`, though obviously not as elegant when `cond` fully exploits the stack and higher order words. `select * from case when exists (select * from ifrel where p) then t else f end` is malformed syntax because `case when` is of the `expression` grammar but only the `table-or-subquery` grammar is valid after `from`. that form would've been less elegant and probably less efficient than the `union all` version anyway. also each branch of the `case` must return a single-attribute relation. this makes `case when` like coproduct types and relations like product types. _naturally_ if none of your predicates match then your result will be the ∅. in most langs the lack of a matching predicate without an `else` clause produces `nop : void`. of course that value does not exist in sql. sql's natural empty value is ∅, which, unlike `void`, has algebraic properties (as 0 in ring (+,×,-,0)).

NOTE: generally the predicate generalizes from 0 & 1 to any integer, so that we can group rows by the predicates that they satisfy, assuming that all predicates are mutually exclusive.

so we compute a whole relation then select a predicate from it by `where`. therefore what in most langs is a complicated tree of nested `cond` blocks where only relevant statements are evaluated, in sql, as in apl, all possibilities are calculated, some of which may have a `null` value if they've no sensible definition depending on which predicates match or not, perhaps theoretically the cond tree is more efficient since it eschews irrelevant computations, but jumping (e.g. `jle`) greatly slows a program because the program loaded must try to anticipate which code of the binary's _executable_ section must be loaded, and i/o is hella slow. the sql version computes more but:

. they may be small computations
. there's no jumping but traversing a relation takes time
. invalid branches may contain `null`, which makes any computation O(1) by null propogation
. logic is obvious: all branches are flatly enumerated
  .. the program is simpler to reason about or study formally
  .. the flatness implies ability to parallelize computation. indeed, the relational algebra can be easily handled by a specialized processor, which is the ultimate speedup.

the solution (2) is to use bit vectors in cpu registers to avoid jumps and heap access.

perhaps mare formally we know that we can express if in terms of ∧ & ∨, and that those are product & coproduct respectively, which means that they correspond to ∩ & ∪. using simple term rewriting & implication instead of philosophical reasoning:

`(if p x y)` = `(or (and p x) y)` ≅ `p × x + y` ≅ `p ∩ {x} ∪ {y}`. `p ∩ {x}` = `{x} s.t. p`, which implies that commonly `p ∩ {x}` ≠ `∅` i.e. `p ⊆ p ∩ {x} ∧ {x} ⊆ p ∩ {x}`. we generalize `x` & to sets for power & symmetry: `p ⊆ p ∩ x ∧ x ⊆ p ∩ x`. say `t := p ∩ x`; then `p,x ∈ t` i.e. `select p,x from t`, which sees `p ∩ x` as a 2-relation, which is equivalent to `p join x on π`. `π` is a nonce here b/c p=π, which means that `p join x` = `select x from x where p`. (this is a case of a ∩ a@p as a redundant version of a@p.) to be sensible, `p` must be `p(x)` i.e. each `p` must vary with `x`. in sql `where` accepts only one predicate, and we want one `p` per `x`; therefore we must use the 2-relation if(`p`,`x`).

we see `limit 1` as "here are the valid solutions; take 1." we can see branches propogate like `μ` for the list monad. again, we filter branches with falsy (`0`) or empty (`null`) values, and traversal of branches terminates when branch set is ∅. this is similar to flattening a tree of conditional forms into a stack, then looping until the stack is empty, executing the top of the stack on each loop, where those computations may push more to the stack.

`where` is just predication, which is obviously `if`, since `if` is the only primitive of any language that accepts a predicate, as `where` is the only grammar in sql that accepts a predicate. `if`:one::`filter`:many, and `where` filters. therefore it's clearly insensible to discuss `if` anymore; we should use relations instead, which generalize `if` to `cond` by multiplicity then re-express `cond` from a special form to a loop over a sequence of duples `[(p,x)]` which is reduced to a loop over `(filter p x)`.

let's use this knowledge to translate:

[source,scheme]
----
(let-values ([(at-least-as-attractive? more-attractive ext oppext) (if (> count 0)
                                                                       (values <= min low high)
                                                                       (values >= max high low))])
  (and (or (sql-null? stop)
           (if (sql-null? TABLEXT)
               (unless (at-least-as-attractive? stop oppext)
                 (query-exec D "update orders set stop = null where oid = ?" oid))
               (let-values ([(newext test-limit?) (if (>= (abs (- TABLEXT oppext)) (abs stop))
                                                      (values sql-null #t)
                                                      (values (more-attractive TABLEXT ext) #f))])
                 (query-exec D "update orders set ext = ? where oid = ?" newext oid)
                 test-limit?)))
       (let ([most-attractive (more-attractive open limit)])
         (and (at-least-as-attractive? ext limit)
              `(,most-attractive . ,o)))))
----

first, let's reduce for simplicity:

[source,scheme]
----
(let-values ([(f g a b) (if (> count 0)
                            (values <= min low high)
                            (values >= max high low))])
  (and (or (sql-null? x)
           (if (sql-null? y)
               (unless (f x b)
                 (query-exec D "update t set x = null where z = ?" z))
               (let-values ([(newext test-limit?) (if (>= (abs (- y b)) (abs x))
                                                      (values sql-null #t)
                                                      (values (g y a) #f))])
                 (query-exec D "update t set a = ? where z = ?" newext z)
                 test-limit?)))
       (let ([most-attractive (g open limit)])
         (and (f a limit)
              `(,most-attractive . ,o)))))
----

so sql/apl's array/set compute/select conditional branching method is better than nested conditional jumps. that leaves the question of how's sql's alternative to functions as data. firstly, let's, as always, not think of functions but quoted programs; rather than say that sql does not support functions as data, say that it does not support quoted expressions/subprograms. anyway, subprogram variables is a topic related to conditional branches: they're both conditional/variable, but conditional branches are ad-hoc and expressed by 2-relations, whereas subprograms are quantified over an infinite set specified by a predicate, namely a function type or expression grammar.

variable/quoted subprograms have 3 uses:

. immediately applied to arguments, in which case there's no point in it being variable or first-class
. being passed to another subprogram (which does not need to be quoted) which, after being passed around across subprograms, is ultimately applied to some arguments, at which point, again, it may as well not be variable.
. being modified before execution (metaprogramming)

variables are the same as conditions, just symmetric (predicates, namely types or grammars) instead of ad-hoc (literal data). if subprograms are all specified statically (cf dynamically i.e. during execution) then we can statically translate to a non-variable form. unfortunately this is a form of needless coupling. the benefit, though, is that we get to do everything in sql. so instead of a relation of predicates and functions, we'll need a relation of predicates and the values obtained by applying functions to their arguments:

invalid but ideal: `insert into r1(f,g,a,b) case when count>0 values(<=,min,low,high) else values(>=,max,high,low) end`. pure (non-mutative), still invalid form: `select (values(<= as f, min as g, low as a, high as b)) where count>0 union all values(>=,max,high,low) limit 1`. to make valid we must identify each of `f`'s & `g`'s arguments:

* f: values(x,b),(a   ,limit)
* g: values(y,a),(open,limit)

assume that `open` & `limit` are in scope. `a` & `b` are already associated with `f` & `g`; combine `let-values` & `if` into a single relation `t(p, f_x_b, f_a_limit, g_y_a, g_open_limit)`:

[options="header"]
|==============================================================
| p | f_x_b     | f_a_limit     | g_y_a       | g_open_limit
| 1 | x <= high | low  <= limit | min(y,low)  | min(open,limit)
| 0 | x >= low  | high >= limit | max(y,high) | max(open,limit)
|==============================================================

then making the attribute names sensible, especially considering that `stop_at_least_as_attractive_as_oppext` is only used before a `not`:

[options="header"]
|===========================================================================================================================
| p | stop_less_attractive_than_oppext | ext_at_least_as_attractive_as_limit | more_attractive_of_y_or_ext | most_attractive
| 1 | stop > high                      | low  <= limit                       | min(y,low)                  | min(open,limit)
| 0 | stop < low                       | high >= limit                       | max(y,high)                 | max(open,limit)
|===========================================================================================================================

it's more coupled, which i, before i translated the scheme code to sql, thought would be bad, but i actually prefer this because i can see _all_ of the places in which `f`, `g`, `a`, & `b` are used immediately rather than needing to read through nested code to find where & how each is used! this is appropriate because these data _are_ coupled! it's not like i'm defining separate functions in a library. i'm using `let`, binding particular data & functions for a particular purpose, for them to be used together in a small block of code. they are defined for each other, naturally coupled. not only that, but this tabular formatting is clean.

you may think that we'll join this table with the table that provides `high`, `low`, &c `on p=count>0`. let's see how that looks with a simpler table, `things(x,y,z)`:

[source,sql]
----
select *, case when p then v1 else v2 end as v1, case when p then v3 else v4 end as v2 from
  (select *,x>z as p from things) -- query with conditional logical value
join
  (select     x>z as p, y+z as v1, max(x,y) as v2 from things) -- values for p, condition #1
using (p)
join
  (select not x>z as p, y*z as v3, max(y,z) as v4 from things) -- values for not p (the else clause, or condition #2)
using (p);
----

that's bad for two reasons: 1. i need to join using p for each conditional branch and 2. i still need to use `case when` for each attribute, so i may as well skip the joins and just inline the conditionals:

[source,sql]
----
select *, case when x>z then y+z else y*z end as v1, case when x>z then else max(x,y) end as max(y,z) from things
----

or shorter by using `iif` instead of `case when`:

[source,sql]
----
select *,iif(x>z,y+z,y*z) as v1, iif(x>z,max(x,y),max(y,z)) as v2 from things
----

to make this work as nicely as we want, either `case when` would need to allow returning multiple-attribute relations, or we need something other than join, since join unions relations' attribute sets, but for each row we don't want all the possible values! we want only values that match the predicate. `union` should do the trick! rather than `join on`, we'll filter by `where` then union the results. `union` does not care about attribute names and will allow us to choose v1 vs v3 & v2 vs v4:

[source,sql]
----
select * y+z as v1, max(x,y) as v2 from things where x>z
union all
select *, y+z,max(x,y) from things where not x>z
----

now the redundant part is specifying the same table twice.

i suppose that generally the ideal would be `group by` to determine predicate case value, then joining on that with a table of something effectively select statements with variables. this is the point where sql's limitation binds us; there are many solutions but all of them require changing sql. TODO: how would this be encoded in prolog, and then how to translate that into sql? precisely, generally, how limited is sql, or what must be done in sql in order to encode things, inelegantly? also describe the conference of union & join in algebraic terms then discuss how sql is more limited than just pure data OR how to encode everything in relations in such a way that, constrained to sql operations, we can express any computation. is this result decent & pragmatic?

so let's finish the translation by translating the query (the `let-values` body). first notice the `query-exec`'s; those're queries executed for effect. to do those in sql we use triggers. both triggers are conditional and share a common triggering event, though that event is not specified in this example.

(and (or (sql-null? x)
           (if (sql-null? y)
               (unless (f x b)
                 (query-exec D "update t set x = null where z = ?" z))
               (let-values ([(newext test-limit?) (if (>= (abs (- y b)) (abs x))
                                                      (values sql-null #t)
                                                      (values (g y a) #f))])
                 (query-exec D "update t set a = ? where z = ?" newext z)
                 test-limit?)))
       (let ([most-attractive (g open limit)])
         (and (f a limit)
              `(,most-attractive . ,o))))

[source,sql]
----
create trigger T1 after <some event> on orders when ext isnull and stop_less_attractive_than_oppext begin update orders set stop = null where oid = oid; end -- the `on` clause brings order's attributes into query scope
case when x isnull or (case when y isnull then end) then _ else _ end
----

=== constraint solving

a constraint solver is possible in sql just as in a logic language. however, sql works on literal values, not predicates which may represent infinities of values. to account for this, we must use ranges of values in sql. for all sets of ranges {(a,b)} ∃ min(a) & max(b). thus predicates like x≥4 can be expressed in a limited but useful capacity as [4,max(b)]. intervals may be considered as 1D boundaries. [a,b] generalizes to a subset of [a,b]×[c,d], etc. these are subsets of cartesian products—the very same definition as joins. thus the n-dimensional geometry model obviously corresponds to sql joins. of course we can take geometric intersections, which is the same as set intersection, which is available plainly in sql as `intersect`. aside from intersection obviously there's the shortest distance between two geometries.

a really cool property of intersection & union is that they tend toward convergence or divergence—a nice interpretation of their dualism!

=== statistics

=== literature

writing a nonfiction book, commonly a reference.
