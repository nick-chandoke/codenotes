= the factor programming language
notes by nicholas chandoke. currently undergoing significant revision.
:toc:

.cool vocabs

NOTE: many of these—namely those starting with "factor.extra"—don't export documentation; but you can read their source code at `<factor install dir>/extra/...` and some have a `MAIN:`.

* those having the "algorithms" tag
  ** lcs
* `furnace`, for coding web servers

see the factor docs for "Networking".

to check-out:

* jamshred. good, simple example of an opengl game. requires (before starting factor) libalut (arch package `freealut`)
* factor.extra.gamelib.demos
* factor.extra.game
* factor.extra.gpu.demos.raytrace
* factor.extra.gpu.render
* factor.extra.audio.loader
* factor.extra.audio.engine
* factor.extra.rosetta-code
* factor.extra.spider
* factor.extra.directory-to-file
* factor.extra.file-to-directory
* factor.extra.curses
* factor.extra.did-you-mean
* factor.extra.env NB. envars
* factor.extra.geoip
* factor.extra.geobytes
* factor.extra.irc.client
* factor.extra.logic.*
* factor.extra.machine-learning.*
* factor.extra.math.*
* factor.extra.money
* factor.extra.taxes.usa[.*]
* factor.extra.modern.*
  ** modern.slices might be useful for procedural relations among sequences, i guess?
* factor.extra.multisets
* factor.extra.opengl
* factor.extra.successor
* factor.extra.tensors
* factor.extra.terminal[.*]
* factor.extra.terminfo
* factor.extra.tetris
* factor.extra.text-to-pdf
* factor.extra.twitter
* factor.extra.webapps.
  ** blogs
  ** calculator
  ** fjsc
  ** todo
  ** wee-url
  ** wiki
* factor.extra.websites.concatenative
* factor.extra.namespaces.extra
* factor.extra.echo-server NB. almost works. does not respond to browser (tested with firefox) until the factor process is closed. uses HTTP/0.9, as told my cURL.
* io.streams.*
* the `leaks.` word
* io.servers # threaded tcp/ip

SYNTAX: ?:
    [ scan-new-word parse-definition ] with-definition
    dup infer define-declared ;

* `math.functions` includes common functions like `^` (exponentiation), logarithms, and trigonometric functions.
  ** *you should completely familiarize yourself with this vocab*
* `math.intervals` and `math.vectors` are useful, too
  ** try to use `math.vectors.simd` wherever possible!
* rational numbers (abbreviated as "ratios") are always used in factor for division unless floats are explicitly specified. their syntax as `a/b` or `a+b/c` is also supported e.g. `1+1/2 5 *` gives `7+1/2`.

* `$[ ... ]` for static eval

useful words:

* `cleave>array` in `combinators.smart`

`find` & `map-find` are short-circuiting versions of `each` and `map`:

[source,factor]
----
{ 2 4 6 7 1 3 } [ dup even?             [ . f ] [ drop t ] if ]     find ! prints even numbers then leaves 3 7 on the stack
{ 2 4 6 7 1 3 } [ [ 10 * ] [ even? ] bi [ . f ] [ ]        if ] map-find ! prints 10 * even numbers then leaves 70 7 on the stack
----

`q map-find swap` is a bit more convenient than `find over q [ ] if` but i'm surprised that someone bothered to write it, given what little it offers beyond `find`. i mean `find` already allows one to loop through a sequence, supporting short-circuiting and side effects, and `map-find` still returns only one result, unlike `map`, which returns many. a much more useful definition for something that i would personally call "map-find" is:

[source,factor]
----
: map-filter-until ( stop: ( elt -- ? ) q: ( x -- ?y ) seq -- seq )
  [ length <vector> dup [ push ] curry [ when* f ] curry -rotd compose [ if ] curry
    [ drop t ] swap curry compose [ dup ] prepose
  ] keep swap find 2drop ; inline
[ 1 = ] [ dup even? [ 10 * ] [ drop f ] if ] { 2 4 7 6 1 3 } map-filter-until . ! prints V{ 20 40 60 }
----

or even better:

[source,factor]
----
: map-filter-until ( ... stop: ( ... elt -- ... ? ) q: ( ... x -- ... ?y ) seq -- ... seq )
  [ [ keep swap ] curry ] 2dip
  ! the following line is unmodified
  [ length <vector> dup [ push ] curry [ when* f ] curry -rotd compose [ if ] curry
    ! the next line is the same but lacks [ dup ] prepose
    [ drop t ] swap curry compose
  ] keep swap rot [ find 2drop ] dip ; inline
! the following example is the same but instead of exiting at 1, it exits after the 3rd iteration.
0 [ drop dup 3 > ] [ [ 1 + ] dip dup even? [ 10 * ] [ drop f ] if ] { 2 4 7 6 1 3 } map-filter-until . ! prints V{ 20 40 60 } and leaves 4 on the stack
----

this exploits `find` as `until` but with an implicit exit condition: when we reach a sequence's end.

`each-integer` is a general sequence iteration combinator, but does not support short-circiuting.

from the factor faq's:

* factor ships with a deploy tool which creates mac os x .app packages, or as windows and unix executables bundled with an image and some .dlls. to put a factor program into a package so it can be run easily, deploying a vocabulary into an application which will run the vocabulary's main word: `USE: ui.tools.deploy "vocab-name" deploy-tool`.
* if you need two different vocabs that define synonyms but want to use only one vocab's word, then put it after the other in `USING:`. otherwise use qualified imports: `QUALIFIED: v` to load vocab `v` s.t. its words are accessible as `v:word`.
* ffi: 1. ensure that your shared object is compiled for the same architecture that factor was (most commonly 32- vs 64-bit). also, for alien code, `add-library` first. alien supports C's name mangling but not C++'s.

== factor good

firstly, note that anyone coming to a cat/tacit lang from an applicative lang is going to try to reason about catlang programming in terms of applangs, which is failure. it's inevitable and to be overcome in time, but overcome it must be in order for one to actually program cats well. anyway, to mentally assign a name or label to data on the stack, and think about how its stack position changes, is bad; a good catter does not much monitor the stack; instead, they think about what sequences of operations they want. at each step, they must know what the input is, and so there's some stack tracking, but it's local stack tracking; they ever care only about part of the stack. this reflects cats themselves: arbitrary subcats don't affect others! certainly there is still some considerable stack consideration, such as when using `tri*`, `over`, or `rot`. it's still not much, though; rarely is complex shuffling done, and rarely does one ever care about more than 3 items on the stack. you'll know that you've become true to habitually thinking in cat/stack-way when everything seems like currying to you; when you see `swap 10 + *` as "2 things on stack. add 10 to 1 then multiply them", and it should be immediately obvious that you could instead do `[ 10 + ] dip *`, or if you wanted to leave an item on the stack, then `over 10 + *` because you know `over` to be equivalent to "swap but leave one." thinking in factor _feels_ very easy once you unlearn other perceptions of programming. stack programming is very natural to humans! that it reads left to right, is simple, and pretty with little syntax, is quite obvious to anyone who hasn't thought of "what code should look like." link:https://codeberg.org/ngn/k/src/branch/master/j.c[whitney-style c code] is 18% parenthesis! isn't it amazing that nearly 1/5 of the tersest c is just needless syntax? that's before we even consider the bloat due to variable names, type declarations, or other what-have-you.

basically it's like scheme but better. its semantics are like c but the language model is a nicer-yet-equivalent version of the λ calculus; see _the mathematical foundations of joy_. its type & object system is most like clojure, i think; i'm not quite familiar.

* excellent documentation. doc browser is excellently designed, and enables exploring code definitions very fluidly, and it's dynamic, showing current runtime values and docs for vocabs as they're loaded
  ** allows you to persistently write the listener environment, too! see the word `save` in the `Images` help document. the stack isn't saved, so you'll need to set a dynamic variable to its value then restore it later.
* because its standard library is so large, and factor has very few primitives (easily found by searching for words with the `PRIMITIVE` declaration), and all words' definitions are easily viewed, factor has a library of easily-understood codes that you can transcode [into another lang].
* hackable. things declared private are only so _declared_; code cannot actually be inaccessible.
* simple model: the vm is just 2 stacks and a hashtable.
* syntax
  ** all tokens are whitespace-separated so selecting arbitrary subprograms is easy. quoted programs (a generalization of lambdas and macros [lisp]) are delimited by brakets, still, which makes program organization clear. this is more significant than you may think; in practice, the code being so clean is quite less stressful than using an applicative language. there is a latent burden that creeps in and that applicative programmers become used to. the required whitespace does make the code less terse, but easier to read and less stressful. i prefer it over j. i desire some mix of the two, but i haven't imagined how that could work.
  ** no order of operations, so no need for parentheses. the only nested expressions are quoted programs.
  ** homoiconic syntax. b/c the model is simple, this means that's there's practically _no_ syntax (except eDSLs), and the syntax describes the program exactly
* dynamic. because programs are first class objects (just sequences whose execution the compiler can optimize), there's hardly a need for metaprogramming. regardless, macros are available.
  ** whereas lispers usually use macros and call that metaprogramming, factor does the equivalent of quoting sexps & calling `eval` all over the place during or before runtime.
  ** makes dsls easy, especially since it comes with a peg parser.
* (TODO: confirm) compiled factor programs are pretty small (if you don't use vocabs that require higher runlevels) and fast. certainly not as fast as a forth, rust, v, fortran, go (probably). but for all that factor affords, its efficiency is excellent!
* get as close or far from your target hardware as you want. you can inline assembly, use simd, or not. ffi for python or c/dlls is easy, too.

pay attention to these facts! they reduce the complexity of the programming & language model, which makes easier and more efficient: reasoning about how to code programs, refactoring, coding it in the first place, debugging. boilerplate is rare and can be easily avoided by defining a macro in a few words.

=== factor not good

* you want a small codebase or small executables. factor is straight-up large. it comes with packages that have _no_ business being included, instead of being separate code that you may find in a separate repository on github. examples include a _magic: the gathering_ vocab. i think i've said enough! regardless of how efficient or fast your compiled binaries run, they are still large, if you consider tens of megabytes large. i really dislike inefficient code, but the expansive capability that factor comes with, and its walker (debug tool) and interactive documentation and support for dynamicism are amazing. therefore, for large projects at least, it's my goto language. i may forgo it in lieu of a collection of specialized languages/tools, or perhaps use a forth that interfaces with c/++ well, using those libraries and never looking at their code,...but c sucks, too, even insofar as compiling it and using shared objects! guess that leaves rust, go, and maybe fortran?
* you want multithreading. there is yet no multithreading in factor. it has coroutines, but cannot use true parallelism. it's too bulky to use the picolisp strategy of simply spawning multiple pl os threads.
* you want a solver, e.g. prolog or stm
* enormous set of included libraries (good b/c much functionality, bad b/c bloated)

TODO: can i use the elegance of a stacklang with a non-reductive system like prolog? term-rewriting catlangs are common, but reductive. stacklangs have an implicit, reductive model of traversing the stack. are term-rewriting catlangs implicitly, necessarily reductive?

it's been suggested that the stack is not suitable for some coding; comomnly the quadratic formula is suggested. however, i defeat that argument here:

[source,factor]
----
: ± ( a b -- a+b a-b ) [ + ] [ - ] 2bi ;
: solve-quadratic ( a b c -- r1 r2 )
  rot 2 * [ * 2 * ] keep
  [ [ [ neg ] [ dup * ] bi ] dip - sqrt ± ] dip [ / ] curry bi@ ;
----

i consider this code more elegant than applicative versions. it only took me a few minutes to translate from the math notation to this code. true, to a newcomer who isn't familiar with the stack, solving for this code would be difficult. the method for coding a stack lang elegantly is not obvious, but the method is simple and simple to teach, and once one knows it, then coding elegantly in factor is easy. the method is mostly about ordering the stack such that related elements are near each other on the stack; then you must learn how to curry well, and using a particular set of primitives such as `each`, `bi`, and others.

the code annotated:

[source,factor]
----
:                  ! syntax that starts a definition
solve-quadratic    ! the name of the word which we're defining
( a b c -- r1 r2 ) ! "stack effect", analagous to a function/type signature. we require three inputs to be atop the stack, and after consuming them all, the word leaves two outputs on the stack. forth uses the same notation, but in forth it's a comment. factor has a stack checker to statically catch errors. an equivalent, terser, but less documenting notation is used by uiua: an integer e.g. ( x -- ) would be expressed by -1 to mean that this word reduces the number of elements on the stack by 1.
  rot ! ( b c a )
  2 * ! ( b c 2a )
  [ * 2 * ] keep ! b 2a*2*c 2a. keep performs a computation with the stack, then pushes the top of the stack back to the top
  ! the following code dips; 2a is taken off the stack, then this computation is performed, then 2a is put back atop the stack
  [ [ [ neg ] [ dup * ] bi ] dip ! dip below 4ac. replace b by -b b². stack is now -b b² 4ac
    - sqrt ± ] dip ! stack is now -b+√(b²-4ac) -b-√(b²-4ac)
  [ / ] curry bi@ ; ! divide both by 2a
----

of course, the solutions that you see on link:https://rosettacode.org/wiki/Roots_of_a_quadratic_function#Factor[rosetta code] are a bit more complex because they have special consideration for numerical precision.

=== factor ugly

* vocabs _must_ be defined by a particular directory structure: they must be named `<name>/<name>.factor`; to load that vocab, you cannot specify the filepath; instead, you must <name> must be a subdirectory of a directory that you've registered with factor's vocab loader via the `add-vocab-root` word.
* as mentioned above, if you use some particular vocabs, then you must compile with a higher runlevel, which can take a 2MB executable to a 272MB executable!
  ** the runlevel (and other compiler options) needed to compile any given vocab are unknown, undocumented, and not something that you can discover without reading their source code or shotgunning compiler options & recompiling, which takes very much time, and is liable to change across versions of factor.
* interpreted factor takes so long to load vocabs that it's infeasable to use it. this is before we even consider the difference in speed or efficiency between interpreted & compiled factor code.

== introduction

tl;dr: use prolog if you want implict & complex control flow. use factor if your control flow is simple. factor for dataflow, prolog for _solving_ constrained systems.

factor is hardly a language; it's homoiconic just like prolog and lisp; as such, there is no _language_; there's only data and a repl; the repl has a hardcoded traversal & evaluation model for its input data. thus its input data are implicitly programs exactly because they're evaluatable as such by parsers (here, interpreters).

such a general/free models hardly suggest any idiomatic style. indeed, one can make whatever model they wish by making even whatever syntax they desire via metaprogramming. factor is, like lisp and unlike prolog, reductionist & functional but, like any system, at least prolog can be implemented in it. factor should be thought of as lisp except with a different model for relating functions' inputs & outputs:

* in lisp, inputs are specified inline, thus nesting expressions in an ast. in factor we order them linearly, often in advance of when they'll be evaluated. in lisp the evaluation is a depth-first traversal of a tree, whereas in factor it's a loop of fn application until the stack is empty; in factor the program is eaten-up whereas it's traversed in lisp.
* a funny thing about stack langs is that the stack relates all of its elements, whereas functions' arguments are distinct. we see this in functional combinators vs stack effect combinators such as `2tri`, which does not associate each of 3 functions with each of two inputs—the sequence [(f,x,y), (g,x,y), (h,x,y)]—then evaluate each triple of that sequence; instead, it performs stack effects `f`, `g`, & `h` in a given order, which means that the effect of the earlier-executed ones can affect the inputs of the latter-executed ones. this is very much a frequent hassle when using `if`, `cond`. the equivalent in scheme would be an effectful predicate, e.g. using `set!`. one can argue that this is more capable because it enables relating clauses, but that's often not what we want. as much as ever, separation & complection should be specified explicitly & elegantly by relations, where relations are implicitly entailed by terms being present across predicates.
* whenever a computation is to be performed once but its output passed to multiple locations, lisp requires binding clauses, and factor requires `dup`.
  ** btw, in prolog, rather than binding clauses, variables are related implicitly by their presence in predicates which are intersected with others. *this shows intersection as essentially the same as relation itself, and relation the same as application or composition. e.g. we see (a,b;c;d;e) as `map a [b,c,d,e]` because it's `a` intersected with a disjoint union; `a` is related to each element of the disjoint union. this shows disjoint union as a set of distinct elements. `AND` is application & merging whereas `OR` maintains distinctness/separation. all code generally is merely "these distinct" vs "these together!"* i suggest the prolog notation of `,` for "together" and `;` for "apart." and have fns partially applied b/c that can only make things easier. when a fn is loaded with args, it either has a deterministic arity at which point it evaluates (if we're using a reductionist model); or if its evaluation must be made explicit, then relate it to a special primitive that exists expressly to force the fn to eval. i don't think this is practically possible in a stack model, though.
* in factor, because it evaluates in the order which it reads—left to right—functions must be quoted in order to not be immediately evaluated. in a lang such as uiua, where evaluation is from the right but parsing is from the left, and perhaps enabled by a lack of metaprogrammability, function arguments do not need to be quoted; when parsing from the left, when a higher-order function is encountered, its stack effect (fn signature) tells how many of the following stack items should be interpreted as functions and not immediately evaluated.

NOTE: lisp is the de facto applicative functional notation, and factor is nearly the de facto stack functional notation (technically the _Joy_ proglang is the de facto). there are many varieties of each lisp and forth, and to some lesser extent, prolog.

.actual evaluation

there's a certain amount of trouble in any design—except maybe prolog; that's yet to be known. the only solution to this is a parser/dsl paradigm: to have a plethora of evaluation models and syntaxes which all share the same underlying model. the reason that prolog may be necessarily ideal is that its model—facts expressed as relations & constraints of free vars—is exactly the general substance of meaning itself. the only trouble then is that general systems, by definition, have little information encoded in them; this means that _we_ must specify information rather than it being tacit. this being said, metaprogrammable models are suited for creating such dsls. languages that can modify themselves dynamically (during runtime) are most free. commonly lisp cannot do this; picolisp is the only one that i know of that supports it. forth, maybe factor, and prolog support it.

the good news is that we can impose models, such as the array model, which does not affect the basic case e.g. `1+2` is `3` regardless of whether the array model is imposed or not. yet things that would otherwise be nonsense (uninterperable) are interpreted sensibly by it e.g. `1 2 3 + 4` produces `5 6 7`. we can freely union additional orthogonal parsers (orthogonal meaning here that each parser's parsing expressions do not overlap) without worry about changing the interpretation (meaning) of our code. we're also free to install new non-orthogonal models and compute the overlap then choose the order in which sets of overlapping rules are tried, and we can run it on code to identify which subsets of our code's meaning may change by installing the new parser. obviously <installing a new parser whose rules are tried only after the prior parser's overlapping rule fails> will affect only if the original parser fails, which may or may not be expected in your code, depending on how you wrote it.

link:https://toml.io/en/v1.0.0#array-of-tables[toml's array of tables syntax] is like stack langs whereas JSON is like applicative languages. in stack langs we accumulate programs imperatively then eventually execute them. applicative languages specify large program segments as _one_ complex (and deeply nested!) structure of data relations. stack programs are not nested. stack is a 1-dimensional data structure, whereas (abstract syntax) trees are two dimensional and irregular. granted, asts can be built of stack programs, too. the tradeoff is that one must maintain awareness of the stack's state at a given point in time but the syntax is nice (which makes refactoring nice), whereas applangs display the whole program all at once, which...gives the whole picture at once, but it's still complex! imperatively building programs allows us to go one step at a time. *showing the whole program at once does not make it easy to trace through.* also the mere fact of syntax being more complex is a burden. it's one usually taken for granted, but there's no reason for that.

''''

* check-out vocabs: `models`, `ui`

== deployment/compilation

why? for speed, encapsulation (single dynamically-linked binary), or code hiding. the produced binary executable may seem large, but given that it doesn't require factor to run, it's relatively good, both b/c it's convenient for the user (they don't have to download yet another runtime just to run someone else's [your] code) and because factor itself (as a downloaded snapshot or installed by a package manager) is hundreds of megabytes.

. vocabulary does not need to be already loaded
. in the listener, evaluate `"name" deploy-tool`. a graphical dialog appears. i use sway, and i had to goto a blank workspace in order for the dialog to display properly. the dialog appeared instantaneously but was painted over by firefox.
. click "deploy" and the compliation will begin
. when compilation finishes, a terminal is opened to the directory where the compiled binary was left. for me, that's in factor's install directory, `~/factor/`.

=== functions and the deploy options that they require

* if one deploys a program which uses `printf` or `.` with compilation level less than 3, then the program feps-out, declaring that it's crashed and that such crash is a bug.
  ** actually, scratch that; even this `printf` trick still fails seemingly exactly the same!
* `mirrors` requires level 5+. `all-slots` also requires level 5+, so you can't use tuple inspection words to make your own version that does not require such a high level. however, `tuple-slots` does not require anything above level 1, so if you hardcode tuple slot names then you can effectively use mirrors at run level 1.
* `peg` requires level 6 plus retain all word properties and definitions. it used to only require lvl 5. idk how much worse 6 is as far as binary sizes. ...but i have a program here that's 76MiB, and it hardly does shit. the program itself is computationally simple; it could be done in a handful of lines of C. if only factor's peg could be built at run level 1, it'd be only 29MiB, which is still way the fuck too large for what the program does. compare it to link:https://dev.ronware.org/p/reva/wiki?name=Manual[reva forth], whose binary is 30k, or link:https://codeberg.org/ngn/k[ngn/k] whose binary is 272k. these two are powerful programs! for any binary to be even 1MiB is *ridiculous*! when i load the program, i see the following are loaded, though hopefully it's at least some weird thing where they're loaded but not included at all in the compiled binary: regex, calendar, websockets, prettyprint, things of the `help` vocab including ui & fonts, openssl...so what the hell? the program can be expressed in a line of sed! in fact, here it is:

[source,factor]
----
USING: io kernel peg sequences sequences.extras io.encodings.utf8 io.launcher sequences.generalizations strings unicode util ;
IN: pacfmt

: parse-yay ( yay-pkg-descs -- html-str )
  [ 32 = not ] satisfy repeat1 [ >string ] action
  [ 10 = not ] satisfy repeat1 [ >lower "installed" subseq-index >boolean ] action
  "\n" token hide
  [ " \t\n" member? ] satisfy repeat1 hide [ 10 = not ] satisfy repeat1 [ >string ] action
  "\n" token optional hide
  6 narray seq repeat0 parse [ first3 [ [ >upper ] [ ] if ] dip [ "td" (wt) ] bi@ append "tr" (wt) ] map-concat "packages" f wrap-html ;

: main ( -- ) read-contents [ parse-yay "w3m" utf8 [ write ] with-process-writer ] [ "empty stdin was given. exiting." print ] if* ; MAIN: main
----

== usability

being left-to-right is nice in that we can put comment initiators (`!`) anywhere in our code in order to halt the computation early.

.dynamic evaluation

* if you define word A in terms of word B then redefine A, then B's definition is implicitly changed.
* `with-datastack` is like lisp's `apply`

.caveats and common errors

* anytime that you use syntax to specify any mutable structure (most commonly vectors, string buffers, and hash tables), always use `clone` afterward! otherwise you risk multiple references to one object across your codebase when you really meant for the objects to be individual!
* when using packed tuple arrays, you get "matching failed." you should use `{ } map-as` instead of `map`
* setting a dynamic variable has no effect, or dynamic variable is `f` even though you just set it. check whether you're executing it within a namespace combinator e.g. `with-file-writer` is ultimately defined in terms of `with-variables`, so any setting within its quotation will not affect the namespace outside the quotation! for example, consider `SYMBOL: myVar "~/test.txt" ascii [ 0 myVar set [ myVar get dup even? [ myVar inc ] when ] with-my-db myVar get 5 + . ] with-file-writer` where `with-my-db` is defined as described in `db.tuples` document, _Tuple database tutorial_. execution throws an error: "No suitable arithmetic method. left: f; right: 5; generic: +" `myVar` was set only within the context of the inner namespace—the one of `with-my-db`. within the namespace of `with-file-writer`, it was still unset. more precisely, after ``with-my-db``'s quotation finished, `myVar` was set back to the value that it had had before that quotation was evaluated. indeed, even when we move `0 myVar set` to the outer quotation, "5" is written to the file, not "6", because the increment occurred only within the inner quotation! `myVar` is reset to 0 after that quotation finishes!
* confounding `map` errors: `map` maps into the same type as the thing being mapped over. if you want to map into an array then use `{ } map-as`. this is especially common if you're trying to map over a string.
* for words like `set-at` which consume a structure and don't leave it on the stack, use `keep`: `H{ } [ "val" "k" rot set-at ] keep` leaves H{ { "k" "val" } } on the stack
  ** use `over adjoin`
  ** use `[ _initAssoc set-at ]` or `[ set-at ] curry` or `[ set-at ] keep`
  ** use `over [ change-at ] dip` or `_q curry [ change-at ] pick [ 3curry call ] dip`
* `inline` can make reading tracebacks more difficult e.g. with ``: a ( x -- y ) 0 / ; inline : b ( x -- y ) a ;``, evaluating `b` with any input will throw an error, and the traceback will go as deep as `b`.
* `read-contents` hangs
  ** you meant `utf8 file-contents`
* assocs: you do something like `f "key" value { } 2sequence assoc-union` and get a weird result. you meant to do `f "key" value { } 2sequence { } 1sequence assoc-union` or `f value "key" associate assoc-union`
* `call-n` doesn't work like you'd expect. did you mean `napply`?
* the stack checker sometimes fails for complex row-polymorphic functions. consider the following: `[ second length 3 > ] [ first2 dupd [ myfn ] [ 0 > ] bi 3array ] filter-map` was a mismatch, saying that the filter clause was `( x -- x )` but that the map was `( x x -- x x x )`. that's obviously wrong. the problem is that `myfn` was defined `inline` and had `map` in its definition; thus when the compiler inlined it, the composite effect was beyond its reasoning, despite `myfn` having successfully compiled with stack effect `( x -- x )`.
  ** as it turns-out, the problem was `map-filter` being too polymorphic. i learned this by changing both the filter and map clauses to `[ ]` yet i still got the error! i suppose that the lesson here is to start from the outside then specify inward as needed. the specific thing that i did is take the erroring `<quot> <seq> <quot> rot map-filter` and change it to `<quot> <seq> <quot> rot \ map-filter execute( s q: ( a -- b ) f: ( b -- ? ) -- s' )`
* no output expected to stdout: use `flush`.
* forgetting `get` after a symbol; remember that symbols are symbol literals and are not themselves dynamic variables, though they can be used as such
* mixing `set-global` & `set` or `get-global` & `get`
* using `::` but forgetting to put leading args
* using a quotation in `::` without `compose` or `call` (thus giving a larger return stack than expected)
* "cannot create slice from 1 to 0": slice on empty sequence, commonly by `unclip-slice`

.run-time computed values

first check that you did `prepose`, not `prepend`. `prepend` is for sequences in general; `prepose` is considered specially for quotations by the compiler. `prepose` works where `prepend` gives the "cannot apply such-and-such to run time computed values" error.

the help document "Stack effect checking escape hatches". it mentions `call(` & `execute(` for quotations and words with statically-known stack effects; and `with-datastack` for general manipulation. there are some other strategies:

* the `literals` vocab is parse-time computation, like macros, except that macros result in callables whereas literals result in values.
* macros are very convenient, too, for specifying dynamically-computed values that are known before runtime.

the following code failed b/c `ndip` (and probably `npick`, too) can't take a run-time computed value:

[source,factor]
----
: reduce-collect ( ..a seq q: ( ..a e -- ..a ?collectval ) -- ..a collection )
  dup infer in>> length 1 +
  [ [ V{ } clone ] swap ndip ] ! accumulation vector
  [ 1 + npick ] bi [ push ] curry [ when* ] curry compose each ; inline
----

so what to do? well, fortunately i expect the quotation to always be specified inline, which means that its effect can be known at parse time, before runtime. macros allow us to dynamically compute values which are, at runtime, literals, thus solving the runtime-computed value problem:

[source,factor]
----
<PRIVATE
: (reduce-collect) ( ..a seq q: ( ..a e -- ..a ?collectval ) ndip -- ..a collection )
  [ [ V{ } clone ] swap ndip ] ! accumulation vector
  [ 1 + npick ] bi [ push ] curry [ when* ] curry compose each ; inline
PRIVATE>

! i don't need to specify q's stack effect here. i do so for documentation's sake only,
! so that the user can know what kind of quotation to pass.
! same for the ..a's outside of q's effect.
MACRO: reduce-collect ( ..a seq q: ( ..a e -- ..a ?collectval ) -- ..a collection )
  dup infer in>> length 1 + [ (reduce-collect) ] 3curry ;
----

actually, a little later, when i passed a run-time-computed quotation to `reduce-collect`, i found that `npick` is then a runtime-computed value, too! so i had to amend the code:

[source,factor]
----
: (reduce-collect) ( ..a seq q: ( ..a e -- ..a ?collectval ) ndip -- ..a collection )
  [ [ V{ } clone ] swap ndip ] ! accumulation vector
  [ npick ] bi* [ push ] curry [ when* ] curry compose each ; inline

MACRO: reduce-collect ( ..a seq q: ( ..a e -- ..a ?collectval ) -- ..a collection )
  dup infer in>> length 1 + dup 1 + [ (reduce-collect) ] 4 ncurry ;
----

.conditional combinator problems (this section probably needs revision or elimination)

* `p q when`: consume `p` and if `p` then `call` `q` else `drop` `q`. `p` is not passed to `q`; `when*` makes that so.
  ** `unless` is the same but with `p not`
  ** though these cannot leave new data on the stack, they can affect the stack by mutating things on the stack e.g. `dup empty? [ dup 0 swap [ 1 + ] change-nth ] unless` to increment the 1st element of a non-empty sequence.
* `unless*` has a different stack effect than the others; the others leave the stack how it was; they can be used only for programs of effect ( -- ) or ( x -- ) for `when*`. `unless*` leaves a new datum atop the stack. this is because it retains [dups] the predicate before checking its falsity. `a [ b ] unless*` means `a b or` but short-circuiting and accepts quotations rather than single values, or, more generally, `a b unless*` means `a b or` where `b` has effect `( -- x )` i.e. produces a value e.g. `x y [ dup ] unless*` leaves `x y` if `y` else `x x`

there's nothing like ``Maybe``'s `fmap`. should there be? `: fmapMaybe ( ..a q: ( ..a x -- x ) -- y ) [ f ] if* ; inline`. would you ever want to preserve the `f`? certainly you may want to perform a mutation on the top of the stack if it's not `f`, and you may want to do that for multiple conditions. however, given that `fmap` leaves `f` if it starts with `f`, then one mutation occurring implies that the rest must also occur, and so they all can be combined into one mutation. therefore a more sensible word is one that operates on the stack unless its top is `f`, in which case the `f` is dropped: `[ ] if*`. however, this fails because the branches have different stack effects. `[ f ] if*` balances them, and is the definition of `fmapMaybe`. so it appears that we should have `fmapMaybe`. one must now choose between `fmapMaybe` and `when` depending on the stack effect. it'd be nice to have one word that drops a `f` value and one that consumes it and any other things. for example, the fact that we must code like the following is annoying:

[source,factor]
----
[ . ] [ ] if*
[ . . ] [ drop ] if*
[ . . . ] [ drop drop ] if*
----

`smart-if*` does not help because it relies on the predicate consuming a certain number of values, which cannot be done if the quotation has effect `( ..a x -- ? )`; in that case youd need `ndup` & `ndrop`, but in that case you may as well use `if` directly.

* stack effect problem for `when` (or `unless`): strange as it seems from looking at `when`'s definition, `P T when` is not equal to `P T [ drop ] if`. to understand: note, in `when`'s definition, that `[ drop ]` and `[ call ]` operate on the same object: `when`'s quotation! `when*` is the conveniently terser word for `t [ drop ] if*`. however, i've found myself most commonly doing `dup pred true [ drop ] if`, which discards the predicate but retains the subject of the predicate for use in the true clause. there is no builtin combinator for that. consider the following versions of it:

[source,factor]
----
:: with-if ( pred: ( x -- ? ) true-prog def -- y ) dup pred call true-prog [ drop def ] if ; inline

! PROG                                  ! OUTVAL     ! STYLE
4 dup even? [ 1 swap / ] [ drop 10 ] if ! 80         ! if pred(x) then f(x) else defval
1 [ even? ] [ 1 swap / ] 10 with-if     ! 10         ! shortened by 4 characters by with-if
0 [ even? ] [ 1 swap / ] [ 4 * ] tri ?  ! error: 1/0 ! tri & ?
4 dup even? [ 1 swap / ] [ 4 * ] if     ! 1/4        ! plain ol' if
----

* `smart-if*` can sometimes be useful
* the plain form is best. they're all pretty much the same number of characters, and the plain form makes specifying a default value as easy as a false-branch function, including `[ drop ]`. whatever the case, the false branch's stack effect must equal the true's.
* the `tri` form executes both branches, which can be problematic, and is inefficient
* `with-if` saves us from typing `dup` & `drop` each once, but definitely returns a constant in the false case rather than a function on, and isn't appreciably shorter
* even `?if` uses the condition's output, not the subject of the condition, in the true branch.

i guess that the expected idiom for `if*`, `when*`, `?if`, &c is `keep and` e.g. `obj [ pred ] keep and [ fn ] [ else ] if*`. that's not really better than `obj dup pred [ fn ] [ drop else ] if`. they're equivalent, though, so if you find yourself using `if*` &c, then use `keep and`.

the lesson is that `if` is the primitive selective evaluator and is perfectly simple and free, and it's not worth the time trying to find something nicer than plain `if`, except some obvious ones like `if-empty`. i'm unsure for stack machines, and especially specifically for factor's implementation, how `bi ?`'s speed compares with `if`'s. i imagine that naïve code is optimized well in any stack language, and especially in factor, which is designed to be fast. i'm not worrying about the efficiency of things like an `if` inside a `loop`. if you're so concerned about speed, and you can put a fixed size to your data, then use `math.vectors`, whose ops are auto-optimized to simd when possible; or use a gpu or array primitives implemented in factor.

== design

=== language

"language?" ...data with an evaluation model is more like it.

* designed for metaprogrammability, simplicity, and flexibility/dynamicism, like elisp but better because the facilities available to the user are the exact same as are used to implement the factor language
* all factor metaprogramming is compile-time
* constrained design is generally bad (viz here using the stack—a quite constrained data structure). however, constraint is useful when we don't need to go outside the constraints anyway. in this way factor provides a simple model (stack) for the common cases but allows a simple arg-binding syntax for when that's more elegant.
  ** the stack's simplicity allows extremely efficient program optimization and execution strategies
  ** effectively implicit composition of arbitrary-arity functions
  ** stack based (also called _concatenative_) languages are usually superior to functional ones. factor's support for globals, mutable objects, and local binds make factor clearly a good language, certainly strictly better than any functional language
  ** there are no "void" words. ( ..a ... -- ..a ) is effectively void, but the "return value" is still `..a`, thus allowing composition of functions like `[ 1 + ] dup print [ 2/ ]`. no applicative language supports putting `print` or any other void function in a composition chain!
* not an array lang. lang features plurality. however, at least it uses virtual sequences, i.e. functions from index to element—especially _cords_, vseqs that appear as a concatenation but have O(1) concat
* stack
  ** neither functional nor stateful
  ** no scope. just position in the stack.
* macros are quotation monomorphisms, and their parameters must "known as constants" by the stack checker, though their values may be only dynamically known
  ** `inline` combinators may be partially applied to macros in one context so long as its parameters are appropriately compile-time e.g. `: length-case ( seq cases -- ) over length swap case ; inline`
* _functors_ are like macros but more powerful...? idk how they differ.
* lang is a core written in factor with a vm written in c++. factor began on the jvm, being used as a scripting lang for a larger java program.
* ffi can call c, fortran, and obj-c, and additional libs enable ffi w/js, lua, and c++. the ffi is easy (at least for c): just type the function e.g. `FUNCTION: SSL* SSL_new ( SSL_CTX* ctx ) ;`
* supports binary data well, viz as byte arrays, structs (`classes.struct` vocab), simd vectors, and _specialized arrays_ (unboxed/packed) and tuple arrays. this should make factor a good lang for hacking binaries. by the optimizing compiler, operations on tehse binary structures can approach c's speed. un/boxing is implicit.
* the _destructors_ lib supports deterministic cleanup/finalization of {see §5) external resources (e.g. file handle, network connection). this contrasts the usual gc model.
* syntax macros are called _parsing words_. these words are evaluated at parse time and may perform arbitrary computations. the `syntax` vocabulary contains many.
* extremely good [syntax] macros!
  ** backslash is needed to refer to a fn without execution e.g. `\ drop` pushes `drop`; `drop` alone would execute it. `\ drop` is different from `[ drop ]`. idk why, aside from being slightly briefer, one would use `\` instead of quotation.
  ** quotations are sequences
  ** the following are is implemented as factor macros, so they're expanded before runtime: named local binds, square and curly brackets, quote marks, and colon for fndefs. (meta-circular)
* like lisp, factor is a data-based lang. however, factor [stack] is simple enough that we can easily examine the whole program state in the debugger!
  ** debugging steps through ops and shows the stack at each op
* can pass around macros like any other data; unlike in lisp, macros are first-class data. truly all of factor's linguistic objects are symmetrical about computability; they're all data & transforms thereof.
* good, _flexible_ (somewhat implicit by generic words, mixin classes & instances) oop support (like cl)
  ** this is how we do ad-hoc relations. this makes encoding ad-hoc polymorphism easy, so we can have haskell-like concision but without haskell's restrictions.
    *** programs are often prolog-like: small facts (except here fns) that are used like a vocabulary; more code re-use than big, specialized chunks of code.
* code is compiled on the fly into highly optimized single static assignment (SSA IR). such a simple lang supports extreme optimization.
  ** use `optimized.` (instead of `.`) to see optimization details of some code
* extremely good ide: simple, debugger/stepper, inline docs (all local), quickly see everywhere that any word is used, and any word's definition
* uses arrays with pseudo-indexing (i->a) e.g. `<reversed>`
* comes with memoization library
* λ syntax is `::`
* good unicode support
* supports dynamic scope!
* postfix; read left to right, e.g. `2 even? [ "OK" ] [ "Cosmic rays detected" ] if` means `2|2 ? "OK" : "Cosmic ..."`
  ** pipeline [unix cmd pipe] design
* like haskell, data are just nullary functions
  ** all syntactic objects are simply called _words_
* excepting row-polymorphic combinators and macros, all words must accept and output a fixed number of words
* latently typed w/dynamic checking, static stack effect checking. duck typed oop/generics.
* modules are called _vocabularies_
  ** for maximum flexibility & interactivity, even private identifiers are usable in greater contexts if explicitly referenced
  ** like java public classes, each vocabulary must be defined in a file of the same name
* factor is oop, but all methods are generic; no class "owns" methods; instead, everything is interfaces [java] / purely abstract classes [c++] / type classes [haskell] and instances. instance lookup is dynamic.
* identifiers can be marked as private, but this is a suggestion, not enforced linguistically

=== implementation

* the `tools.deploy` vocab allows compiling to native executables which neither require factor to be installed on host nor expose source code!

=== other considerations

* the documentation is usually _astounding_, except that it _never_ features examples. some vocabs have only the technical, auto-generated docs.
  ** includes word definitions as source code
* the listener (repl) is super-capable and integrated well with the docs
* there are _many_ libs builtin (see factor handbook > libraries > vocabular index), and *they're all documented offline in the docs*
* the docs are updated realtime as vocabs are loaded
* ffi w/lua
* has python bindings

== environment

* `USE: <lib>` imports one lib. `USING: <lib> ... ;` imports many.
  ** *put space between last lib and `;`*
* `FROM: vocab => word ... ;` disambiguates imported words. it overrides `USE:`/`USING:`, and can be used in lieu of those
* see `QUALIFIED:`, `FROM:`, `EXCLUDE:`, AND `RENAME:`, too.
* `<PRIVATE code ... PRIVATE>` exports `code ...` with the suffix `.private`
* module A may use module B even if B has errors, as long as A doesn't use any of B's words in which the errors exist
  ** or maybe not? perhaps _sometimes_....
* `IN:` defines a module. *required when writing any module*
* you must import `kernel` when running scripts. yeah, even `drop` must be imported.
* _quotation's stack effect does not match call site_ is an inconsiderable runtime error displayed when a script finishes with a non-empty stack. even `MAIN:` is hard-coded to check against `( -- )`. either put `clear` at the end of your script or make your script have stack effect `( -- )`. this is probably the most idiotic thing i've seen factor do yet.
* `save` saves the entire program state to a file. this is useful for scripts, since they're usually re-evaluated on each run. of course, for programs that do not need re-evaluation, it's best to use the ui deployment tool (`deploy-tool`) to make native, speedy executables.
* command line args: `USE: command-line command-line get-global`. *arg0 (program name) is not included!*
* envars: `USE: env`; then singleton `env` is an assoc

see factor handbook > the language > vocabulary loader > vocabulary roots. you can get there by searching for `vocab-roots`.

vocabularies have metadata. this is encoded by directories: each vocabulary has its own directory e.g. `foo`, and inside it contains at least `foo.factor`, among any special metadata files (e.g. docs, author) or other files. any of the 3 methods in _working with code outside of the factor source tree_ are good for making directories available for use with `USE:` &c. otherwise you can use `add-vocab-root` *with an absolute path* (leading homedir tilde is supported.) *this are supported only in the listener.* in a source file, `USING:` is processed before the rest of the source file regardless of the order of words. this means that you can't set `FACTOR_ROOTS` in `env`, either.

so `FACTOR_ROOTS` is useless for scripts, unless you're fine with wrapping every executable factor script in a single-line shell script that sets `FACTOR_ROOTS` before running the script. using `add-vocab-root` in `~/.factor-rc` is the best solution.

NEXT: try `require` after `add-vocab-root`, just to see how it works

.example

suppose i'm keeping a `util` module at `~/programming/util/util.factor`, and i want to use it in the listener.

[source,factor]
----
"~/programming" add-vocab-root
USE: util
----

`util` here refers to the directory; that's why it's `util` and not `programming.util`. however, even if i name the module as `IN: programming.util`, i still can only `USE: util`, not `USE: programming.util`. that's unexpected. anyway, declaring names without periods is simpler anyway. still, TODO: explore how module (and corresponding directory) hierarchies correspond to `USE:` statements.

.no transient imports of generic words

because generic words are potentially many (and can often collide) the module system requires that you, at least in the listener, `USE:` providing vocabs despite having already `USE:`'d a module which itself `USE:`'d that same module. e.g. if my `util` module uses `io` for `stream-contents` (which is not generic but is defined in terms of `stream-contents*` which _is_ generic), then if you `USE: util` in the listener, you'll be prompted to `USE: io` so that `stream-contents` can be resolved. this affects only generic words. this is a price of dynamicism.

== running factor scripts

* if envar `DISPLAY` is not set then factor will run in a text repl
* there's no man nor info page, and `factor -h` sets the global var `h` to `t`, which is definitely not what we'd expect. to learn about invoking the factor interpreter, see "command line arguments" in the docs.
* see "scripting cookbook" in the docs for more info
* when you run factor, you'll probably want to put in `~/.local/bin` a script that `cd`'s to the factor install location then runs `./factor -i=factor.image "$@"`. because there's a gnu coreutil called `factor` (which factors prime numbers) ensure that `$HOME/.local/bin` is early in your `PATH`.
  ** strangely, though, even though `factor` in a terminal runs the number factorer, the `#!/usr/bin/env factor` in a factor script runs factor.
* scripts don't need `MAIN:`; the program is executed like most scripting langs

== exploring code & learning factor

NOTE: _ciif_ := "code in input field"

* `#concatenative` on irc.libera.chat (or irc.freenode.net? i'm seeing more ppl on libera)
* start with the factor repl's `help` menu item
  ** see _developer tools_
  ** see _all tips of the day_ (factor handbook > developer tools > help system > tips of the day)
* read the factor source code
* ^i: see the stack effect of ciif
* ^w: step through ciif
* ^t: time execution of ciif 
* `apropos` e.g. `"group" apropos` (equivalent to searching in the factor handbook [help] search box)
* familiarize yourself with word naming conventions (handbook > the language > conventions § word naming conventions)
* `:error` gives most recent error. `:c` to see its callstack

== semantics

* see `DEFER:` for mutual recursion
* scope is not often a consideration. however, `set` is scoped only within a source file (b/c files are parsed with `with-scope`)
* strings are sequences of unicode code points, not of bytes. factor supports encodings well. writing bytes is merely a matter of using the correct encoding (namely the `binary` encoding)
  ** bitstring literals are enterable by `B{`, the byte array literal syntax. you can use `B{` with `write` e.g. `path binary [ B{ 96 0xa 65 } write ] with-file-writer`
    *** `0x` syntax is directly supported by factor. no need for even number of hex digits, btw.
* pushing quotations does not use memory
* `f` is the false value; all others are truthy
  ** `t` is the canonical truthy value
* `{ 1 2 3 } dup [ [ 1 + ] map! ] dip . .` prints `{ 2 3 4 } { 2 3 4 }`. therefore `dup` duplicates, at least for non-primitives, a pointer, and arrays are mutable...? this seems to suggest so, but `{ } 3 suffix!` confoundingly fails with _sequence index out of bounds_. this example fails when i use `3 [0,b]` instead because ranges are immutable.

.concurrency & parallelism

see vocab `threads`, vocabs tagged with `concurrency`. parallelism words are in `concurrency.combinators`.

=== vs picolisp

factor & pil are equally simple, dynamic, and support purity & mutation, and both are extremely efficient (though i've yet to contest them). lambdas are equally easy in both. factor's concatenativity and pil's applicativity is the big difference, and is what makes factor the clear winner. though lists are stacks and pil has `apply`, pil (or other lisps) can be a stack machine only if every function can choose how many data to take from the stack. some take a certain number (either common words, which is a fixed positive integer, or combinators, whose arities are ultimately functions of their parameter functions' arities) or are, like `loop`, variable (these classes can be phrased as static vs dynamic arities.) if we can calculate/get that, then a simple fexpr would make pil into a stack lang. yet factor's parameterization of words is slightly nicer than pil's parameterization of data: pil asymmetrically considers nullary functions & data differently, which means that parameterizing a datum is non-trivial.

NOTE: i've yet to consider pil's universal dynamic binding, and how it can use various kinds of symbols

* factor's state is usually stored on the stack, and pil's in appropriate variables. however, both can use stacks or variables easily.
* both langs use loop primitives instead of manual recursion (usually)
* pil hasn't generics; instead, _everything_ is lists.
* macros are first-class in both factor and pil

factor is easier to learn than pil, namely because:

* pil's documentation isn't nearly as easy to navigate
* the pil repl isn't nearly as helpful as factor's
* pil is far more likely to unceremoniously produce unexpected behavior instead of halting with a helpful error, as factor usually does.
* pil's handling of symbols (internal, transient, &c) is uncommon and complex or not obvious, nor easily explained, at least by the official docs

.pil's advantages over factor

* seems smaller (comes with fewer primitives)
* is simpler; again, _everything_ is only lists & `eval`, and the vm is amazingly simple & efficient
* not more dynamic, but dynamic & hacky behaviors are easier in pil
* is terser (variable names)

it doesn't really matter which of factor or pil you use, but factor is easier to learn and use, comes with a large set of libraries, runs on both windows and *nix, and supports writing guis, so you should probably use factor, though pico is probably worth learning.

== special builtins

these are contrasted with non-special builtins; these builtins are not useful in writing programs, but are used to examine programs or otherwise concern the vm or language itself.

* `call`: lisp's `eval`. runs a quotation, curried fn, or fry expression.
* `\ f`: pushes `f` onto the stack. `f` is then callable via `execute`
  ** `execute` cannot be used with dynamically bound variables; in that case you must use `execute(`

== the repl (the "listener") and the (documentation) browser

* browser keybinds: //note: mac uses use command key instead of alt
  ** alt-f: focus search bar
  ** ctrl-k: open "jump to" dialog
* *just because a program runs in the listener does not mean that it is correct*. e.g. `f [ 1 ] unless` runs but trying to get its stack effect produces a stack effect mismatch error! replacing it by `unless*` runs the same as `unless` but has a correct stack effect.
* set font: e.g. `"monospace" 20 set-listener-font`. you can `save` the image or put in `~/.factor-rc`
  ** btw the browser font size is *not* adjusted by using ctrl-- & ctrl-+, despite what's been said in the mailing list
* press `shift+return` to start a new line in an expression; press `return` to evaluate.
* when the cursor is left in a word for 1s, its stack effect is displayed in the status bar
* the `refresh-all` word reloads all loaded source files. unlike clojure/cider, reloading the file does not merely execute statements; suppose that a file defines a word; then that file is loaded, modified to have the word definition removed, then reloaded; the word is no longer defined in the listener.
  ** TODO: determine when/how/why `refresh-all` fails. never trust it too much.
* supports tab completion
* supports ^p & ^n but not up & down arrows
* runs as a gui rather than cli program
* is a client that connects to a repl server
* tracks the stack for you, which makes easy both working with state and debugging

== stack evaluation model

NOTE: the _retain stack_ stores values to push back later. it's used by words like `dip` (or `keep`, which is defined in terms of `dip`). see it in action in the walker (`^w` instead of `return` in the listener)

there is no function _composition_. there are only combinators (higher order functions) and application (β-reduction.) combinators are obvious because they always use qutations. unlike functional languages, words are always applied unless quoted (i.e. in a quotation); unquoted words are always applied. this differs from scheme, where `f` is different from `(f)` and `f` may be passed as an argument. factor is different from haskell, where `f x` evaluates to a result but `f` may still be passed as an argument to a higher-order function. in factor `f` is always applied to the stack below it. furthermore there is no distinguishment between data and functions; like haskell, words are all the same and each has variable natural number arity. `+ = 1 -1 ?` uses neither higher order functions nor composition _per se_; it is equivalent to composition, though composition exists only in a functional model and has no meaning in a stack model, since there composition is equivalent to application which are/is always implicit. binary `+` is applied, then binary `=` is applied. notice that i did not say "applied to `+`'s result." there are no function outputs in the stack model! the only input and output is the stack. any word may affect the stack in any way. here `+` is applied to the top two stack elements, then `=` is applied to the top two stack elements. therefore the stack effect of `+ =` is `( x x x -- x)`; `1 2 3 + =` is `1 == 2 + 3` in common pseudocode, and `+ = 1 -1 ?` is `λx y z. if x == y + z then 1 else -1`.

* `[ + = 1 0 ? ]` has stack effect `( -- x)` i.e. it's just a datum; but `[ + = 1 0 ? ] curry` has stack effect `( x -- x)`.
* non-higher order functions cannot be variadic, though higher order functions can be; their arity is a function of their argument function(s)'.

NOTE: fns are curried. e.g. `{ { 0 1 } } at` is illegal if the stack is empty; however, `: X ( x -- x ) { { 0 1 } } at ;` is fine b/c it defines but not evaluates `X`. functions may be defined in terms of other [curried] functions, which in turn are curried. you can tell that a function is curried by using an unquoted function that would usually cause stack underflow if applied to an empty stack.

== syntax

the only true syntax of the language itself, rather than a syntax implemented in factor itself, is that words are whitespace-delimited. defining words is a user-definable syntax, as are definition suffixes like `flushable`; consider the definition `: pp ( a -- ) . ; flushable`. here we're pushing each word to the stack. `:`, `(`, `--`, `)`, `;` are all just words. after `;` is pushed & evaluated, a definition is left atop the stack. that definition is an argument to `flushable`. one beautiful benefit of such uniform design is that the documentation for _all_ parts of the factor language is uniform and equally accessible by simply clicking on the word in the help docs.

furthermore factor beats lisp(s except picolisp and possibly some other uncommon, simple lisps) at its own game: factor actually does not distinguish between code & data; all language objects are _words_, which are just strings associated with properties. the only truly core parts of the language are hashtables, tuples, and other primitive data structures. this means that the language is not at its core a language, but instead a simple system of data manipulations i.e. creating & re/moving data and elementary arithmetic; the only other unique aspect of the language that makes it factor is the implicit & simple fact of how the stack is evaluated, viz β-reduction, and its static stack effect checking.

NOTE: primitive words are marked by featuring the `PRIMITIVE:` word in their definitions e.g. `datastack-for` in `kernel.private` vocab.

the _continuation implementation details_ page is very refreshingly overtly simple: "a continuation is simply a tuple holding the contents of the five stacks: [... each of which] can be read and written." no black box. no trepidation about internal complexity, and certainly no external complexity. maybe i've been scarred by racket's docs on continuations, but i know that all languages besides factor that i've encountered have even attempted to be so clean.

== oop / generics / ad-hoc polymorphism

if you aren't using generic methods or other oopy things, then prefer hash tables over collections of tuples because 1. they support the whole `assocs` vocabulary, and 2. they don't require special syntax; keys can be dynamically generated easily, and can be any value.

TODO: discuss _protocols_ e.g. `assoc`

probably the easiest & most flexible oop ever:

[source,factor]
----
TUPLE: circle r ;
TUPLE: rect l w ;
GENERIC: area ( shape -- area )
M: circle area r>> dup * pi * ;
M: rect area [ l>> ] [ w>> ] bi * ;
----

NOTE >>foo writes, foo>> reads. i guess that words [functions] are used because, if true, as class hierarchies are built, mere accesses become arbitrarily or greatly augmented. such degree of augmentation seems unlikely, though. i would expect, especially in a language like factor that touts its dynamicism, that hash keys would be preferred over accessor & setter words, as it's done in clojure. it seems that factor is perhaps not so flexible or dynamic as picolisp. TODO: how are tuples advantageous over mere hash maps? actually, they cannot be, since maps are the plainest general structure.

these are called _tuple_ classes. `r`, `l`, & `w` are called _instance variables_, so named for the interpretation of these named tuples as _classes_ and a constructed tuple (rather than its type/spec/shape) being seen as an _instance_ [object] of the tuple class. a _method call_ is a generic function that applies to a tuple e.g. `r>>` or `area`, both of which apply to any object that supports them (viz any tuple instantiated of a class having an `r` instance variable and a class that supports `area` respectively, where support is determined dynamically.

ways to instance a tuple: `boa`, `new`, `T{`, or by using the `constructors vocab.

i know not of classes other than tuples. tuples are considered as sets of attributes.

_derived classes_:

* _predicate classes_ are subclasses satisfying a predicate.
  ** is a subclass not merely a union? e.g. `TUPLE: a a b c ; subclass b a d ;` sees `b` as a's attributes ∪ {d}, yeah?
* _union & intersection classes_ are the union or intersection of classes.
  ** _mixins_ are a variety of union class. i have no idea what they add to union classes.

* _primitive_ classes represent data primitives and cannot be subclassed
* what are
  ** multiple dispatch (planned inclusion in factor, but currently implemented by a library)
  ** predicate classes

three functions from class to class:

* derivation
* union (n-ary)
* intersection (n-ary)

three types of classes:

* primitive
* tuple
* derived
* predicate (subclass B of A where A consists of instances satisfying a predicate)

primitive & tuple classes use >> & << (but not derived ones?)

== common words

.`sequence` vocab

* `nth`: elem at index or error. `nths` is like mapping curried `nth`
* `set-nth`. mutative, so whereas `CHAR: c 1 "-s" set-nth` leaves the stack empty, `"-s" CHAR: c 1 pick set-nth` leaves "-c" atop
  ** `change-nth` may be preferable. like `set-nth`, it's mutative, so you need some odd `dup`'s e.g. `{ "CAT" } dup 0 swap [ dup CHAR: c 1 rot set-nth ] change-nth` leaves `{ "CcT" }` on the stack.
    *** `swap over` ( a b -- b a b ) may be useful here
* `?nth`: elem at index or `f`
* `prefix`, `suffix`: adjoin at head or tail
  ** `prefix?` & `suffix?` are not defined; instead use `subseq-start 0 =` for `prefix?` and `[ subseq-start ] [ [ length ] bi@ swap - = ] 2bi` for `suffix?`
    *** regarding `subseq-start` &al, the factor docs use _subsequence_ to mean _substring_
* `insert-nth`: insert at provided index, moving latter elements rightward by one index
* `prepend`, `append`: concatenate 2 topmost sequences
* `concat`: concatenate elements of a sequence of sequences
* `join`: intercalate then concat

there's no complement of n-array; however, `2array` &c has complements `first2` &c. `nths` pushes 1 sequence, not n elements, to the stack.

example: find 1st element matching some predicates: `[ preds 1&& ] find nip` e.g. `{ "kak" "file" 36 41 } [ { [ number? ] [ even? ] } 1&& ] find nip` returns `36`.

=== pattern matching

there's a primitive built-in pattern matcher, but you're better-off rolling something better, or using PEG.

[source,factor]
----
USE: match
MATCH-VARS: ?x ?y ;
: my-match ( seq -- )
{ { [ _ "2" ?y ] [ 14 number>string write ?y print ] }     ! case 1
  { [ ?x _  ?y ] [ ?x 7 * number>string write ?y print ] } ! case 2
  { [ _ ] [ "<no match>" print ] } }                       ! else
match-cond ;
{ "1" "2" " is the number" } my-match ! writes 14 is the number
{  6  "6" " is a number"   } my-match ! writes 42 is a number
----

case 1 is more specific than case 2; were case 2 earlier, it would match even if case 1 were a better match.

== stack tech

.tips

* if `seq q map` is used as per usual, then `seq q each` pushes the results of the map to the stack rather than collecting them into a seq. sadly, this is a hack; it works only in the listener, which does not stack check thoroughly; `each` requires its quotation to have effect `... x -- ...`. therefore we must use `with-datastack` e.g. to perform a 4-ary fn `f: ( a b c d -- x )` on data from an assoc: `[ at ] curry { "a" "b" "c" "d" } swap map [ f ] with-datastack first`. you can instead use `firstn` in `sequences.generalizations`.

=== good stack words

* `preserving` (of the very useful `combinators.smart` vocab): when running a word, don't consume its args from the stack e.g. `1 2 [ + ] preserving` leaves `1 2 3` atop the stack.
* `?if` is a seemingly particular one: it's `a -> (a -> Maybe b) -> (b -> c) -> (a -> c) -> c`. it's the same functionality as haskell's `either`.
* `[ x ] 2dip` is clearer than `x -rot`. you should rarely use `[-]rot`; there's usually a better way to structure your code!

.impure `cond`

`cond` performs stack effects in order until the top is truthy. prior conditional predicate quotations affect later ones. this example demonstrates it, as does the following one:

[source,factor]
----
{ { [ dup empty? ]              [ drop 1000 ] }
  { [ dup first 6 * dup 50 <= ] [ ] }
  { [ drop t ]                  [ drop "none" ] }
} cond
----

[options="header"]
|=============================
| argument   | resultant stack
| `{ }`      | 1000
| `{ 5 3 }`  | 25
| `{ 15 3 }` | "none"
|=============================

note its ``dup``s & ``drop``s. the 1st condition must `dup` so that, if not empty, the sequence will remain on the stack for the 2nd condition to test, and so on. consequently, each branch replaces the sequence by some other value. factoring-out the ``dup``s to before the `cond` assoc is incorrect; that'd be the same as moving the first `dup` and removing the second. `dup` must be performed before each of `empty?` and `first`; a sequence must be atop the stack before each of those predicates is performed, and each predicate must ensure that it keeps [that] sequence atop the stack for the next predicate to evaluate, unless the assoc is designed to mutate the stack as it goes through the predicates. admittedly, though mutating state while going through predicates is _generally_ useful, it's _commonly_ not, and a pure version of `cond` would be nice to have additionally.

stateful `cond` is especially useful in writing parsers e.g.

TODO: rewrite this in relational style

[source,factor]
----
USING: kernel namespaces system command-line ;
SYMBOL: PARAM1 PARAM1 off
command-line get-global
[ [ f ]
  [ unclip-slice { { [ dup "--param1" = ] [ drop PARAM1 swap set-global t ] }
                   { [ dup "--help" = ] [ print-help 0 exit ] }
                   { [ drop t ] [ write " is an invalid arg" print -1 exit f ] } }
                 cond ]
  if-empty ]
loop
----

=== sequence & looping

* `collector-as` (guard is filter) & `selector-as` (guard is short-circuit) are the most general looping functions that collect into a sequence. they do not require input sequences; they use whatever state the stack has as input.
  ** `q collector` leaves a quotation that applies `q` then pushes that result to a resizable seq, and that resizable seq (to keep it in scope)
    *** `collector` is more convenient than `loop`: less shuffling and terser.
* `seq [ ] each` pushes each elt of seq to the stack

`reduce` with stack modification example: test whether all items in a sequence equal. ``reduce``'s identity starts at `t` and is a boolean of whether all elements so far are equal. the part left on the stack for the reduction quotation to implicitly use is the previous element encountered, initializing to the first.
 
[source,factor]
----
: all-eq? ( seq -- ? ) [ first ] keep t [ pick = and ] reduce nip ;
{ 1 0 3 } all-eq? ! f
{ 1 1 1 } all-eq? ! t
----

* `reduce` accepts only one `identity`, so we need to have the other part(s) of our accumulator already on the stack before the input sequence.
* `nip` to remove the non-output part of the accumulator. generally you'd `[ drop ... drop ] dip`

actually, though, this particular example is more elegantly expressed as:

[source,factor]
----
: all-eq? ( seq -- ? ) dup unclip-slice suffix = ; inline
----

TODO: make a loop combinator that processes whatever `e`, which may conveniently be an input sequence as given by a combinator `seq>loop` of effect `( seq -- e )`, and: 1. if `SYMBOL: stop` is returned then the loop stops; 2. returning `f` will not push the element into the output sequence; 3. other values are pushed into the output seq. `seq>loop` will output `short` if empty. this general filter/map/stop loop pattern is practically universal! it can mutate state arbitrarily, accumulate from any state into a sequence, retaining or discarding elts. i should be able to have the argument function return multiple values, too, thus allowing it to return multiple values, and those can be inserted inline into the output seq. in fact, i should be able to have my accumulator be any structure that supports insertion, e.g. a splay tree.

deep-each example: `{ { { 1 2 { 3 4 } 5 6 } { 7 8 } } } [ . ] deep-each` outputs:

----
{ { { 1 2 { 3 4 } 5 6 } { 7 8 } } }
{ { 1 2 { 3 4 } 5 6 } { 7 8 } }
{ 1 2 { 3 4 } 5 6 }
1
2
{ 3 4 }
3
4
5
6
{ 7 8 }
7
8
----

the `sequences.squish` vocab defines `squish` which takes a function of `{ 1 2 { 3 4 } 5 6 }` & `{ 7 8 }` whereas `deep-map` tries applying a quotation to `{ 3 4 }` & `5`, probably b/c `{ 3 4 }` is the deepest sequence and `5` follows it. idk what the general pattern is; i'll explore that when i have nothing better to do. idk what "preorder" means.

==== folds with short-circuiting

stack langs are extremely powerfully flexible in that the whole stack is available to loop bodies. thus the whole `map` vs `2map` problem is not really a problem, once those are recognized as convenience functions, not essential combinators. generally we use `while`, or `loop` if the continuation condition is of the iteration's output, for non-sequences, and `each` for sequences. although `map` is optimized a bit (using `nth-unsafe`), `collector` with `each` is just about as good. still, note that ``map``'s definition is not in terms of `unclip-slice`! *factor does not use linked lists.* `map` is defined in terms of `map-integers-as`, which accepts only an integer—not a sequence—as its input! rather than linked lists, factor uses growable sequences, which grow from the _end_ in O(1) time and have O(1) lookup. these are much more natural. of course a sequence is added to at the end, not the beginning! any non-coder would suppose so, just as they'd suppose that left folds are natural, not right ones.

all this to say: _never_ use `loop` and `unclip-slice` together. this isn't haskell or lisp, and thank god. well, ok, you _can_ use _unclip-slice_ and it's still natural in some cases, probably, but `unclip-slice` is just a shorthand for `[ 1 index-to-tail <slice> ] [ 0 swap nth ] bi` which obviously generalizes when we use numbers other than 1 & 0. furthermore, `nth` is random access, as is slicing eventually. i suppose that the motivation for looping with `unclip-slice` is that we check `empty?` which is easier than checking whether an index is less than length. regardless, there are looping combinators for:

. looping through sequences
. looping until a predicate yields `f`
. short-circuiting
. collecting loop iteration results

and it's better to use direct access than sequential access because it considers elements independently of others, enables getting multiple elements at once (array programming) and not tracking context. consider zippers (data structure). they represent the context at one and only one index, and they need a whole data structure for that! contrast this with a set of indices, which represents any number of contexts simply. the obviousness of it is supreme.

an example of "augh! this looping control flow is too complex. let's just modify the stack." is, given two lists A & B, generating `{ { a bs } ... }` where a∈A and bs(a) is the substring of `B` all of whose values are greater than or equal to `a`, when `A` & `B` are both sorted ascending. your first idea may be to use `accumulate*` because B progressively becomes a substring of itself. scans/folds are associated with iterative mutation. however, we're also mapping over `A`! `2reduce` doesn't help b/c we aren't _mapping_ over `B`; we're progressively modifying it in total. this is not a 1:n map. it's a 1:n reduction. it's worth mentioning that an efficient solution (enabled by `sorted-index`) is given by arrays; see the definition of `join<` in _§using the stack well_. such solutions should always be preferred. however, suppose that we use an alternative method which is not sensible for this scenario, but similar scenarios would entail these kinds of control flow concerns: at each iteration, return (a,{b|b∈B,a>=b}), removing all b<a from B for the next iteration.

TODO: write this code when i have time
[source,factor]
----
! : join< ( B A -- joined ) ! precond: A is ordered ascending. each of A & B is `values` of their pk->val assocs
{ 2 3 6 10 12 18 24 36 42 83 91 102 }
{ 10 12 34 56 87 } ! next: test when a>sup(B)
[ [ swap >= ] curry find drop [ tail-slice ] [ { } ] if* ] ! ( B' a -- B' )
accumulate*
! ; inline
----

`combinators.short-circuit` is a helpful vocab. example: `[ { [ sequence? ] [ integer? not ] } 1&&`, meaning scheme `(λ (x) (and (sequence? x) (not (integer? x))))`.

==== other general looping stuff

here's an interesting pattern:

[source,factor]
----
V{ } clone dup [ last . ] [ push ] bi-curry
[ 400 ] dip [ call ] keep ! push 400 into the vec
over call ! prints 400
[ 2 ] dip [ call ] keep ! push 2 into the vec
over call ! prints 2
2drop . ! prints V{ 400 2 }
----

== namespaces

=== globals

like lua's `_G`, factor has a global namespace called `global`. namespaces instance the `assoc` class.

[source,factor]
----
SYMBOL: x      ! declare
4 x set-global ! set
x get-global   ! access
----

=== locals

[source,factor]
----
60 [let 2 5 + :> x 49 x / * ] ! pushes 420
60 [let :> x x x * ] ! pushes 64. :> binds the top of the stack to an identifier while dropping it
----

locals do not care about nesting:

[source,factor]
----
[let 40 :> x x even? [ x 2 * :> y y 2 * ] [ ] if ] ! pushes 160 to the stack
----

==== mutable vars

[source,factor]
----
USE: locals
! 3 f => 11
:: f ( x! -- t ) ! x! makes x mutable by enabling x! to set x (see below)
  x 2 * x! ! x<-2x
  5 x + ;  ! return 5+x
----

`x!` pops into `x`. exclamation marks ("shrieks") are particular here.

this syntax can be used in `[let` also e.g. `[let 24 :> x! x x * x! x 400 - ]` which outputs 176.

=== multiple simultaneous output streams

NOTE: see <<_logging>> in this document if that's your particular use case.

[source,factor]
----
USING: io.encodings.ascii destructors ;
SYMBOL: extra-out
: ./eo ( x -- ) extra-out get [ . ] with-output-stream* ; ! note the asterisk! i don't want to close the output stream in ./eo!
"extra.log" ascii <file-writer> [ extra-out [ 10 . 20 ./eo ] with-variable ] with-disposal
----

writes 10 to stdout and 20 to ./extra.log.

this can easily be extended to more output streams, though for arbitrarily many you'll probably want to make your own version of `with-disposal`.

== caveats

lines like `f number>string drop` cause scripts to exit silently WITH EXIT CODE 0 nonetheless! also i spent quite a bit of time trying to debug a script, only to find that `{ "systemctl suspend" } run-detached` was the issue; it should've been `{ "systemctl" "suspend" }`! it silently tried executing the invalid program name. even the resultant `process` object did not have anything indicating an issue. however, the resultant `process` object of `{ "systemctl suspend" } run-process` had `{ status 255 }`.

code in the listener that uses `if` may successfully or unsuccessfully run despite having improper stack effects. `ctrl+i` recognizes the mismatched stack effects. were i to put this in a function in a vocab then try to load the vocab, i'd get a stack mismatch error. thus this issue really exists only when running code directly in the listener.

== libs & specific words

* for graphics, use cairo; it has bindings to factor
* see factor documentation > libraries. it's a wealth of functionality in one big listing!

== tricks

* `USE: math.ranges CHAR: a CHAR: z [a,b]` works b/c characters are integers
* `USING: math.parser random ; "(ddd) ddd-dddd" [ { { CHAR: d [ 10 random number>string ] } [ 1string ] } case ] { } map-as concat`
* `USING: calendar calendar.format ; now 1 months time+ { YYYY " " MONTH " " DD " " hh ":" mm ":" ss "\n" } formatted`. `formatted` is a macro.
  ** `millis>timestamp`, and its complement, `unix-1970 time- duration>milliseconds >integer`

`io.styles` vocab e.g.

[source,factor]
----
USING: colors.gray io.styles hashtables sequences kernel math ;
10 <iota> [
    "Hello world\n"
    swap 10 / 1 <gray> foreground associate format
] each
----

== `math`

* `bitxor`, `bitand` &c. see the docs for related fns like `2/` (right shift by 1 bit), `bitcount`, and `even-parity?`

== os

=== subprocesses (`io.launcher` vocab)

generally one may make a `process` tuple then modify its properties then run it. however, usually we want the default process behavior. notice that words like `run`, `run-detached`, and `<process-reader>` accept "desc"s, not process object _per se_. indeed, these words use generic word `>process`. to convert an object into a process. therefore we commonly do e.g. `{ "echo" "hello, there!" } run` rather than `<process> { "echo" "hello, there!" } >>command run`.

.read a process into a string

[source,factor]
----
USING: io io.launcher io.encodings.utf8 ;
{ "echo" "hello, there!" } utf8 <process-reader> stream-contents .
----

.stdin & stdout redirection

[source,factor]
----
<process> { "cat" } >>command "outfile.txt" >>stdout ascii
[ "this output is in a file!" print ]
with-process-writer* ! or omit the asterisk if you don't need the process object nor its status
----

if you run `"cat" ascii [ "hello!" print ] with-process-writer` in factor in a terminal (i.e. by launching factor with `export -n DISPLAY`), then you see the output directly in the terminal. you can replace `cat` by a curses program e.g. `w3m` and see that curses programs work fine, too.

NOTE: `binary` encoding does not work with `with-process-writer` nor `with-file-writer`! it gives some odd error: `element-size` does not define a method for the <such-and-such> class (class depends on what you're writing) dispatching on <item of that class>.

which waits for the process to finish (returning `cancel-operation` if the process takes longer than its `timeout` attribute) and returns the process object and its exit status. look at the definition of `with-process-writer*` for details.

* `run-detached` truly runs a separate bg proc, so that you never need to use `nohup`
* `obj run-process` is the simple synchronous execution of a cmdline string or array of strings.
* `stream-contents` replaces the process on the stack with its output.
* though `echo` supports `-e` to not output trailing newline, remember that we can trim trailing newline by `[ CHAR: \n = ] trim-tail`

TODO: how to stream one process's output as input to another process, or stream to stdout? the trouble is that `<process-reader>` returns an input stream, but `write` takes only binary data or a string. do i need to read n bytes at a time from the input stream then `[stream-]write` that?

.exec

a common use of factor is as a powerful alternative to bash, often simply preparing command lines then executing them, replacing itself by that child process (exec). this is done by using `exec-args-with-path` (of the `unix.process` vocab) instead of `run-process`.

=== filesystem

* vocabs: `io.files`, `io.directories`, `io.encodings`
* load files as streams: `with-file-[reader|writer]`
* load whole file: `[set-]file-[contents|lines]`
* `current-directory` dynvar

examples:

* `"filepath.txt" utf8 [set-]file-contents` to read or write to a file.
* `"writeme" utf8 [ "readme" mac-roman [ [ print ] each-line ] with-file-reader ] with-file-writer`

== peg

factor's `peg` vocab is a link:https://bford.info/packrat/[packrat parser].

peg is like regex but makes extracting substrings and implicitly putting them in an ast much easier. peg also works on sequences of any type:

.intro demo
[source,factor]
----
USING: peg peg.search ;
{ { -47 4 } { 2 34 } { -1 6.6 } { 3 766 } }
[ first 0 < ] satisfy [ second ] action repeat1 search .
----

NOTE: `search` is defined in terms of `any-char-parser`—a special parser defined in `peg.search.private` which, despite its name, parses _any thing_ not just _any character_. `any-char-parser` is, at least in factor v0.99, equivalently defined as ``peg.parsers``'s `any-char`.

prints V{ V{ 4 } V{ 6.6 } }. pattern matching on number sequences can be extremely useful for e.g. technical stock trading, finding subsequences of blobs.

* `parse ( input parser -- ast )` where input may be a string
* common parsers (e.g. `any-char`) are in `peg.parsers`
* `hide`
* `satisfy` matches a character against a predicate quotation
* `token` is a parser that tries to match a string literal
* `sp` modifies a parser to accept & ignore leading whitespace e.g. `"  hi" "hi" token sp parse .` prints "hi"

primitive (by definition—not theoretical canonical basis) peg parser words:

* `action`
* `range`
* `satisfy`. accepts only a sequence as input. tests only a single element of the input sequence.
* `sp`
* combinators (they take 1+ parser(s) as inputs):
  ** `seq`
  ** `choice` (or). *not commutative!* tries the 2nd only if the 1st fails!
  ** `optional` (or hide)
  ** `repeat0` (kleene star)
  ** `repeat1` (kleene plus)
  ** `semantic` (define a parser's validity in terms of its output. for a parser composed by `seq` or other combinators, `semantic` enables you to decide whether a group of parsers altogether are valid)
    *** `semantic` is `satisfy` except that it accepts another parser as input

.tips & examples

* parse 3-substrings of increasing value: `any-char 3 exactly-n [ [ < ] monotonic? ] semantic`
* you can use `[ ... ] satisfy [ ... ] action` to perform a function on a matched value, or you can do `[ ... ] action [ ] semantic` to perform a function then check whether its output is valid. this pattern is helpful when your `satisfy` & `action` quotations would share a lot of code; you can write the code only once as an argument to `action` then use `[ ] semantic` to filter the results.

* there's no "and" to complement "or" (`choice`). this is because pegs parse-out values. and & or complements naturally exist for testing whether a parser suceeds, but not for combining results.
* `any-char` is regex `.`. idk why it's in `peg.parsers` instead of in `peg`, what that implies. for a particular character, just make it a singleton string then pass to `token` e.g. `"0" token` to parse a zero. `peg.parsers` has a word, `1token`, which does exactly that.

special parsers that affect not what's parsed, but the parsing itself:

* `box`
* `hide`
* `check-parse-result`
* `delay`
* `ensure`

.regex as peg words

these are in `peg`:

[options="header"]
|================================================================
| regex    | peg word(s)
| [A-Za-z] | `range` & <and AND combinator to be defined>; or `range-pattern` (in `peg.parsers`)
| ab       | `seq`, `token` (`token` is `seq` on string literals)
| a?       | `optional`
| a*       | `repeat0`
| a+       | `repeat1`
| (a|b)    | `choice`
|================================================================

the following are in `peg.parsers`:

[options="header"]
|=====================================================================================
| regex            | peg word(s)
| a                | `1token`
| .                | `any-char`
| {m,n}            | `at-least-n`, `at-most-n`, `from-m-to-n`, `exactly-n`
|                  | `epsilon` (empty sequence)
| `[0-9]`          | `digit-parser`
| `[0-9]+`         | `integer-parser`
| ((<pat>)<sep>?)* | `list-of` e.g. `"2,32,64" integer-parser "," token list-of parse`
| "([^"])"         | `string-parser`
|=====================================================================================

.ideas sensible only in peg, not regex

* `ensure[-not]`
* `satisfy`
* `semantic`
* `hide`
* `action`
* `surrounded-by`
* `add-error`

[TODO]
* how to run a parser just to see if it succeeded or not?
* how to combine a parser `p` with `satisfy` as `[ p quot and ] satisfy`?

* `satisfy repeat[0|1]` returns a vector of characters
* `1token`, defined in terms of `1string`, returns a singleton string

`ensure-not` allows us to check whether we're at the end of input:

* `"X" any-char any-char ensure-not 2seq parse` pushes `V{ 88 }`
* `"" any-char ensure-not parse` pushes `ignore`

* it seems that adding `ensure[-not]` to `choice` makes a `cond`-like parser

examples:

[source,factor]
----
! COMMON PARSERS
: any ( q -- parser ) satisfy repeat0 [ >string ] action ; inline
! to is to-end if predicate is never hit
: to ( q -- parser ) [ not ] compose any ; inline
: to/c ( c -- parser ) [ = not ] curry any ; inline
! BUG: to-end fails on empty string; it should then return the empty string
: to-end ( -- parser ) any-char repeat1 [ >string ] action ; inline
: many ( q -- parser ) satisfy repeat1 [ >string ] action ; inline
: a* ( c -- parser ) [ = ] curry any ; inline
: a+ ( c -- parser ) [ = ] curry many ; inline
! sp is probably more efficient when you can use it; ws* & ws+ are
! intended to be used at least for list-of.
: ws* ( -- parser ) CHAR: space a* hide ; inline
: ws+ ( -- parser ) CHAR: space a+ hide ; inline
: WORD ( -- parser ) [ CHAR: space = not ] many ; inline
: words ( -- parser ) WORD ws+ list-of ; inline

! EXAMPLE COMPOUND PARSER
: my-clause-parser ( -- parser )
  f ! empty seq
  CHAR: - to/c [ [ CHAR: space = ] trim ] action suffix
  "->" token sp hide                             suffix
  WORD sp                                        suffix
  CHAR: : to/c [ words sp parse ] action sp      suffix
      [ CHAR: : = ] satisfy ensure
      ":" token sp hide
      to-end sp
    3seq
    any-char ensure-not
  2choice                                        suffix
seq ; inline
----

[options="header"]
|===========================================================================================================================
| input                                              | output
| "expr -> mytbl apple   booty cow  dargon : x >= 5" | V{ "expr" "mytbl" V{ "apple" "booty" "cow" "dargon" } V{ "x >= 5" } }
| "expr -> mytbl apple   booty cow  dargon"          | V{ "expr" "mytbl" V{ "apple" "booty" "cow" "dargon" } }
|===========================================================================================================================

* "x >= 5" is in a vector because of `3seq`; e.g. `"A" any-char parse .` returns 65 as expected, but `"A" any-char 1array seq parse .` returns `V{ 65 }`.
* how to parse recursive syntaxes? there should be a peg json parser example on the web for an example.

caveats & mistakes:

* `"thing horo nee" any-char repeat1 ws+ list-of parse` returns a singleton vector of a vector! this is because `list-of` calls `any-char repeat1` which matches the whole string; then `list-of` tries to break on spaces, but there's no more input, so it returns that single vector of characters in a vector.
* error about gensym: then check to see if you forgot `suffix` after your parser
* error about `length` not having method for `parser`: you probably put 2+ parsers on the stack but forgot to put them into a sequence. especially with `ensure`, ensure that you do `<q> ensure <parser> 2seq`

.search & replace

[source,factor]
----
USING: peg peg.search kernel make sequences strings ;
! simple modification of string-parser in peg.parsers vocab
:: delimited ( start end -- parser )
    [ [ start = ] satisfy hide , [ end = not ] satisfy repeat1 ,
      [ end = ] satisfy hide ,
    ] seq* [ first >string ] action ;

CONSTANT: props H{ } clone
"TSLA" "INST" props set-at
"i feel like trading {INST} today. {INST} is a fine stock."
CHAR: { CHAR: } delimited [ props at ] action replace .

"queueing either makes one happy or not."
[ "aeiou" member? ] satisfy repeat1 [ first ] action ! parse the 1st of a string of vowels
any-char 2 at-most-n 2seq [ first2 >upper 2array ] action ! (A). capitalize the 2 (or fewer if end of input) characters following the last vowel
replace .
----

prints "i feel like trading TSLA today. TSLA is a fine stock." and "quNG eTHeR maKEs oNE haPPy oR noT.". strange how we need `first` before `>string`. somewhy the vector of characters matched by `repeat1` is itself wrapped in a vector.

notice that action (A) returns an array of a character and another array. `[ first2 [ 1array ] [ >upper ] bi* 2array 1array ]` has the same effect; arrays are effectively flattened; `replace` is defined in terms of `tree-write`.

NOTE: `replace` works only on strings! if you want to work on non-strings, just use the majority of ``replace``'s definition inline: `any-char 2choice repeat0 parse`

.generalized `replace` example

[source,factor]
----
TUPLE: myt fst snd ;
"eixayz"
[ "aeiou" member? ] satisfy repeat1 [ first ] action
any-char 2seq [ first2 [ 1string ] bi@ myt boa ] action
any-char 2choice repeat0 parse .
----

prints

----
V{ T{ myt { fst "e" } { snd "x" } }
   T{ myt { fst "a" } { snd "y" } }
   122 }
----

=== EBNF

basically, unless i'm given a correct, formal description of `peg.ebnf`'s ebnf's grammar, then it's unusable. use manual parsers instead.

peg's ebnf syntax produces a parser that you could've written by hand, but i'm unsure that ebnf can describe all that manual parser combiniation can. i'm not even sure when ebnf is really more convenient than manually writing a parser. for example, can ebnf elegantly describe tokens delimited by `/[[:space:]]+/` or a group of tokens delimited by commas with optional space?

* `EBNF:` in `peg.ebnf`

syntax is like regex:

* `|`
* `[abc]` & `[^abc]` (don't quote characters)
* use double-quotes for literals
* `?`, `*`, `+`
* `EBNF[[ y=[W-Z] x=[T-X] ]]` creates rules `y` & `x` and is a quotation that applies a parser that checks `y AND x` i.e. a single character in `[W-X]`.
* need to use `<tokenizer-name>=`; no unnamed tokenizers.

in trying to learn the ebnf grammar by reading source, i'm learning about using non-ebnf parser( combinators) e.g. `choice*`, and i'm finding those easy to use though more verbose and less readable than ebnf.

the errors can be astonishingly stupid: `"A" EBNF[[ aa = "A" aa|"B" ]]` errors with "Expected 'A' or 'B'. Got 'A'", though it parses `"B"` just fine. however, after some poking around, i see that `|` does not mean "or": `"AAAAB" EBNF[[ aa = "A" aa|"B" ]] .` prints `V{ "A" V{ "A" V{ "A" V{ "A" "B" } } } }`. with such complexity, i decide to no longer try to try to learn the ebnf grammar by looking through source code.

.lookahead

`"a ∈ mytbl -> t(b,c,d)" EBNF[[ y= .+ => " -> " .+ ]]` fails b/c `.+` matches whole string before required token `" -> "` is attempted to be parsed; b/c there's no more input, `" -> "` fails to match, causing the whole parser to fail. `ensure[-not]` can be used for lookahead. then again, we usually want something more specific than `.+`; for example, here "a ∈ mytbl" should be matched against some parser that chooses from multiple valid expressions; the expression should be terminated by its own grammar rather than `" -> "` terminating that expression; therefore the expression should match without worry about accidentally parsing `" -> "` before the appropriate occasion. that `.+` may match `" -> "` and more is not a defect of ebnf; it's no easier to manually write a parser that has not that problem.

this being said, it probably is sometimes reasonable to want to parse until a given string. TODO: how to do that?

.decoding ebnf grammar

terminal: blank or ∈ ["'|{}=()[].!&*+?:~<>]

== debugging

firstly, using `prettyprint` is not apt for debugging a running program, though it's fine in the listener to print values that you don't want kept on the stack. to inspect values during runtime, use `break` and the inspector (context-click an object then select "Inspector") or factor's logging framework. to produce values for logging, use `unparse` or `unparse-short` of the `prettyprint` vocab.

one trick is to push some values before a breakpoint, then drop them sometime later. that way they'll be on the stack in the walker. this is easier than making & setting dynamic variables, and besides, the "variables" dialog shows only symbols declared in `scratchpad`. `IN:` does not change this fact.

* see factor docs: "Watching variables in the listener". namely you'll want `show-vars`. this, however, prints the variables after every listener statement's execution finishes, rather than upon the vars' change(s).
* `^w` in listener to walk through a quotation
* see the doc "Watching variables in the listener"
* if using a higher order fn, mimic it by running its argument at the top level e.g. if `[ f ] each` isn't working, test `f` with the arguments that you expect
  ** if `each`, `map`, `reduce`, or any other traversal over a sequence, is failing, then the easiest way & most direct way to debug it is to stick a `1 head` after the input sequence.
* check the stack signature (ctrl+i)
* when testing code that mutates structures, use `clone`, so that each test starts from the correct initial structure! e.g. do `H ( -- h ) H{ { 0 HS{ } } { 2 HS{ } } } ;` to define initial hashmap, then in all of your tests, do `H clone words ...`. `clone` makes a shallow copy.
  ** depending on your code, you may want `clone` for production execution anyway!

consider the following code which *incorrectly* tries to implement j's key (`/.`) (it accumulates into a hash set instead of a vector):

[source,factor]
----
: groupby ( vals keys -- groups ) ! like /. in j or `group by` in sql
  H{ } over [ swap [ HS{ } ] 2dip [ set-at ] keep ] each spin ! h ks vs
  [ swapd [ over adjoin ] curry [ change-at ] pick [ curry call ] dip ] 2each ; inline

{ 0 2 4 0 7 1 100 56 35 } { 0 2 4 0 2 4 0 2 4 } groupby
----

i kept getting the output:

----
H{
    { 0 HS{ 0 1 2 35 4 100 7 56 } }
    { 2 HS{ 0 1 2 35 4 100 7 56 } }
    { 4 HS{ 0 1 2 35 4 100 7 56 } }
}
----

i took a couple of hours to realize that it was because the `HS{ }` was one object, used as all values for the hash map! using `HS{ } clone` fixed the problem, giving the correct output:

----
H{ { 0 HS{ 0 100 } }
   { 2 HS{ 56 2 7 } }
   { 4 HS{ 1 35 4 } } }
----

btw, yes, i'd later discover that this exact functionality is implemented by `collect-by` of the `assocs` vocab. and look at its definition—how much better it was written! clearly i still have much to learn about how to write good factor code.

=== logging

vocabs `logging`, `logging.server`; and less importantly: `logging.analysis`, `logging.insomniac`, `logging.parser`.

factor comes with a framework for logging to log files.

.example
[source,factor]
----
"myApp" ! here we name the log
[ 10 0 [ / ] [ \ / log-error ] recover ] ! log-error logs a traceback. notice that log error takes a word as its top input.
with-logging ! with-logging is needed to actually write to a log
"myApp" log-path . ! where the log was written
----

to disable logging temporarily,...huh, i thought there was some simple built-in functionality for that, such as setting off a dynamic variable. well, you can just replace `with-logging` by `log? get-global [ with-logging ] [ nip call ] if` after you've created the `log?` symbol.

=== reading tracebacks

my comments are prefixed with a `!`.

.example

the error was "sequence index out of bounds" where the index was `1` and the seq was `f`.

----
(U) [ set-namestack init-catchstack self quot>> call => stop ]
! i ran the code in the listener
(O) listener-thread
(O) listener
(O) listener-loop
(O) listener-step
(U) [
        [ ~quotation~ dip swap ~quotation~ dip ] dip swap
        [ call get-datastack ] dip => swap [ set-datastack ] dip
    ]
(U) [ call => get-datastack ]
! the error was thrown inside execute-parser which was called by `perform-parse` which was called by `parse`, " `safe-search` " `parse-signals` " `backtest`
(O) backtest
(O) parse-signals
(O) safe-search
(O) parse
(O) perform-parse
(O) execute-parser
(O) ( gensym )
(O) execute-parser
(O) ( gensym )
(O) execute-parser
(O) ( gensym )
(O) execute-parser
(O) check-action
! the error was thrown inside this quotation (which was defined using lexical variables)
! this quotation was called from `check-action` which is used in the word `action` of the peg vocab
(U) [
        2 load-locals first2 2 load-locals 0 -1 get-local 1 - 0 max
        -3 get-local <slice> <reversed> 0 get-local [
            load-local second l>> 0 get-local l>> -
            abs w-tolerance get-global < 1 drop-locals
        ] curry find => nip -1 get-local -2 get-local [
            2 load-locals first -1 get-local 0 get-local <slice>
            dup ~quotation~ keep ~quotation~ keep drop
            ~quotation~ dip drop ~quotation~ ~quotation~ if
            2 drop-locals
        ] curry curry and* 4 drop-locals
    ]
! the error was thrown inside `find`
(U) [ [ 0 ] 2dip do-find-from => index/element ]
    find
! more specifically (i think), it was thrown in the following stack shuffling soup:
(U) [ 2keep => drop ]
    keepd
(U) [ swap [ dip ] dip => ]
    2dip
(U) [ swap [ call ] dip => ]
    dip
(U) [
        [ nip call ] 3keep => roll
        [ 2drop ] [ ~quotation~ 2dip find-integer-from ] if
    ]
(U) [ swap [ 2dip ] dip => ]
    3dip
(U) [ swap [ dip ] dip => ]
    2dip
(U) [ swap [ call ] dip => ]
    dip
! ah! wait, i recognize this part! i wrote this code!
! it's the quotation that i passed to `find`.
! and right after this quotation is the description of the
! bounds error. using a little inference, i see that `second`,
! which is the same as `1 nth`, was apparently called on `f`
! where a non-empty sequence was expected.
(U) [
        load-local second => l>> 0 get-local l>> -
        abs w-tolerance get-global < 1 drop-locals
    ]
(O) M\ sequence nth
(O) bounds-error
(O) M\ object throw
! this last quotation is printed for all tracebacks of `break`. ignore it.
(U) [
        OBJ-CURRENT-THREAD special-object error-thread set-global
        current-continuation => error-continuation set-global
        [ original-error set-global ] [ rethrow ] bi
    ]
----

.codes

* `M\ x y` means that `y` is generic and indicates ``x``'s implementation of it
* `(O)` seemingly indicates an ordinary word
* `(U)` seemingly indicates a quotation

== relational and array factor

basically:

. reason in terms of arrays if data are related by indices
. reason in terms of general relations if data are related not by indices

data types of choice:

* interval sets. used for BETWEEN and specific varieties `<` & `=`.
* string buffers (`SBUF""`). they're mutable, growable strings.
* vectors. they're mutable, growable arrays.
* (avl) (generally binary search) trees. they're mutable, ordered, assocs, supporting O(1) min & max, fast range selection, and O(nlog(n)) element selection.
  ** linked assocs wrap an assoc and support retrieval of items in insertion order via `>alist`.
* hash maps. use when your keys' order isn't concerned. they've O(1) amoratized lookup—quite a bit better than trees.
* shaped arrays? i suppose they should be used for (multi)linear algebra, but if such computations are many or large, then they should be done through a more efficient mechanism such as a gpu-compatible library anyway, likely accessed in factor via c ffi.
  ** the `tensors` vocab is like shaped arrays but their only multidimensional operations are multiplication & transposition; elemntwise elementary & modular arithmetic, and of course reshaping, are also supported. why to use them: well, they use simd.... the `tensors.demos.private` vocab defines `gradient-descent`, `compute-cost`, and `normalize`, so i suppose that tensors are especially suited or intended for neural network training; contrast with shaped arrays which are like j/apl arrays.
* the following are special purpose or _especially_ efficient—like "embedded systems" efficient. this being said, idk if factor itself (vm+core+basis) is efficient enough to be used in embedded systems anyway.
  ** arrays
  ** bit & byte arrays. used to pass binary data between factor & c.
  ** _specialized arrays_ & vectors. these can be passed to c fns that take `float*`, `int[<n>]`, &c
    *** the following c types are supported: `char` `float` `int` `long` `uchar` `uint` `uintptr_t` `void*`
    *** the memory of these (and perhaps other types) can be manually managed. see _manual memory management_ in the factor docs.
  ** strings? are they more efficient than string buffers? probably, since they're more limited in functionality.

TODO: see:
* sets.extras
* splitting.extras
* stream.extras
* string-server

``assocs.extras``'s `assoc-merge` is akin to an outer join. i think that joins generally rely on two things: 1. keeping indexes (sorted data); 2. longest common subsequence algorithms, such as the following:

[source,factor]
----
! map through keys, replacing each by itself or itself with its corresponding value.
! KV's key sequence must be a subsequence of K, and all the keys should be unique.
! can be generalized easily from lookup to join.
:: lookup-map ( {k} {{k,v}} kname vname -- {{k}/{k,v}} )
  {k} {{k,v}} swap 0 -rot
  [ [ [ ?nth ] 2keep rot ] dip swap ! i KV k ?kv
    [ [ first = ] 2keep rot
      [ nip kname vname 2array swap H{ } zip-as [ 1 + ] 2dip ]
      [ drop kname associate ]
      if ]
    [ kname associate ] if*
  ] { } map-as 2nip ;
----

otherwise, if we're using hash tables, then inner join can be done like the following wip does: mapping through the shorter of two assocs, looking-up in the other assoc:

[source,factor]
----
H{ { "cat" 6 } { "bat" 7 } }
H{ { "merry" 10 } { "bat" 12 } }
[ + ] ! operation to apply to keys found of both assocs
[ 2dup [ assoc-size ] bi@ < ] dip ! iterate over the shorter of the two assocs, setting the longer assoc
pick swap [ change-at ] 2curry
[ [ at ] curry ] dip
[ [ drop ] if* ] curry compose [ over ] prepose 
assoc-each
----

again, though, join is really a harmful idea. it's an arbitrary & over-constraining thought. just intersect predicates. remember what a join is: a relation of keys, and another of values. here i use _keys_ to refer to the attributes entailed in the join, and the _values_ to be all other attributes of the entailed relations. the key relation relates rows. the value is an (output) expression per row. in either a logical or reductive model, you should have an efficient join algorithm. keeping data _indexed_ is essential for that. an index is a sorted map i.e. a tree such as avl, b+,  b, t, from sorted value to rowid/pk. then you access the property vectors at that id. after indexes, _ranges_ are essential for efficient lookup/join. if you want to intersect relation on an attribute, then, well, ok there it's best to iterate through the shorter of the two since it's only those values, if it's an inner join. for an outer join you're considering everything, so you must iterate over both anyway. left or right joins explicitly mandate over which to iterate.

TODO: compare π-calculus, dataflow, relational, and frp models. compare erlang to what the `concurrency.messaging` vocab enables.

.important concepts
. a _relation_ [rel alg] is a set of relations [mathematics] of attributes with a single attribute designated as a _primary key_, thus transitively associating attributes by primary key equality (in sql parlance, "joining the attributes `using` the primary key"). note: to discuss a single _attribute_, is to discuss a set of values, indexed by primary key. i'll use _element_ to refer to an element of a set. for example, in the standand mathematical expression `f(x,y)`, `x` & `y` are attributes but refer to their entire domains. an example of transient relation of multiple values via primary key equality: `{first:{{ 0 "chuck" } { 1 "richard" } { 2 "joe" }} last:{{ 0 "moore" } { 1 "stallman" } { 2 "armstrong" }}}` relates first & last names; the primary key is the first field, an integer. note that arrays can be generalized to associative lists whose keys are indices (natural numbers), or strings, or any value. because relations are more general than arrays—1. natural number indexes to anything permitting order or equality, and 2. that removing elements from an attribute dictionary does not affect the attribute's relation to other attributes which it does for arrays—they're better, excepting where arrays' constraints can be exploited for speed e.g. in calculations done by a gpu. hash maps require only uniqueness. search trees require ordering. storing attributes separately reduces complexity. for example, we can filter one attribute without caring about other attributes;  after one attribute is filtered, it's (inner-) joined with another `using` the primary key. even if we delete an attribute, there's no problem: then it just won't be in the join; or if we're doing an outer join, then its value will be `null`. for example, if i delete first name at primary key `1` ("joe") then when i join first & last names using primary key, i'll still get `0 "chuck" "moore"; 1 "richard" "stallman"`. another variety of deletion is to merely set the attribute value to `null`. to delete a whole row, just delete the primary key value for a given row, since without it, the relation cannot exist. (a relational system would delete all values if the programmer instructed to delete the primary key for a given element.)
  .. inner join is intersection. outer join is "corresponding element or empty." they're very closely related conceptually and mathematically. indeed, interesction is just an optimized version of `[ at ] curry map sift` whereas left/right join is `[ at ] curry map`. idk how to do an efficient full join. fortunately full join is rarely needed; i've still not encountered a use for it.
  .. all relations have primary keys. if not defined, then they assume one that corresponds to the insertion order.
  .. this model sees that the smallest unit of information in the relational model is a 2-attribute relation from primary key to value. being that that's a unit, what makes attributes of one relation and not another is that attributes join on a common primary key. however! this is no different from joining on a primary key from another table: there's no difference between joining attributes of a common relation on primary key vs joining across relations on primary key vs joining an attribute's INDEX with a primary key! it's all just intersecting maps keys, regardless of whether the map is to or from any relation's primary key! thus this model of having only attributes eliminates having multiple tables! instead of relations [tables], we've only attributes! in other words, we only join attributes, not tables! granted, one can have an attribute of multiple values, and this can be stored as a separate attribute again joined on primary key, but if you're always going to join them together anyway, then you may as well store them in a common structure i.e. as a single value in a k/v avl tree store. this assumes that you'll naturally also always run predicates (and create indexes) on the whole collection of associated values.
  .. TODO: is there a reason to identify 1:1 maps (relations) differently from 1:n maps (joins)? i can express a 1:1 relation `{ { a x } { b x } }` as a 1:n relation: `{ a b } join { x }`. even 1:n generalizes to m:n, which is obviously the most general.
. because we're using trees, queries on ranges are efficient. they're quite common, too. we can create pseudo-attributes, called _indexes_ that are treemaps from values to primary keys. for example, i can create an index on the first letter of the first name, so that when i search for things where the first letter of the first name is between 'a' & 'f', that can be done in O(nlog(n)) time.
  .. because attributes are commonly joined on primary key, they're treemaps from primary key to value. if the attribute(s) are part of a query filter, then we must index them for O(nlog(n)) lookup.
  .. `a=v` is the same as a∈[v,v]
  .. `a<v` is the same as a∈(-inf,v)
  .. b/c we're using trees, we have O(1) min & max
  .. b/c values are not generally unique, indexes are maps from value to many primary keys.

so the basic procedure is to:

. define attribute treemaps. you may define them as part of a relation e.g. `H{ { "attr1" ~treemap~ } { "attr2" ~treemap~ } ... }`, or you may define them by `CONSTANT:` and name them e.g. `rel1.attr` and `rel2.attr` if the same attribute appears in multiple relations where its primary keys are not equal in both relations. rather than specify relations in code, it's freeer to specify them in documentation then have them implicitly belong to common relation(s) in code simply by their treemap keys (which we may call "primary keys") correspond.
. you may define a word to make defining indexes more convenient. the word would take the name of an attribute then create a new name: that attribute's name with an "IDX" suffix. of course if you're naming attributes in an assoc as strings then just `"IDX" suffix`; if you're using words, then do that followed by `create-word-in` then create an index of the attribute then associate it with the identifier by putting it with the identifier in an assoc, or use `define-constant`.
. to find attributes corresponding to a given value:
  .. if you don't have the primary key(s), then look them up in this attribute's v->k assoc.
  .. with the primary key, look up desired attributes at the primary key in their k->v assocs.
. to join on a predicate of a value `a` (which may be an attribute or a function thereof) in one table with the value `b` of another, where the predicate is always "greater or less than or equal to, or within a range":
  .. if a treemap from `a` to `b` or `b` to `a` exists:
    ... note that `A join B on a <R> b` is equivalent to `A join B on b <R-complement> a`, so if i want to join on e.g. `a<b` then i can rephrase taht as `b>=a` then efficiently compute the join by the method in codeblock _join<_ below.
      .... looking-up in trees is not the only variety of lookup that we may desire! lookup in a suffix array is also useful and similar to `a<b`. indeed, we may also define our own types and `<=>` for them.
  .. if any of the values is not indexed:

.join<
[source,factor]
----
! A join B on a<b
: join< ( A B -- joined ) ! precond: A is ordered ascending. each of A & B is `values` of their pk->val assocs
  dupd [ I. ] [ swap tail-slice ] bi-curry compose map zip ; inline
----

test:

----
{ 10 12 34 56 87 500 }
{ 2 3 6 10 12 18 24 36 42 83 91 102 }
join< [ >array ] map-values .
----

prints

----
{
    { 10 { 10 12 18 24 36 42 83 91 102 } }
    { 12 { 12 18 24 36 42 83 91 102 } }
    { 34 { 36 42 83 91 102 } }
    { 56 { 83 91 102 } }
    { 87 { 91 102 } }
    { 500 { } }
}
----

although factor comes with binary search, `insert-nth` uses `append` and so copies part of the input vector which is slow. avl trees are good (though apparently not as good as t-trees) for in-memory operations. besides, the `trees` functionality is excellent and perfectly suited to queries. it is of course advisable to use sqlite if you need persistence, multi-agency, acid, or if you must work with a large amount of data at once. see the `db.tuples` vocab.

deletion of an element: remove it from all attributes of a relation, and amend entailed indexes to not have the primary key in its cod. if removing the pk from the cod results in an empty cod, then delete that whole assoc entry.

whereas sql uses `expr as name`, prolog uses `name is expr(bind1,bind2,...)`. i want prolog's style b/c it's terser and more natural.

.contextual programming

each quoted program in factor has a context. the same is true for lexical closures in applicative languages. data should be coupled with subprograms in which they're used i.e. data should be in the smallest scope possible. this is especially true in stack langs b/c more data in scope means more data on the stack which means more to manage, because unlike in applicative languages, variables are related to each other by position; in a stack, they're sequential access, not direct access. usually sequential access is worse design than direct access; however, reductionist programs tend to be expressions of subprograms and variants of programs, which stacks express well (assuming that quoted programs can be put on the stack as data, and support currying, composition, and evaluation).

i should really exploit the quotation-traversal pattern e.g. `map` is a traversal parameterized by a quotation. the quotation evaluates within a particular _context_. contexts are easy in factor because everything in factor is contextual; the context is the stack. however, factor also has dynamically scoped variables: `SYMBOL:` & `get`. in sql "map" is called "select" and within the selection clause's context, the "from" clause's table's set of attribute names are bound [available]. you refer to all rows by referring to attributes independent of "set vs element". contextual programming is decomplected and thus tacit; the expression is separated from the context. this is a beautiful way to code. what's more: a good programming model enables us to freely intersect and union contexts. when contexts are sets/predicates [prolog] of attributes then union & intersection are obviously defined. when the context is a stack, then union & intersection don't obviously/immediately apply. one variety of union for a stack context is `bi` (generally `cleave`) which unions two (or any number of) programs and evaluates them in order, leaving their outputs in that order on the stack. there is no escaping the essential ordered nature of the stack. it can be helpful or not depending on how we want to relate subprograms.

''''

* `x(I,X),y(I,Y),R=X+Y` in prolog is `select X+Y from x join y using (I)` in sql. the prolog one is terser, more uniform, and more obvious. rather than doing a cartesian product per se, i `reduce` the sequence of predicates into one predicate then evaluate it by looking through indexes! furthermore, prolog derives a set from a predicate, which makes virtual sets natural! outer joins mean, "for a given set, find corresponding elements of other sets." outer joins are naturally expressed by the predicate: `r1(I,A),r2(I,B);r1(I,A),r2(NULL)`. sql expresses "there's no corresponding element" (usually seen as `at` returning `f`) as "the corresponding element is null," which is fine; and if a corresponding element is null then implicitly its primary key is null, and all of the attributes at the null primary key are defaults, commonly all null.

.relational system design from first principles

all things can be virtualized by having, rather than data structure lookups necessarily, virtual things by quotations. quotations can wrap lookups in data structures if arbitrary data must be stored. virtual structures can be represented by tuples or assocs or w/e, and we may define functions on these e.g. OR, AND, IMPLIES. (implication is a good way to refine (shrink) the computation or search space! for example, i may specify the property "monotonic" of a thing to remove a `sort` which would otherwise be present, since it's generally needed.) where sql breaks-down is in answering the question, "why define operations on data structures, given that we're already specifying queries, and given that our data obey arbitrary relations? why not just specify the relations and derive symbols' values?" indeed, this shows that sql, despite being "declarative" is not _as_ declarative as prolog! programs are generally specified by _facts_, not data! to be constrained by some arbitrary set of primitives (namely sql's, vs the necessary set of primitives * & +) is downright, straight-up foolishness. sql's separation of data & expressions is unnecessary asymmetry, too.

what makes my methods more sql-like than prolog like is that i'm using lookup rather than unification. this being said, if i have double lookup and inverses, then i can navigate queries however i like, making it effectively prolog, right? prolog's model is inherently flat: each time that all of n symbols have values, they're all returned, and that's one point in an n-dimensional subspace.

in prolog the following 3 ops are done simultaneously: 1. identify data; 2. intersect; 3. identify corresponding element e.g. mapping a thing to a computation thereof; this being said, we'd not express this as a virtual sequence, but instead as a function in the selection expression e.g. just having attribute `x` then doing `select x,x+2 from ...` which in factor will be `[ dup 2 + ]` or something; i'ven't decided how to structure computations relative to selected data points. in rel(A,B,C),r2(B,D), f(B,D,A,R) we see that 1. all points in (A,B,C) share a primary key value—a constraint implied by them being of a common relation; 2. B's value in rel equals B's value in r2 i.e. we select all b∈B in rel then, for each unique b, lookup b and its corresponding D in r2; 3. D is implied by B, since D was otherwise free.

this is expressed as: `{ A B C } rel { B D } r2 AND [ B D A f ]`. NOTICE THAT LOOKING-UP IN A TABLE AND EVOKING A FN ARE THE SAME! i can say that r2 is a relation, or that r2(B,D) is a fn that looks-up B and returns D. i can just as well say that D looks-up B, since in both cases i'm identifying a subset of each then intersecting them by primary key: if i select both unconstrained, then that's the intersection of all primary keys with itself which returns itself, and if i predicate only one then i'm intersecting all pks with that subset's, which returns the subset, and finally if i select subsets of both then obviously their intersection will not contain more elements than either. *so all i'm doing is* selecting subsets of each attribute then intersecting them on primary key, then applying an operation to them altogether and/or in part (since i'll want to incrementally build-up programs e.g. by selecting D then currying it with [ + ]), then group and sort (efficient b/c each group is still associated with the primary keys, and we can use the primary keys to index into the INDEX to sort by). then rel(A,B,C). i can start from the left, identify an A, then corresponding B, then corresponding C, and this is fine; this is the cartesian product. i can skip any A, B, or C that fail a predicate. also, when i identify A, B, and C, then i identify the corresponding D, too! the thing is that i want efficiency, so i'll be using sets and intersection then, as needed, lookup. i'll be using treesets rather than just rules. i'll still use rules that have bodies, too!

* see "sorting by multiple keys" in factor docs.
* remember that i can use `rename-at` to do like sql's `as`.

to join on anything other than primary key, just evaluate expressions then determine which primary keys they map to, then join.

`where` predicates are generally of the form "expr in range." where `expr` is an attribute or a function of it or multiple; either way, `expr` can be INDEXed *for a given single relation.* of course the primary keys are not related among separate relations! *this is the defining property of a "relation": that each of the attributes can be joined on primary key.* anyway, an INDEX is just a map from a value to a key. or, heck—how about a range of values to a key? sure! indeed, the sqlite docs describe exactly how INDEXes work: an INDEX on a value is a sorted map from value to primary key; after the primary key is looked-up, it's used to index into the table, where it's a primary key. it acknowledges that it must do two binary searches per query, but that that's still much faster than a single full table scan.

given two INDEXes, we can look-up values then get the corresponding primary keys, then intersect those sets, then look those up in the original table! we see intersect here as being the implementation of AND: `indexedattr1=val1 AND indexedattr2=val2`. the way that multi-column INDEXes are done by sqlite: the leftmost column is the INDEX's primary key and the rest of the rows are used to "break ties" i.e. to resolve collisions [hash tables]. if we were using sequences then a multi-attribute INDEX would be sorted by those keys in order. since we're using hash tables, though...<TODO>. a multi-attribue INDEX just does one binary search in order to get the table's primary key! also, a multi-attribute INDEX(a,b,c,...) can be used to find any prefixes, too—here the sets {a,b} and {a}. a _covering index_ is one that features all of a relation's attributes; the relation itself is never consulted. thus the covering index achieves by sorting attributes and thus being able to binary search, at least by its primary key.

NOTE: for a multidimensional INDEX, sqlite stores each attribute in sorted order! thus if we've INDEX(a,b) and we `select x from t where a=v order by b` then no sorting is done; we simply retrieve the matching `a`. note that this works only if we use equality, not an interval, since ``b``'s are sorted only per `a`. consider sub-map of INDEX(person,exes) `HS{ "kat" { "billy" "tom" "zack" } "kathryn" { "larry" "moe" } }`; i must sort if i `select exes from t where person between "kat" and "kathryn"`.

NOTE: each group of intersected predicates, for which an INDEX exists, corresponds to a contiguous chunk of that index. the number of disjoint predicates is the number of lookups into the corresponding INDEXes.

NOTE: always take each INDEX as far as it can go; for every `where` clause that you ultimately use in your program, at least one of its entailed attributes should be INDEXed! sqlite's "Query Planning" documentation's example has an index on (fruit,state), but the query has `order by fruit,price`; thus for each group of fruit, a sort is done. this is better than one sort of it all. for an index on (a,b,c), `where a=val and b in (1,2,3)`, index by a then linear filter on `b`.

NOTE: i won't rewrite x=a OR x=b ... into x in (a,b). i expect the programmer to always try using x∈range or x∈set patterns whenever possible. i can use factor's type system (`range?`) to check if i can use ranges rather than merely sets. note that ranges are sets, too, so i cannot use `set?`; i must use `range?`. not yet sure if `interval-sets` can be useful.

btw, it's possible to optimize on matching against sequences by storing them in a trie.

a good engine should not support both AND and intersect as separate operations! this being said, we'd still want a result expression list b/c we don't want all symbols to be in the output expression. in prolog this is done by either ignoring symbols or by eliding them from the head of a clause, or putting them in the head arguments e.g. `reverse(A,B)` has B as the output and any effectively local symbol (e.g. an `X is f(A,B)`) inside the body is not in the head, and so not "returned." the sql query `select a,b,c from t where a>b and not c` would be `t(A,B,C),A>B,not(C)` in prolog. a _view_ (a named query) of it `create view v(a,b,c) as select [...]` would be `v(A,B,C):-[...]` in prolog. this shows queries being like goals which generalize lambdas; therefore sql queries are lambdas. they don't generalize lambdas because they're reductions: the sql symbols are mere symbols of data literals rather than logical, computable, semantic objects. to name the lambda and thus make it a "function" is to make a query a view or rule.

each OR'd predicate's attribute set is ordered then looked-up in an INDEX (ordered to canonicalize it in order to find a corresponding index e.g. `b=4 and a=5` would look-up in the `(a,b)` INDEX) then folded into a union of primary keys, then those primary keys are looked-up in the relation: basically `[ _find-INDEX ] [ union! ] map-reduce _lookup-in-relation` which is applied to the sequence of things OR'd together e.g. the expression `b=4 or a=5 and c<4 or c<2 and name.first CHAR: t =` becomes an input to the reduction: `{ { b a } { c } { name.first } }`. we'd define the `name.first` INDEX as an assoc whose keys are `[name first]`, which is really to add it as an attribute (still by the name `name.first`) to whichever relation contains `name`. obviously not everything will be INDEXed, but this is how we INDEX general expressions. *if any unINDEXed attribute is used in a predicate that is OR'd with others, then we must do a full table scan, and thus we'll do it only once, ignoring any indexing!* we use INDEXes only if _all_ OR'd parts are INDEXed! again, in summary: `"OR" split harvest [ [ CHAR: space = ] trim ] map dup [ hasINDEX ] all? [ <somehow do like get each then union all but more efficient> ] [ fullTableScan ] if lookupInRelation`. earlier i said "basically"; that basic version does not handle: 1. what if no INDEX exists for the predicate (yes, manual table lookup, but...well, ok, actually that's pretty simple: we just do _corresponding-INDEX relation or` except delayed until the end, i think, to reduce redundant computation. oh, also that doesn't work b/c INDEXes go from value to primary keys whereas applying a predicate to a relation is a linear search); 2. converting all predicates to ranges e.g. `b<a` becomes `b a [-inf,b)`; 3. actually reifying predicate's attributes to values in order to look-up in the INDEX. also, if `a` is INDEXed but `f(a)` isn't, and `f(a)` is in a predicate, then we straight-up don't have an INDEX for it, and must do a full table scan, UNLESS we know that `f(a)` is monotonic!

all INDEXed filter clauses are ultimately of intervals [a,b]; to lookup by interval, just use `subtree>alist[]` of the `trees` vocab!

i can use INDEXes to sort, too, since the avl tree is already sorted, i can traverse it in order, where the traversal is merely getting `at` a primary key from the relation. this is O(nlog(n)), just like sorting is, except that it does not require extra storage like sorting does. it also allows us to use the same logic for filtered selection and indexing. sorting by a covering index is O(n). most importantly, we should recognize that sorting & filtering are basically the same mechanism!

NOTE: `filter` is predication. whereas sql filters a definite set into a subset, prolog derives, from a predicate, a corresponding set. we see this in that, for sets, `intersect` is defined in terms of `filter`, and that for ``bit-set``s, it's defined in terms of `bitand`, and that in prolog, it's the `,` operator. TODO: does using prolog's scheme save me from organizing my data and needing to reason about queries?

filter each attribute by predicates of only that variable; then, for predicates of two variables, join them then apply predicate. then for those of three vars ...

left or right outer join: `: right ( ? ) flip left ; inline`. left: ∀a∈A ∃ b⊆B. rather than flattening into a table, i can associate with each `a` b as a data structure, and i can partition those `b` into those that match or not a join predicate. there's never a need to flatten, though i may do that as a final step. simply: when i refer to `b` as an attribute name, it really refers to `a.b` ∀a. these "compressed relations" are more appropriate than their expanded counterparts. what technically will happen is that b will be suffixed into a, and a separate table...will be made to store the primary keys of a & b? and then as i join more relations in, that table will be updated to have `(a,b,c,...)` where each of `b,c,...` is a collection but `a`, being the first of a left join, is an atom? a full join would see `a` also being a collection. maybe i should just use `<product-sequence>`. considerations: efficiency, elegance, naturality.

* assocs are binary; they support natural joins. note that even two single-attribute tables support inner join with no predicate, or with a predicate other than `=`. `=` is mootly supported in this case. otherwise even full outer joins can be used. joins express cartesian product, which we know commonly as A×B, but literally it's `{f(a,b)|a∈A,b∈B,p(a,b)}`, generalized to `{f(a,...)|a...∈A...,p(a,...)}`.
* generall relations are graphs. of one attribute, we may relate elements to others; and we may relate any elements of any attribute to any elements of any other attributes. these relations are *predicates*, since predicates generalize equality. assocs are sets of keys & values related 1:1.
* in sql a single-attribute table is hardly useful. it cannot be joined! such joins are no more useful than filtering then unioning.

parts of a sql query in order of evaluation:

. join (relate attributes). remember that there are many varieties: [natural] [outer:<left|right|full>]|inner. natural affects output column space and mandates no `on` nor `using`. inner is default and includes only rows that meet the predicate. outer has nulls where the predicate fails.
  .. outer joins are equivalent to inner joins if no predicate is given.
  .. `on` is most general. `using (a,b,...)` is `on t1.a=t2.a and t1.b=t2.b AND ...` but omits (t2.a,t2.b,...) from the attribute union (append). `natural` is `using` whose argument attribute set is the intersection of tables' attribute sets.
  .. left & right (outer) join feature NULL for t1's or t2's attributes respectively where the join predicate fails. the count of left or right join is the count of the left or right table respectively. the full join is the left join union right join.
  .. natural/using join determines the returned attribute set. *for a common predicate: inner join ⊆ left or right join ⊆ outer join ⊆ unpredicated join (cartesian product)* in fact, this is clearly seen by: *outer join = left join U right join; and inner join = left join ∩ right join; and outer join = cartesian product ∩ predicate.* cartesian product is the natural relation of symbols: `[X,Y]` implicitly has a `∀X,Y` qualification. predicated join is `[X,Y],p(X,Y)`. outer joins, however, are `[X,Y],p(X,Y);p(X,NULL);p(NULL,Y)`. `;` can be read as "union" or "or", or generally, "coproduct," denoted by `+`, which is my preferred notation. note that `[X,Y]` is sensible only if intersected with `p(X,Y)`. in order to produce a full set of `[X,Y]` prolog would need to run through all X and Y, which is exactly what cartesian product is!
. select indices (elements)
. `where` (filter rows/records/points)
. `group by`. the expressions used in a group by clause are the same as those available to the result expression list.
  .. selection expression is verb argument to `/.`
  .. `having` (filter result of `u/.`) `having` is a fn, of a group's transient table, that returns a boolean, such as `count(*)>n` or "x is a member"
    ... if a non-aggregate predicate is given to `having`, which is always silly afaict, then it's evaluated for an arbitrary row of the group. yet, if multiple non-aggregate expressions are supplied, then they're all applied to the SAME arbitrary row! this is sql always maintaining relations!
. `order by`. if `group by` was used, then it's sensible only to order by a selected expression or series thereof; else one may sort by <selected expressions> union <table's attributes>.
  .. each `order by` clause may be followed by <asc|desc> [nulls <first|last>]
. `limit` & `offset` (subsequence selection aka _slicing_)
. `distinct` (nub, called ~. in j and `members` in factor) is applied.

NOTE: there's no mechanism to filter aggregates' results,

the philosophy of join: relate a row to other rows. it's nothing more than a 1:n relation. it relates each item to a set/seq. _join_ is _correspondence_. it's equivalent to "query a table with this row." we can do joins nicely in factor by using this "sequential subquery" method: we query, get n results, then pass them to an n-ary query (quotation), and so on. the `A(a,b,c) join X(x)` pattern is quite silly. we do it in sql b/c there it's sql's only way to bring a datum into scope. in any other language we'd just have `x` in our scope. the scope may be static or dynamic. in sql's array model, all expressions ultimately are of constants or a `bind-parameter` (attribute name.)

btw, see "Intervals" in the factor docs.

`-1/0.` is negative infinity. `1/0.` is positive infinity. these are builtin but searching the documentation for them returns no results. sadly `1/0.` is not an integer, so `1/0. <iota>` does not work. i suppose that that's just as well; i'd rather use predicates (generators defined of predicates) instead of virtual sequences anyway. generators are available in the `generators` vocabulary. remember that generators are also called "coroutines." also exception handling is done by continuations. see `recover`. generators are more useful than virtual sequences only if we need infinite sets or we want to terminate computation early! it's true that predicates produce values, but we do not need generators for that! we can use `produce` instead!

prolog analogue:

. a query is intersecting & unioning relations (namely implications) of predicates. predicates are, at query time, called "goals." predicates are named tuples. each tuple slot contains a constant or *symbol* (*not* _variable_). b/c we use symbols instead of variables, there is no scoping. there is no dereferencing. symbols must be sufficiently instantiated ("constrained"?). example goals: `people(NAME,AGE,_,_,ADDR)`, `X#>4` which is syntactic sugar for `#>(X,4).
. predicates

`join` is obviously the part that needs the most optimization! this is natural in that it's the 1st part of the query, which means that it's where the most data are. thereafter each step of the query does not increase the amount of concerned data. an obvious optimization is to not produce the full cartesian product literally; collect only rows matching the join predicate. this being said, an inner join predicate can be intersected with the `where` predicate; they may as well be together virtual. (b/c the join clause may use `using` or `natural` to change the column space, which `where` cannot do. therefore `join` has its own clause. also outer joins have behavior that `where` cannot describe. only unnatural inner join's predicate is redundant with `where`. the implementation should merge their predicates together.) ultimately each row is computed independently so we may as well `_estimate <vector>  q reduce` with a quotation that does the result expression list & filter simultaneously. if there's a group by present then we associate each produced output with a group id. `from` has one table argument. `join` creates a virtual sequence.

another JOIN optimization: all PARTS of an inner join / where predicate (parts being a tree of binary compositions of OR and AND) that each concern only one attribute, can be filtered in parallel then their results may be joined together.

cartesian product is itself commutative, but it's not practically commutative depending on its associated predicate. consider `A join B on b>a`. the cardinality of the cardprod is `A(*&#)B`. if B is a treeset or some kind of array that maintains sort order then we can identify, for a given `a`, the related `b` easily: it's TODO: `<slice>`. again we see the pattern of the "main" variable being atop the stack. we want (all) `b` for a _given_ `a`; b/c `a` is given, it tops the stack. well, i say that, but we could do `a B with map`. anyway, this is getting into _query planning_, which is always just an optimization. the fact is that query planning is useful and requires queries to be considered as semantic objects. semantic objects generally enable optimization & _reasoning_ (e.g. rewriting) rather than mere _evaluation_. as prolog demonstrates, it's best to give predicates (intersections) to shrink (b/c ∩ never returns a larger set than either of its arguments) the consideration space. the more details in our query / fact system, the more efficient our traversals. what about indexes?—an _efficient lookup_ device, which is what we precisely want.

NOTE: fast lookups mandate that the key is unique! looking up slices/ranges of values mandates use of ordered keys (e.g. treeset not hashset.)

using nested select statements may or may not directly represent a programmer's query. ah! i see: with more than 2 joins, b/c join filters, the query planner must order joins. the question is how optimization is different if we store data only in attribute vectors, and tables are virtual collections thereof. for example, multi-attribute indexes don't make sense in this model, so comparing a 4-element `index` against a `where` clause's longest prefix of expressions that match the index is insensible. the question is: does the freedom given by decoupling data ultimately make things more or less efficient? that things are generally decoupled means that we must specify couplings, but these are independent and implicit; is that efficiency greater than the one where the data are coupled which makes working more freely with them harder—and is it so much harder that we can't make it as efficient as the freeer solution?

using factor enables us easy optimization by tacitity itself. for example, `e1 between e2 and e3` is identical to `e1 >= e2 and e1 <= e3` in factor b/c we'd define `between` as syntactic sugar for `and` over `>=` & `<=`: `: between ( x a b -- ? ) [ >= ] [ <= ] bi-curry* bi bi and`.

that's the 1st loop. we then apply any `having` predicate to each group, then apply aggregate fns, _then_ order? definitely limit & offset is last.

i should be able to use dynamic variables and macros to easily mimic sql in factor. dynamic scope like `make` should do it, either. because namespaces are just assocs, any value can be the key. usually symbols are used. this being said, i'ven't been able to identify how to _use_ any non-symbol keys in namespaces.

what should the select statement return? i suppose something called a "table"; the j-style (whether using locales or inverted tables) is a hash map with attribute name keys and vector values. in factor these would be so but values would be pointers. consider `with-variables` which shadows symbols' values within a quotation. one must still use `get` to get the symbols' values. `with-scope` is similar: it doesn't initialize symbols' values, but it makes setting them last only within the quotation. `change` is very useful. i'm doubtful that namespaces will be more helpful than just using a hashtable, especially since i can't seem to use non-symbol keys; all strings are available implicitly, vs symbols which must be declared.

TODO: compare `namespaces` and `vocabularies`.

NOTE: `CONSTANT: C V{ } 14 C push C .` shows that C has been updated. it's a constant pointer, but not constant value! this is a convenient way to make global attribute vectors!

NOTE: `symbol` is a subclass of `word`.

still i wonder about b+-trees being used by sqlite.

a thought: maps map atoms to atoms. aggregates map sequences to atoms. (like sort, filter) map sequences to sequences. of course, this is probably of little consideration if we use only singletons, never atoms.

.join example
[source,factor]
----
SYMBOLS: names ages sexes ;
V{ "tom" "bill" "harry" } names set
V{ m f m } sexes set
V{ 10 12 30 } ages set
{ "tom" "bill" "harry" }
H{ { "name" names } { "age" ages } }  ! relation 1
H{ { "name" names } { "sex" sexes } } ! relation 2
2dup [ keys ] bi@ diffs
! TODO: finish
----

relations are sets of pointwise-related sequences or uid-related hashtables. should i allow relations to each have names like above? or should i see relations as sets of symbols?

== j in factor

TODO: consider array, factor, and haskell / type class varieties of loops & control flow, namely using Maybe as 0 for + (<|>, <> (Nothing adds mempty to a monoid)) & × (<*>) to.

* shaped arrays
* virtual sequences:
  ** `<shifted>` (`sequences.shifted` vocab)
  ** `<clumps>`
  ** `<groups>`
  ** `<circular-clumps>`
    *** also the `circular` & `sequences.rotated` vocabs exist, though idk what relative benefit they add if any
  ** virtual cartesian products by the `sequences.product` vocab
  ** `sequences.padded` vocab, for when you need to pad a sequence with some fixed length number of a given element
  ** `sequences.cords` vocab—virtual append/concat
  ** `sequences.inserters` vocab—useful for building upon seqs rather than from scratch e.g. `{ 100 200 300 } [ 50 / ] V{ 16 32 } <appender> map-as` => `V{ 16 32 2 4 6 }`
  ** `sequences.repeating` vocab
  ** `sequences.snipped` vocab. whereas ``sequences``'s `snip-slice` splits a string at one index, `<snipped>` is the inverse of `<slice>`: it removes the slice! furthermore, the `snipped` tuple can be constructed instead of a starting index and length, and underlying sequence, of course. `<removed>` removes a single item from the underlying sequence.
  ** `sequences.merged` & `sequences.interleaved` vocabs—dunno how they compare
  ** `sequences.n-based` vocab: assoc from indices [a..] to input seq elts
  ** `<zipped>` (`sequences.zipped` vocab)
  ** `reversed` class
  ** slices. see "subsequences and slices" in the factor docs.
    *** `head-slice`
  ** `<iota>`
  ** define your own instances of the virtual sequence protocol, namely implementing `virtual@`. rotations would be defined easily as virtual sequences.
  ** numeric ranges:
    *** `[a..b]`, `[0..b)`, &al
    *** `<range>`
* `map` everywhere implicitly like j
  ** make a version of rank/join (generally: relate)
  ** singleton arrays, not atoms; this enables them to support map & rank
* [each|map|reduce]-index are useful words!
* vector operations
* `strings.tables` vocab for printing arrays e.g. `sa{ { 0 2 3 4 } { -6 6547 1 0 } } [ number>string ] shaped-map shaped-array>array format-box.`.
  ** `format-box` formats like j boxed arrays
  ** `format-table` formats like j unboxed arrays

=== cartesian product in factor and j

arrays make the following regular, unpredicated maps easy: 1:1 (pointwise relation), 1:n, m:n (cartesian product). 1:n is m:n where `m=1`:

[source,factor]
----
{ { 0 } { 1 2 3 } } [ ] product-map
  0   { 1 2 3 } [ { } 2sequence ] with map
= ! t
----

`cartesian-product`, like j, returns a cube; each row corresponds to:

----
   0 5 (,"_ _1"_1 _) 1 2 3
0 1
0 2
0 3

5 1
5 2
5 3

! version more amenable to custom prettyprint: cartesian-product [ [ unparse ] map "\n" join ] map "\n\n" join "%s\n" printf
! of course, a more proper solution is to implement the `prettyprinting` protocol
{ 0 5 } { 1 2 3 } cartesian-product [ [ . ] each "\n" printf ] each
{ 0 1 }
{ 0 2 }
{ 0 3 }

{ 5 1 }
{ 5 2 }
{ 5 3 }

{ 0 5 } { 1 2 3 } cartesian-product [ . ] each
{ { 0 1 } { 0 2 } { 0 3 } }
{ { 5 1 } { 5 2 } { 5 3 } }
----

=== factor/j bilateral translation table

this section firstmost describes array operations in factor, and secondarily compares factor to j's array model and array models generally, discussing the shortcomings of the array model and how to nicely code solutions in factor that array models do not elegantly permit. i'm using j as the array language of comparison because it's the only array language that i've used. in retrospect k would have been a better choice at least for its linked list structure instead of boxed arrays. likely it's comparably fast, has a smaller executable, and less-complex source code. any other array language, such as apl, k, elymas, uiua, is just as fine a substitute for j.

.symmetric traversals and array shape (metadata)

an _array_ is a map from indices to individual elements. each operation in this section concerns either the array's metadata (namely shape) or element values i.e. it concerns the relation between index and value, or relation among indices, or relation between indices and natural numbers.

arrays symmetry—what enables them to support array operations—is that considering an array is to consider its elements individually—without relation to each other. arguably reshaping an array changes the elements' relation, but such relation is _implied_ by the shape, but not mandated. for example, a (2 3) array may be said to be rows, but also columns, or even diagonals. multiple considerations are possible, but none is enforced.

clumps, prefixes, &c can be interpreted as reshaping or traversals. this demonstrates that shape & traversal are the same, since shape only changes how arrays are traversed, since the collection of elements remains the same.

whereas j uses indices, factor uses slices. they're effectively equivalent. `subseq*` (`sequences.extras`) enables slicing by negative indices (but uses `subseq` instead of `<slice>` to make the output sequence, so it's not virtual).

[options="header"]
|======================================================================================
| factor                                                   | j
| `shaped-shaped-binary-op`                                | dyadic verbs
| `shape`                                                  | `$ y`
| `length`                                                 | `#y`
| `reshape`                                                | `x $ y`
| the `circular` or `sequences.rotated` vocabularies^[3]^  | `x\|.y`
| the `sequences.shifted` vocabulary                       | `x\|.!.f y`
| `<clumps>`^[1]^                                          | `x u\ y`, x>0
| `<groups>`^[1]^                                          | `x u\ y`, x<0
| `<prefixes>`^[2]^                                        | `u\y`
| `<suffixes>`^[2]^                                        | `u\.y`
| `reverse`                                                | `\|.y`
| `flip`                                                   | `\|:y`
| `sort`, `inv-sort`, `sort-strings`                       | `/:~`, `\:~`
| `sort-by`                                                | `x/:y`
| `set-nth`, `map`                                         | `x m} y`
| `q filter`                                               | `(#~q)y`
| `cartesian-map`, `2nested-map` (`sequences.extras`)      | `x u/y`
| `v?`                                                     | `}y`
|======================================================================================

.subsequences/elements

most general to least general: `split-indices`, `subseq-index`, `find`, `member?`.

[options="header"]
|=========================================================================================
| factor                                                          | j
| `split-indices`, `snip-slice`, `cut-when` (`sequences.extras`)  | `x u;.±<1|2> y`
| `subseq-index`, suffix arrays, `start-all` (`sequences.extras`) | `E.`
| `q find`                                                        | <idk j anymore so w/e>
| `member?`                                                       | `e.`
| `nths`, `nth`                                                   | `{`
| `[ index ] keep length or`                                      | `x i. y`
| `[ reverse ] dip [ index ] keep length or`                      | `x i: y`
| #x≥2: `natural-search`; else see _§using the stack well_        | `x I. y`
| `<groups>`, `<clumps>`, `delete-slice`                          | `x u\.y`
| `all-subseqs`                                                   | ???
|=========================================================================================

.asymmetric traversals

these relate elements.

[options="header"]
|======================================================================================
| factor                                                   | j
| `reduce`                                                 | `u/y`
| `generators` vocab, or solution below                    | `F:.` with `Z:`
| `collect-by` (of the `assocs` vocab)                     | `x u/. y`
|======================================================================================

.particular traversals

these traversals do not occur often, and illustrate functions' "black box" / "lack of fusion" deficiency. these traversals are simple conceptually, and are _almost_ implementated by common primitives such as `map`, `find`, `filter`, but must nonetheless be coded manually. an example of such a traversal is `map-filter`, which is obviously a fusion of `map`-then-`filter`, but must be coded separately simply to have one pass instead of two.

many of these traversals are in factor's `sequences.extras` vocab:

* `change-nths` reveals a bit of how to use `each` as a primitive for custom traversals: `[ change-nth ] 2curry each`.
* `collapse` replaces each <substr all of whose elements match a predicate> by its head
  ** `compact` is the same but also removes the substrs leading or trailing the input seq
* `deduplicate` replaces all /a+/ subseqs by /a/, like unix coreutil `uniq(1)`
* `drop-while`
* `exchange-subseq`
* `extract!`: the inverse of `reject!`: retain subseq of elts matching a predicate
* `find-all`: `filter` but retains the associated indices
* `find-pred`: (a -> b) -> (b -> Bool) -> Maybe (b,a,Index)
* `interleaved`
* `longest-subseq`
* `loop>sequence` (like k's collecting "while")
* `map-concat`
* `map-from` &al words starting with `map-`
* `mismatch` (part of `sequences`, not `sequences.extras`)
* `zip-longest`: zip, padding the shorter seq
* i don't know of a word that of type `[a] -> (a -> b) -> Map a b`, but that's pretty easily defined: `: f ( seq q: ( x -- v k ) -- ht ) over length <hashtable> [ [ set-at ] curry compose each ] keep ; inline`

.custom traversals

if the particulars above aren't sufficient, then here are some primitives and relation techniques to help you code any traversal:

* `kernel` vocab:
  ** `if`
  ** `loop`
* `math` vocab:
  ** `each-integer-from`
  ** `find-integer-from`
* `sequences.private` vocab:
  ** `sequence-operator`

this is very apl/c-like, using integers and O(1) access at the nth elements and iterating using `i.`/`ɩ` (like c `for(i=FROM;i<END;i++)`). because all these are effectful, their sequences' elements' order matters. each of `each` & `find` itself considers only one element at a time, unrelated to other elements, though of course because of their row-polymorphic effects, each element can be related to any part of the program state by implicitly being related by being on the stack. `all?` relates elements within the input sequence. `all?` & `find` short-circuit. `any?` is just a limited version of `find`. `all?` is redundant; `[ q ] all?` is equivalent to `[ q not ] any? not`, which means that it's equivalent to `[ q not ] find drop not`.

`[ q f ] find` is equivalent to `[ q ] each`, making `each` obselete, but worth retaining for efficiency's sake, since it avoids conditional branching. however, for coding, reason only in terms of the short-circuiting stateful loop primitive, `find-integer-from`. the natural numbers express all orders, so `find-integer-from` works for all (orderd) sequences i.e. traversals. generally, all (non-parallel) traversals are one element at a time, ordered, effectful. this means that _any_ abstract or data structure can be expressed as a loop body and permutation of natural numbers, which means that it's available to `find-integer-from`. you can make it available to `find` by making a virtual sequence, but that's just unhelpful cruft.

`find` is like `loop` except with an integer argument, which makes it convenient for sequences.

*all this to say that `find-integer-from` is our go-to. however, before you try that, and now that you understand the traversal primitives, first look for solutions in `sequences.generalizations`. that vocab is the most powerful one in all of factor, effectively making factor an array language.* also the `generalizations` and `combinators.smart` are your best friends. they enable symmetry for arbitrary-degree relations.

* `combinators.extras` vocab:
  ** `chain` composes n quotations, short-circuiting if any returns `f`, *and applies it*: `6 { [ 1 + ] [ 10 * ] [ 3 / ] } chain` returns `6 1 + 10 * 3 /` and the literal composed quotation is `[ 1 + [ 10 * [ 3 / ] [ f ] if* ] [ f ] if* ]`.
  ** `loop1` is do-while or something maybe? it keeps hanging for me and i keep flubbing-up the stepping.
  ** `throttle`...i'd think kills a computation if it doesn't complete in the given timeframe, but apparently not...?
  ** `cond-case` is like `cond` but you don't need to lead all your predicates with `dup`

.`nfind` example

`"hello" "there" "boys" [ [ "aeiouy" member? ] tri@ or or 1 0 ? ] { } 3map-as sum 3 >=`, but with short-circuiting:

[source,factor]
----
0 "hello" "there" "boys" [ [ "aeiouy" member? ] tri@ or or 1 0 ? + dup 3 >= ] 3 nfind 4drop 3 >=
----

is `f`, but `t` if i change "boys" to "boysx", since then the 5th characters of the 1st 2 sequences are used. `nfind` traverses only insofar as the shortest sequence. this is a bit silly use of nfind, since we're using `4drop`; you'd likely suppose that we should use `until`, but the trouble there is that `until` does not provide easy indexing into sequences nor stopping at the end of the (shortest) sequence. the need to specify `3 >=` twice is also buffoonish. i think about how nice this would be in j: `2<+('aeiouy'&i.)"1 'hello','there',:'boys'` (something like that; i haven't tested this code). in this case, the code is nice because it exploits the fact that we're applying the vowel predicate to each letter of all sequences. this independence, along with addition's commutativity, enables much more elegant code than the general case of `q nfind` where q is not commutative.

you may suggest that ideally a compiler would detect conditionals following iterators, and change `map` into `find` automatically, so that the user can write expressions uniformly, easily, without redundant computation. however, suppose that you want to use `map` to apply an effect to each item of a sequence, collecting the results. in that case short-circuiting would be incorrect. again, the best solution is to always use `find-integer-from` or `nfind`, and the compiler can detect whether the last entry of the argument quotation is the constant `f`, and if so, convert it to `neach`. once `find-integer-from` is ultimately the only sequence combinator, it should be easy to optimize code written in terms of it. remember: `nfind` is effectively `find-integer-from` for sequences.

NOTE: `napply` generalizes `bi@`, and `dupn` & `ndup` differ! the former replicates the top elt n times, whereas ndup is like `2dup` &c.

.lossy array operations

these operations change the array shape. they remove information. `append` removes the distinction between its arguments, otherwise preserves the arguments information, but changes the right argument's indices, namely by shifting them. simply retaining the left argument's length is enough to make `append` non-lossy.

[options="header"]
|======================================================================================
| factor                                                   | j
| `suffix`,`prefix`,`append`                               | `x,y`
| `concat`                                                 | `,/`
| `flatten`                                                | `,y`
| `without`                                                | `x-.y`
| `intersect`                                              | `x([-.-.)y`
| `members`                                                | `~.`
|======================================================================================

.functions
[options="header"]
|======================================================================================
| factor                                                   | j
| `nip`                                                    | `x]y`
| `drop`                                                   | `x[y`
| `curry`                                                  | `m&v`, `u&n`
| `call`                                                   | `".y`
| `swap`, `dup`                                            | `~`
| juxtaposition, `compose`, `prepose`                      | `@`
| fried quotations (discouraged)                           | `=.`
| `y dup v u`                                              | `(u v)y`
| `y v x swap u`                                           | `x(u v)y`
| `f h bi g`                                               | `(f g h)y`
| `f h 2bi g`                                              | `x(f g h)y`
| the `inverse` vocabulary                                 | `&.`
| `drop y`                                                 | `y"_`
| `v bi@ u`                                                | dyadic `u&v`
|======================================================================================

.primitives
[options="header"]
|======================================================================================
| factor                                                   | j
| `=`                                                      | `-:`, `=`
| `CONSTANT:`, `SYMBOL:`, `set`, `:` & `;`                 | `=:`
|======================================================================================

.control flow
[options="header"]
|======================================================================================
| `loop` &al                                               | `^:`
| `if`, `if*`, `?`, `cond`, `at`                           | `@.`
|======================================================================================

.other

TODO: `x#y` below is incorrect

[options="header"]
|======================================================================================
| factor                                                   | j
| `dup length <iota> <zipped> sort-keys unzip nip`^[1]^    | `/:y`
| `y string>number x or`                                   | `x".y`
| the `random` vocabulary                                  | `?`, `?.`
| the `prettyprint` vocab                                  | `":`
| `>base`                                                  | `x#<.|:>y` (unmixed bases)
| the `math.bitwise` vocab                                 | `b.`
| `classify` (`sequences.extra`)                           | `=y`, i think
| `<iota>`                                                 | `i.y`
| `[ and ] filter`, `[ [ ] curry replicate ] 2map harvest` | `x#y`
| `dup length <iota> vand sift`                            | `I.y` (where y is a boolean vector)
| <implemented below>                                      | `~:y`
| <implemented below>                                      | `x#[.:]y` (mixed bases)
| <see bulleted notes>                                     | `"`
|======================================================================================

^[1]^ or `rank` from the `math.statistics` vocabulary, which has the same order but starts at 1 instead of 0.

the following are basic primitives and convenient variants:

[options="header"]
|==========================================================================
| primitive | variant     | variation
| `loop`    | `while`     | factors-out looping predicate
| `loop`    | `follow`    | accumulates iteration values
| `follow`  | `produce`   | factors-out looping predicate
| `loop`    | `times`     | `loop` with implicit counter/increment behavior
| `times`   | `replicate` | accumulates iteration values
|==========================================================================

^[1]^ of the `grouping` vocab.

^[2]^ of the `grouping.extras` vocab. that vocab also has `group-by`, but whereas `collect-by` collects into subsequences of predicate-matching elements:, `group-by` collects substrings of predicate-matching elements. (reminder: _subsequence_ is a subset retaining order. a _substring_ is a subsequence that retains contiguity.)

[source,factor]
----
"A STRIng with MaNY cAsES" [ dup ch>upper = ] collect-by [ >string ] assoc-map .
! H{ { t "A STRI  MNY AES" }
!    { f "ngwithacs"       } }
"A STRIng with MaNY cAsES" [ dup ch>upper = ] group-by [ >string ] assoc-map .
! V{ { t "A STRI" }
!    { f "ng"     }
!    { t " "      }
!    { f "with"   }
!    { t " M"     }
!    { f "a"      }
!    { t "NY "    }
!    { f "c"      }
!    { t "A"      }
!    { f "s"      }
!    { t "ES"     } }
----

NOTE: ``grouping.extra``'s `group-map` and `clump-map` apply a quotation to the group or clump *not as a sequence, but as though those values were pushed directly to the stack*:

[source,factor]
----
! factor                  ! corresponding j
dup 1 0 <shifted> v= all? ! (1|.!.0 x)=x
[ = ] 2 clump-map all?    ! *./2=/\x
----

clump-map can be composed easily, too:

[source,factor]
----
! vector ops version
dup 1 0 <shifted> v- 1 tail [ sgn ] map dup 1 0 <shifted> [ = not ] 2map
! clump version
[ swap - sgn ] 2 clump-map [ = not ] 2 clump-map
----

if you're using simd or bitvectors, though, then of course comparing a vector with its shifted version is far more efficient.

^[3]^ see `<circular-clumps>` of the `grouping` vocab for related functionality.

TODO: generalize fork & hook to n-ary.

* `y x natural-search`, where x is sorted and #x≥2, returns the index of, and element there at, the least e∈x s.t. y≥e.
  ** this behavior implies that `natural-search` never returns a negative index. `0.33 0.66 I. 0.2 0.5 0.75` returns `0 1 2`. `{ 1/5 1/2 3/4 } [ { 1/3 2/3 } natural-search drop ] map` returns `{ 0 0 1 }`. to make it j-like, add in negative infinity (`-1/0.`): `{ 1/5 1/2 3/4 } [ { -1/0. 1/3 2/3 } natural-search drop ] map`.
* the factor expression of `/:y` leaves sorted `y` and its grading sequence on the stack. of course the grading sequence is always used for sorting `y`, so we may as well apply it, and we can use it to sort other sequences, too.
* anything that takes a quotation implicitly works on arrays, too. j style grouping is `{ "tom" "dick" "harry" "wilbert" "billio" } { 0 1 1 0 2 2 } collect-by`, we can do `{ "tom" "dick" "harry" "wilbert" "billio" } { 0 1 1 0 2 2 } [ { } 2sequence ] 2map [ second ] group-by [ second [ first ] map ] map`. if ``group-by``'s quotation were to also take an index then this would be much more elegant: `{ "tom" "dick" "harry" "wilbert" "billio" } [ { 0 1 1 0 2 2 } nth ] group-by-index`. we can do `{ "tom" "dick" "harry" "wilbert" "billio" } V{ 0 1 1 0 2 2 } reverse [ pop ] curry [ drop ] prepose group-by` which is an elegant idea but looks clumsy b/c we must reverse the key sequence and curry with prepose. this being said, we most commonly group one set by a function of itself anyway, so predicated `group-by` is usually appropriate.
* rank isn't much help in practice. the few commonly used maps—1:1, 1:n, n:n—are defined below.
* `loop` loops forever until the loop body returns false
* `>base` isn't literally like j's `#.` & `#:`. `20 2 >base` produces the _string_ "10100". we can then run `string>digits` to produce `B{ 1 0 1 0 0}`. `B{ }` is a byte vector.
* `q map-index` is a terser version of `[ length <iota> ] keep q 2map`
  ** `<enumerated>` (of the `assocs` vocab) makes a virtual assoc of a sequence where the seq is values and integer indices are keys.
* usually we'll use `index` or `find-index`, not `index-or-length`. `search`, of the `binary-search` vocab, is `find` (takes a predicate) but uses binary search.
* the following are in j but are not useful in j or factor: `x|:y`. i'm curious how `/:y` and `~:y` can be useful both generally and specifically in j, and for those j-specific uses, what can we use in factor instead?
* the following are in factor but not in j and are nice: `join`, `interleave`, `index`. `flip-text` is an example of the `[ [ M at ] transmute ] map` pattern, which is a relational primitive. the `random` vocabulary is extensive.
* i'm curious how often rank is used. in factor (and indeed, scheme, haskell, etc), rank _1 is implicit. for rank 0 just `flatten` (if needed) then `cartesian-map`. pointwise is `2map`. to do like `+/` on a matrix which effectively sums columns in parallel: `TODO`. there are even virtual sequences for cartesian products, `x ;"0 y`.

."under" example
[source,factor]
----
! i haven't considered variadic stack effects
: &. ( x map: ( x -- y ) op: ( a -- b ) -- bx ) over [ compose ] dip [undo] [ call ] dip call ; inline
20 [ 4 + ] [ 3 * ] &. ! equivalent to j (3&*)&.(4&+)20
----

.`F:.` with `Z:`

if we use `follow`, we must `unclip-slice` before `follow`, because its argument quotation is `( ... prev -- ... result/f )`; thus it operates on the current element of the list, and the rest of the list must be part of the `...`. using `follow` requires the input sequence to be non-empty. also, we can't use `follow` at all, because inside its quotation, we must decide whether to include the current element or not. however we stop at a break condition, we generally include the current element in the output; however, eventually the input sequence may be exhausted. eventually we whittle-down to an empty sequence and the current element; we still accept the current element but cannot `unclip-slice`. however, we cannot leave the sequence empty, because we already have the rule that if it's empty, then accept the current element! thus we must replace the empty sequence by a designated value, e.g. `f`, then test it, and if it's `f` then we don't accept the current element! it's an ungraceful handling of an off-by-1 error!

`loop` actually provides the most elegant efficient solution. in most functional languages manual looping would be cumbersome, but in factor, because it allows accessing the whole stack from within a quotation, and makes stateful updates easy, and leverages currying & composition, it's quite nice:

[source,factor]
----
! accumulate until our sum is >=500
0
{ 1 10 420 65 74 100 54 }
[ 2dup [ 500 >= ] [ empty? ] bi* or
  [ f ]
  [ unclip-slice [ + ] curry dip t
  ] if
] loop >array
----

returns

----
570
{ 100 54 }
----

originally i wrote it in terms of `if-empty`, and even then it was improper. i was very confused trying to figure-out why my stack effects didn't check, until i realized:

. ``loop``'s stack effect implies that when the loop is done, the stack must be as it was before the loop began.
. the loop is executed on each loop. therefore if i were to put the vector and input sequence in the loop body, then those literal values would be pushed on every iteration.
. `if-empty` is not good to use inside `loop` because its stack effect discards the input sequence when it's empty, but per point (1), i must leave the sequence on the stack so as to not add nor remove anything from how it was before `loop` was called.
. i'd used `push` instead of `[ push ] keep`. my thinking was simply incongruous about keeping the vector on the stack but using `push ( elt seq -- )`.

the only reason that we need this complicated device is when we want to stop a reduction (not a map) when some predicate of _the accumulator_ is satisfied. if the predicate were of an input in the sequence, then we could just `dup <pred> find drop [ head ] when* <initval> <quot> reduce`.

a neat side-effect of this is that it effectively returns a zipper: an accumulation and the remainder of the sequence that was not accumulated.

this sholud be a combinator. writing it without fried quotations would be quite difficult.

.`#[.:]`
[source,factor]
----
: #: ( y x -- x#:y )
  <reversed> 1 head* 1 prefix
  dup length <vector> [ [ dup ?last 1 or ] dip * over push ] reduce <reversed>
  [ /mod swap ] map nip ; inline

! seems an odd but correct implementation
: #. ( y x -- x#.y )
  [ <reversed> ] bi@
  1 head* 1 prefix
  dup length <vector> [ [ dup ?last 1 or ] dip * over push ] reduce
  vdot ; inline
----

then `{ 1 30 6 } { 24 60 60 } #.` returns `5406`, and `5406 { 24 60 60 } #:` returns `{ 1 30 6 }`. in factor, the control argument is nearer the top of the stack, so the argument order is inverted compared to j.

.`x u\ y` when x<0 (*mistakenly written*; `group` already does this!)
[source,factor]
----
! [ u ] map afterward if you please
: xu\y ( x y -- xu\y )
  [ length <iota> [ 1 + ] map ] keep
  [ [ [ mod 0 = ] curry ] dip swap filter ] dip
  swap split-indices harvest ; inline

4 10 <iota> xu\y . ! outputs { { 0 1 2 3 } { 4 5 6 7 } { 8 9 } }
----

NEXT: outfix (x u\. y) should be trivial to implement in terms of infix

.nub sieve
[source,factor]
----
: ~: ( seq -- mask )
  ! `p q 2bi swap`, not `q p 2bi`, b/c update set AFTER testing `in?`
  HS{ } clone swap [ swap [ in? not ] [ [ adjoin ] keep ] 2bi swap ]
  ?{ } map-as nip ; inline
----

"catamaran" ~: gives `?{ t t t f t f t f t }`, an efficiently-packed bit array.

NOTE: this is a good example of `map` with tacit state!

.i. & multidimensional i.
[source,factor]
----
! print-mat works only on 2d arrays of numbers. we can easily extend it to any dimension, though.
: print-mat ( a -- ) [ number>string ] shaped-map shaped-array>array format-box. ; inline

: print-cube ( c -- )
  ! we must format the whole table then add line breaks so that all of the columns widths are equal even across
  ! line breaks.
  [ shape second ] keep [ number>string ] shaped-map shaped-array>array concat format-table
  [ length <iota> [ 1 + ] map ] keep
  [ [ [ mod 0 = ] curry ] dip swap filter ] dip
  swap split-indices harvest { "" } join [ print ] each ; inline

: print-cube/boxed ( c -- ) [ [ number>string ] map { } 1sequence format-table ] shaped-map shaped-array>array
  [ [ concat ] map format-box. ] each ; inline

! each ATOM is a multidimensional index. e.g. { 2 2 3 } multi-i. { 36 } reshape is a shape error but { 12 } works, even though its `shape` is { 2 2 3 }. in this way sa's are like everything's boxed, except that there's no raze or unbox.

! creates a shaped array of multidimensional normal (unshaped) arrays.
! i could make them shaped arrays but i can't find any way to flatten them, so i see no point in doing so;
! it only introduces an extra step of converting back to a normal array.
: mi. ( shape -- arr ) zeros [ second ] map-shaped-index ; inline ! firsts were 0; idk what they're for.

! a block version. it seems that block arrays can't be used for anything, so...*shrug*
: mi.-block ( shape -- arr ) [ zeros >shaped-array ] [ length 1array [ <block-array> ] curry ] bi [ second ] prepose map-shaped-index ; inline
: i. ( shape -- arr ) [ product <iota> >shaped-array ] [ reshape ] bi ; inline

  { 4 2 3 } mi. [ [ 10 + ] map ] shaped-map print-cube/boxed
┌──────────┬──────────┬──────────┐
│ 10 10 10 │ 10 10 11 │ 10 10 12 │
├──────────┼──────────┼──────────┤
│ 10 11 10 │ 10 11 11 │ 10 11 12 │
└──────────┴──────────┴──────────┘
┌──────────┬──────────┬──────────┐
│ 11 10 10 │ 11 10 11 │ 11 10 12 │
├──────────┼──────────┼──────────┤
│ 11 11 10 │ 11 11 11 │ 11 11 12 │
└──────────┴──────────┴──────────┘
┌──────────┬──────────┬──────────┐
│ 12 10 10 │ 12 10 11 │ 12 10 12 │
├──────────┼──────────┼──────────┤
│ 12 11 10 │ 12 11 11 │ 12 11 12 │
└──────────┴──────────┴──────────┘
┌──────────┬──────────┬──────────┐
│ 13 10 10 │ 13 10 11 │ 13 10 12 │
├──────────┼──────────┼──────────┤
│ 13 11 10 │ 13 11 11 │ 13 11 12 │
└──────────┴──────────┴──────────┘
  { 2 2 2 } mi. [ length { } 1sequence ] shaped-map print-cube/boxed
┌───┬───┐
│ 3 │ 3 │
├───┼───┤
│ 3 │ 3 │
└───┴───┘
┌───┬───┐
│ 3 │ 3 │
├───┼───┤
│ 3 │ 3 │
└───┴───┘
  { 4 2 3 } i. print-cube
0  1  2
3  4  5

6  7  8
9  10 11

12 13 14
15 16 17

18 19 20
21 22 23
----

[TODO]
* compare `>array` vs `shaped-array>array`
* consider relations, beyond mere arrays. consider using mutable states in arrays. relations are more general than rank.

it seems that `>array` can be used to flatten like `,/` in j.

.padding example
[source,factor]
----
sa{ { 0 2 3 4 } { -6 6547 1 } } ! error: no-abnormally-shaped-arrays
sa{ 0 2 3 4 } sa{ -6 6547 1 } pad-shapes ! well, actually pad-shapes seems to do nothing somehow.
----

=== examples of deriving relational or array-stack programs

.parse command line

goal: separate part of the command line from the rest. namely: separate switches "-c" & "-s" and their following arguments from everything else, preserving order.

a scalar-functional-applicative programmer's go-to method would be looping with accumulator states:

----
(let loop ([xs cmdargs] [acc1 '()] [acc2 '()])
  (if (null? xs)
      (values (reverse acc1) (reverse acc2))
      (let ([x (car xs)] [rst (cdr xs)])
        (if (or (equal? x "-c") (equal? x "-s"))
            (loop (cdr rst) (cons (car rst) (cons x acc1)) acc2)
            (loop rst acc1 (cons x acc2))))))
----

which i could of course write in factor, but scalar code in factor (and generally) is horrible, so i'd like to save myself the likely pain.

kakoune method: "find '-[cs]', extend selection to next word, delete, paste in new place." one doesn't even need to think about how to derive a program because it's so natural. it's not altogether immediately obvious, but each step is immediately obvious given the current state! obviously we want to find strings like "-c" or "-s" which is obviously is expressed by the regex `-[cs]`; then we want the next words. then we're done selecting. to separate, of course we delete. now that we've deleted it, we must retain it, so we must paste it, but obviously we want to paste it in a location distinct from their original one. done.

j method:

. "find '-c' or '-s'". "find" means "select", "match", "identify". to identify, j uses bit masks. so we do `{{((<'-c')=y)+.(<'-s')=y}}"0 args`. lol, jk. it's funny how that's what my mind went to after writing the scheme code above. good j uses arrays and forks wherever possible, which is usually. thus to derive programs, we ask ourselves "which array primitives work best for my needs?" it's a good variety of constraint! `;` is just better than boxing both things. now we've an array. a common pattern is that "x=a or x=b" is "any? (= x) [a b]", which, when x is a set, is generally expressed as "not null x∩[a b]". anyway, all this thought derives `mask=.(e.&('-c';'-s'))args`.
. then we want to select the following indices, too. this means that all `1`'s successors are set [to 1], too. it's not obvious to me how to do this; ideally i'd use some kind of verb that maps selected indices values to other indices, here that set bits are nth, and (n+1)th indices should be set to 1. but no such one exists. this would be easily done as `>:&.I.args` if an inverse were defined for `I.`. anyway, permutations are 1:1 maps between elements, which is what we want here. specifically, we want the nth indices to be related to their successors. this particular, simple permutation is expressed by rotation or shifting: `mask=.(+._1&(|.!.0))mask`.
. now we've a mask that has two values—0 or 1—which partitions our data. to finish our solution, we only need to apply this mask as a partition: `mask</.args`. done.

the whole solution altogether:

[source,j]
---------------------------------------------------------------------------
NB. the solution that we derived. it's explicit b/c we built in in parts,
NB. then stuck all those parts together.
((+._1&(|.!.0))@(e.&('-c';'-s')) args)</.args

NB. refactored to tersest, tacit form.
(</.~(+._1|.!.0])@e.&('-c';'-s'))args
---------------------------------------------------------------------------

using array style in factor is much like using j except that we're freer. remember that the most general description of the solution is to:

. identify -c & -s
. extend the identification to their following args
. separate this identified subsequence from the rest

the j solution is perfect, then, at least insofar as its logic: it identifies by a mask, then modifies that mask by relating the mask to a permutation of itself i.e. we relate the mask to a relation of the mask's indices to themselves. then we apply that mask to partition.

[source,factor]
----
{ "-f" "-c" "sess" "-n" "-c" "SE" "-s" "SA" "+45:6" }
dup [ swap { "-c" "-s" } member? and ] map-index
dup f prefix [ or ] 2map collect-by
----

which leaves the following hash table on the stack:

----
H{
    { t V{ "-c" "sess" "-c" "SE" "-s" "SA" } }
    { f V{ "-f" "-n" "+45:6" } }
}
----

i prefer to define my own symbolic primitives e.g. `ALIAS: ⇄ swap` to make code terse & clean. by these, the solution is:

----
↟ [ ⇄ { "-c" "-s" } ∈ A ] ∀i ↟ f prefix vor collect-by
----

my factor solution uses `f prefix` analagously to `_1|.!.0`. i'm afforded that b/c `2map` iterates a number of times equal to the length of the shorter list, not caring if two lists have different lengths.

this solution is most elegant because it unions the selection vector to its offset and retains pointwise association of group id with elements.

the actual ideal solution is a literal translation of "separate indices where item matches -[cs], and their successors, from the rest, retaining original order." the following factor represents this:

[source,factor]
----
[ [ # i. ] ⎌              ! push i.@#
  [ { "-c" "-s" } ∈ ] ∃': ! selection vector
  [ ↟ 1 + { } 2s ] ∀      ! vector of duples { selection successor }
  ♭                       ! flatten into selection vector "selvec"
  [ \ ] ⎌ ]               ! (i.@#)-.selvec while retaining selvec (atop) the stack
[ [ @: ] & ]              ! "quotation B"
bi                        ! these two quotations are applied to the input
bi@                       ! quotation "B" which `bi` leaves atop the stack is applied
                          ! to the two selection vectors that quotation "B" left on the stack
----

sadly this does not read as well as the english. why not?

. there's a lot of nested, ordered state retention
. the phrase "indices and their successors" is expressed oddly: "each index with its successor put into a sequence then flatten that whole array"
. there's currying and delayed execution (data are far from the word that consumes them) which, when finally executed, is done with `bi bi@`. this is done so that i can apply `@:` to both selection sequences without having the input on the stack. namely, i'd have to have it twice on the stack, and in the right place, in order to call `@:` twice on it. specifically, i'd need `selvec1 inputseq selvec2 inputseq [ nths ] bi@`, which would be horrible to express on the stack. currying is practically necessary (and elegant anyway) for keeping the stack managable.
. instead of partitioning, i apply each of the selection vectors to the original input. that's two selection operations instead of one partition operation. it's because i chose to code it this way that i must retain the input for two operations, which is easiest when i curry it. the relation between the input sequence and selection vectors is 1:n, namely 1:2. `1:n` is defined as `curry map` so currying is appropriate, though it'd look nicer if we'd express the relation as `1:n` which would return one sequence of 2 sequences: the selected items and the rest—instead of pushing these two to the stack as outputs.

how the english description breaks-down:

----
partition [ indices such that item matches -[cs] union their successors ]
retaining original order
----

why would the english be easier than factor? i think that it shouldn't be. so i push myself to make the factor code better. just like factor, english builds on prior-mentioned things. i can surely be tacit here because the english expression does not use names. the only parameter is tacit:

----
partition [ indices such that [each of the input's] item matches -[cs] union their successors ]
retaining [the input's] original order.
----

i should have my factor code mirror this _exactly_. because factor code is totally tacit, i may begin constructing my program at any part of the english phrase. first i notice that `[ # i. ] ⎌` inside the first `bi` quotation can be factored-out:

[source,factor]
----
[ # i. ]
[ [ { "-c" "-s" } ∈ ] ∃': [ ↟ 1 + { } 2s ] ∀ ♭ [ \ ] ⎌ ]
[ [ @: ] & ]
tri
bi@
----

flatter, better, obviously shows that input is considered in three ways. there's not really any unnecessary nesting anymore.

i can try to retain order in an arguably more natural way: by unioning (appending) the index vector with its successor vector then sorting them:

[source,factor]
----
[ # i. ]                  ! related to "separate from the rest"...
[ [ { "-c" "-s" } ∈ ] ∃': ! "indices where item matches -[cs]"
  ! the next three lines read as "and their successors"
  ↟                       ! "their"
  [ 1 + ] ∀               ! "successors"
  ++                      ! "and"
  ⍏                       ! "retaining original order"
  [ \ ] ⎌ ]               ! "separate from the rest"
[ [ @: ] & ] tri bi@
----

i've mapped each part of the english phrase to the corresponding factor words directly. note that "separate from the rest" uses the `# i.` vector left quite earlier on the stack. this actually corresponds directly to the english! "the rest" only has meaning (because its meaning is relative) after specifying "indices ... and their successors!" the factor code which implements the idea of "separate from the rest" is "all minus indices & successors" which actually merely _calculates_ the rest. this demonstrates that "separate from the rest" is redundant! the fact of them being the _rest_ already _distinguishes_ them from the rest. being that "rest of the rest" is an identity, we see that "rest" means "complement" which is a set operation. "rest" is thus a fn of a set and a total. in my code, `[ # i. ]` and `\` are distant from each other. to more closely follow the english, i should have curried `# i.` to `\` (TODO: or `\:`? that's what's in the code below).

[source,factor]
----
[ [ { "-c" "-s" } ∈ ] ∃': ↟ [ 1 + ] ∀ ++ ⍏ ]
[ # i. ⇄ [ \: ] ⎌ ]
[ [ @: ] & ] tri bi@
----

so i noticed that actually i can just swap the order to—big surprise—_actually match the english's order_: "rest" following "indices ..." then there's no need to curry!

NOTE: because indices themselves represent the order, sorting the indices is always just as fine as producing the indices in order.

more literally following the english description:

[source,factor]
----
↟ [ { "-c" "-s" } ∈ ] ∃': [ ∈ ] & partition
----

the only problem is that `partition` takes a predicate of elements instead of indices. that being said, it takes a quotation that returns booleans, which generalizes to a quotation that returns integers, or most generally, returns distinct things. and we may decouple applying a quotation from partitioning on its outputs. `collect-by` does this. this particular case of "class and else" is nice because we can use one class to derive two classes. i may think about ways to use variants of, or prepare inputs for, `partition` or `collect-by` but i don't want to bother. that's because...

the easiest way is what the scheme solution does: loop, putting an item into one list or an item with its successor into another list. that's the most straightforward description and naturally retains order, and is one traversal. the inconvenience with coding this is that we cannot use list iterators because we must consider multiple elements at a time. this means that we must loop. honestly there should be a combinator like map, reduce, etc, that takes the remainder of the list as an argument rather than just the next one item. given that i've not defined such a combinator and that i don't want to spend any more time on this yet, here's the solution basically equal to the scheme one, but a bit nicer b/c it uses `make` instead of needing to keep both vectors on the stack; and because it destructively/effectively appends to both vectors.

[source,factor]
----
V{ } clone                                            ! one of the two accumulators
{ "-f" "-c" "sess" "-n" "-c" "SE" "-s" "SA" "+45:6" } ! input
[ [ ↟ ∅ ]
  [ unclip-slice ↟ { "-c" "-s" } ∈
                   [ , unclip-slice , ]
                   [ [ suffix! ] & ⋎ ]
                   if
  ] until
] { } make 2↶
----

recall that this is exactly what we did in kakoune. as its elegance suggests, it's the best solution because the iteration relates each element to all of the following ones, allowing us to move it and its successor to a separate "else" vector. the traversal being in order of increasing index (i.e. in original order) is good b/c we want that order both in that we retain it in both of our output partitions, but also in that the order relates the identified elements with their successors and that we'll be indexing both of them anyway!

there's a powerful combinator that this code generalizes to. however, the question is how to identify such a generalization that is not overconstrained.

and for fun, the sql version:

[source,sql]
----
create table r(i integer primary key autoincrement, e string);
insert into r(e) values('-f'),('-c'),('sess'),('-n'),('-c'),('SE'),('-s'),('SA'),('+45:6');
with x(a) as (select i from r where e='-c' or e='-s') -- need a local bind b/c we union x with itself
select group_concat(e) from r group by i in (select a from x union all select a+1 from x) order by i;
┌─────────────────────┐
│   group_concat(e)   │
├─────────────────────┤
│ -f,-n,+45:6         │
│ -c,sess,-c,SE,-s,SA │
└─────────────────────┘
----

the equivalent factor:

[source,factor]
----
{ "-f" "-c" "sess" "-n" "-c" "SE" "-s" "SA" "+45:6" }
[ # i. ] ⎌ zip                         ! table r is on the stack
↟ [ 2nd { "-c" "-s" } ∈ ] ∀f [ 1st ] ∀ ! now r x is on the stack
↟ [ 1 + ] ∀ ++ ⍏                       ! now r xU(x+1) is on the stack
[ ∈ ] & [ 1st ] prepose partition      ! yay! our partitions!
[ [ 2nd ] ∀ ] bi@                      ! remove associated indices
----

and similar factor:

[source,factor]
----
{ "-f" "-c" "sess" "-n" "-c" "SE" "-s" "SA" "+45:6" }
[ ↟ # i. zip ]
[ [ { "-c" "-s" } ∈ ] ∃': ↟ [ 1 + ] ∀ ++ ⍏ [ ∈ ] & [ 2nd ] prepose ]
bi partition [ [ 1st ] ∀ ] bi@
----

prints:

----
{ "-f" "-n" "+45:6" }
{ "-c" "sess" "-c" "SE" "-s" "SA" }
----

this relational style (coupling attribute vectors into a table, here a list of duples) isn't good outside of sql, because we must use accessors like `1st`. the relational style worked "nicely" here b/c i used `partition` in order to derive two groups from one predicate. however, generally (for more complex or detailed relations) it's best to use `collect-by` and keep attribute vectors separate, related by whatever maps relate them—the maps corresponding to joins, e.g. `{ { 1 65 } { 2 4 } { 2 3 } { 6 20 } }` which relates two tables' (as seen by each by each item in the list having length 2) indices.

TODO: join is a sql thing. how would prolog do it? sql uses data literals. prolog uses composable, logical generator fns.

window fns don't work:

[source,factor]
----
{ "-f" "-c" "sess" "-n" "-c" "SE" "-s" "SA" "+45:6" }
2 <clumps> [ 1st { "-c" "-s" } ∈ ] partition
[ [ >array ] ∀ . ] bi@
----

prints

----
{ { "-c" "sess" } { "-c" "SE" } { "-s" "SA" } }
{
    { "-f" "-c" }
    { "sess" "-n" }
    { "-n" "-c" }
    { "SE" "-s" }
    { "SA" "+45:6" }
}
----

so of course we can't use that 2nd sequence of duples.

.useful considerations of rank

[source,factor]
----
: 1:1 ( x y q -- z ) 2map ; inline
: n:n ( x y q -- z ) cartesian-map ; inline ! defined as [ with map ] 2curry map
: 1:n ( x y q -- z ) curry map ; inline

{ 1 2 3 4 } "cats" [ { } 2sequence ] 1:1 .
{ { 1 99 } { 2 97 } { 3 116 } { 4 115 } }

{ 1 2 3 4 } "cats" [ { } 2sequence ] 1:n .
{ { 1 "cats" } { 2 "cats" } { 3 "cats" } { 4 "cats" } }

{ 1 2 3 4 } "cats" [ { } 2sequence ] n:n .
{
    { { 1 99 } { 1 97 } { 1 116 } { 1 115 } }
    { { 2 99 } { 2 97 } { 2 116 } { 2 115 } }
    { { 3 99 } { 3 97 } { 3 116 } { 3 115 } }
    { { 4 99 } { 4 97 } { 4 116 } { 4 115 } }
}
----

== using the stack well

* small programs defined as words are common both for modularity but also b/c if a word evaluates then it's verified to be correct, assumedly for its functionality, but certainly for its stack effect. breaking a program into many word definitions makes tracking the stack easy. calling a word is like a checkpoint.
* despite aptly being called "factor", coupling is important, too! however data are always used together, couple them into a single item on the stack, as early as you can.
  ** curry when you can. this reduces the number on items on the stack and appropriately couples a fn with its arg. build-up programs incrementally as early as you can. an example is `1 xs [ / ] with map` or the equivalent `xs [ 1 swap / ] map`. the naive way is to think of `/` as a binary operation which takes two args, and assume that those two args should be provided as arguments on the stack. however, here one arg is an atom and the other a sequence, so we can't simply apply it nor can we use `2map`. a novice might put `1` on the stack then retain it beyond its consumption by `/`: `1 xs [ over [ / ] dip swap ] map`. that's unnecessary hell. or they may do a less elegant thing: creating a sequence of a constant: `xs dup length 1 <repetition> swap [ / ] 2map`. these just make stack langs look bad.
    *** i expect that this is why words like `nth` take the index then the sequence, or why `set-at` takes the hash map off the top of the stack, or why `member?` takes the sequence off the top of the stack.
    *** exploit that programs are sequences e.g. `[ "v1" "v2" ] [ "k1" "k2" ] [ H{ } set-at ] [ 2each ] keep first .` set-at is effectful, consuming our hash table. `keep` leaves the program `[ H{ } set-at ]` atop the stack; the hash table is thus still on the stack, contained in the sequence; `first .` prints it.
* any time that data have an attribute in common, where that attribute is relevant to some fn that the data are passed to, put the data in a sequence then iterate over it.
* if you're having trouble orienting composition/currying of fns throughout a stack process, then just define a helper fn or combinator. stack langs work best when each word has a short definition.
* manual recursion really is rare! surprisingly, even `(split)`'s definition is non-recursive!
* if you're resorting to `fry` (fried quotations), you're almost certainly not using the stack well
* if you're nesting combinators and getting stack effect mismatch errors, then check the stack effects (ctrl+i in the graphical listener) of each part individually and algebraically reason through what outer quotations mandate vs what your inner quotations effects are.

basically: express symmetries as sequences and asymmetries as relations of sequences. these relations are commonly themselves sequences. `[ H{ } set-at ]` is a sequence/relation of two distinct things paired particularly, and this relation is passed to `2each`, which relates a matrix (having two axes [of symmetry]) to an operation (program).

if you don't immediately find obvious how to tacitly write a program, then code it in applicative style, then factor it. reading factored code is easy, but identifying it without first seeing the distributed code is often difficult, and is a variety of optimizing too early. indeed, factoring is a function of some code altogether. a good exercise for this is making `sorted-index` like j's `I.`.

.deriving tacit I.

i start with the following non-tacit form:

[source,factor]
----
:: I. ( elt seq -- i )
  elt seq natural-search :> i v
  { { [ v not          ] [ i ] }
    { [ v elt <=> +lt+ ] [ i 1 + seq length min ] }
    ! if v>=elt then we've already the correct index.
    { [ t              ] [ i ] } } cond ;
----

factoring:

. the longest common substring of ``cond``'s bodies is `i`; therefore i should leave i on the stack before calling `cond`
. all of ``cond``'s heads (can) start with `v` (once i rewrite `t` to `v drop t`)
. because ``cond``'s heads are used before their bodies, `v` should be atop the stack; then because `i` is used after, it should be beneath `v`
. the main branch of `cond` requires both `seq` and `elt`, which the other branches do not entail. for the sake of this one branch, i must put them on the stack sometime before calling `cond`. if it's before calling `cond`, then that implies that they must be taken off the stack by the other two branches. this expresses all of the branches in terms of common factor `seq elt`.
. combining points (3) & (4), the stack must be `i seq elt v` before calling `cond`
  .. or we can do the relation of `i` & `seq` ahead of time once we decide that `v` is not `f`, but that'd mean breaking the `cond` into two ``if``'s

[source,factor]
----
: I. ( elt seq -- i )
  2dup [ swap ] 2dip natural-search -rotd ! i seq elt v
  { { [ dup not         ] [ 4drop 0                    ] } ! replace i (f) by 0
    { [ swap <=> +lt+ = ] [ [ 1 + ] [ length ] bi* min ] }
    ! if v>=elt then we've already the correct index.
    { [ drop t          ] [                            ] } } cond ; inline
----

test:

----
A=10 12 34 56 87 500
B=2 3 6 10 12 18 24 36 42 83 91 102 

   B I. A NB. j
3 4 7 9 10 12

   A B [ sorted-index ] curry map . ! factor
{ 3 4 6 8 9 } ! incorrect

   A B [ I. ] curry map . ! factor
{ 3 4 7 9 10 12 } ! yay! success!
----

== examples of good programming

.state, currying, a/symmetry obvious in code
[source,factor]
----
{ HS{ } HS{ } } ! list 1
{ "hs1" "hs2" } ! list 2
[ over [ H{ } set-at ] dip ] [ 2map ] keep ! accumulating into a list of hash sets, to keep them in scope for later modification
                                           ! running 2map under keep leaves the hash table quotation atop the stack
second first ! extract hash map from quotation
dup . ! print hash table
[ { "beans" "jeans" } [ over adjoin ] 2map . ] dip ! insert into, then print, each hash set
. ! print hash table
----

this code generalizes easily to any number of key/value pairs. this elegant encoding keeps few elements on the stack, even if it's "strange" that we retain the hash table within the quotation passed to `2each`. after all, we're always setting values of hash table, so why not couple it with `set-at`? it's so simple that one may fail to realize that it's a variety of metaprogramming that macros cannot enable. for this, one truly must use call/eval and be able to extract subprograms from a (quoted) program. it's even easier in factor because the hash map is not a program that we need to evaluate; it's a datum itself!

.a quintessential example of factor coding

[source,factor]
----
: collector-as ( quot exemplar -- quot' vec )
    dup new-resizable-like [ [ push ] curry compose ] keep ;
    inline
----

this is from the `sequences` vocab. it uses `curry`, `compose`, `keep`, `dup`, and state (`push` is effectful only.)

reading & reasoning about the code:

. don't read `dup` by itself. read `dup new-resizable-like`. `quot exemplar` becomes `quot exemplar new-resizable-like-exemplar`. `dup f` (at least when `f` is unary) should be read as "push f(top of stack) to stack"
. next we hit a quotation; we immediately look to see of which word this quotation is an argument. it's `keep`. `keep` should be read as "under pop" in the sense of "pop, do stuff, push back." if the stack were a list, then it's like saying "apply quotation to `init`" where `init` is from haskell, meaning "the subsequence defined as all but the last element."
  .. now we're picturing the stack `( quot exemplar [ push ] curry compose ) new-resizable-like-exemplar` where the parenthesis has its usual meaning: "result of this expression goes here"
. we hit a quotation again. we see that it matches `curry`. we apply it, resulting in: `( quot [ exemplar  push ] compose ) new-resizable-like-exemplar`
. we hit a quotation again. we see that it matches `compose`. we apply it, resulting in: `( [ quot exemplar push ] ) new-resizable-like-exemplar`
. the parenthesis surround a quotation; they've done their job, so we remove them, leaving the result of `collector-as`: `[ quot exemplar push ] new-resizable-like-exemplar`

tacit data structure combinators tend to have definitions with many uses of `curry` & `compose`. the use of `push` is clever: it has stack signature `( elt seq -- )`. by currying _a particular_ sequence with it, we've created a quotation that takes an element and pushes it to a sequence. `push` is purely effectful, which means that we can pass around this "push to sequence" quotation around anywhere without needing to also pass the sequence, _and_ we don't need to consider how to manipulate the stack in order to keep the sequence in the right position, since it's not on the stack at all anymore! the first benefit is accomplishable simply by currying, which can be done in many languages. the second point is a consideration only in stack langs. however, factor is the only language that i know that supports _uncurrying_! `quot first` leaves the sequence on the stack! we can say "'sequence' becomes 'push to sequence', and later becomes 'sequence' again." true, haskell has `uncurry` but it: 1. works only on duples (a structure of only 2 data); 2. the data must be structured in that particular way—as a duple—which is not the natural way to express computations in haskell. fortunately lisp generalizes lists & cons pairs to one structure, and has `apply`, which is similar to `curry`, but even lisp does not support uncurrying, because a currying a function with an argument results in a function, and functions support only application (β-reduction.) however, technically lisp does support uncurrying just like factor does: we can build an argument vector then `cons` a function name to it and `eval` it, while retaining the input arguments. this is, however, inelegant (clunky) in lisp and very unidiomatic! idiomatic lisp uses macros instead of `eval`, and pure rather than mutative operations (whereas factor supports both equally), and even the macros must consider macro hygiene because they use identifiers whereas factor is totally tacit and thus has no analagous concerns. factor is just like lisp but better.

.more practice reading factor

this code is from the `calendar` vocab:

[source,factor]
----
: weekdays-between ( date1 date2 -- n )
    [
        [ swap time- duration>days 5 * ]
        [ [ day-of-week ] bi@ - 2 * ] 2bi - 7 /i 1 +
    ] 2keep day-of-week 6 = [ [ 1 - ] dip ] when day-of-week 0 =
    [ 1 - ] when ;
----

. first we encounter a quotation; thus we look to see of what word the quotation is an argument. here, it's `2keep`. ok, so we're computing over `date1` & `date2` and storing the result under them on the stack for later.
  .. to know what that computation is:
    ... we see a quotation, so we look to see of which word it's an argument. oh! the quotation is followed by another quotation! ok, so we'll see of which word these 2 quotations are arguments. (generally, repeat, collecting quotations as arguments, until a word is encountered.) ah, it's `2bi`.

if you followed that, then you can certainly parse the rest of `weekdays-between`.

== generators

`USE: generators`

installs `return` in a quotation such that the repeatedly.

generators are tuples built on continuations/coroutines. `<generator>` accepts a nullary quotation that loops, but the loop does not actually evaluate completely because sometime within the loop, `yield` is called. `yield` stops evaluation of the quotation like `return` does in many programming languages.

idk how efficient generators are. i know that they're like a non-strict-evaluation version of `make`, and that `make`, while strict-eval, is still slower than looping combinators like `reduce`. one may suggest that virtual sequences are non-strict, but this is not correct: virtual sequences are not computations! they're _representations_! virtual sequences may be used in strict or non-strict computations.

all generator quotations:

* "return" comes in two varieties:
  ** `yield`: return value.
  ** `stop-generator` return the fact that there's nothing to be returned.
* usually have some looping construct
  ** or else just generate intermediate values of a computation

words that run generators: `?next`, `skip`, `take`, `take-all`.

.wrapping `produce` trivially
[source,factor]
----
[ 1337 [ dup 0 > ] [ 2/ dup yield* ] produce ] <generator> 6 take
----

produces the same as `produce 6 head` but non-strictly; only the first 6 values are computed. `produce` does not fully evaluate!

.wrapping `produce` helpfully
[source,factor]
----
: time ( -- s ) now [ hour>> ] [ minute>> ] [ second>> floor ] tri [ "%02d:%02d:%02d" printf ] with-string-writer ; inline
[ [ t ] [ time yield* 4 seconds sleep ] produce ] <generator> 5 take .
----

prints `{ "23:23:54" "23:23:58" "23:24:02" "23:24:06" "23:24:10" }`.

this is easier than `produce` because we don't have to keep a loop counter. it's very probably less efficient, though! of course, an unpredicated reduce is better expressed by `loop`: `[ [ time yield 4 seconds sleep t ] loop ] <generator>`.

.a finite generator: pop off a (resizeable) sequence
[source,factor]
----
[ V{ 1 2 3 4 } [ dup empty? ] [ dup pop yield ] until ]
<generator> take-all
----

produces `{ 4 3 2 1 }`.

.an infinite generator: natural numbers
[source,factor]
----
[ 0 [ [ 1 + ] keep yield t ] loop ] <generator>
----

.generator composition: call one generator to yield values, from within another
[source,factor]
----
[ 0 [ dup 7 = [ [ V{ 1 2 3 4 } [ dup empty? ] [ dup pop yield ] until ] <generator> yield-from ]
              [ [ 1 + ] keep yield ]
              if t ]
    loop ] <generator>
----

`11 take` from this produces { 0 ... 6 4 ... 1 }. `12 take` loops forever! the problem is that i never incremented past 7! the incrementer increments to 7, then the if's true branch is taken, which yields the 4 values, but then hsa no more values to yield, but keeps trying to! the solution is to add an increment operation after `yield-from`:

[source,factor]
----
[ 0 [ dup 7 = [ [ V{ 1 2 3 4 } [ dup empty? ] [ dup pop yield ] until ] <generator> yield-from 1 + ]
              [ [ 1 + ] keep yield ]
              if t ]
    loop ] <generator>
----

`20 take` on this produces `{ 0 1 2 3 4 5 6 4 3 2 1 8 9 10 11 12 13 14 15 16 }`.

`[ next ] [ skip ] [ next ] tri { } 2sequence` on it leaves { 0 2 } on the stack.

== suffix arrays

[source,factor]
----
"cat" "con" "mar"
[ dup SA{ "cats" "catamaran" "concat" "cafe con leche" "mary" } query { } 2sequence ]
tri@ { } 3sequence [ ... ] each
----

prints

----
{ "cat" { "concat" "catamaran" "cats" } }
{ "con" { "cafe con leche" "concat" } }
{ "mar" { "catamaran" "mary" } }
----

== html

=== parsing

[source,factor]
----
USE: http.client
"https://github.com/" http-get nip
----

on this data, ``modern.html``'s `string>html` fails with a malformed html error, whereas ``html.parser``'s `parse-html` succeeds. however, the former supports `string>html` while the latter does not, though i could write such a function easily. anyway, ``modern.html``'s parser seems intolerant of some common varieties of html. this is a shame considering that `modern.html` features words like `walk-html` and `find-links`.

i find the internal representation of `modern.html` odd—namely that it includes separate open & close tags, and fields `open` & `close` for "<" & ">", as if they could ever differ. idk why anyone would ever not prefer html as represented by sexps (what would be sequences and assocs in factor, like how it represents json) in racket scheme.

the `html5` vocab is yet unfinished. idk why it was started; html5 validation software already exists, and for parsing, an html soup parser would work.

== http servers

.responders (the req->resp fn)

* in a responder, `HTTP request variables` (see docs) are set: `request`, `url`, `params`, `responder-nesting`,
* to practically write a response, use `furnace.actions`. invoke `<action>` then set its properties, which are described in `Furnace action configuration`. `display` is the main property for GET requests, analagously: `submit` for POSTs, `replace` for PUTs, `update` for PATCHes; set it to a quotation that takes a request and returns a response.

.template/example
[source,factor]
----
USING: io.servers                   ! tcp/ip
       http                         ! provides the `request` class
       http.server                  ! http
       furnace.actions              ! easy http responses
       http.server.responses        ! simple http response bodies
       html.templates.chloe         ! xhtml template device
     ! http.server.dispatchers      ! what's this?
       http.server.static           ! ftp emulation
       http2.server
     ! http.server.cgi              ! cgi: external subproc handles req->resp
     ! logging.server
       present                      ! to convert url to string
       io.streams.string formatting ! printf to string
;

! specify the req->resp handler. somewhy we do so by setting a global name.
<action>
[ request get [ url>> present ] [ header>> ] bi
  [ "you requested resource: %s with these headers:\n%u" printf ] with-string-writer <text-content> ] >>display
main-responder set-global

8080 httpd  ! start an httpd server at port 8080
stop-server ! run this whenever you're ready to stop it. see io.servers for more useful words.
----

* `<http-server> 8080 >>insecure f >>secure start-server` creates then starts a new http server on port 8080, leaving the http-server object on the stack.
  ** you *must* specify both `insecure` and `secure`; otherwise the server will fail to run, citing error "permission denied."
* `httpd` is a convenience function for all that; however, because it does not execute in a coroutine, it blocks until the server dies. this is very bad for development outside of windows os, since only windows supports ^C to stop the running factor thread!
* `handle-get` apparently defines responses of other actions. idk when one would do this.
* i suppose that `wait-for-server` is useful only if we want to block until the server terminates itself, or receives such a signal e.g. by someone requesting URL path `/stop` of the server.
* `http-server` subclasses `threaded-server`, which has attrs `max-connections`, `encoding`, `timeout`.

TODO: compare factor's server against mongoose and redbean. linking factor with mongoose may be a good solution. [nov 2023] mongoose is marketing itself as an "embedded web server for electronic devices." a few years ago it was just a webserver, but a well-designed one: small, elegant, (mostly) pure c99, few or no external libs. mongoose is GPL or commercial license, however: either totally free or pay. redbean is zlib/isc/bsd/mit.

as a reminder: URLs are akin to command lines: they're a sequence and map of parameters e.g. `cp -rut place f1 f2 f3` is represented by URL `/cp/f1/f2/f3?r&u&t=place`.

i don't understand ``action``'s `rest` parameter, nor do i understand `http.server.rewrite`.

=== html templates

chloe is contrived but more certain. fhtml is freer and more liable to error.

docs are found at the following vocab pages after loading them:

* `html.templates.chloe`
* `html.templates.fhtml`

both implement the `template` interface of the `html.templates` vocab. chloe templates support conversion to responses via `<chloe-content>`. this being said, one can simply do `<fhtml> [ call-template* ] with-string-writer <html-content>`

== filesystem

vocab `io.files.info` provides word `file-info` which returns a `file-info-tuple` tuple (yes, it has "tuple" in the name) which has attributes: `type`, `size`, `permissions`, `created`, `modified`, and `accessed`, and `size-on-disk`, if that's ever useful.

== stack machines and assembly

in assembly we've registers, the stack, the heap, and opcodes. execution is sequential and may involve jumps. a common issue in assembly is that computations are defined in terms of other computations, or that computations are large enough that we must use a register for multiple unrelated purposes; and to do so without losing information, we must put that information into a buffer for recall later. for example, for x86, i may `add` which sets `eax`, and i may want to use that value later and still add some other values; i must store the result of the first add somewhere. i may put it on the stack, or in an otherwise unused register, such as a general purpose one. opcodes take their args from certain registers. functions, as invoked by `call`, take their arguments from the stack. unlike the registers, each of which may store a value, the stack may store any number of values (up to hw, ofc.) registers are powerful (because of their speed as used by opcodes) but stacks are flexible.

this issue of needing the registers for multiple unrelated purposes while preserving data is the same as scoping, generally a concern of programming languages. the stack is the solution in assembly. stack machines use the stack for all computations so that we don't have to decide when to use the stack or not. the stack does not have scoping concerns, but instead requires the stack to be manipulated to keep its elements in proper order.

a stack program is homoiconic: a sequence of _words_ pushed to the stack. words are un/quoted (sequences of) symbols, or datum literals. words are executed when encountered, or pushed as literals if quoted. 

subprograms are common in stack machines. quoted programs are just literal word sequences but may be evaluated later.

the de facto elegant turing-complete set of 6 stack primitives is: `eval`, `quote`; `drop`, `dup`; `compose`, `swap`. there is the basis {`cake`,`k`}, which i haven't looked at; see von thun's paper.

concatenativity: stack programs—the sequence (composition) of subprograms (word sequences)—is associative; a program can be split at arbitrary points, and each of those subprograms evaluated, then their results replacing where they were in the original sequence, and that sequence, when evaluated, produces one result. this enables parallelism exceedingly well!

== statistics

* `USE: math.statistics`

.about quantiles

from the `math.statistics` module source code:

quantile can be any n-tile. quartile is n = 4, percentile is n = 100
a,b,c,d parameters, N - number of samples, q is quantile (1/2 for median, 1/4 for 1st quartile)
<https://mathworld.wolfram.com/Quantile.html>

so the various 8 builtin quantile functions (notice that `quantile2` is missing!) all do the same thing, but are different implementations, giving slightly different values due to how they round, interpolate, or resolve real-valued indices. only `quantile1` & `quantile3` always return values from the input distribution.

therefore if you simply want the element at the xth percentile: `percentile ( seq p -- elt ) 1array quantile1 first` *where `p` is a rational*.

== database

query tuple slots accept values for which `where` (of the `db.queries` vocab) has a method. you can read the method definitions to see how the data are transformed into query clauses.

[source,factor]
----
USING: db.sqlite db db.tuples db.types ;
[ f ! any value
  { "vietnamese" "bingsu" } ! any of these multiple values. the `sequence` instance of `where`
  NULL ! attr is null
  8 [a,inf] ! attr>=8. the `interval` instance of `where`. you can see inside the method's definition that infinite intervals are handled specially.
  mytuple boa
  >query "genre" >>order select-tuples . ] with-my-db
----

you can view a representation of the select, update, or delete statement, computed of the tuple, that will be passed to the db by using `<select-by-slots-statement>`, `<update-tuple-statement>`, or `<delete-tuples-statement>`.

(because sql's syntax is the same for selecting from tables as it is from views) you can select from views just as well:

[source,sql]
----
CREATE TABLE x(a,b);
CREATE TABLE y(b,c);
CREATE VIEW z(a,b,c) as select * from x natural join y;
insert into x values(1,10),(35,50);
insert into y values(10,103),(50,90),(1024,2048);
----

[source,factor]
----
: wdb ( q -- ) "~/test.db" <sqlite-db> swap with-db ; inline
TUPLE: z a b c ;
z "z" { { "a" "a" INTEGER } { "b" "b" INTEGER } { "c" "c" INTEGER } } define-persistent
[ z new select-tuples ] wdb .
----

----
{ T{ z { a 1  } { b 10 } { c 103 } }
  T{ z { a 35 } { b 50 } { c 90  } } }
----

you can't create intervals of timestamps, which is bad design, since timestamps implement `<=>`; for intervals to use only numbers instead of comparables is limiting without benefit (afaict). the only good work-around is to get all results then filter them via `filter`. `sql-query` not only returns an array of strings. even `[ mytuple slots>tuple ] map` isn't smart enough to parse the right types.

NOTE: if some attribute values may be greater than 2^32^, then use either `SIGNED-BIG-INTEGER` or `UNSIGNED-BIG-INTEGER`; you'll get incorrect values if you use `INTEGER` and try to select a tuple whose attribute has too large a value.

of course, we can instance `where` as we like e.g:

[source,factor]
----
USE: strings.parser
TUPLE: like str ;
M: like where str>> over column-name>> 0% " like " 0% bind# ;
SYNTAX: LIKE" lexer get skip-blank parse-string like boa suffix! ;
[ f f f f 8 LIKE" %belt%" NULL entry boa select-tuples . ] with-my-db
----

the code that really makes `select-tuples` work is the `<select-by-slots-statement>` hook. it specifies how a tuple is converted into a sqlite statement. we don't need to bother understanding `make-query*`; just do `>query` to make your usual query tuple into a `query`, then set its attributes e.g. `[ my-query-tuple >query "column4" >>group select-tuples . ] with-my-db`.

`<prepared-statement> execute-statement` may be useful somehow. and why is that hard to use compared to `sql-command` despite it being higher-level?

TODO: how to use binds? e.g. how can i "insert or ignore into tbl values($1,$2,$3)" with a tuple `{ "bats" now f }`?

=== `NULL` values

[source,factor]
----
: tdb ( q -- ) "test.db" <sqlite-db> swap with-db ; inline
TUPLE: tt f1 f2 ;
tt "t" { { "f1" "f1" INTEGER }
         { "f2" "f2" INTEGER } } define-persistent
[ 10 NULL tt boa insert-tuple
  100 200 tt boa insert-tuple
  tt new select-tuples .
] tdb
----

prints `{ T{ tt { f1 10 } } T{ tt { f1 100 } { f2 200 } } }`. thus `tt new select-tuples [ f1>> ] map` returns `{ f 200 }`.

=== updating tuples

the `+primary-key+` modifier cannot be specified in `define-persistent` e.g.:

[source,factor]
----
TUPLE: mytuple a b c ;
mytuple "mytable" { { "a" "a" INTEGER +primary-key+ } { "b" "b" VARCHAR } { "c" "c" INTEGER } } define-persistent
----

is incorrect despite no error being thrown. the trouble is that +primary-key+ is a union class, not a singleton class. as such, it represents a set of things. the way that `define-persistent` works is that any primary key—any one of those singleton classes—may be specified in any one column. 

anyway, incorrectly using `+primary-key+` allows `insert-tuples` to work as expected: trying to insert a tuple with an extant primary key appropriately raises the "constraint violation" error. however, `update-tuple` fails with a strange error: "incomplete input."

aside: as it turns-out, this is an error provided by sqlite. the sqlite c library sets an error pointer to that string, which is then read by factor's sqlite alien binding. searching the net for "sqlite incomplete input" mostly shows people with syntax errors trying to execute sql statements. understanding why factor is throwing the error for such a simple case clearly requires in-depth knowledge of its db vocab, potentially the sqlite-specific part.

also, unlike the sql `UPDATE` statement itself, `update-tuples` requires a primary key to be set.

[source,factor]
----
[ "create table if not exists mytable(a integer, b text, c integer, primary key (a,b))" sql-command
  T{ T f 10 "hi" 70 } insert-tuple
  T{ T f 10 "hi" 700 } update-tuple
] with-my-db
----

if you try updating a tuple whose primary key does not exist, e.g. `T{ T f 20 "hi" 700 } update-tuple`, then nothing happens.

.the three primary key modifiers

`+user-assigned-id+`:: you created a table with a primary key by some method other than `db.tuples`, and you're specifying to `db.tuples` that your table has a primary key. you may specify `+user-assigned-id+` in any one column; it doesn't matter which.
`+db-assigned-id+`:: if your tuple has a `f` in the primary key slot(s) then the sql db chooses a value
`+random-id+`:: if your tuple has a `f` in the primary key slot(s) then factor chooses an arbitrary value

also, apparently you can totally do the following in sqlite:

[source,sql]
----
create table dat(x text, y integer, w text, primary key (x,y));
insert into dat values('HOLLA',NULL,NULL);
insert into dat values('HOLLA',NULL,NULL);
insert into dat(x) values(null);
select * from dat;
┌───────┬───┬───┐
│   x   │ y │ w │
├───────┼───┼───┤
│ HOLLA │   │   │
│ HOLLA │   │   │
└───────┴───┴───┘
----

this surprises me for two reasons:

. `y`, despite being part of the primary key, is allowed to be null
. the same (x,y) are allowed, when i guess _any_ one or more of the primary key values is null?

but then

[source,sql]
----
insert into dat values('cat',10,'bat');
insert into dat values('cat',10,null);
----

but _now_ we get a constraint violation error! anyway, weird.

NOTE: on one occasion, `update-tuple` simply would not update the last column, which i'd created by using `alter table ... add column`. i have no idea why, and i could not recreate the behavior in a fresh test db.

== audio

uses OpenAL/FFI. supports raw pcm, vorbis, aiff, wav. you can also programatically generate pcm. 3D audio such as one would desire for games is built-in. if you are only concerned with playing audio such as a music player does, then ignore the spatial audio tuples and assume their default values.

=== tuples

* `audio`: represents pcm. fields: `channels` (e.g. 2 for stereo), `sample-bits` (e.g. 16), `sample-rate` (e.g. 48000), `size` (length of data) `data`.
* `audio-engine`: like a server responsible for playing audio clips. unless you want to specify an output device yourself, you'll create an audio engine by `f n <audio-engine>` where `n` is the number of tracks that the engine may need to play simultaneously.
* `audio-clip`: actual audio data. it's an opaque type; don't try to construct it yourself nor should you be concerned with its attributes.
  ** `static-audio-clip`: audio which can be loaded at once, such as a song file.
  ** `streaming-audio-clip`: audio generated dynamically (in realtime), such as procedurally-generated audio
* spatial audio:
  ** `audio-listener`: represents volume and location for 3D spatial audio. fields: `position` (a 3D vector), `gain` (volume), `velocity` (for doppler effect; useful for games or simulations), `orientation` (see below; just use the default).
  ** `audio-orientation-state`: 3D, not necessarily normalized vectors that represent the user's position for 3D spatial audio. fields: `forward` `up`. idk why one would ever not use the default unit vectors.
  ** `audio-source`: firstly, "source" does *not* mean "i/o source!" "source" refers to spatial audio. a trivial implementation of the _audio source protocol_—a collection of generic words which control spatial audio: those implemented trivially by `audio-listener`: `position`, `gain`, `velocity`; and the rest: `audio-relative?` (whether velocity & position are coordinates relative to listener vs absolute coordinates), `audio-distance` (distance below which a listener hears a sound at full volume), `audio-rolloff` (degree to which audio lessens as listener is away from the source).

.`audio` vs `audio-clip`

these are two different types! `<static-audio-clip>` & `<streaming-audio-clip>` convert ``audio``s into ``audio-clip``s, which are then actually usable by the `play-clip` &al words. *remember that `start-audio` does not start playing audio objects!* `audio` objects cannot be played; only ``audio-clip``s can be played! i suppose that the `audio` tuple is revealed to the coder only so that they can implement their own audio types, e.g. enabling loading ape's or mp3's. for the most part, you will not concern the `audio` type! furthermore, i assume that it's only for technical reasons that it's ever possible to produce an `audio` object rather than directly producing the actually usable corresponding `audio-clip` object!

`<static-audio-clip>` & `<streaming-audio-clip>` both take an engine [read: audio server] and source [read: location].

where they're different:

[options="header"]
|======================================================
| type      | 3rd param       | 4th param
| static    | `audio` object  | whether to loop forever
| streaming | audio generator | number of buffers
|======================================================

.wav & aiff vs vorbis

wav & aiff are static audio. vorbis is a stream. thus you load wav or aiff objects as ``audio``s by `read-audio` then convert them into ``audio-clip``s by `<static-audio-clip>`. however, the ogg/vorbis loader reads into a stream, which implements an audio generator, even if you're reading from a file! idk why it's designed like that. `<vorbis-stream>` reads from a binary input stream, in case you really are streaming some vorbis-encoded audio. for files, use `read-vorbis-stream` then convert into an audio clip by `<streaming-audio-clip>`.

=== words

engine:

* `start-audio` (a convenient version of `start-audio*`) starts [read: enables] an audio engine. it must be started before any audio can be played.
* `update-audio` applies any new values of spatial audio dynamic variables and refills `streaming-audio-clip` buffers. you'll call `update-audio` only if you used `start-audio*`, not `start-audio`.
* `stop-audio` stops an audio engine. i do not know why someone would want to do this.

the clip words are self-explanatory.

=== spatial audio

the audio engine automatically reads dynamic spatial audio variables each time that it updates, so just update your spatial audio attributes and they're automatically practically immediately reflected in the audio that's playing.

`<streaming-audio-clip>` & `<static-audio-clip>` return `audio-clip` objects. aside from calling `play-clip` &al on them, you can access their `source` attributes then update their source values to change the volume, position, &al.

=== loading & playing audio

* `read-audio` (of the `audio.loader`) vocabulary is the easy way to load wav or aiff audio from filepaths. examine dynamic variable `audio-types` to see exactly which file extensions it supports and which words it uses to load each type of audio file.
* `start-audio*` is useful only if you want to synchronize the audio with some other timer e.g. a game loop: updating the audio buffer in sync with updating game world state and painting to the screen.

.simple example

just load & play some audio. no spatial audio nor manual timers or anything:

[source,factor]
----
USING: audio.engine audio.vorbis destructors ;
f 1 <audio-engine> dup                                          ! engine, which i've started. dup again to retain on the stack for disposal later
dup start-audio                                                 ! gotta start the engine before doing anything else!
audio-source new                                                ! default audio source, because we aren't using spatial audio
"/path/to/song.ogg"
stream-buffer-size                                              ! a constant defined to 4096. guess that's a good buffer size.
read-vorbis-stream                                              ! vorbis, so it's an audio generator
3                                                               ! (1)
play-streaming-audio-clip                                       ! convert the audio generator into a `streaming-audio-clip` then play it
drop                                                            ! drop the audio clip. the engine remains on the stack. i can call dispose on it before or after the song is done playing
----

note (1): i don't know how to determine in advance how many buffers to use. the audio didn't play with 1 buffer, and i got really confused about why it wasn't playing, since i was just starting-out trying to use the audio vocab. at 2 buffers just a second or so of the song played before i got silence. 3 worked fine. what also worked was using 2 with doubling the buffer size.

.the audio.engine.test code, annotated
[source,factor]
----
! Copyright (C) 2009 Joe Groff.
! See https://factorcode.org/license.txt for BSD license.
USING: accessors alien.c-types audio.engine audio.loader
calendar destructors io kernel math math.functions math.vectors
random ranges sequences specialized-arrays timers ;
SPECIALIZED-ARRAY: short
IN: audio.engine.test

TUPLE: noise-generator ;

M: noise-generator generator-audio-format
    drop 1 16 8000 ;
M: noise-generator generate-audio
    drop
    4096 [ -4096 4096 [a..b] random ] short-array{ } replicate-as
    8192 ;
M: noise-generator dispose
    drop ;

CONSTANT: location1 T{ audio-source f {  1.0 0.0 0.0 } 1.0 { 0.0 0.0 0.0 } f }
CONSTANT: location2 T{ audio-source f { -1.0 0.0 0.0 } 1.0 { 0.0 0.0 0.0 } f }
CONSTANT: location3 T{ audio-source f {  0.0 0.0 0.0 } 1.0 { 0.0 0.0 0.0 } f }

:: audio-engine-test ( -- )
    ! read data from files into `audio` objects
    "vocab:audio/engine/test/loop.aiff" read-audio :> loop-sound
    "vocab:audio/engine/test/once.wav" read-audio :> once-sound
    0 :> i                       ! ! time variable. used later to vary locations with time.
    f 4 <audio-engine> :> engine ! an engine which can play up to 4 clips simultaneously
    engine start-audio*          ! start the engine before doing anything else!

    ! convert the audio objects into clips
    engine location1 loop-sound          t play-static-audio-clip    :> loop-clip
    engine location2 noise-generator new 2 play-streaming-audio-clip :> noise-clip

    [ ! every 20ms: update spatial locations then play audio
        i 1 + i! ! increment time variable
        ! compute new locations as functions of time
        i 0.05 * [ sin ] [ cos ] bi :> ( s c )
        loop-clip  source>> { c 0.0 s }          >>position drop
        noise-clip source>> { c 0.0 s } -2.0 v*n >>position drop

        i 50 mod 0 = [ engine location3 once-sound f play-static-audio-clip drop ] when

        engine update-audio ! we must call update-audio b/c we used start-audio*, not start-audio
    ] 20 milliseconds every :> timer
    "Press Enter to stop the test." print
    readln drop
    timer stop-timer
    engine dispose ; ! always dispose of your engine to avoid memory leaks!
----

if you don't dispose your engine, then not only is it a memory leak, but also it'll remain in your operating system's list of audio streams (as seen in e.g pavucontrol on linux, or the list of applications with sound streams in windows), polluting that list.

== efficiency/tuning

aspects: speed, storage. you can use efficient factor code, or interface with an efficient `alien` [ffi] library. if you must write the code, then you'll likely prefer a stack/cat lang, so writing c, fortran, or asm would be kinda uncomfortable.

* you can write assembly inline in factor via `alien-assembly`

=== storage

* for c ffi: structs (`classes.struct` vocab), byte arrays, specialized arrays. these all are unboxed machine values.
* tuple arrays are like how sql relations would be implemented in apl: it is effectively an array of tuples, but it's stored unpacked (no pointers) i.e. instead of storing an array of pointers to tuple objects on the heap, it stores tuple values in memory directly.
  ** a struct stores a single tuple as an array, so it's practically a singleton tuple array. structs are more tuned: they require you to specify each slot's type in a way where its size is statically known.
* simd vectors, and _specialized arrays_ (unboxed/packed) and tuple arrays. this should make factor a good lang for hacking binaries. by the optimizing compiler, operations on tehse binary structures can approach c's speed. un/boxing is implicit.

specialized arrays' valid c types are described in `alien.c-types` vocabulary's documentation. many of the words are undocumented and seem redundant, e.g. `u16`, which i can only guess is equivalent to `ushort`.

.how to choose specialized arrays, structs, or tuple arrays

* c ffi => not tuple arrays
* not storing the same size of data in each tuple => tuple arrays
* passing struct to a c fn => struct
* storing an array of structs => specialized array (or tuple array if you prefer that interface and don't care so much about efficiency nor types)

at this point the question is whether to store an array of structs, or instead, a structured array for each struct attribute: `struct(a,b,c)[n]` vs `a[n], b[n], c[n]`. this is actually a stupid question. this is 2-dimensional data. are indices are attribute name and index. we may write it as (tag,idx) or (idx,tag). consider the general version of this problem: how should we store n-dimensional data? the original question does not generalize sensibly because there are far too many possible ways to nest arrays & structs. this reveals that having multiple (here two) equivalent devices is bad design because it creates this "needless choice" problem in which all choices are equally good, but the code differs, which creates syntactic asymmetries, which makes coding & refactoring harder.

thus the conclusion is: if you're storing one struct, then use the struct type; if you're using an array of structs, then use a specialized array per struct attribute.

by the way, the ideal way to store a set of n-dimensional data is as a map from an index dictionary to subset e.g. the index `{name='jim',age>26}` to whatever elements correspond. this is a query satisfied efficiently by traversing a database of facts stored as predicates, reducing the total set to a subset until all predicates are satisfied. this is exactly what prolog does. how to make this traversal efficient depends on the data; it can be specialized, but there are surely various good general schemes, too.

.using specialized arrays

* they support the following methods: `clone`, `length`, `like`, `new-sequence`, `resize`, `nth`, `set-nth`.
* the fact of them being sequences means that they can be used with `map-as` or any word that uses `like` and/or `new-sequence`.
* see document "Specialized array words" for other useful words.

.using tuple arrays

`0 <class-array> map-as` produces an a "cannot call run-time computed value" error. you must do `map >class-array`.

==== encoding numbers

the `endian` vocabulary has efficient words for coding (un)signed integers and floats as byte arrays of either endianness. `>le` &al's 2nd parameter is the number of bytes needed to encode the number. it truncates or pads as appropriate: padding: `10 2 >le` => `B{ 10 0 }`; truncation: `260 1 >le` => `B{ 4 }`; normal: `2000 2 >le` => `B{ 208 7 }`. notice how negative numbers are encoded: `-5 2 >le` => `B{ 251 255 }`. it's 2's complement of the positive version. `bitnot` is equivalent to `neg 1 -`. because factor fixnums are not stored as fixed-size bit arrays, if you want to flip the kth bit of some length n representation, then you must convert them to a length n bit array or byte array via the `integer>bit-array` (which determines number of bits from  `bit-length` i.e. basically `log2 1 +`) or `>le` &c words, then flip bits either by using boolean words for bit sets, or manually by using words like `bitxor` (or, more conveniently, words of the `math.bitwise` vocab), possibly using a masks such as `255 bitand` as necessary.

NOTE: remember that factor supports syntax like `0b1001`. also, `.b`, `.o`, `.h` print in binary, octal, or hexadecimal, and that you can set dynamic variable `number-base` to make all pretty-printed numbers display in a given base. because they use dynamic variables, they work for byte arrays, too: `2024 03 28 julian-day-number 3 >le .h` prints "B{ 0xee 0x8a 0x25 }". even values shown in the walker are in the set base!

==== writing binary files

being that binary files are so particular, it'll often be useful to overwrite only some bytes in a file. to do this, open the file in append mode then seek to the position that you want to begin overwriting, then write. "append" also means "overwrite", here. if you must replace a subsequence of bytes, starting at a position _p_, by a different-length one, then you must rewrite all the bytes of the file starting at _p_ e.g. the most efficient way to replace "c" in "abcde" by "xy", resulting in "abxyde", is to leave "ab" as it is, seek to "c", load "de" into a buffer, write "xy", then write the buffer contents.

in factor, we use `seek-input` & `seek-output`. the seek types that they take are `seek-relative`, `seek-absolute`, and `seek-end`, as found in the doc for `stream-seek`. if you give a position past the end then the between subsequence is zeroes. *to use seek, you must use the `binary` encoding class, and you must write a sequence of class that implements `element-size` e.g. `byte-array` or `uint-array`, but not strings!* see the example:

.replace last n characters of a file
[source,factor]
----
: overwrite-file-end ( str path -- )
  binary [ >byte-array dup element-size 1 + neg seek-end seek-output write ] with-file-appender ;
"ZE" "/home/nic/samplefile" overwrite-file-end ! initial file content was "stuff"; now it's "stuZE"
----

important notes:

* the file did not end with a newline. if you're getting off-by-1 errors, check whether anything writing the file is writing a newline
* i seeked to one byte before the string's length because `0 seek-end` sets the position to append to the file's end
* seeking is sensible only in append mode / with a file appender, not in write mode / with a file writer. `<file-writer>` opens a file with mode `"wb"` (see `fopen(3)`) which truncates the file (discards its entire content then seeks to 0). `<file-appender>` uses mode `"ab"` which opens in append mode, which really should be called "overwrite mode", even though it does start the stream position at the file's end. the `b` is for _binary_, btw. append/overwrite overwrites anything already existing or else appends, expanding the file size.

tl;dr: think of `with-file-writer` as "set file content" and `with-file-appender` as "modify file content."

==== reading binary files

think of the integer argument passed to `seek-absolute` as "skip this number of bytes, then read." the same is true of writing, naturally. with a file called `TEST` whose content is `12345` followed by a newline, the following makes it `123ZZ` followed by a newline:

[source,factor]
----
"TEST" binary [ 3 seek-absolute seek-output "ZZ" B{ } like write ] with-file-appender
----

==== inspecting binary files

i prefer link:https://justine.lol/braille/[bd aka braille dump] for hex dumping (until i write my own), or hexyl if i want to print only part of a file. like other hex dumpers, bd.com _squeezes_ its output: it puts asterisks to abbreviate series of duplicate lines (most commonly when each of the lines is all zeroes). maybe this started with hexdump(1) or something earlier. in this example, i write 201 bytes of zeroes.

.`"newfile.txt" binary [ 200 seek-absolute seek-output B{ 0 } write ] with-file-writer`
------------------------------------------------------------------------------
00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  │▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁│
*
000000c9
------------------------------------------------------------------------------

.`"newfile.txt" binary [ 200 255 randoms B{ } like write ] with-file-writer`
------------------------------------------------------------------------------
00000000  c4 27 79 97 4e 1b 3c cb  7d f1 ee 17 75 63 b2 17  │⠪'y⣡N←<⣎}⡛⢾↨uc⢉↨│
00000010  97 81 4e 24 ea 59 b8 40  fc f2 05 b4 80 48 d1 c2  │⣡⡀N$⢞Y⠍@⠿⢛♣⠩█H⡑⢊│
00000020  7b 48 02 1d 67 1b 3b f8  7b ec 7d 59 63 57 be 3e  │{H☻↔g←;⠟{⠾}YcW⢭>│
00000030  ce 70 43 f1 ba e8 68 f1  c8 31 0f 26 bf 7e 57 0a  │⢮pC⡛⢍⠞h⡛⠎1☼&⣭~W◙│
00000040  d8 5c 40 96 0c 3c e6 cf  c9 e7 c4 f7 ea da d4 f0  │⠕\@⢡♀<⢺⣮⡎⣺⠪⣻⢞⢕⠱⠛│
00000050  ba 9b 23 d5 28 7e e9 14  45 b0 24 59 09 cc be db  │⢍⣅#⡱(~⡞¶E⠉$Y○⠮⢭⣕│
00000060  39 7e 7f 6e 81 59 f9 6f  43 41 6b d5 c3 8e 23 a5  │9~⌂n⡀Y⡟oCAk⡱⣊⢤#⡣│
00000070  1d 98 d4 cc 08 0e 5e 63  08 94 87 70 95 f3 8f 86  │↔⠅⠱⠮◘♫^c◘⠡⣠p⡡⣛⣤⢠│
00000080  65 da 2d 90 88 8e b4 e5  ca 45 83 f5 6e 9e 77 c3  │e⢕-⠁⠄⢤⠩⡺⢎E⣀⡻n⢥w⣊│
00000090  56 79 23 09 b9 3e 1b 8a  cc 82 2e a4 93 14 3e 14  │Vy#○⡍>←⢄⠮⢀.⠣⣁¶>¶│
000000a0  7d 90 75 3c f1 48 38 6d  6b f9 27 40 c4 a5 6c da  │}⠁u<⡛H8mk⡟'@⠪⡣l⢕│
000000b0  88 8b 29 dd 4c ba e5 bd  ea 3c 09 57 d4 d1 bf c6  │⠄⣄)⡵L⢍⡺⡭⢞<○W⠱⡑⣭⢪│
000000c0  3f f5 4d 01 bf 59 ca ae                           │?⡻M☺⣭Y⢎⢧│
000000c8
------------------------------------------------------------------------------

i note this so that you won't be confused like i was when i saw so few bytes but `wc -c` said 201 like i'd expected.

== PRs

* make db.tuples' `do-each-tuple` row-polymorphic, thus enabling things like indexing rows
* when loading a program, list all errors (namely unfound symbol errors) at once, and suggest import locations as the listener does, outputting an import string like when you try _running_ a program. my example is loading `.factor-rc`; i was told many times that a word was "found in the current vocabulary search path."
* make a word to reload `.factor-rc` without restarting the listener (actually, i think that i encountered this already but forgot what the word is called)

== factor hackery ("fhacktory")

=== (custom) images

bootstrap:: create an image from source rather than as a memory (heap) snapshot of a running factor process. see "Bootstrapping new images" doc.

NOTE: when saving images for quickly resuming state, i advise that you never overwrite the original image file; generally, it's good to preserve all canonical versions, and modding only copies.

the image saves only the heap, not the stack. afaict yet, custom images' priamry use is saving time by effectively caching precompiled preloaded vocabularies. i haven't learned anything about the bootstrapping process yet, though.

you can't load an image in the listener; you must invoke factor with the `-i` parameter.

a neat thing: when you load from an image, it sets `image-path` which is used by `save`, so factor tracks which session you're in, so that `save-image-and-exit` is akin to hibernating.

TIP: `save` saves the ui theme, so you can associate a theme to each session/project so that when you start factor, you can immediately know which projcet image is loaded. fonts aren't saved, unfortunately.

NOTE: `image-path` is a word that gets the value of dynamic variable `\ image-path`! note the preceeding backslash to treat `image-path` as a symbol instead of invoking the word!

TODO: somehow noly my saved image cannot find the `trees.avl` nor the `aws` vocabularies. why? why are some vocabs present but others not? i suppose that i should learn more about exactly what images are and how vocabularies relate thereto, and generally work, before i expect to be able to answer. maybe it had to do with me deleting the original image then rebuilding it....

=== modding the listener

see `ui.tools.listener` vocab.

=== general

* open vocabs in your text editor; see a section on text editor integration, and TODO: write that section if i haven't already
* poke around the following vocabs:
  ** inspector
  ** ui.theme
  ** help.vocabs

=== theme-change hack

in the listener, evaluate:

[source,factor]
----
SYMBOL: S_
get-datastack S_ set-global
dark-mode listener-window
----

then, in the new window: evaluate `S_ get-global set-datastack`.
