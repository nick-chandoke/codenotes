== factor

=== factor: good or bad?

TODO: now knowing array & relational methods, try again to write a program that extracts the common prefix from two strings then returns it with the remainders of each of the two strings, in factor. use this exercise as an exploration of why factor is a pain to use despite being a stack lang. then write it in racket, too!

.introduction

factor is hardly a language; it's homoiconic just like prolog and lisp; as such, there is no _language_; there's only data and a repl; the repl has a hardcoded traversal & evaluation model for its input data. thus its input data are implicitly programs exactly because they're evaluatable as such by parsers (here, interpreters).

such a general/free models hardly suggest any idiomatic style. indeed, one can make whatever model they wish by making even whatever syntax they desire via metaprogramming. factor is, like lisp and unlike prolog, reductionist & functional but, like any system, at least prolog can be implemented in it. factor should be thought of as lisp except with a different model for relating functions' inputs & outputs:

* in lisp, inputs are specified inline, thus nesting expressions in an ast. in factor we order them linearly, often in advance of when they'll be evaluated. in lisp the evaluation is a depth-first traversal of a tree, whereas in factor it's a loop of fn application until the stack is empty; in factor the program is eaten-up whereas it's traversed in lisp.
* a funny thing about stack langs is that the stack relates all of its elements, whereas functions' arguments are distinct. we see this in functional combinators vs stack effect combinators such as `2tri`, which does not associate each of 3 functions with each of two inputs—the sequence [(f,x,y), (g,x,y), (h,x,y)]—then evaluate each triple of that sequence; instead, it performs stack effects `f`, `g`, & `h` in a given order, which means that the effect of the earlier-executed ones can affect the inputs of the latter-executed ones. this is very much a frequent hassle when using `if`, `cond`. the equivalent in scheme would be an effectful predicate, e.g. using `set!`. one can argue that this is more capable because it enables relating clauses, but that's often not what we want. as much as ever, separation & complection should be specified explicitly & elegantly by relations, where relations are implicitly entailed by terms being present across predicates.
* whenever a computation is to be performed once but its output passed to multiple locations, lisp requires binding clauses, and factor requires `dup`.
  ** btw, in prolog, rather than binding clauses, variables are related implicitly by their presence in predicates which are intersected with others. *this shows intersection as essentially the same as relation itself, and relation the same as application or composition. e.g. we see (a,b;c;d;e) as `map a [b,c,d,e]` becasue it's `a` intersected with a disjoint union; `a` is related to each element of the disjoint union. this shows disjoint union as a set of distinct elements. `AND` is application & merging whereas `OR` maintains distinctness/separation. all code generally is merely "these distinct" vs "these together!"* i suggest the prolog notation of `,` for "together" and `;` for "apart." and have fns partially applied b/c that can only make things easier. when a fn is loaded with args, it either has a deterministic arity at which point it evaluates (if we're using a reductionist model); or if its evaluation must be made explicit, then relate it to a special primitive that exists expressly to force the fn to eval. i don't think this is practically possible in a stack model, though.
* TODO: i know when `dup` is needed; but when are other stack shuffle words needed? what do they correspond to in lisp? and which shuffle words are essential vs merely convenient? it seems like we need only adding:{pushing, `dup`}, removing:{`pop`}, and reordering:{`swap`} (is it technically possible to structure any program to not need `swap`?) and if `swap` is needed, can we make do with only swapping the top 2 elts?
* in factor, because it evaluates in the order which it reads—left to right—functions must be quoted in order to not be immediately evaluated. in a lang such as uiua, where evaluation is from the right but parsing is from the left, and perhaps enabled by a lack of metaprogrammability, function arguments do not need to be quoted; when parsing from the left, when a higher-order function is encountered, its stack effect (fn signature) tells how many of the following stack items should be interpreted as functions and not immediately evaluated.

NOTE: lisp is the de facto applicative functional notation, and factor is nearly the de facto stack functional notation (technically the _Joy_ proglang is the de facto). there are many varieties of each lisp and forth, and to some lesser extent, prolog.

the stack is nice when it's nice, and it's not when it's not. to say that it's always good is silly. sometimes we like exploiting order, and sometimes we don't. sometimes the stack is good; sometimes another structure of different dimensionality or defining or natural [implication] property is good. choose appropriate structures for each situation.

.actual evaluation

there's a certain amount of trouble in any design—except maybe prolog; that's yet to be known. the only solution to this is a parser/dsl paradigm: to have a plethora of evaluation models and syntaxes which all share the same underlying model. the reason that prolog may be necessarily ideal is that its model—facts expressed as relations & constraints of free vars—is exactly the general substance of meaning itself. the only trouble then is that general systems, by definition, have little information encoded in them; this means that _we_ must specify information rather than it being tacit. this being said, metaprogrammable models are suited for creating such dsls. languages that can modify themselves dynamically (during runtime) are most free. commonly lisp cannot do this; picolisp is the only one that i know of that supports it. forth, maybe factor, and prolog support it.

the good news is that we can impose models, such as the array model, which does not affect the basic case e.g. `1+2` is `3` regardless of whether the array model is imposed or not. yet things that would otherwise be nonsense (uninterperable) are interpreted sensibly by it e.g. `1 2 3 + 4` produces `5 6 7`. we can freely union additional orthogonal parsers (orthogonal meaning here that each parser's parsing expressions do not overlap) without worry about changing the interpretation (meaning) of our code. we're also free to install new non-orthogonal models and compute the overlap then choose the order in which sets of overlapping rules are tried, and we can run it on code to identify which subsets of our code's meaning may change by installing the new parser. obviously <installing a new parser whose rules are tried only after the prior parser's overlapping rule fails> will affect only if the original parser fails, which may or may not be expected in your code, depending on how you wrote it.

''''

[TODO]
* how to do haskell style applicative programming in factor? there's power & terseness there, enabled by global ad-hoc definitions (especially how things compose/relate) and function composition (namely unordered composition.) adts & type classes are at the heart of such programming! factor oop is equivalent. think of how `either` has left as associated state and is a [bi]functor.
* concurrency
* cf link:https://forth-standard.org/[forth] viz wrt efficiency
* check-out vocabs: `models`

=== usability

better than app.prog but still inferior to sql because sql is symmetric and implicit, whereas the stack relates all elements to each other, positionally. chunks of code are not cleanly separated into statements like they are in sql.

factor improves on app langs by using the stack instead of `let` clauses, which nest and lack ordinal position which connotes when they'll be used (items near the top of the stack being used now.) it's nice to see a factor stack reduce, the elegance of data being _left_ on the stack for use by subsequent words. but we coders must still position words correctly. rather than defining a _set_ of edges, we must sequence mutations ourselves, and the stack's elegance really isn't so great when we're doing remotely complex control flow, such as: suppose that three data top the stack; then there's a `cond` block where one or two of the data optionally mutate the third. fortunately factor supports `locals` syntax, ... but that's really less elegant than sql.

fortunately factor can use sql for great computations, and comes with easy-to-use has set-theoretic functions. yet one must always be careful to limit themselves to these functions, not being guided to factor's default designs of plurality-dependent operations, things like `find` instead of `filter`, or nesting looping words.

factor programs can be difficult to overview; being able to overview them depends on naming words well. this is stylistic and not difficult, but also can't be done systematically and there's no natural solution to this; the ways to do this well are quite arbitrary.

''''

why does this error:

[source,factor]
----
USING: accessors combinators command-line io io.encodings.binary io.files io.launcher kernel namespaces prettyprint sequences ;
IN: MODULE

: main ( -- )
<process> command-line get-global last ! sqlite tmp file path
{ "gpg" "-er" "nic" "--output" "-" } over suffix swap .s clear ! >>command binary <process-reader>
! binary [ write flush ] with-file-writer
; MAIN: main
----

? if i replace `clear` by `drop drop drop` then `.s` does nothing. wtf? i can replace it all by `. . .` which prints the stack with tap of stack printed first.

.common mistakes

* builtins can't be found. you need to import them. for `cond`, `USE: combinators`. for `+` (yes, fucking *plus*) you must `USE: math`. for basic stack words, you `USE: kernel`. never assume that _anything_ does not require using some vocab.
* no output / print does nothing, especially when near `exit`: use `flush`.
* using factor when sql can do
* forgetting `get` after a symbol; remember that symbols are symbol literals and are not themselves dynamic variables, though they can be used as such
* mixing `set-global` & `set` or `get-global` & `get`
* using `::` but forgetting to put leading args
* using a quotation in `::` without `compose` or `call` (thus giving a larger return stack than expected)
* "cannot create slice from 1 to 0": slice on empty sequence. you probably did `unclip-slice` outside of an `if-empty`.
* `split-slice` "...does not support length". `split-slice` takes a sequence of split elements, not a single element!
* stack effect off by one when an `if` statement is involved: you probably quoted the predicate when you shouldn't've e.g. `dup [ pred ] [ t ] [ f ] if`

.when &c

* `p q when`: consume `p` and if `p` then `call` `q` else `drop` `q`. `p` is not passed to `q`; `when*` makes that so.
  ** `unless` is the same but with `p not`
  ** though these cannot leave new data on the stack, they can affect the stack by mutating things on the stack e.g. `dup empty? [ dup 0 swap [ 1 + ] change-nth ] unless` to increment the 1st element of a non-empty sequence.
* `unless*` has a different stack effect than the others; the others leave the stack how it was; they can be used only for programs of effect ( -- ) or ( x -- ) for `when*`. `unless*` leaves a new datum atop the stack. this is because it retains [dups] the predicate before checking its falsity. `a [ b ] unless*` means `a b or` but short-circuiting and accepts quotations rather than single values, or, more generally, `a b unless*` means `a b or` where `b` has effect `( -- x )` i.e. produces a value e.g. `x y [ dup ] unless*` leaves `x y` if `y` else `x x`

there's nothing like `Maybe`'s `fmap`. should there be? `: fmapMaybe ( ..a q: ( ..a x -- x ) -- y ) [ f ] if* ; inline`. would you ever want to preserve the `f`? certainly you may want to perform a mutation on the top of the stack if it's not `f`, and you may want to do that for multiple conditions. however, given that `fmap` leaves `f` if it starts with `f`, then one mutation occurring implies that the rest must also occur, and so they all can be combined into one mutation. therefore a more sensible word is one that operates on the stack unless its top is `f`, in which case the `f` is dropped: `[ ] if*`. however, this fails because the branches have different stack effects. `[ f ] if*` balances them, and is the definition of `fmapMaybe`. so it appears that we should have `fmapMaybe`. one must now choose between `fmapMaybe` and `when` depending on the stack effect. it'd be nice to have one word that drops a `f` value and one that consumes it and any other things. for example, the fact that we must code like the following is annoying:

[source,factor]
----
[ . ] [ ] if*
[ . . ] [ drop ] if*
[ . . . ] [ drop drop ] if*
----

`smart-if*` does not help because it relies on the predicate consuming a certain number of values, which cannot be done if the quotation has effect `( ..a x -- ? )`; in that case youd need `ndup` & `ndrop`, but in that case you may as well use `if` directly.

* stack effect problem for `when` (or `unless`): strange as it seems from looking at `when`'s definition, `P T when` is not equal to `P T [ drop ] if`. to understand: note, in `when`'s definition, that `[ drop ]` and `[ call ]` operate on the same object: `when`'s quotation! `when*` is the conveniently terser word for `t [ drop ] if*`. however, i've found myself most commonly doing `dup pred true [ drop ] if`, which discards the predicate but retains the subject of the predicate for use in the true clause. there is no builtin combinator for that. consider the following versions of it:

[source,factor]
----
:: with-if ( pred: ( x -- ? ) true-prog def -- y ) dup pred call true-prog [ drop def ] if ; inline

! PROG                                  ! OUTVAL     ! STYLE
4 dup even? [ 1 swap / ] [ drop 10 ] if ! 80         ! if pred(x) then f(x) else defval
1 [ even? ] [ 1 swap / ] 10 with-if     ! 10         ! shortened by 4 characters by with-if
0 [ even? ] [ 1 swap / ] [ 4 * ] tri ?  ! error: 1/0 ! tri & ?
4 dup even? [ 1 swap / ] [ 4 * ] if     ! 1/4        ! plain ol' if
----

* `smart-if*` can sometimes be useful
* the plain form is best. they're all pretty much the same number of characters, and the plain form makes specifying a default value as easy as a false-branch function, including `[ drop ]`. whatever the case, the false branch's stack effect must equal the true's.
* the `tri` form executes both branches, which can be problematic, and is inefficient
* `with-if` saves us from typing `dup` & `drop` each once, but definitely returns a constant in the false case rather than a function on, and isn't appreciably shorter
* even `?if` uses the condition's output, not the subject of the condition, in the true branch.

i guess that the expected idiom for `if*`, `when*`, `?if`, &c is `keep and` e.g. `obj [ pred ] keep and [ fn ] [ else ] if*`. that's not really better than `obj dup pred [ fn ] [ drop else ] if`. they're equivalent, though, so if you find yourself using `if*` &c, then use `keep and`.

the lesson is that `if` is the primitive selective evaluator and is perfectly simple and free, and it's not worth the time trying to find something nicer than plain `if`, except some obvious ones like `if-empty`. i'm unsure for stack machines, and especially specifically for factor's implementation, how `bi ?`'s speed compares with `if`'s. i imagine that naïve code is optimized well in any stack language, and especially in factor, which is designed to be fast. i'm not worrying about the efficiency of things like an `if` inside a `loop`. if you're so concerned about speed, and you can put a fixed size to your data, then use `math.vectors`, whose ops are auto-optimized to simd when possible; or use a gpu or array primitives implemented in factor.

=== design

==== language

"language?" ...data with an evaluation model is more like it.

* designed for metaprogrammability, simplicity, and flexibility/dynamicism, like elisp but better because the facilities available to the user are the exact same as are used to implement the factor language
* all factor metaprogramming is compile-time
* constrained design is generally bad (viz here using the stack—a quite constrained data structure). however, constraint is useful when we don't need to go outside the constraints anyway. in this way factor provides a simple model (stack) for the common cases but allows a simple arg-binding syntax for when that's more elegant.
  ** the stack's simplicity allows extremely efficient program optimization and execution strategies
  ** effectively implicit composition of arbitrary-arity functions
  ** stack based (also called _concatenative_) languages are usually superior to functional ones. factor's support for globals, mutable objects, and local binds make factor clearly a good language, certainly strictly better than any functional language
  ** there are no "void" words. ( ..a ... -- ..a ) is effectively void, but the "return value" is still `..a`, thus allowing composition of functions like `[ 1 + ] dup print [ 2/ ]`. no applicative language supports putting `print` or any other void function in a composition chain!
* not an array lang. lang features plurality. however, at least it uses virtual sequences, i.e. functions from index to element—especially _cords_, vseqs that appear as a concatenation but have O(1) concat
* stack
  ** neither functional nor stateful
  ** no scope. just position in the stack.
* macros are quotation monomorphisms, and their parameters must "known as constants" by the stack checker, though their values may be only dynamically known
  ** `inline` combinators may be partially applied to macros in one context so long as its parameters are appropriately compile-time e.g. `: length-case ( seq cases -- ) over length swap case ; inline`
* _functors_ are like macros but more powerful...? idk how they differ.
* lang is a core written in factor with a vm written in c++. factor began on the jvm, being used as a scripting lang for a larger java program.
* ffi can call c, fortran, and obj-c, and additional libs enable ffi w/js, lua, and c++. the ffi is easy (at least for c): just type the function e.g. `FUNCTION: SSL* SSL_new ( SSL_CTX* ctx ) ;`
* supports binary data well, viz as structs, simd vectors, and specialized [packed per data type] arrays. this should make factor a good lang for hacking binaries. by the optimizing compiler, operations on tehse binary structures can approach c's speed. un/boxing is implicit.
* the _destructors_ lib supports deterministic cleanup/finalization of {see §5) external resources (e.g. file handle, network connection). this contrasts the usual gc model.
* syntax macros are called _parsing words_. these words are evaluated at parse time and may perform arbitrary computations. the `syntax` vocabulary contains many.
* extremely good [syntax] macros!
  ** backslash is needed to refer to a fn without execution e.g. `\ drop` pushes `drop`; `drop` alone would execute it. `\ drop` is different from `[ drop ]`. idk why, aside from being slightly briefer, one would use `\` instead of quotation.
  ** quotations are sequences
  ** the following are is implemented as factor macros, so they're expanded before runtime: named local binds, square and curly brackets, quote marks, and colon for fndefs. (meta-circular)
* like lisp, factor is a data-based lang. however, factor [stack] is simple enough that we can easily examine the whole program state in the debugger!
  ** debugging steps through ops and shows the stack at each op
* can pass around macros like any other data; unlike in lisp, macros are first-class data. truly all of factor's linguistic objects are symmetrical about computability; they're all data & transforms thereof.
* good, _flexible_ (somewhat implicit by generic words, mixin classes & instances) oop support (like cl)
  ** this is how we do ad-hoc relations. this makes encoding ad-hoc polymorphism easy, so we can have haskell-like concision but without haskell's restrictions.
    *** programs are often prolog-like: small facts (except here fns) that are used like a vocabulary; more code re-use than big, specialized chunks of code.
* code is compiled on the fly into highly optimized single static assignment (SSA IR). such a simple lang supports extreme optimization.
  ** use `optimized.` (instead of `.`) to see optimization details of some code
* extremely good ide: simple, debugger/stepper, inline docs (all local), quickly see everywhere that any word is used, and any word's definition
* uses arrays with pseudo-indexing (i->a) e.g. `<reversed>`
* comes with memoization library
* λ syntax is `::`
* good unicode support
* supports dynamic scope!
* postfix; read left to right, e.g. `2 even? [ "OK" ] [ "Cosmic rays detected" ] if` means `2|2 ? "OK" : "Cosmic ..."`
  ** pipeline [unix cmd pipe] design
* like haskell, data are just nullary functions
  ** all syntactic objects are simply called _words_
* excepting row-polymorphic combinators and macros, all words must accept and output a fixed number of words
* latently typed w/dynamic checking, static stack effect checking. duck typed oop/generics.
* modules are called _vocabularies_
  ** for maximum flexibility & interactivity, even private identifiers are usable in greater contexts if explicitly referenced
  ** like java public classes, each vocabulary must be defined in a file of the same name
* factor is oop, but all methods are generic; no class "owns" methods; instead, everything is interfaces [java] / purely abstract classes [c++] / type classes [haskell] and instances. instance lookup is dynamic.
* identifiers can be marked as private, but this is a suggestion, not enforced linguistically

==== implementation

* the `tools.deploy` vocab allows compiling to native executables which neither require factor to be installed on host nor expose source code!

==== other considerations

* the documentation is usually _astounding_, except that it _never_ features examples. some vocabs have only the technical, auto-generated docs.
  ** includes word definitions as source code
* the listener (repl) is super-capable and integrated well with the docs
* there are _many_ libs builtin (see factor handbook > libraries > vocabular index), and *they're all documented offline in the docs*
* the docs are updated realtime as vocabs are loaded
* ffi w/lua
* has python bindings

=== environment

* `USE: <lib>` imports one lib. `USING: <lib> ... ;` imports many.
  ** *put space between last lib and `;`*
* `FROM: vocab => word ... ;` disambiguates imported words. it overrides `USE:`/`USING:`, and can be used in lieu of those
* see `QUALIFIED:`, `FROM:`, `EXCLUDE:`, AND `RENAME:`, too.
* module A may use module B even if B has errors, as long as A doesn't use any of B's words in which the errors exist
  ** or maybe not? perhaps _sometimes_....
* `IN:` defines a module. *required when writing any module*
* you must import `kernel` when running scripts. yeah, even `drop` must be imported.
* _quotation's stack effect does not match call site_ is an inconsiderable runtime error displayed when a script finishes with a non-empty stack. even `MAIN:` is hard-coded to check against `( -- )`. either put `clear` at the end of your script or make your script have stack effect `( -- )`. this is probably the most idiotic thing i've seen factor do yet.
* `save` saves the entire program state to a file. this is useful for scripts, since they're usually re-evaluated on each run. of course, for programs that do not need re-evaluation, it's best to use the ui deployment tool (`deploy-tool`) to make native, speedy executables.
* command line args: `USE: command-line command-line get-global`. *arg0 (program name) is not included!*
* envars: `USE: env`; then singleton `env` is an assoc

see factor handbook > the language > vocabulary loader > vocabulary roots. you can get there by searching for `vocab-roots`.

vocabularies have metadata. this is encoded by directories: each vocabulary has its own directory e.g. `foo`, and inside it contains at least `foo.factor`, among any special metadata files (e.g. docs, author) or other files. any of the 3 methods in _working with code outside of the factor source tree_ are good for making directories available for use with `USE:` &c. otherwise you can use `add-vocab-root` *with an absolute path* (leading homedir tilde is supported.) *this are supported only in the listener.* in a source file, `USING:` is processed before the rest of the source file regardless of the order of words. this means that you can't set `FACTOR_ROOTS` in `env`, either.

so `FACTOR_ROOTS` is useless for scripts, unless you're fine with wrapping every executable factor script in a single-line shell script that sets `FACTOR_ROOTS` before running the script. using `add-vocab-root` in `~/.factor-rc` is the best solution.

NEXT: try `require` after `add-vocab-root`, just to see how it works

.example

suppose i'm keeping a `util` module at `~/programming/util/util.factor`, and i want to use it in the listener.

[source,factor]
----
"~/programming" add-vocab-root
USE: util
----

`util` here refers to the directory; that's why it's `util` and not `programming.util`. however, even if i name the module as `IN: programming.util`, i still can only `USE: util`, not `USE: programming.util`. that's unexpected. anyway, declaring names without periods is simpler anyway. still, TODO: explore how module (and corresponding directory) hierarchies correspond to `USE:` statements.

.no transient imports of generic words

because generic words are potentially many (and can often collide) the module system requires that you, at least in the listener, `USE:` providing vocabs despite having already `USE:`'d a module which itself `USE:`'d that same module. e.g. if my `util` module uses `io` for `stream-contents` (which is not generic but is defined in terms of `stream-contents*` which _is_ generic), then if you `USE: util` in the listener, you'll be prompted to `USE: io` so that `stream-contents` can be resolved. this affects only generic words. this is a price of dynamicism.

=== exploring code & learning factor

NOTE: _ciif_ := "code in input field"

* `#concatenative` on irc.libera.chat (or irc.freenode.net? i'm seeing more ppl on libera)
* start with the factor repl's `help` menu item
  ** see _developer tools_
  ** see _all tips of the day_ (factor handbook > developer tools > help system > tips of the day)
* read the factor source code
* ^i: see the stack effect of ciif
* ^w: step through ciif
* ^t: time execution of ciif 
* `apropos` e.g. `"group" apropos` (equivalent to searching in the factor handbook [help] search box)
* familiarize yourself with word naming conventions (handbook > the language > conventions § word naming conventions)
* `:error` gives most recent error. `:c` to see its callstack

=== semantics

* see `DEFER:` for mutual recursion
* scope is not often a consideration. however, `set` is scoped only within a source file (b/c files are parsed with `with-scope`)
* strings are sequences of unicode code points, not of bytes. factor supports encodings well. writing bytes is merely a matter of using the correct encoding (namely the `binary` encoding)
  ** bitstring literals are enterable by `B{`, the byte array literal syntax. you can use `B{` with `write` e.g. `path binary [ B{ 96 0xa 65 } write ] with-file-writer`
    *** `0x` syntax is directly supported by factor. no need for even number of hex digits, btw.
* pushing quotations does not use memory
* `f` is the false value; all others are truthy
  ** `t` is the canonical truthy value
* `{ 1 2 3 } dup [ [ 1 + ] map! ] dip . .` prints `{ 2 3 4 } { 2 3 4 }`. therefore `dup` duplicates, at least for non-primitives, a pointer, and arrays are mutable...? this seems to suggest so, but `{ } 3 suffix!` confoundingly fails with _sequence index out of bounds_. this example fails when i use `3 [0,b]` instead because ranges are immutable.

.concurrency & parallelism

see vocab `threads`, vocabs tagged with `concurrency`. parallelism words are in `concurrency.combinators`.

==== vs picolisp

factor & pil are equally simple, dynamic, and support purity & mutation, and both are extremely efficient (though i've yet to contest them). lambdas are equally easy in both. factor's concatenativity and pil's applicativity is the big difference, and is what makes factor the clear winner. though lists are stacks and pil has `apply`, pil (or other lisps) can be a stack machine only if every function can choose how many data to take from the stack. some take a certain number (either common words, which is a fixed positive integer, or combinators, whose arities are ultimately functions of their parameter functions' arities) or are, like `loop`, variable (these classes can be phrased as static vs dynamic arities.) if we can calculate/get that, then a simple fexpr would make pil into a stack lang. yet factor's parameterization of words is slightly nicer than pil's parameterization of data: pil asymmetrically considers nullary functions & data differently, which means that parameterizing a datum is non-trivial.

NOTE: i've yet to consider pil's universal dynamic binding, and how it can use various kinds of symbols

* factor's state is usually stored on the stack, and pil's in appropriate variables. however, both can use stacks or variables easily.
* both langs use loop primitives instead of manual recursion (usually)
* pil hasn't generics; instead, _everything_ is lists.
* macros are first-class in both factor and pil

factor is easier to learn than pil, namely because:

* pil's documentation isn't nearly as easy to navigate
* the pil repl isn't nearly as helpful as factor's
* pil is far more likely to unceremoniously produce unexpected behavior instead of halting with a helpful error, as factor usually does.
* pil's handling of symbols (internal, transient, &c) is uncommon and complex or not obvious, nor easily explained, at least by the official docs

.pil's advantages over factor

* seems smaller (comes with fewer primitives)
* is simpler; again, _everything_ is only lists & `eval`, and the vm is amazingly simple & efficient
* not more dynamic, but dynamic & hacky behaviors are easier in pil
* is terser (variable names)
* easier to read if you're not already familiar with reading concatenative programs
* designed for unix-like oses, and makes system calls easy; i'd probably prefer to write a *nix interactive shell in pil.
* refactoring is easy b/c blocks of code are easily selected b/c they're delimited by parens
* pil fexprs are easier to learn & use than factor's metaprogramming
* supports parallel implicit mutation e.g. `(while (read) (println @))`. changing multiple independent states (viz variables) is easier than one state (viz the stack.)

it doesn't really matter which of factor or pil you use, but factor is easier to learn and use, comes with a large set of libraries, runs on both windows and *nix, and supports writing guis, so you should probably use factor, though pico is probably worth learning.

=== special builtins

these are contrasted with non-special builtins; these builtins are not useful in writing programs, but are used to examine programs or otherwise concern the vm or language itself.

* `call`: lisp's `eval`. runs a quotation, curried fn, or fry expression.
* `\ f`: pushes `f` onto the stack. `f` is then callable via `execute`
  ** `execute` cannot be used with dynamically bound variables; in that case you must use `execute(`

=== the repl (the _listener_)

* *just because a program runs in the listener does not mean that it is correct*. e.g. `f [ 1 ] unless` runs but trying to get its stack effect produces a stack effect mismatch error! replacing it by `unless*` runs the same as `unless` but has a correct stack effect.
* set font: e.g. `"monospace" 20 set-listener-font`. you can `save` the image or put in `~/.factor-rc`
  ** btw the browser font size is *not* adjusted by using ctrl-- & ctrl-+, despite what's been said in the mailing list
* press `shift+return` to start a new line in an expression; press `return` to evaluate.
* when the cursor is left in a word for 1s, its stack effect is displayed in the status bar
* the `refresh-all` word reloads all loaded source files. unlike clojure/cider, reloading the file does not merely execute statements; suppose that a file defines a word; then that file is loaded, modified to have the word definition removed, then reloaded; the word is no longer defined in the listener.
  ** TODO: determine when/how/why `refresh-all` fails. never trust it too much.
* supports tab completion
* supports ^p & ^n but not up & down arrows
* runs as a gui rather than cli program
* is a client that connects to a repl server
* tracks the stack for you, which makes easy both working with state and debugging
* to enable dark mode (no idea how this was found): run `USE: tools.scaffold scaffold-factor-boot-rc` then add `USE: ui.theme.switching dark-mode` to `~/.factor-boot-rc`, then run `run-bootstrap-init save`, then restart the listener. on nixos i got a _read-only filesystem_ error, so this didn't work totally.

=== stack evaluation model

there is no function _composition_. there are only combinators (higher order functions) and application (β-reduction.) combinators are obvious because they always use qutations. unlike functional languages, words are always applied unless quoted (i.e. in a quotation); unquoted words are always applied. this differs from scheme, where `f` is different from `(f)` and `f` may be passed as an argument. factor is different from haskell, where `f x` evaluates to a result but `f` may still be passed as an argument to a higher-order function. in factor `f` is always applied to the stack below it. furthermore there is no distinguishment between data and functions; like haskell, words are all the same and each has variable natural number arity. `+ = 1 -1 ?` uses neither higher order functions nor composition _per se_; it is equivalent to composition, though composition exists only in a functional model and has no meaning in a stack model, since there composition is equivalent to application which are/is always implicit. binary `+` is applied, then binary `=` is applied. notice that i did not say "applied to `+`'s result." there are no function outputs in the stack model! the only input and output is the stack. any word may affect the stack in any way. here `+` is applied to the top two stack elements, then `=` is applied to the top two stack elements. therefore the stack effect of `+ =` is `( x x x -- x)`; `1 2 3 + =` is `1 == 2 + 3` in common pseudocode, and `+ = 1 -1 ?` is `λx y z. if x == y + z then 1 else -1`.

* `[ + = 1 0 ? ]` has stack effect `( -- x)` i.e. it's just a datum; but `[ + = 1 0 ? ] curry` has stack effect `( x -- x)`.
* non-higher order functions cannot be variadic, though higher order functions can be; their arity is a function of their argument function(s)'.

NOTE: fns are curried. e.g. `{ { 0 1 } } at` is illegal if the stack is empty; however, `: X ( x -- x ) { { 0 1 } } at ;` is fine b/c it defines but not evaluates `X`. functions may be defined in terms of other [curried] functions, which in turn are curried. you can tell that a function is curried by using an unquoted function that would usually cause stack underflow if applied to an empty stack.

=== syntax

the only true syntax of the language itself, rather than a syntax implemented in factor itself, is that words are whitespace-delimited. defining words is a user-definable syntax, as are definition suffixes like `flushable`; consider the definition `: pp ( a -- ) . ; flushable`. here we're pushing each word to the stack. `:`, `(`, `--`, `)`, `;` are all just words. after `;` is pushed & evaluated, a definition is left atop the stack. that definition is an argument to `flushable`. one beautiful benefit of such uniform design is that the documentation for _all_ parts of the factor language is uniform and equally accessible by simply clicking on the word in the help docs.

furthermore factor beats lisp(s except picolisp and possibly some other uncommon, simple lisps) at its own game: factor actually does not distinguish between code & data; all language objects are _words_, which are just strings associated with properties. the only truly core parts of the language are hashtables, tuples, and other primitive data structures. this means that the language is not at its core a language, but instead a simple system of data manipulations i.e. creating & re/moving data and elementary arithmetic; the only other unique aspect of the language that makes it factor is the implicit & simple fact of how the stack is evaluated, viz β-reduction, and its static stack effect checking.

NOTE: primitive words are marked by featuring the `PRIMITIVE:` word in their definitions e.g. `datastack-for` in `kernel.private` vocab.

the _continuation implementation details_ page is very refreshingly overtly simple: "a continuation is simply a tuple holding the contents of the five stacks: [... each of which] can be read and written." no black box. no trepidation about internal complexity, and certainly no external complexity. maybe i've been scarred by racket's docs on continuations, but i know that all languages besides factor that i've encountered have even attempted to be so clean.

* `!` starts single-line comments. multiline comments are /* ... */, after `USE: multiline`
* `$ word` executes `word` at parse time, adding its results to the parser accumulator [stack?]. seems similar to macros.
* there is no built-in syntax except that there must be spaces between syntax objects. all delimiters and even strings are [reader] macros.
* local binds: `[| m n | m n + ]` binds m & n to next-to-top and top stack elems respectively, then uses them to push m+n.

.common delimiter syntaxes
|===========================
| {}             | array literal
| []             | quotation (like lisp)
| '[ ... _ ... ] | threading macro, e.g. `5 '[ _ + ]` is equivalent to `[ 5 + ]`. requires `fry` library.
|===========================

`5 '[ _ + ]` is equivalent to `[ 5 + ]`.

i wish that these terser syntaxses were available; they'd make e.g. `cond` easier:

[source,factor]
----
[ a ] [ b ] ... => [a:b:...]
{ a } { b } ... => {a:b:...}
----

refactoring these into their more general cases (mixing arrays & quotations) is not an _extra_ cost; it's a _delayed_ cost; you'd need to take that cost the first time anyway!

==== defining words by other words

* `curry` combines a word and a quotation e.g. `2 [ - ] curry`. it always reduces the quotation's arity by 1.
* `compose` combines two quotations e.g. `[ 2 + ] [ 4 * ] compose`
* `::` inserts quotation parameters literally e.g. `:: test ( x q: ( x x -- y ) -- y ) x dup q ;` is wrong; by this definition, the stack effect is `( x x -- x x x )` and `2 [ 5 + ] test` pushes `2 2 [ 5 + ]` to the stack. the solution is to do `:: test ( ... ) x dup q call ; inline`.

=== oop / generics / ad-hoc polymorphism

TODO: discuss _protocols_ e.g. `assoc`

probably the easiest & most flexible oop ever:

[source,factor]
----
TUPLE: circle r ;
TUPLE: rect l w ;
GENERIC: area ( shape -- area )
M: circle area r>> dup * pi * ;
M: rect area [ l>> ] [ w>> ] bi * ;
----

NOTE >>foo writes, foo>> reads. it's unfortunate that these are words which must be imported rather than syntax for getting or setting a hash table. hash tables are better than tuples. i guess that words [functions] are used because, if true, as class hierarchies are built, mere accesses become arbitrarily or greatly augmented. such degree of augmentation seems unlikely, though. i would expect, especially in a language like factor that touts its dynamicism, that hash keys would be preferred over accessor & setter words, as it's done in clojure. it seems that factor is perhaps not so flexible or dynamic as picolisp. TODO: how are tuples advantageous over mere hash maps? actually, they cannot be, since maps are the plainest general structure.

these are called _tuple_ classes. `r`, `l`, & `w` are called _instance variables_, so named for the interpretation of these named tuples as _classes_ and a constructed tuple (rather than its type/spec/shape) being seen as an _instance_ [object] of the tuple class. a _method call_ is a generic function that applies to a tuple e.g. `r>>` or `area`, both of which apply to any object that supports them (viz any tuple instantiated of a class having an `r` instance variable and a class that supports `area` respectively, where support is determined dynamically.

ways to instance a tuple: `boa`, `new`, `T{`, or by using the `constructors vocab.

i know not of classes other than tuples. tuples are considered as sets of attributes.

_derived classes_:

* _predicate classes_ are subclasses satisfying a predicate.
  ** is a subclass not merely a union? e.g. `TUPLE: a a b c ; subclass b a d ;` sees `b` as a's attributes ∪ {d}, yeah?
* _union & intersection classes_ are the union or intersection of classes.
  ** _mixins_ are a variety of union class. i have no idea what they add to union classes.

* _primitive_ classes represent data primitives and cannot be subclassed
* what are
  ** multiple dispatch (planned inclusion in factor, but currently implemented by a library)
  ** predicate classes

three functions from class to class:

* derivation
* union (n-ary)
* intersection (n-ary)

three types of classes:

* primitive
* tuple
* derived
* predicate (subclass B of A where A consists of instances satisfying a predicate)

primitive & tuple classes use >> & << (but not derived ones?)

=== common words

.`sequence` vocab

* `nth`: elem at index or error. `nths` is like mapping curried `nth`
* `set-nth`. mutative, so whereas `CHAR: c 1 "-s" set-nth` leaves the stack empty, `"-s" CHAR: c 1 pick set-nth` leaves "-c" atop
  ** `change-nth` may be preferable. like `set-nth`, it's mutative, so you need some odd `dup`'s e.g. `{ "CAT" } dup 0 swap [ dup CHAR: c 1 rot set-nth ] change-nth` leaves `{ "CcT" }` on the stack.
    *** `swap over` ( a b -- b a b ) may be useful here
* `?nth`: elem at index or `f`
* `prefix`, `suffix`: adjoin at head or tail
  ** `prefix?` & `suffix?` are not defined; instead use `subseq-start 0 =` for `prefix?` and `[ subseq-start ] [ [ length ] bi@ swap - = ] 2bi` for `suffix?`
    *** regarding `subseq-start` &al, the factor docs use _subsequence_ to mean _substring_
* `insert-nth`: insert at provided index, moving latter elements rightward by one index
* `prepend`, `append`: concatenate 2 topmost sequences
* `concat`: concatenate elements of a sequence of sequences
* `join`: intercalate then concat

there's no complement of n-array; however, `2array` &c has complements `first2` &c. `nths` pushes 1 sequence, not n elements, to the stack.

example: find 1st element matching some predicates: `[ preds 1&& ] find nip` e.g. `{ "kak" "file" 36 41 } [ { [ number? ] [ even? ] } 1&& ] find nip` returns `36`.

==== pattern matching

[source,factor]
----
USE: match
MATCH-VARS: ?x ?y ;
: my-match ( seq -- )
{ { [ _ "2" ?y ] [ 14 number>string write ?y print ] }     ! case 1
  { [ ?x _  ?y ] [ ?x 7 * number>string write ?y print ] } ! case 2
  { [ _ ] [ "<no match>" print ] } }                       ! else
match-cond ;
{ "1" "2" " is the number" } my-match ! writes 14 is the number
{  6  "6" " is a number"   } my-match ! writes 42 is a number
----

case 1 is more specific than case 2; were case 2 earlier, it would match even if case 1 were a better match.

=== stack tech

==== basic stack words

to write amazing code, master the following: drop, dup, dip, swap; cleave[-curry], apply[-curry], spread[-curry]; curry, compose, prepose; with; smart words. in factor, though many stuffle words are defined in `kernel` as ``PRIMITIVE:``s, these words can be defined by drop, dup, dip, & swap. being primitives, though, they're probably a tad faster than using those four manually, so use e.g. `pick` or `over` instead of `[ [ dup ] dip swap ] dip swap` or `[ dup ] dip swap`, and use non-primitives like `keep` &al because they're common and terse; however, use them only as brevity devices, not as their own technique! always _reason_ in terms of the four.

* `preserving` (of the very useful `combinators.smart` vocab): when running a word, don't consume its args from the stack e.g. `1 2 [ + ] preserving` leaves `1 2 3` atop the stack.
* `drop`: remove top elem
* `nip`: remove 2nd elem. nip = [ drop ] dip
* [2|3|5]nip: remove top n elems
* `nipd`: remove 3rd elem
  ** generally any word ending with `d` is that word under a `dip`
* `dip`: pop, apply, push back
* `x -rot` = `[ x ] 2dip`; think of `x y z -rot` as inserting `z` before `x y`
  ** think of `rot` as moving `x y z` in front of `z`. i personally can imagine more easily moving one object rather than rotating a sequence. so `rot` (non-negative `rot`) moves `x` forward; `-rot` moves `z` backward.
* `q keep` = `dup q dip`. "keep" means "squirrel away"—store f(x) lower on the stack for later use.
* `bi`,  `tri`,  `cleave`: apply many fns upon top elem
* `bi*`, `tri*`, `spread`: apply pointwise fns upon data
* `bi@`, `tri@`,  `apply`: apply one fn upon many data
  ** `both?` & `either?`: `liftA2 (&&)` & `liftA2 (||)` in the `(->a)` category
  ** use `[bi|tri]-curry[*|@]` to encode tacit data pipelines
* over: x y -> x y x
* pick: x y z -> x y z x. `pick` = `[ over ] dip swap`, \= `[ over ] keep`, btw.
* `10 56 t [ 2/ ] when` -> 10 28. like `unless`.
* at: lisp's `assoc`. use `at*` if you need non-`f` value on lookup failure
* ?: `if` but accepts value literals instead of quotations. its only benefit over `if` is eschewing bracket syntax cruft 
  ** `if*` keeps the test value on the stack *only when the true branch is taken*, effectively `maybe` in haskell
* `when*` is `when` but the conditional is the thing to be modified. effectively haskell's fmap in Maybe
* `a b qt qf ?if` applies `qt` to `b` if `b`, else `qf` to `a`. so it's `if` when there's already an alternative/fallback value left on the stack; usually we'd specify the fallback value in `if`'s false quotation.

NOTE: `keep` supports only quotations e.g. `: add1 ( x -- x ) 1 + ; 1 add1 keep` errors but `1 [ add1 ] keep` is correct

functions start at f and are named with a leading arity. functions grouped together are suffixed by indices e.g. 1f1 1f2 for two related unary functions.

* `[ x ] 2dip` is clearer than `x -rot`
* `dup 1f 2g` = `[ ] [ 1f ] bi 2g`
* though `0 { 1 2 3 } { 4 5 6 } [ 1 + ] [ [ tail ] bi@ ] bi` fails b/c it tries to apply `[ 1 + ]` to `{ 1 2 3 }`, we can do `{ 1 2 3 } { 4 5 6 } 0 1 + [ [ 1 tail ] bi@ ] dip` to get `{ 2 3 } { 5 6 } 1` or `0 { 1 2 3 } { 4 5 6 } [ 1 + ] 2dip [ 1 tail ] bi@` to get `1 { 2 3 } { 5 6 }`.

.impure `cond`

`cond` performs stack effects in order until the top is truthy. prior conditional predicate quotations affect later ones. this example demonstrates it, as does the following one:

[source,factor]
----
{ { [ dup empty? ]              [ drop 1000 ] }
  { [ dup first 6 * dup 50 <= ] [ ] }
  { [ drop t ]                  [ drop "none" ] }
} cond
----

[options="header"]
|=============================
| argument   | resultant stack
| `{ }`      | 1000
| `{ 5 3 }`  | 25
| `{ 15 3 }` | "none"
|=============================

note its ``dup``s & ``drop``s. the 1st condition must `dup` so that, if not empty, the sequence will remain on the stack for the 2nd condition to test, and so on. consequently, each branch replaces the sequence by some other value. factoring-out the ``dup``s to before the `cond` assoc is incorrect; that'd be the same as moving the first `dup` and removing the second. `dup` must be performed before each of `empty?` and `first`; a sequence must be atop the stack before each of those predicates is performed, and each predicate must ensure that it keeps [that] sequence atop the stack for the next predicate to evaluate, unless the assoc is designed to mutate the stack as it goes through the predicates. admittedly, though mutating state while going through predicates is _generally_ useful, it's _commonly_ not, and a pure version of `cond` would be nice to have additionally.

stateful `cond` is especially useful in writing parsers e.g.

[source,factor]
----
USING: kernel namespaces system command-line ;
SYMBOL: PARAM1 PARAM1 off
command-line get-global
[ [ f ]
  [ unclip-slice { { [ dup "--param1" = ] [ drop PARAM1 swap set-global t ] }
                   { [ dup "--help" = ] [ print-help 0 exit ] }
                   { [ drop t ] [ write " is an invalid arg" print -1 exit f ] } }
                 cond ]
  if-empty ]
loop
----

==== sequence & looping words

.general loops

* `loop`: general loop construct; repeats a quotation until the quotation evaluates to `f`
* `while` & `until`: `loop` but partitioned into predicate & body.

.loops over sequences or quotations

* `collector-as` (guard is filter) & `selector-as` (guard is short-circuit) are the most general looping functions that collect into a sequence. they do not require input sequences; they use whatever state the stack has as input.
  ** implemented in terms of `push` & `push-if` respectively
    *** `suffix!` = `over push`
  ** `q collector` leaves a quotation that applies `q` then pushes that result to a resizable seq, and that resizable seq (to keep it in scope)
    *** `collector` is more convenient than `loop`: less shuffling and terser.
* `combinators.short-circuit` for short-circuiting `and` & `or`
* `each`, `map`
* `reduce`: fold
  ** `foldl` & `foldr` are for _lists_, not _sequences_ (two different types)
* `replicate` seq of elts produced by calling a quotation _n_ times
  ** `follow` is like `replicate` but mixed with `while`: it calls a quotation until that outputs `f`, collecting results into a seq
    *** `follow` is implemented in terms of `produce`, which is implemented in terms of `collector-as`. `produce`:`until`::`follow`:`loop`.
* `accumulate`: scan
* `map-find`: dual of `filter-map` as `find` is dual of `filter`
* `seq [ ] each` pushes each elt of seq to the stack

.multi-parameter fold accumulator example

we'll test whether all items in a sequence equal by using a 2-part accumulator; one part is the element to test equality against, and the other is a boolean of whether all of the elements so far have equaled:
 
[source,factor]
----
: all-eq? ( seq -- ? ) [ first ] [ ] bi t [ pick = and ] reduce nip ;
{ 1 0 3 } all-eq? ! f
{ 1 1 1 } all-eq? ! t
----

* `reduce` accepts only one `identity`, so we need to have the other part(s) of our accumulator already on the stack before the input sequence.
* `nip` to remove the non-output part of the accumulator. generally you'd `[ drop ... drop ] dip`

actually, though, this particular example is more elegantly expressed as:

[source,factor]
----
: all-eq? ( seq -- ? ) dup 1 tail-slice t [ = and ] 2reduce ;
----

which is efficient b/c `tail-slice` produces a virtual sequence. factor's common use of virtual sequences makes mapping or other folds easy to express without sacrificing efficiency.

TODO: try swapping the boolean and first element to see how that goes.

collector & selector examples:

there appears to be no `filter-map`, so one would use `collector` instead. it's nicer than `map-reduce`. 

[source,factor]
----
[ 2 * dup when ] collector ! leaves [ 2 * ~vector~ push ] V{ }
10 [0,b] -rot [ each ] dip . ! prints V{ 0 2 4 6 8 10 12 14 16 18 20 }
----

NEXT: quite frankly there should be just a loop that processes whatever `e`, which may conveniently be an input sequence as given by a combinator `seq>loop` of effect `( seq -- e )`, and: 1. if `SYMBOL: stop` is returned then the loop stops; 2. returning `f` will not push the element into the output sequence; 3. other values are pushed into the output seq. `seq>loop` will output `short` if empty. this general filter/map/stop loop pattern is practically universal! it can mutate state arbitrarily, accumulate from any state into a sequence, retaining or discarding elts. i should be able to have the argument function return multiple values, too, thus allowing it to return multiple values, and those can be inserted inline into the output seq. in fact, i should be able to have my accumulator be any structure that supports insertion, e.g. a splay tree.

.non-looping sequence words

* `a b s snip` leaves `s[0:a]` & `s[b:]`

deep-each example: `{ { { 1 2 { 3 4 } 5 6 } { 7 8 } } } [ . ] deep-each` outputs:

----
{ { { 1 2 { 3 4 } 5 6 } { 7 8 } } }
{ { 1 2 { 3 4 } 5 6 } { 7 8 } }
{ 1 2 { 3 4 } 5 6 }
1
2
{ 3 4 }
3
4
5
6
{ 7 8 }
7
8
----

==== general loop supporting short-circuiting

use `each` if you won't short-circuit; else use whichever of `until`, `while`, or `loop` is most elegant for your problem. they all short-circuit by having the body push a bool atop the stack, but `until` & `while` also support a predicate given outside the body. they are more powerful, but can look a little clumsier than `loop` if only the body gives the continuation condition.

`unclip[-slice]` is just a shorthand for `[ rest[-slice] ] [ first ] bi`. if putting the tail & head adjacently isn't particularly helpful, then use each individually where appropriate. it's best to use `[ f ] [ unclip-slice ... ] if-empty` for easy stack effect balancing.

the ideal stack solution to any problem is determined by identifying the reduced AST. let's look at how we'd write haskell `[y | x <- seq, let y = 12*x, y <= 50]` in factor. first, consider the tree of morphisms that we'll need:

* seq
  ** empty? (in loop predicate)
  ** [ first 12 * ] (bound to `y` in the list comprehension)
    *** [ 50 <= ] (guard condition)
  ** rest-slice (needed to loop)

note that this bullet list matches how it'd be written in factor, minus `cleave`:

[source,factor]
----
seq
  { [ empty? ]
    [ first 12 * dup { [ 50 <= ] } cleave ]
    [ rest-slice ] }
cleave
----

then we just need to shuffle the stack. this demonstrates equivalence of bullet notation and `cleave` in source code. neither accounts for evaluation conditionality. in actual code you'nd never use `cleave` on a singleton; you'd just use `[ first 12 * dup 50 <= ]`. however, if a macro were to expand a bulleted list, it'd expand to `cleave` on a singleton, unless it were trying to be clever.

anyway, continuing, we install some sensible combinators:

* `keep and` ( x pred -- maybe-x ). e.g. `6 [ even? ] keep and` --> 6. `6 [ odd? ] keep and` --> f.
* a preserving `bi`. rather than `[ keep ] dip call`, use `[ keep ] dip keep -rot` or `[ keep swap over ] dip call`. we'll call it `bik`.
* `if-empty` (of the `sequences` vocab)
* a form of `and` that accepts quotations or the maybe monad would be good for short-circuiting, but wouldn't help here since the shorting is determined easily already by just pushing `f` for `loop`

order of word application is irrelevant up to conditional evaluation.

[source,factor]
----
! only shuffle words. terser, less clear
{ } seq [ [ f ] [ dup first 12 * dup 50 <= [ swapd suffix swap rest-slice t ] [ 2drop f ] if ] if-empty ] loop
! uses locals syntax. clearer & more verbose.
{ } seq [ [ f ] [ dup first 12 * dup 50 <= [| acc src v | acc v suffix src rest-slice t ] [ 2drop f ] if ] if-empty ] loop
! `until` version
{ } seq f [ [ dup empty? ] [ ] bi* or ] [ dup first 12 * dup 50 <= [| acc src v | acc v suffix src rest-slice f ] [ drop t ] if ] until drop
----

* the `until` version needs a leading boolean
* using `if-empty` means that `empty?` isn't in a cleave, but it wouldn't be able to be in a cleave anyway, since the remainder of the cleave quotations are conditional on emptiness.

granted, that's much longer than haskell `[y | x <- [0..n], let y = 12*x, y <= 50]`! that being said, that's special syntax, not a loop using haskell primitives. we can and should use ``Alternative``s and `foldMap` in factor for elegant loops that support short-circuiting. factor does not come with such libraries, but it does come with a rudimentary `monad` vocab.

.monadic version
[source,factor]
----
USE: monads
FROM: monads do ;
{ [ seq >array ] [ 12 * dup 50 <= [ array-monad return ] [ array-monad fail ] if ] } do
----

it's long, but only in characters, not tokens. the way this works is that:

* `x q bind` is the same as `x >>= q` in haskell.
* `return` uses 1. the kleisli's output as considered inside the monad, and 2. a singleton saying how to interpret the value (e.g. `array-monad`), to produce a monad-specific interpretation of that value e.g. a list.
  ** we can use `fail` in place of `return` to mean `empty` of the haskell `Alternative` class (or `mzero` if you're into that.)
* you must use `fail`; there's no `guard`, and using `{ }` is not interpreted as `empty`; those empty lists will be returned in the resultant list.
* btw `Just x` is `T{ just f x }` in factor, as discovered by evaluating `100 maybe-monad return` since idk factor oop well yet.

NOTE: arrays are strict and lists are lazy. therefore `>array` is needed for arrays, and `>list` (and `list-monad`) for lists. lists would be more appropriate since they're more efficient, but i can't figure-out how to print, or generally loop through, them.

conclusion: clearly a combinator would be best. the obvious winner is the monadic version, preferably a lazy list version. otherwise `loop`-with-only-shuffle-words version is best. regardless, a combinator should be written to make easy work of shorting loops:

[source,factor]
----
:: map-until ( seq g: ( x -- y stop? ) -- seq ) { } seq
  [ [ f ]
    [ unclip-slice g call [ 2drop f ] [ swap [ suffix ] dip t ] if ]
    if-empty ]
  loop ; inline

10 [0,b] [ 12 * dup 50 >= ] map-until
----

it looks long, but i feel better about deciding to learn factor when i compare it to the scheme version:

[source,scm]
----
(define (map-until f s)
  (let r ([s s])
    (if (null? s)
        '()
        (let-values ([(y stop?) (f (car s))])
          (if stop?
              '()
              `(,y . ,(r (cdr s))))))))

(map-until (λ (x) (let ([y (* 12 x)]) (values y (>= y 50)))) (range 10))
----

besides, `map-until` should really be written in a loop combinator that combines `loop` and `if-empty`, since that's so common:

[source,factor]
----
:: loopseq ( ..a seq g: ( ..a -- ..b ? ) -- ..b seq ) seq [ [ f ] g if-empty ] loop ; inline
: loopseq ( ..a seq g: ( ..a -- ..b ? ) -- ..b seq ) [ f ] swap [ if-empty ] 2curry loop ; inline ! alt def
:: map-until ( seq g: ( x -- y stop? ) -- seq ) { } seq [ unclip-slice g call [ 2drop f ] [ swap [ suffix ] dip t ] if ] loopseq ; inline
----

NOTE: using locals may be easier, but remember to think pointedly! i spent a supid amount of time trying to figure-out why my locals version of `loopseq` caused the `map-until` unit test to output nothing; it turned-out that i'd forgotten to include `seq` at the start of its definition, since i'm so used to thinking pointfree! and you'd think that omitting `seq` would make `if-empty` give a stack underflow error, right? nope; `map-until` puts `{ } seq` on the stack. `seq` gets omitted, leaving the empty sequence. thus `if-empty` chooses `[ f ]`, thus terminating the `loop`, producing no effect.

.example of multi-arity word composition
[source,factor]
----
{ 1 2 3 4 } [ 2 mod 0 = ] filter
----

we see that effectively each item in the list is inserted before filter's predicate; then the predicate is applied. thus we get e.g. `1 2 mod 0 =`.

`'[ _ 2 mod 0 = ]` with the `fry` vocabulary tries to do `{ 1 2 3 4 } 2 mod 0 =`; fried expressions expand to unquoted expressions.

==== sequences

* TODO: try using push & pop
* use destructive sequence operations when accumulating a sequence in a loop! this will prevent copying the sequence, staying in linear time rather than quadratic!

=== globals

like lua's `_G`, factor has a global namespace called `global`. namespaces instance the `assoc` class.

[source,factor]
----
SYMBOL: x      ! declare
4 x set-global ! set
x get-global   ! access
----

==== locals

[source,factor]
----
60 [let 2 5 + :> x 49 x / * ] ! pushes 420
60 [let :> x x x * ] ! pushes 64. :> binds the top of the stack to an identifier while dropping it
----

locals do not care about nesting:

[source,factor]
----
[let 40 :> x x even? [ x 2 * :> y y 2 * ] [ ] if ] ! pushes 160 to the stack
----

===== mutable vars

[source,factor]
----
USE: locals
! 3 f => 11
:: f ( x! -- t ) ! x! makes x mutable by enabling x! to set x (see below)
  x 2 * x! ! x<-2x
  5 x + ;  ! return 5+x
----

`x!` pops into `x`. exclamation marks ("shrieks") are particular here.

this syntax can be used in `[let` also e.g. `[let 24 :> x! x x * x! x 400 - ]` which outputs 176.

=== caveats

none (in this general section) documented yet! frankly, though, "caveat" is an attributive of some properties, and they're commonly caveats about other things, which makes "caveat" a property of a statement, and those statements concern particular subjects; therefore one should query a db for caveats ∩ subject.

=== libs & specific words

* for graphics, use cairo; it has bindings to factor
* see factor documentation > libraries. it's a wealth of functionality in one big listing!

=== tricks

[source,factor]
----
26 <iota> [ CHAR: a + ] map            ! list of a..z
USE: math.ranges CHAR: a CHAR: z [a,b] ! same
USING: math.parser random ; "(ddd) ddd-dddd" [ { { CHAR: d [ 10 random number>string ] } [ 1string ] } case ] { } map-as concat
USING: calendar calendar.format ; now 30 days time+ { YYYY " " MONTH " " DD " " hh ":" mm ":" ss } formatted>string .
----

=== `math`

* `bitxor`, `bitand` &c. see the docs for related fns like `2/` (right shift by 1 bit), `bitcount`, and `even-parity?`

=== os

==== subprocesses (`io.launcher` vocab)

.read a process into a string

[source,factor]
----
USING: io.launcher io.encodings.utf8 ;
<process> ! new blank process object
  { "echo" "hello, there!" } >>command
  utf8 <process-reader> stream-contents
"and hello to you, too!" append print
----

NOTE: use `with-disposal`, or more likely, `with-<input|output>-stream`.

* `obj run-process` is the simple synchronous execution of a cmdline string or array of strings.
* `stream-contents` replaces the process on the stack with its output.
* though `echo` supports `-e` to not output trailing newline, remember that we can trim trailing newline by `[ CHAR: \n = ] trim-tail`

TODO: how to stream one process's output as input to another process, or stream to stdout? the trouble is that `<process-reader>` returns an input stream, but `write` takes only binary data or a string. do i need to read n bytes at a time from the input stream then `[stream-]write` that?

==== filesystem

* vocabs: `io.files`, `io.directories`, `io.encodings`
* load files as streams: `with-file-[reader|writer]`
* load whole file: `[set-]file-[contents|lines]`
* `current-directory` dynvar

examples:

* `"filepath.txt" utf8 [set-]file-contents` to read or write to a file.
* `"writeme" utf8 [ "readme" mac-roman [ [ print ] each-line ] with-file-reader ] with-file-writer`

=== peg

factor's `peg` vocab is a link:https://bford.info/packrat/[packrat parser].

peg is like regex but makes extracting substrings and implicitly putting them in an ast much easier.

* `parse (input parser -- ast)` where input may be a string
* common parsers (e.g. `any-char`) are in `peg.parsers`
* `hide`
* `satisfy` matches a character against a predicate quotation
* `token` is a parser that tries to match a string literal
* `sp` modifies a parser to accept & ignore leading whitespace e.g. `"  hi" "hi" token sp parse .` prints "hi"

.regex as peg words

these are in `peg`:

[options="header"]
|================================================================
| regex    | peg word(s)
| [A-Za-z] | `range` & <and AND combinator to be defined>; or `range-pattern` (in `peg.parsers`)
| ab       | `seq`, `token` (`token` is `seq` on string literals)
| a?       | `optional`
| a*       | `repeat0`
| a+       | `repeat1`
| (a|b)    | `choice`
|================================================================

the following are in `peg.parsers`:

[options="header"]
|=====================================================================================
| regex            | peg word(s)
| a                | `1token`
| .                | `any-char`
| {m,n}            | `at-least-n`, `at-most-n`, `from-m-to-n`, `exactly-n`
|                  | `epsilon` (empty sequence)
| `[0-9]`          | `digit-parser`
| `[0-9]+`         | `integer-parser`
| ((<pat>)<sep>?)* | `list-of` e.g. `"2,32,64" integer-parser "," token list-of parse`
| "([^"])"         | `string-parser`
|=====================================================================================

.ideas sensible only in peg, not regex

* `ensure[-not]`
* `satisfy`
* `semantic`
* `hide`
* `action`
* `surrounded-by`
* `add-error`

[TODO]
* how to run a parser just to see if it succeeded or not?
* how to combine a parser `p` with `satisfy` as `[ p quot and ] satisfy`?

* `satisfy repeat[0|1]` returns a vector of characters
* `1token`, defined in terms of `1string`, returns a singleton string

`ensure-not` allows us to check whether we're at the end of input:

* `"X" any-char any-char ensure-not 2seq parse` pushes `V{ 88 }`
* `"" any-char ensure-not parse` pushes `ignore`

* it seems that adding `ensure[-not]` to `choice` makes a `cond`-like parser

examples:

[source,factor]
----
! COMMON PARSERS
: any ( q -- parser ) satisfy repeat0 [ >string ] action ; inline
! to is to-end if predicate is never hit
: to ( q -- parser ) [ not ] compose any ; inline
: to/c ( c -- parser ) [ = not ] curry any ; inline
! BUG: to-end fails on empty string; it should then return the empty string
: to-end ( -- parser ) any-char repeat1 [ >string ] action ; inline
: many ( q -- parser ) satisfy repeat1 [ >string ] action ; inline
: a* ( c -- parser ) [ = ] curry any ; inline
: a+ ( c -- parser ) [ = ] curry many ; inline
! sp is probably more efficient when you can use it; ws* & ws+ are
! intended to be used at least for list-of.
: ws* ( -- parser ) CHAR: space a* hide ; inline
: ws+ ( -- parser ) CHAR: space a+ hide ; inline
: WORD ( -- parser ) [ CHAR: space = not ] many ; inline
: words ( -- parser ) WORD ws+ list-of ; inline

! EXAMPLE COMPOUND PARSER
: my-clause-parser ( -- parser )
  f ! empty seq
  CHAR: - to/c [ [ CHAR: space = ] trim ] action suffix
  "->" token sp hide                             suffix
  WORD sp                                        suffix
  CHAR: : to/c [ words sp parse ] action sp      suffix
      [ CHAR: : = ] satisfy ensure
      ":" token sp hide
      to-end sp
    3seq
    any-char ensure-not
  2choice                                        suffix
seq ; inline
----

[options="header"]
|===========================================================================================================================
| input                                              | output
| "expr -> mytbl apple   booty cow  dargon : x >= 5" | V{ "expr" "mytbl" V{ "apple" "booty" "cow" "dargon" } V{ "x >= 5" } }
| "expr -> mytbl apple   booty cow  dargon"          | V{ "expr" "mytbl" V{ "apple" "booty" "cow" "dargon" } }
|===========================================================================================================================

* "x >= 5" is in a vector because of `3seq`; e.g. `"A" any-char parse .` returns 65 as expected, but `"A" any-char 1array seq parse .` returns `V{ 65 }`.
* how to parse recursive syntaxes? there should be a peg json parser example on the web for an example.

caveats & mistakes:

* `"thing horo nee" any-char repeat1 ws+ list-of parse` returns a singleton vector of a vector! this is because `list-of` calls `any-char repeat1` which matches the whole string; then `list-of` tries to break on spaces, but there's no more input, so it returns that single vector of characters in a vector.
* error about gensym: then check to see if you forgot `suffix` after your parser
* error about `length` not having method for `parser`: you probably put 2+ parsers on the stack but forgot to put them into a sequence. especially with `ensure`, ensure that you do `<q> ensure <parser> 2seq`
* for words like `set-at` which consume a structure and don't leave it on the stack, use `keep`: `H{ } [ "val" "k" rot set-at ] keep` leaves H{ { "k" "val" } } on the stack
  ** use `over adjoin`
  ** use `[ _initAssoc set-at ]` or `[ set-at ] curry` or `[ set-at ] keep`
  ** use `over [ change-at ] dip` or `_q curry [ change-at ] pick [ 3curry call ] dip`

----
2000 1000 [ 10 65 4 7 ] [ [ + ] curry bi@ 2dup . . ] each
----

1010
2010
1075
2075
1079
2079
1086
2086

--- Data stack:
2086
1086

==== EBNF

basically, unless i'm given a correct, formal description of `peg.ebnf`'s ebnf's grammar, then it's unusable. use manual parsers instead.

peg's ebnf syntax produces a parser that you could've written by hand, but i'm unsure that ebnf can describe all that manual parser combiniation can. i'm not even sure when ebnf is really more convenient than manually writing a parser. for example, can ebnf elegantly describe tokens delimited by `/[[:space:]]+/` or a group of tokens delimited by commas with optional space?

* `EBNF:` in `peg.ebnf`

syntax is like regex:

* `|`
* `[abc]` & `[^abc]` (don't quote characters)
* use double-quotes for literals
* `?`, `*`, `+`
* `EBNF[[ y=[W-Z] x=[T-X] ]]` creates rules `y` & `x` and is a quotation that applies a parser that checks `y AND x` i.e. a single character in `[W-X]`.
* need to use `<tokenizer-name>=`; no unnamed tokenizers.

in trying to learn the ebnf grammar by reading source, i'm learning about using non-ebnf parser( combinators) e.g. `choice*`, and i'm finding those easy to use though more verbose and less readable than ebnf.

the errors can be astonishingly stupid: `"A" EBNF[[ aa = "A" aa|"B" ]]` errors with "Expected 'A' or 'B'. Got 'A'", though it parses `"B"` just fine. however, after some poking around, i see that `|` does not mean "or": `"AAAAB" EBNF[[ aa = "A" aa|"B" ]] .` prints `V{ "A" V{ "A" V{ "A" V{ "A" "B" } } } }`. with such complexity, i decide to no longer try to try to learn the ebnf grammar by looking through source code.

.lookahead

`"a ∈ mytbl -> t(b,c,d)" EBNF[[ y= .+ => " -> " .+ ]]` fails b/c `.+` matches whole string before required token `" -> "` is attempted to be parsed; b/c there's no more input, `" -> "` fails to match, causing the whole parser to fail. `ensure[-not]` can be used for lookahead. then again, we usually want something more specific than `.+`; for example, here "a ∈ mytbl" should be matched against some parser that chooses from multiple valid expressions; the expression should be terminated by its own grammar rather than `" -> "` terminating that expression; therefore the expression should match without worry about accidentally parsing `" -> "` before the appropriate occasion. that `.+` may match `" -> "` and more is not a defect of ebnf; it's no easier to manually write a parser that has not that problem.

this being said, it probably is sometimes reasonable to want to parse until a given string. TODO: how to do that?

.decoding ebnf grammar

terminal: blank or ∈ ["'|{}=()[].!&*+?:~<>]

=== debugging

* if using a higher order fn, mimic it by running its argument at the top level e.g. if `[ f ] each` isn't working, test `f` with the arguments that you expect
  ** if `each`, `map`, `reduce`, or any other traversal over a sequence, is failing, then the easiest way & most direct way to debug it is to stick a `1 head` after the input sequence.
* check the stack signature (ctrl+i)
* when testing code that mutates structures, use `clone`, so that each test starts from the correct initial structure! e.g. do `H ( -- h ) H{ { 0 HS{ } } { 2 HS{ } } } ;` to define initial hashmap, then in all of your tests, do `H clone words ...`. `clone` makes a shallow copy.
  ** depending on your code, you may want `clone` for production execution anyway!

consider the following code:
[source,factor]
----
: groupby ( vals keys -- groups ) ! like /. in j or `group by` in sql
  H{ } over [ swap [ HS{ } ] 2dip [ set-at ] keep ] each -rot swap ! h ks vs
  [ swapd [ over adjoin ] curry [ change-at ] pick [ curry call ] dip ] 2each ; inline

{ 0 2 4 0 7 1 100 56 35 } { 0 2 4 0 2 4 0 2 4 } groupby
----

i kept getting the output:

----
H{
    { 0 HS{ 0 1 2 35 4 100 7 56 } }
    { 2 HS{ 0 1 2 35 4 100 7 56 } }
    { 4 HS{ 0 1 2 35 4 100 7 56 } }
}
----

i took a couple of hours to realize that it was because the `HS{ }` was one object, used as all values for the hash map! using `HS{ } clone` fixed the problem, giving the correct output: `H{ { 0 HS{ 0 100 } } { 2 HS{ 56 2 7 } } { 4 HS{ 1 35 4 } } }`.

=== relational-array factor

* virtual sequences:
  ** `reversed` class
  ** slices. see "subsequences and slices" in the factor docs.
    *** `head-slice`
  ** `<iota>`
  ** define your own instances of the virtual sequence protocol, namely implementing `virtual@`. rotations would be defined easily as virtual sequences.
  ** numeric ranges:
    *** `[a..b]`, `[0..b)`, &al
    *** `<range>`
* `map` everywhere implicitly like j
  ** make a version of rank/join (generally: relate)
  ** singleton arrays, not atoms; this enables them to support map & rank
* [each|map|redue]-index are useful words!
* vector operations

=== using the stack well

* despite aptly being called "factor", coupling is important, too! however data are always used together, couple them into a single item on the stack, as early as you can.
  ** curry when you can. this reduces the number on items on the stack and appropriately couples a fn with its arg. build-up programs incrementally as early as you can.
    *** exploit that programs are sequences e.g. `[ "v1" "v2" ] [ "k1" "k2" ] [ H{ } set-at ] [ 2each ] keep first .` set-at is effectful, consuming our hash table. `keep` leaves the program `[ H{ } set-at ]` atop the stack; the hash table is thus still on the stack, contained in the sequence; `first .` prints it.
* any time that data have an attribute in common, where that attribute is relevant to some fn that the data are passed to, put the data in a sequence then iterate over it.

basically: express symmetries as sequences and asymmetries as relations of sequences. these relations are commonly themselves sequences. `[ H{ } set-at ]` is a sequence/relation of two distinct things paired particularly, and this relation is passed to `2each`, which relates a matrix (having two axes [of symmetry]) to an operation (program).

=== examples of good programming

.state, currying, a/symmetry obvious in code
[source,factor]
----
[ HS{ } HS{ } ] ! list 1
[ "hs1" "hs2" ] ! list 2
[ over [ H{ } set-at ] dip ] [ 2map ] keep ! accumulating into a list of hash sets, to keep them in scope for later modification
                                           ! running 2map under keep leaves the hash table quotation atop the stack
second first ! extract hash map from quotation
dup . ! print hash table
[ [ "beans" "jeans" ] [ over adjoin ] 2map . ] dip ! insert into, then print, each hash set
. ! print hash table
----

this code generalizes easily to any number of key/value pairs. this elegant encoding keeps few elements on the stack, even if it's "strange" that we retain the hash table within the quotation passed to `2each`. after all, we're always setting values of hash table, so why not couple it with `set-at`? it's so simple that one may fail to realize that it's a variety of metaprogramming that macros cannot enable. for this, one truly must use call/eval and be able to extract subprograms from a (quoted) program. it's even easier in factor because the hash map is not a program that we need to evaluate; it's a datum itself!
