NB. throughout this document i refer to sets and sequences interchangeably. they are equivalent: all seqs may be represented by indexed sets, and all sets may be represented by sequences whose order and whose elements' multiplicity is ignored. ...this being said, there must be some computation to check whether each element has already been considered.... from an information-theoretic perspective, we must store a bit (or number) for each element to store whether it's already been encountered (or how many times). this requires a structure additional to the input seq. said structure would need fast finding of elements, so likely a hashmap, ordered growable array, tree (or other graph), skip list (or other probabilistic data structure).

a general note: expectation of structure is doomed. we see this when people's simple life plans don't work-out, always due to unforseen events or that their plan worked but it wasn't as fulfilling as they'd expected. the same is as true in code as in life: adaptability is paramount. even fault-tolerant systems are just that: fault _tolerant_; they aren't to _avoid_ faults; they're to handle them elegantly and keep operating despite them. _design_ itself is a bad idea! go a step short of design: just the study of items (or subsystems) in systems. know how they work, so that you can use them _when you need them_. it is always a mistake to assume use of something before the very moment that it's found to be necessary.

sets/predicates are essential for refactorability; for example, changing `push` to `push-all`, or needing to use an array where there was a singleton before, or needing to change array dimensions, all suck, if you're simply adding extra elements that can be in a set! k is actually pretty nearly there already, btw. also btw, the system can simply check for e.g. conditions that were on an atom but are now on a set; the system will spot atomic predicates on on sets, and inform the programmer that they must choose a subset to satisfy the predicate e.g. any (1), all (n), or all or any of some subset. note that "any" and "all" are distinct and complementary; any means "next()" and all means "next() until exhaustion".

as you consider this document, try to keep the perspective not of "how can i structure this data?" but instead, "what queries do i want to answer?" rather than thinking about where & how data are stored, just imagine that all data are stored in some nebulous, abstract space, then let your mind naturally notice relations among data, which suggests how data should be stored.

.selling the language

_another_ lang? why? well, this isn't a new language so much as it's a new programming paradigm, and naturally i need a way to encode programs—hence the language. this language is not:

. some college nor hobby project
. a mere variant of <your favorite language here> or <obscure language here>. it's truly novel, designed by first principles and deduction, not on some arbitrary or "interesting" ideas, nor is it a mere collection of features
. an experiment that might ultimately go no where and be abandoned. it may go around a bit before it's finished & stable, but as long as i'm alive i'll be creating it in pursuit of coding perfection.

the language is designed to be as close to ideal as possible: terse, readable, elegant, refactorable, efficient (cpu & memory). if you think that "a newcomer can't best the greats" or that "one language can't have it all" then leave here and go back to c, python, lisp, haskell, or whatever. by the way, if that's you, then you are not a hacker; you're just someone incapable who latched-on to someone popular and found safety there, takes their prescriptions/suggestions, and feels threatened by anyone who questions this. a _hacker_ knows that ultimately all is just them and the object of their hacking, and that nothing changes that—not even (and perhaps especially not) any manuals that came with it, nor the opinions of anyone dead nor alive.

not only will the language be documented with a formal specification, but its design and all the reasoning that led to it will be completely and well documented, too, including things that did not make it into the design. this coding system / language is the natural result of studying _coding_ generally; thus the "documentation" is not actually documentation, but a study in its own right, and this language is a product that simply reflects conclusions from that study about what is needed or elegant to code various patterns.

== lessons from other languages

* picolisp demonstrates that linked lists can often be used efficiently b/c they're either short and so O(n)≈O(1) for small n, or are being traversed in order anyway
* factor, sql, & prolog demonstrate that data can be neatly stored in one scope: the stack, tables or relations
  ** apls do not domenstrate this; their lack of being able to nest scopes is often inelegant
* sql demonstrates that indexed data can be both simultaneously efficiently & simply traversed by using some few declarative relational primitives, and that custom traversals usually don't need to be specified
* sql & prolog demonstrate that one variable can elegantly refer to a set. apls aren't so elegant, since they impose restrictions on variables' shapes.
* factor demonstrates that a mix of stack combinators & shuffle words and locals elegantly expresses relations among (few data or relations thereof)
* multi-dispatch (generics) is the same as predicate or pattern matching which is the same as case/cond/if. a single "match" form should be a language's only expression of this device.

= index[ed set]-centric computing

tl;dr: make a lang whose only data types are 2: 1. set of index intervals (at their smallest, set may be empty and intervals may contain 1 elt); and 2. thing efficiently indexable by such. 

one troublesome aspect of apls is shape. all arrays have shape, and must conform to particular shapes for use in operations. sure, it's easy to create a new array of equal shape to another and populate it with a default value then start applying operations to it, but: 1. this is crufty and annoying to do often; 2. arrays cannot be infinite; 3. perhaps not in this case, but in some cases, as the shape changes, so must your code. were we only to specify relations, then the shapes would be implied, rather than us explicitly dealing with them. what's more, if our code is all relations/constraints anyway, then it's natural to specify some optimization constraints (e.g. x∈[0,1]) inline amongst the rest. the system would use such constraints to know the tersest encoding scheme that it can safely use. e.g. `(x<200,#x<16)` would satisfy `ceil(log2(max(x)))*#x<=128` allow it to use an avx (or w/e other vector) register.

the "computational properties" system would be used for sigfig-centric arithmetic, too. reals can be stored as literals (e.g "π" which is considered like in a CAS), or fixed-point, or rationals whose num & denom are of given sizes in bits. i recommend fixed-point sigfigs b/c they are easy & apparent, and make sigfigs easy. you'd store a number and its number of sigfigs (up to 8 for efficiency). sigfigs would be appropriate for e.g. storing stock prices (for trading, rather than record-keeping, reasons); we don't care to store more than 4, sig figs; we don't care about any amount less than 10 cents once we're dealing with prices of $10 or more, since that's at most a negligible 1% difference. this "up to 1% error" is a proprety of 4 sigfigs, and so applies regardless of the value; it applies for $20k just as much as $1. furthermore, it permits the same binary encoding for all values: say a signed nibble for the decimal place (accomodates up to 10^±7^), and 14 bits for the 4 decimal digits. sadly, in this case, 18 bits is just shy of byte-alignment, but oh well. at least the encoding remains the same for all values of 4 sigfigs.

some common reasons i've heard for why c is faster than other langs are manual memory management instead of garbage collection, or that there's no language runtime, or that it uses machine code instead of bytecode. these are all true, but they should be framed commonly: that the language forces few things between the machine and the programmer. c code makes it easy to have total control over the _computation_. most people only talk about _programs_—instruction sequences or relations that reduce to them—not the actual physical computations that occur. in c the fact of allocating memory is explicit in the code, and is thus just another instruction that the programmer specifies. this forces the programmer to be more considerate of where & how the data are stored, manipulated, & accessed, and naturally also gives the programmer control over all these aspects. giving the programmer this ability means giving responsibility & control, and thus power.

rather than nested iterators, i want to iterate once over the cartesian product of indices, except generally not the full cartesian product; i may choose during the iteration to not try any further multidimensional indices matching a given predicate. this is the same as the prolog thing of having iteration naturally continue so long as next() returns non-NULL. this system should, when used for such a simple scheme, require no more computation than a loop in assembly: next() would plainly be, in x86, the duple ([cmp ecx len; jlt], inc ecx). we don't need an else; if a predicate fails, then of course we'll try other predicates, if any are left. and any predicate may modify the set of remaining predicates; this design is like a mix of a `for` loop and a `while` loop, but of course is generally represented by `if` & conditional jump. another concern is managing which data must be preserved vs may be overwritten/repurposed throughout computations. again, like in <align-seqs>, to be clear, this is for control flow only, unlike prolog which has a global fact db. my technique works in any language (incl. asm) and is for variably local control flow. it may be global, but there are no special affordances for that. this is meant to be as inline as a loop block.

TIP: multidimensional rectangular arrays' index sets are given by cartesian products i.e. mixed radix numbers e.g. a 3×5×7 is {(i,j,k)|i∈[0,3),j∈[0,5),k∈[0,7)} i.e. i∈[0,<largest 3-digit base-3,5,7 number>).

TIP: the "in" predicate is really intersection. rather than returning a boolean, it should return the input if found. really, then, this is `find`, or more generally, `find-integer-from`, since `a b q find-integer-from` is `find` (which is the same as intersection which is the same as inclusion after lifting the element into a singleton set) but on an interval i.e. after intersecting some set or slice with another slice `[a,b)`.

specifying iteration as anything other than "next step" is a great design flaw. common examples are `for` loops, or especially iterative combinators without early exit, such as `reduce`, or words commonly implemented in terms thereof: `map` or `filter`. this immediately leads to antipatterns such as expressing "first non-negative" as `[ 0 >= ] filter first` which is inefficient. of course now you, the reader, say, "obviously they should use `find`!" but `find` is just a common combinator that happens to exist and fit this common, simple situation well. what if i want to search over two lists and perform `find`? a factor user might suggest `2 nfind`, but that still trims the input lists to be equal length. and what if i don't want pointwise traversal? what if my search space is irregular or modifies itself as it's iterated over? the variety of traversals is obviously so great that it cannot be represented elegantly by combination of any finite set of iterative combinators! what makes `next()` different is that _it is exactly iteration itself_. indeed, this reveals iteration to be no different from computation, just as a program is a sequence of instructions that execute, or a traversal is a search over some space. most generally it may be parallelized, thus being not a mere sequence, but generally a directed acyclic graph. the acyclic part is ironic, since one could earily argue that computations ubiquitously have cycles which we call "loops" but any cycle can be "unrolled" into a non-cycle, except infinite loops, which purposefully exist only in daemons. loops are noteworthy for their symmetry, but this thought is always observed when one imagines a simple loop, such as a `for` loop or even a `while`, but never does not imagine some various nested loops with complex state manipulation across the loops. at such point of complexity, one starts to call it "control flow" instead of "loops." so if "loops" are just some naive simplified representation of computation, then let's forget it and focus on the general problem, "control flow." it seems that hardly anyone has tried to actually identify the intrensic trouble in most elegantly expressing complex control flow, and have instead resorted to arbitrary pretty models such as functional programming and iterative combinators, or array programming, which are just as pretty as they are incapable of expressing complex control flow; they express it at the cost of being syntactically long, computationally redundant, and awkward—awkward because doing so requires you to identify the symmetries and asymmetries rather explicitly (the symmetries being expressed by loop combinators) but when the asymmetries are many, and are complex, then one feels dissatisfied by the loop combinators not living up to their promise of making the code look elegant. to be clear, loop combinators, like `map`, are good for expressing common, simple control flows that they are designed for, so they themselves are not bad, but rather they are insufficient to elegantly express arbitrary control flow, and a model that does so elegantly express obviates the need for such combinators. while it's good to know your code and the problem that it solves—and identifying a/symmetries is a part of that—there is no need to partition into a|symmetries; better is to specify a set of facts and let the traversal be ever implicit, like in prolog.

NB. false/empty values are represent by a `next()` which returns immediately. if it will be considered algebraically, then it has obvious properties & algebraic values.

the actual source code should read like english, and wrt symmetries, with w/o using the word "per" b/c ALL RELATIONS MUST BE IMPLICIT. especially, there are often multiple ways to express relations e.g. x per y or y per x being equivalent b/c it's just a cartesian product either way. e.g. "n-clumps of sessions, times of candles whose v or n accumulated since the session's start exceeds the prior 3 session's total v or n respectively."

an index-centric model would avoid bad design such as factor's `2each-from`, which obviously generalizes to parallel traversal of n seqs from a given offset. still, however, this function, is even poorer design because it is a particular traversal. traversals should not be in functions; they should be virtual sequences, e.g. `zipped` instead of `zip` and `cord` instead of `append`. all would be virtual; the programmer would have no ability to override this. like in k, computations would have naturally elegant information reductions e.g. reducing to `0N` which is truthy but propogates and can be converted to a falsy value by `^:`. it should not be defined, and though its definition, `-roll 2length-operator each-integer-from`, is efficient, clean, and short, it is better expressed as `[ tail-slice ] curry [ bi@ ] curry dip 2each`. the latter version generalizes to any virtual sequences of any argument seq. furthermore, i say the same for `each-index`; prefer `each` over a virtual zipping of a seq w/its corresponding iota.

case studies for implicit iteration:

* inner or outer join
* asymmetric relations, especially those that change during iteration
* combinations or permutations
* matching the elegance of `: converge ( ... x q: ( ... x -- ... y ) -- ... y ) [ keep dupd = not ] curry loop ; inline`
* enable a hashtable to retain insertion order. this is a stupidly simple operation: add an extra integer field, and modify insert to insert size() (evaluated before adding the key) along with the key. in black-box programming, this would need to be done by adding structure [read: "relation"] around an underlying hashtable that relates the underlying elements to this seq of integers. with white-box programming, there are no scopes, and...well, no black boxes! rather than subroutines, which are sequences of instructions, we use predicates, which are inherently non-hierarchical (though ofc they incidentally permit hierarchies by various traversals) and rather than support concatenation, support union, intersection, subtraction, etc. the problem is how to conveniently retain only certain relations through refactoring. catlangs make this trivial, and stack langs have good data sharing via the global state: the stack. (i suppose that stack langs w/row polymorphic word effects are arguably "gray-box", then.)
* if n elts of a relation are have a particular attr be nil, then print those items, then prompt the user to enter a list of values; validate that there n values and that all are valid, then set each of the ith attr to the ith user-provided input.
* parsers, which are the sensible, powerful stateful combination of find & replace or otherwise just any general computation on sequences. primitives are slice, find substr, and concat. snip is defined in terms of slice. insertion at idx n is defined as snip[i:i] then 3append; this obviously generalizes to replacement: snip[i:j] then 3append. removal, like insert, changes length; therefore, as replacement, it's defined as "replace snip[i:j] by the empty seq". is defined this same, and so can be naturally expressed as "replacing. there should be 2 separate functions, insert and replace, where the former changes the length and the latter does not. substr considerations generalize to subseqs, which generalize to permutations which generalize to indices some of which may appear multiple times.
  ** anything that changes seq length is just as well done for many elements as for one. only replacement does not change length, and should be done via the primitive `set-nth`, which is either done in a `for` loop or not.
  ** subseq operations commonly do such stateful things as generalizing "replace first occurrence" to "replace all", which is just "replace first" but done iteratively until exhaustion, where each iteration has a state: the index whence to start searching.
  ** i still really like the "append under rotate" idiom, though this probably isn't appropriate for the model that i'll use
  ** it should be just as easy to replace the nth occurrence by the nth element of some other sequence.
* subsequence-and-not-substring operations (and why can't these be done with factor folds (generally expressed by `each` and selective pushing into a collection vector)?)
  ** regrouping (the version of apls' en/decode that we actually want; we don't want a number of a given radix e.g. to convert to h:m:s, we want hours to be variable, i.e. for there to be any number of hours)
  ** in factor's `tzinfo.private` vocab, effectively `[ find-transition ] map` but that efficiently operates over an ascending-ordered input seq
* deep nesting e.g. `(activity-spike)` below

[source,factor]
----------------------------------------------------------------------------------------------------
: (activity-spike) ( cs -- masks f )
  [ d>> ] group-by
  [ second-unsafe [ s>> ] group-by ] map ! V{ V{ { AM V{ c ... } } ... } }
  [ 4 <iota>
    [ tuck of [ [ [ v>> ] map-sum ] [ [ n>> ] map-sum ] bi 2array 2array ] [ f 2array ] if*
    ] cartesian-map ! V{ V{ { AM V{ ?{ Σv Σn } } } ... } }. ? here means "or f"
  ] keep 3 [ <clumps> ] [ tail-slice ] bi-curry bi* ! 2map over days[i-k:i-1] & days[i] for k-slices
  ! q passed to map over sessions: ( session# clump current-day -- ? )
  [ [ overd at
      [ spin [ of ] curry map
        dup [ empty? ] any? ! is this session in all of the clump's days or not?
        [ 2drop f ]
        [ unzip [ mean 3/2 * ] bi@ [ swapd [ > ] 2bi@ or ] 2curry
          [ [ v>> ] [ n>> ] bi swapd [ + ] 2bi@ 2dup ] prepose [ 0 0 ] 2dip find 3nip
        ] if
      ] [ 2drop f ] if*
    ] 2curry 4 <iota> swap map
  ] 2map V{ } concat-as sift! [ c>t ] map! f ;
----------------------------------------------------------------------------------------------------

* replace all numbers in a string by a unary fn of each. solution in factor:

[source,factor]
-----------------------------------------------------------------------------
USING: unicode math.parser ;
: decrement-numbers ( s -- s' )
  SBUF" " clone tuck SBUF" " clone -rot
  '[ dup digit?
     [ suffix! ]
     [ [ [ f ] [ string>number -1 + >dec ] if-empty _ [ push-all ] keep ] dip
       swap push SBUF" " clone ] if ] each
  append! >string ;
-----------------------------------------------------------------------------

* empty sbuf occurs only once, so having empty checking in a loop is not ideal
* creating a new string buffer is dump; the current should be retained & cleared. this would be easy to code in applicatively.

applicative version:

[source,factor]
---------------------------------------------------------------------------
USING: unicode math.parser ;
: decrement-numbers ( s -- s' )
  [let SBUF" " dup [ clone ] bi@ :> ( acc b )
    [| x | x digit?
      [ x b push ]
      [ b [ string>number -1 + >dec acc push-all 0 b shorten ] unless-empty
        x acc push ] if ] each
  acc b append! >string ]
---------------------------------------------------------------------------

notice that the applicative version is, surprisingly to me, actually not terser! it's less symmetric, too! i'm able to apply effects (io) more selectively, which means that my conditional branches differ more than in the stackier version wherein i push `f` then `push-all`. the terseness and refactorability of stacky code is not only due to being tacit, but also due to being more symmetric! this "forced symmetry" is basically to keep all branches the same length (measurable by stack height, or, in functional langs, taking a fixed-arity fn param) or otherwise, more generally, require equality of some attribute(s) across multiple choices of data (where the data may be executable, quoted programs or branches (`if` in factor accepts two quoted program args, but `if` in haskell accepts two clauses of inline source code)). *in other words, it is to pad all choices to be the largest of their shapes.* this is how "spaghetti code" is avoided. of course, usually the padding element is the empty element e.g. returning `false`, `0`, `""`, etc in a functional language, or in a stack lang, pushing `f` to the stack as a dummy return value, as seen in e.g. factor's `find`, which returns either `idx elt` or `f f`. the aforementioned "choice padding" (or "alignment" is an appropriate term) is clearly seen as the presence of redundant information—here namely that `idx` nand `elt` <=> `idx` nor `elt` . expressing all branches by the same shape obviously makes factoring easy. sometimes this seems to be an inconvenience that we'd rather do without, e.g. factor's `loop` requiring its arg quot to preserve stack height. one might say that `loop` is inadequate at expressing what a recursive function can, where the recursive fn can return more outputs than it takes inputs, but simply return them only in base cases, and in the recursive branches not even return them; we'd either implicitly discard or preserve them by their inclusion in the recursive call. it is easier to do that, but we should appreciate that `loop` bluntly reveals such asymmetries. we may think of `loop` as a tiling of rectangles, and more general recursive functions as tesselations of less-regular shapes. another example is how both of haskell's `if` branches must return the same data type, which is either a product type i.e. a vector of a fixed length, or the union of those, which is an ad-hoc (asymmetrical) combination the choice of which must be resolved via a `case` clause. `loop` which does cannot change stack height is more efficient than recursion, just as mutating a fixed-size buffer is more efficient than shrinking or growing it. in such a literal language as c, loops cannot create new variables; in c, loops cannot vary the namespace. however, recursion can, and indeed does, as each recursive call has its own scope, shadowing scopes higher up the call stack; and the cost of retaining all these scopes is that the call stack grows. as always, generally: the more constrained a thing is, the less info is needed to en/de-code it, and the less capable it is. i discourage the term "flexible" because it is only one variety of capability. a 4-bit scheme isn't capable of representing 25 choices, just as `loop` can't represent arbitrary function chains. in the case of source code, "flexible" is commonly used, but this suggests that code be treated differently from other data, though it certainly should not be! each computation is capable of expressing some class of computation (im)practically, and the smaller the class, the more efficiently it can compute. this index-centric model achieves easy, flexible specification of constraint by stating as sets of algebraic rules. the algebra is done of a hierarchy of algebraic classes: either seq or multiset (permutation which may feature multiple copies of elements, which is useful only if their order or count matters) > set (permutation whose order is irrelevant) > permutation > subseq (monotonic inc seq) > substr (interval). each class supports its own sensible variety of product & coproduct (e.g. interval intersection/union (including: appending, which is just a non-disjoint union—a specific variety of what's generally disjoint union (clearly seen if you express a seq as a map from idx to elt; ofc you can union two maps and their key set may be continuous or not); and substring matching & removal, which naturally leaves the seqs leading to & away from the substr) vs set intersection/union) btw, note that i didn't say "unordered permutation"; a permutation always has order; it's only a question of whether its order has meaning or is arbitrary/incidental. no one will ever quite "call a `sort` word"; instead they'll mark a datum's constraint of needing to be sorted. the solver will handle sorting on a "need-to" basis.

similarly, array langs encourage users to code in terms of arrays, which are symmetric structures. homogenous, rectangular arrays are stifilingly symmetric, but hetrogeneous, ragged arrays are flexible while still being easy to reason about in terms of array symmetries. so array code is much more prone to being fewer, simpler, though less-efficient operations than a typical solution coded in a non-array lang.

also, when writing in applicative style, it's easy to forget to account for certain data, whereas usually in stack code if you forget to account for data, then it's just still sitting on the stack, yet to be consumed, which appears as a stack checker error; thus stack code is more suggestive in development. the lack of constraint among of local variables is freeing, but completely not suggestive. the lack of constraint means that any code runs, so the errors found in debugging applicative code will be much more frequently run-time errors than compile-time.

characteristics:

. no nesting/scope
. index/virtual-sequence-based. allows multiple simultaneous multidimensional indices/subsets (generalizes partitions in that they may have non-null intersections) of any structure.
. trivial factoring e.g. sums of two seqs of equal length becomes expressed in terms of one index variable.
. non-black-box traversals. e.g. one should be able to define binary search as its own idea, but effortlessly augment it AT AN ARBITRARY INVOCATION POINT to terminate with a given error value if it compares the target to a prime number. this could be achieved by mandating that each traversal expose its loop condition (i.e. next()) so that it can be modified.

implementation: system like prolog, but computation like factor. we want the stack so that we can do row-polymorphic stuff. code will be expressed by inline combinators. the stack will be used (and will use the cpu's stack literally), but locals will be available, too, and those values will be stored in general purpose cpu registers. it'll be automatic; when something is saved by a variable name (like in a `let` block), then it'll be automatically stored in whatever the next available register is. there will be manual locals management; you must unset a local, which will free its register. simd will be used whenever possible. my goal is to basically have a forth implementation with the semantics of prolog and the efficiency of apl, and all of this being done with the most succinct data structures possible so that we avoid the heap as much as possible. with modern processors, with avx and 64-bit registers, this should be very easily achievable for all code that does not need to store large collections of arbitrary data at a time (such as reading in a large list of json objects all of which must be retained in whole in memory for operations such as median). this is possible because good code never needs more than about 6 objects on the stack, nor needs more than 6 locals at a time. i'm yet unsure how the decision to store data in a local vs a stack is relevant given that we won't be nesting computations; usually i use locals in factor to avoid shuffling, which is only ever an issue for (deeply) nested quotations.

in a stack lang, when you *do* find that you've somehow made your code deeply nested, it's often easier to just put elements far down on the stack, then pull them back up as needed, rather than to try to curry & compose them into a complex tree of quotations. perhaps even better, though, is to, instead of nesting many common traversals such as `map-filter`, create your own traversal that takes n aspects [data] on the stack and uses combinators such as `n&&` to clearly specify a sequence of predicates that return data or f.

the system, like haskell's "at" pattern matching, must make delimitation something that may optionally be used if desired i.e. that we can ever ask which sets a set/obj is a subset/elt of, or for sequences, which indi(ces) a sequence/obj is at or is a substring or subsequence of. i.e. we should be able to efficiently relate data. slices correspond to substrings, index seqs correspond to permutations (which have strictly more info than subsets).

excepting non-commutative folds over ordered data, data subsetting/subsequencing and indexing should be O(1). this is a requirement for full flexability (and application of the very powerful integer arithmetic to creating selection masks) in unnested relating without worry about cost. an example is that we should be able to clump something then collect-by or group-by each clump, which gives us the new slices (for group-by) or subsequence selection vectors (for collect-by) each with their own indexing from 0 (e.g. this is the nth clump) while retaining association with the original index so that i can, without extra computation, for an arbitrary element of the original sequence, its index in the original sequence, and to which clump it belongs to, and its index, if present, in the vector returned by collect-by. that group-by pushes into a vector is terrible design: we create new memory, have more-complex code, and lose the relation between original indices and the groups' indices. the loss of relation is the worst aspect of functional languages, as is the limitation of relation the worst aspect of functions. `collect-by` has a beautifully simple definition, both in code and abstract form; however, the very simple difference of pushing objects instead of their indices loses relation! it's exactly the same as k's `=:` except that it doesn't return indices. consider this apl-like thought wrt this system's prolog form of simply being a sequence of predicates that the system then intersects naturally for one complex traversal. as always, we should explot the extreme flexibility (multiple simultaneous data representations), ubiquity, efficiency (lightweightness), and mathematical properties of integers e.g. order, partition, or arithemtic, e.g. it's easier to use a fold to compute an average by multiplying current elt by 1/n then adding it to an accumulator than to collect into an array then sum it and divide by its length! apls are excellent for using integers for everything, including preserving relation, but they lack in that they can't relate among lambdas (no shared scope, except by using globals. this is at least analagous, if not equivalent, to lacking row-polymorphic stack effects), and in that they...don't make composing relations as simple as prolog...i should study this by coding in k in practice.

at least in the meantime, it's easy to simply do array programming in factor. it might not be quite as nice as a proper array lang, but that's only to say that operations aren't fused or otherwise specialized, and the notation isn't as brief. this is efficient, flexible, and easy. just think "how would i code it in <your apl of choice>" then write that code literally in factor. granted, you start there; if it's obviously more natural to code it in a more "factor" way then just do that; the important thing is that your thinking is array-oriented.

the "changes" fn demonstrates that we should not try to be as efficient as possible; such high efficiency should be achievd only automatically by computational systems. the larger code size, let alone complexity, is not worth the marginal improvement; and such improvements should be considered relative to the hw that the code will run on, the language runtime's efficiency (if any), and other code in your program (optimize only where it makes the MOST difference).

traversals should be implied by the traversed data and their indices. the order of traversal is given by the ordinals, and the set of indices by the intersection of the index sets, plus any ad-hoc, user-specified unions or intersections, or repeats (which is just union with the infinite-sized set of integers mod n). tracking state is troublesome only ever b/c you must change state wrt traversals, and keeping those properly arranged can be difficult. however, if you simply specify variable changes as rules (i.e. "when cond, mutate in such-and-such way") then there's no trouble! binding to locals is not at all inconvenient if done apl-style. with unnested traversals, scope isn't an issue; that vars are freed automatically when scope ends might be fine.

the stack is very computationally and expressively convenient/natural for many expressions, so definitely keep using it to express computations, even if the actual computation is done by registers instead, under-the-hood. forks aren't concatenative nor as flexible as the stack. i aim to avoid using the heap, but if i do, then allocated memory won't actually be freed; it'll just be made available for new uses.

the implementation will simply compile source code directly into machine code. it may do this to produce an executable, or may do this on the fly as jit.

maybe the "find the 1st candle of each session for which each its 3 prior days' sessions of the same type [as this day's given session] has a sum volume greater than the average of the prior 3" code would be nicer to code as regrouping—like mixed-radix, but more general—where i traverse once, building-up relations & sums among days and sessions.

e.g. cs [ day+=priorday!=day; session+=priorsession!=session; f(day,session) ... ] each. then i'd just specify the RELATION OF INDICES AND THEIR CORRESPONDING SETS plainly: nth session of mth day vs nth session of days m-[1:3]. again, the traversal is implicit, or rather, it defaults to all n in sessions and all m in days. an index variable in a natural number is 0..n-1; in a slice [m:n], from [m,n], and for an array, for all its valid indices, traversed in ascending order.

using predicates (higher-order fns / quotations/lambdas) sucks b/c they break relation, but they're good in that they're efficient: they apply the quotation to each element and basically fold that result into few data, which keeps memory usage small. so let's have a system that associates computations with data (as quotations do) yet presents like apl vectors, and has a system that automatically keeps data copies rather than reducing them e.g. if i do vector expr `x*y+x`, thet corresponds to `[ [ + ] 2map ] keep [ * ] 2map`. i really should find a less-trivial example, but this demonstrates that i translate applicative code into concatenative/stack code (though i'd just explicitly code concatenatively w/optional registers anyway) so that it's obvious which data to retain. eh, this being said, i could just as easily go full-applicative by clearing all registers that were bound within a lambda, which naturally & simply implements nested scopes (though not closures, but they aren't needed anyway, as demonstrated by their total lack in stack langs. not once have i even thought about closures nor wished for a more convenient way to code anything in factor).

so my main trouble in coding is that i'll do e.g. `[ [ v>> ] [ n>> ] bi 2array ] map unzip` b/c it's clear & easy, but i totally cringe at the idea of using 2array n times (i mean can you imagine coding malloc & free for each iteration of a loop? awful! it'd be better to malloc once, set many times in each iteration, then free after the loop, but why would we even malloc at all?! of course we'd just set registers! it's only two values, and this is known statically!) then unzipping, when i could just create two n-arrays and populate them with v & n, which is common and should be its own combinator...except that it shouldn't be a combinator, because combinators are TEMPLATES, but rather a language feature for expressing such patterns elegantly by using a bit of LOGIC to convert by effective code (as in "effectively do x, but actually it's y") into literal code. to do what i actually ideally would do in factor is very bloated and unclear for how simple a concept it is:

: map-into-2 ( xs q: ( x -- a b ) -- as bs )
  [ [ length dup [ 0 <array> dup [ set-nth ] curry ] bi@ swapd
    [ [ keep ] curry ] dip compose ] dip
    [ dip ] curry prepose
  ] keepd swap each-index swap ;
{ 3 6 4 5 2 } [ [ 6 * ] [ 20 / ] bi ] map-into-2

=> { 18 36 24 30 12 } { 3/20 3/10 1/5 1/4 1/10 }

compare it to the prolog-like solution `a[i],b[i]=f(x[i])`, which implicitly binds i to RHS x; b/c x is a sequence, i corresponds to a slice, which allows creation of the default contiguous traversable, the "array" data structure; so they're created for a & b, which implictly exist by being LHS exprs. then just evaluate this expression for all i. this system depends on being built with particular consideration of indexed structures, and constraints on those indices e.g. contiguous or not, or integral or natural indices (cf hash map), and whether the indices are ordered. it can exploit these properties and knowledge of integers to make efficient code. btw, indices is the ideal solution, not having a compiler try to recognize certain code patterns then convert them to more efficient alternatives! that's ridiculously ungeneralizable and complicated!

indices are general relation. EVERY data structure should, in code, ever be useful only if its indices are used; without indices, the structure is ignored, and it's considered only as generally & vaguely as any object. data are usually dichotomized into atoms vs structures. i suggest better terminology: indexed vs non-indexed. indices may be multidimensional, and any data may support multiple indexing schemes simultaneously. even data that grows in a linukd-list fashon (e.g. ll's, rose trees) should be indexed; indexed does not imply O(1) access. and ofc, since indices are by default free variables, we may identify subsets of structures by using predicates e.g. `{x[i]|x[i]>5}` applied to a rose tree, which would simultaneously identify i & x[i]. naturally no more x[i] would simultaneously be stored in mmeory than the max arity of expressions entailing x[i].

an indexable mod n, depending on cmp(#x,n), would be clumps or repetition. because the mod n applies to indices, the "mod n" augmentation converts any O(1)-access-&-modify structure into a mutable ring buffer. the most general flexibility comes in manual indexing expressions e.g. n-groups is defined as `λi. x[i*n:i+n]`, and n-clumps as `λi. x[i:i+n]`, and repetition as `λi. x[i mod n]`. notice that there's no need to specify that `i+n<i`; the system automatically restricts the index expression `i+n` to those for which `x[i+n]` is defined, which it can do b/c n is, at the time of evalutation, fixed, as is #x. if x is growable then we'd need to flag whenever its size changes and if this flag is set at time of an evalutation, re-compute the evalution of i. i think that this may likely be insensible in practice, though. index maps are composable, so you can do e.g. n-groups of repetition.

much of these thoughts reflect(s) that i prefer tags over hierarchies; they're soupier: they don't strictly conform to hierarchies, but they may, and they may conform to multiple simultaneously, which may even overlap! consider using a set of graph nodes like i did in sql to traverse a tree; the "tree-ness" is not kept as metadata; such structure is never explicitly stated in the code, and indeed, it is NOT in the code, it is in the data! one must search for it by trying to traverse the data as a tree.

NEXT: about `changes` algorithm: suffix #x-1 only if it isn't the last elt, right?! is this check necessary in the k solution?

adjacent indices give slices all having some common property
if empty, returns the exact same input seq
this code is actually pretty simple, but this syntax is visually unapparent;
were it represented graphically like in quartz composer, its definition would be clear.
in k this is simply {&~=':f x}, which reads 100% literally: "(indices) where f(x) changes."

* k's primitives are natural, so i don't need to add 0 nor (#x)-1; and i don't need to account for x being empty, because i don't have to break x into first & rest, because ': handles that already. this compounds b/c i must apply q to both first and rest.
* i also have to implement where and each-pair myself, though this is done very implicitly by my simple use of each-index and using the stack, and storing the current f(x) as the new prior. then i must drop it afterward.

tl;dr: not natural primitives, and compounding complexity in explicitly coding it as a single traversal, instead of composing ideas then having the single traversal be computed of them.
granted, i could just do the most literal translation of k into factor—`[ map [ = not ] 2 clump-map [ 1 + and ] map-index sift 0 prefix ] curry [ length 1 - ] bi suffix`—but this isn't as efficient. granted, perhaps k actually doesn't do nearly as much optimizing as i'd think, in which case it's just briefer factor with better primitives, and no row-polymorphic stack ops. granted, this is factor, not forth, and i'm running on x86, not a 320MHz risc-v Soc evalution cpu w/16kb data sram, so such optimization is a waste, despite being a good theoretical exercise to develop an ideal stack language. still, said language would be declarative and all would be defined in terms of relations. that solution would be...well, firstly we note that it's necessarily a computation of a sequence rather than a set i.e. indices are implicit in the sequence order. next we consider predicates, starting with what we want: 0,{i|f(x[i])<>f(i[i-1])},#x-1. using i-1 as an index for x implies the domain of i: [1..#x). in this ideal language, we are done. however, unless we somehow cleverly memoize, this computes f 2n-1 times. see the two examples below. the system would need to use induction to infer that it can compute f only n times and store only the prior f(x). how would this generalize to storing multiple data? well, actually it'd be easier, though perhaps more limited, to just have a rule for clumps; obviously per-element computations need to be computed only once per element, so for n-clumps, use a ring buffer for the prior n-1 elts then apply f(x) to the nth elt.

: changes ( seq init-vec-len q: ( elt -- prop ) -- idxs )
  pick empty?
  [ 2drop ]
  [ [ <vector> 0 suffix!
      tuck ! store vec for retval
      [ push ] curry [ [ drop ] if ] curry ] dip ! part of each's quot
    [ [ tuck = not ] compose [ dip 1 + swap ] curry prepose [ each-index drop ] curry
      [ rest-slice ] prepose ]
    [ [ first ] prepose ] bi swap
    [ length 1 - ] tri suffix! ] if ; inline

applicative version:

:: changes ( s #v0 q: ( elt -- prop ) -- idxs )
  s empty? [ s ]
  [ #v0 <vector> 0 suffix! :> V s
    [ first q ]
    [ rest-slice [ q tuck = not [ V push ] [ drop ] if ] each-index drop V ]
    [ length 1 - ] tri suffix! ] if ; inline

derived from the prolog-like solution given above: v.push(0); for i in 1..len(x) if (f(x[i])<>f(x[i-1])) v.push(i); v.push(len(x))}
this computes f 2n-1 times, but is otherwise perfectly efficient:

: changes ( s q -- idxs )
  dupd [ 1 swap [ length ] keep [ nth ] curry ] dip compose
  [ bi@ = not ] curry [ dup dup 1 - ] prepose
  V{ 0 } clone [ [ push ] curry [ [ drop ] if ] curry compose each-integer-from ] keep
  swap length suffix! ;

same, but computes f n times, but traverses twice. O(2n).
it's the same definition except that there's a `map` after `dupd`, and there's no `dip` & `compose`:

: changes ( s q -- idxs )
  dupd map 1 swap [ length ] keep [ nth ] curry
  [ bi@ = not ] curry [ dup dup 1 - ] prepose
  V{ 0 } clone [ [ push ] curry [ [ drop ] if ] curry compose each-integer-from ] keep
  swap length suffix! ;


that one stock problem: "given a seq [(time,val1,val2)], partition into days, then partition each day into hours, then, for each hour, find the first time, if any, that that hour's cumulative val1 or val2 was at least 3 times the average val1 or val2 of the 3 prior days."

* that i must be particular about which array i pass to `find` (as opposed to which arrays i curry into some traversals within find's quotation) is a total pain. i must do it b/c find returns an object from the array that it traverses over.

NB. with all traversal being implicit, and all implicitly being set (like array) so map/filter/produce are implicit, we effectively get traversal fusion for free.

== stack stuff

stack programs' execution is beautifully simple, which makes debugging very simple, nice, easy. granted, having watches on registers is just as clear as watching the stack. the system would know which registers it's tracking, much like how any system would be able to show all the variables per (nested) scope and their values. why they don't do that—why one must _add_ watches on variables—i don't know, but it's a bit of a pain to have to do so.

if i do stack stuff, ofc it doesn't actually need to use a stack. it can be a virtual stack; all that makes a stack is...well, actually nothing! stack langs aren't stacks! they're just tacit buffers! i mean, it's stack-like that evalutation occurs from the top, but we can `ndip`, which is to evaluate _not_ on the top. the stack is merely ordered, in-scope data. shuffle words merely permute the stack. we can easily have a virtual stack of max 8 elements that uses 8 general-purpose registers. i don't even need a return register if i use the shuffle definitions given by allisio in mlatu below. one central design that i won't compromise on even 1mm ever: everything is virtual & algebraic, never literal—so nothing like `compose` not actually composing but instead wrapping data together into a tuple whose class method for `call` just calls one item then the next.

=== mlatu

it's a term rewriting system, which is not reductionist^*^; for terms of referentially transparent rewriting rules, the rewriting is invertable. indeed, terms seem exactly appropriate for some problems, such as solving a rubik's cube—namely where *the string of terms reflects the string of mutations*. this is quite a different scenario from real analysis! indeed, numbers have no place in solving the cube! all aspects of the cube are arbitrary up to its group structure. think that sucks because "term" isn't "number"; it's not specific enough. however, it does invite some questions for me to answer:

^*^by _reduction_ i mean β-reduction [λ calculus], which contrasts with deduction [prolog, term-rewriting systems]. the differn between these two classes is that the former has no algebra but the latter does. algebras support solving for efficient programs. non-algebraic code systems are only shorthands e.g. a _function_ for squaring a number `: sq ( x -- x ) dup x * ;` [forth/factor] is just as well as writing its definition. this is equally true of applicative-style codes after relating variables by name. this is no different from assembly language macros. almost all programming languages are basically this; however, each one tends to add some unique bullshit features that only ever somewhat add semantics to data, namely in the form of type systems, though rust uniquely has its borrow checker. anyway, despite some type systems themselves being algebraic (e.g. those used by haskell), the type systems used in programming are not purely type systems; they're type systems applied to code. therefore the code is restricted by the "almost" type system, and the type system is only nearly complete, which creates edge cases where the type system simply does not make sense. it's a "lose-lose" situation. the only way for a type system to work well in code is for it to be complete, which requires that we have dependent types such that every value definitely is considered as all of the types that it can be, and thus implementing such a type system requires a strong type algebra and typing every value. in practice, type systems have not been used to deduce nor produce programs, but rather are merely stated constraints that can at best avoid _coding_ mistakes that have little to do with the structure of the abstract system being coded, but rather avoid mistaking a variable by its name/scope, or forgetting an operation e.g. trying to pass a number to a function defined only for non-negative inputs, but forgetting to `0 max` first. useful as they are cumbersome, such type systems trade flexibility & ease for correctness & stability.

. which operations are not of numbers? are there any? i must consider this to enable the language to account for even non-numeric values. consider that apls are entirely sequences of numbers/codepoints. this obviously means that anything else is not needed _in apl_, but we know that apl does not handle io or perhaps other special resources.
. my system is algebraic. for numbers, the algebra is understood. but what algebras can i use for arbitrary values? term rewriting systems exactly deal with arbitrary values (symbols i.e. terms), so i should consider such a calculus, provided that i may need to account for non-numbers.
  .. while considering algebras, ensure that you don't get distracted by them; remember that all is seqs or maps. everything else is just unnecessary semantics there-atop which may represent a real idea, but which may be expressed more exactly/plainly directly by the actual underlying info i.e. seqs/sets. all crufty semantics are just concepts (basically notations) to represent relations (of particular subsets.) remember that a common synonym for "relate" is "group" (though relation is usually meant to totally preserve structure, whereas grouping not always is), and relations are expanded (e.g. (a,[b,c]) expands to [(a,b),(a,c)], or are duplicated, or are removed (such as in filter, which is just group/partition but discards (at least) one of the subsets). many languages make the mistake of structuring data as distinct objects; this has the consequence of being treated specially, which means that for each unique type, one must define words that work with that type. ofc that'd be too limiting, so type structures such as ad-hoc polymorphism are created so that certain operations can work for multiple types. other "semantic" structures are created to cope with the limitation imposed by distinct types. the natural solution is to use the only necessary type—relation (which may have an attribute whose order is used for sequencing, or may not be present in which case the relation (n attrs) generalizes an assoc (2 attrs), which generalizes a set (1 attr))—in which case one expresses relations merely by index (which may be any symbol including a lexiographic name or number). aside from relation are actual computations—namely arithmetic—which can be expressed as relations, too. rather than creating data structures, we have _relation templates_: notations that express relations e.g. "pointwise" which could be used to define e.g. a dot product. there's no reason to create a vector class then define an insteance method called "dot". and as anyone who's used sql knows, pointwise is join on equality, which is an efficient operation if the join expression is an indexed value.
. which calculi work for distributed or multithreaded systems? it'd be nice to have the language naturally work for such systems, too. referential transparency might be considerable here. for example, if my language is not only concatenative, then what other evaluation strategies does it permit? referentially transparent concatenative programs may be broken at arbitrary points, each computed in parallel, then their results stuck together into a new concatenative program, which may then be evalutated (although this is probably not quite true and probably has many caveats).
. term rewriting is obviously reductionist. most langs are. even prolog is implemented ultimately by assembly, which is...reductionist, right? so perhaps the "logical vs reductionist" dichotomy that i supposed is actually nonsense, and that "computing" is a better lens. we have data in registers and on the stack and in memory or whatever, and we do stuff with it, and that stuff either replaces the data or keeps it, and then our programs are just traversals, and efficient traversals are chosen for given circumstances. there's nothing more to say. these are the basic, unavoidable facts of the computers that i'm writing for.

of course, all these questions will foremost regard the architecture, which i'll assume to primarily be risc-v, though it'd be little effort to make it work on x86, arm, or any risc or cisc.

mlatu has 6 primitives:

[options="header"]
|====================
| mlatu | factor
| `+`   | `dup`
| `-`   | `drop`
| `>`   | `[ ] curry`
| `~`   | `swap`
| `<`   | `call`
| `,`   | `compose`
|====================

ofc many convenient shuffle words may be defined in terms of these, though i'm unsure yet how universal this primitve set is. certainly keep & dip are important primitives, which leverage a return stack, which mlatu seems to lack. this is likely a worthless consideration since mlatu is made to be referentially transparent and a term rewriting system, which sounds more like a theoretical thing rather than something practical and efficient, tailored for computers.

.common words defined in mlatu, given by user "allisio"

it's easy to accept a challenge of "how would i do a given some constraints/designs?" but this is not a helpful challenge. here we have some commonly-known words defined in mlatu. but why? to show that it can be done in mlatu? surely it can, and one might even feel prideful about having completed the challenge, and might even consider it progress, and might want to continue so satisfying themselves by continuing to define many other words in mlatu, in terms of the words that they've yet defined. one may delude themselves into thinking that solving problems or conquering challenges is anything more than entertainment or a waste of time. the very premise, "how do define these in mlatu" is arbitrary; why should we do it in this way, by the constraints of mlatu? naturally one tends to fiddle around until they get a solution, and then they use that solution to solve other problems. this is hardly a strategy! as one would expect, one ends-up with very many words rather than a single, small, simple, elegant system for expressing arbitrary computation.

this thinking is dangerous; it creates thought pollution, which is distracting! many "new tools" are developed to "solve prior tools' problems", but there was hardly any solving going on. instead, it was just "hey it occurs to me that we could do something in this other, less painful way" and so they do, but that's not design; it's just occurence, and worse, it modifies an arbitrary part of the original design, rather than questioning why this problematic part existed at all in the original design; perhaps a greater subset of the original design is inherently problematic!

a common quality of mastery is doing much with little. conversely, a common quality of being ungraceful is to do much in order to succeed, and be proud of the success, without questioning how you could do it better or what got in the way of making it more graceful, or why you chose a given design over some other alternatives.

many of these words match the antipattern of defining a thing then using said definition to define a 2-thing for it, which generalizes uglyly to n-thing. the good design is to define an n-thing which then you might consider aliasing to some k-things for (m)any k. the n-design is natural in an algebraic system; the "repeat a basic computation" design is natural to a literal, reductive execution system.

[source,mlatu]
----------------------------------------------------------------------
nip: swap drop ;
2nip: nip nip ;
dip: swap quote compose call ;
2dip: swap `dip dip
swapd: `swap dip ;
over: swap dup swapd ;
2dup: over over ;
curry: `quote dip compose ;
2quote: quote curry ;
rot: 2quote swap `call dip ;
rotd: `rot dip ;
roll: rotd swap ;
pick: 2quote over `call dip ;
reach: `pick dip swap ;
uncurry: dup () `call dip uncurry' ; // same as uncons
uncurry': 2dup curry reach over = (drop `nip dip) (2nip uncurry') if ;
conj: quote compose ;
map: () map' ;
map': pick () = `2nip (`uncurry 2dip roll pick call conj map')
----------------------------------------------------------------------

== implementation from first principles

=== assembly

.primitives

* data storage. in total, "state"
  ** registers. multiple, possibly-overlapping, data can be stored in 1+ registers. O(1) access & set.
    *** special e.g. `add` stores sum in `eax` always
    *** general
    *** vector
  ** stack: assembly's only fast data structure whose data are explicitly separate. it's growable and lifo-only access.
  ** heap. slow; avoid.
* programs are opcode seqs. for fast code, avoid jumps.

we must store in a temp buffer if:

. multiple invocations of an opcode that uses special registers e.g. to keep multiple sums, we must copy from eax to elsewhere
. nested fn calls. any nested fns that don't use the same registers can be expressed as one fn. by _function_ i mean a cpu state transition contract.

.stack machines

* stack machine programs rarely put more than 7 data on a stack. given that so few objects must be stored during runtime, implementing a stack machine in assembly should be easy at least *if all word definitions are inlined*. dynamic evaluation (quote & eval) might complicate things, too.
* a stack program is homoiconic: a sequence of _words_ pushed to the stack. words are un/quoted (sequences of) symbols, or datum literals. words are executed when encountered, or pushed as literals if quoted. 
* the de facto elegant turing-complete set of 6 stack primitives is: `eval`, `quote`; `drop`, `dup`; `compose`, `swap` (or `dip` seemingly would be just as well, and would utilize a return stack). there is the basis {`cake`,`k`}, which i haven't looked at; see von thun's paper.
* concatenative: stack programs—the sequence (composition) of subprograms (word sequences)—is associative; all referentially-transparent subprograms can be evaluated in parallel, then their results replacing where they were in the original sequence, and that sequence, when evaluated, produces one result.

== TODO

* how to minimalistically express `p q ?` in computations without redundant computation. e.g. in a loop i don't want to check a condition that doesn't vary with the loop; i prefer to, before the loop, conditionally determine, at runtime, a loop quotation.
* check-out factor's `bit-set`: apparently, efficient storage of intervals [0..n] minus some few arbitrary elts.
* an essential test of the system is that loops are always implicit, and to the degree that they're apparent in code, the loops must be absolutely freely permutable effortlessly; there must be no calculus to rearranging traversals! this implies that there cannot be a `curry` nor an "each right/left". this is fortunate, as it eliminates all "left & right" duplication e.g. join left & join right, each left & each right, etc. left & right are folly anyway, since they assume only 2 args, and again, binary is the basis for reductive code, whereas seqs of arbitrary length is the basis for elegant code.
* consider join and "factor", a form of info compression: `[ \ ] [ ∩ ] [ swap \ ] 2tri`. one datum in many exprs implicitly relates those exprs; their intersection is non-null. "intersect" can be described as "largest common form". certainly all code is merely data being together vs apart. union is just another way of specifying a subset (of all). `AND` is application & merging whereas `OR` maintains distinctness/separation.

note about parsers: we can impose models, such as the array model, which does not affect the basic case e.g. `1+2` is `3` regardless of whether the array model is imposed or not. yet things that would otherwise be nonsense (uninterperable) are interpreted sensibly by it e.g. `1 2 3 + 4` produces `5 6 7`. we can freely union additional orthogonal parsers (orthogonal meaning here that each parser's parsing expressions do not overlap) without worry about changing the interpretation (meaning) of our code. we're also free to install new non-orthogonal models and compute the overlap then choose the order in which sets of overlapping rules are tried, and we can run it on code to identify which subsets of our code's meaning may change by installing the new parser. obviously <installing a new parser whose rules are tried only after the prior parser's overlapping rule fails> will affect only if the original parser fails, which may or may not be expected in your code, depending on how you wrote it.
