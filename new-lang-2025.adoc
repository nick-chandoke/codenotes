TODO: combine this document with other language design ones

TIP: a sign of a bad language is that it enables programmers to define their own primitives or language structures, such as operators or syntaxes. it demonstrates that the language itself did not choose a good set of universal/fundamental primitives and syntax. the ability (and thus, implicitly, encouragement) for a user to extend the language does not make it "powerful"; it just shows that the language as-is is insufficient! k is a good language. it has excellent primitives that work well together, and gives no affordances for the user to extend the language. thus k is confident in that its design suffices for all needs. and indeed, its regular, small syntax, and lack of lingustic-semantic features, instead using data, makes the expression of all code terse & elegant. it's not ideal, but making the language better is an apt solution—not making the language _extendable_! btw, syntactic sugar sucks, too; it's the same issue, except that the extra syntaxes are built into the language rather than being defined by the programmer. the only thing that a user should be able to define is a relation. in fact, one simple rule: *the further that a program is from merely declaring relations of data, the worse-off it is.* this makes prolog similar to ideal, but its syntax, macros, and certain control flow make it poor. features suck. language sucks. this is what makes factor so good: there's no language; it's only relations/data. factor's shortcoming is that its model is _too_ simple, thus requiring many helper words to be defined & used, and for many common patterns to be repeated. you just don't notice the repetition because it's hidden by combinators, which are defined in terms of other combinators, ...and so on. contrast this with an apl-like model that has not only a simple language, but very specific, restricted semantics: the data structure/type, primitive operations, and language ALL work together very particularly so that EVERY variety of these 3 has a distinct meaning. giving meaning to particularities saves the programmer from needing to specify those particularities by what are literally certainly more-verbose ways. io & environment aside, k is incomparably better than factor because it comes only with its primitives, whereas factor has a whole load of standard libraries.

at some point after turing devised his machine, we went from "computers manipulate state, and the logic behind the manipulations is called a 'program'" to "programming _languages_". where in the hell did "language" come from? "code", sure; obviously we must express the program somehow! but what _language_? grammar? technically, yes, but it's leading us away from what a computer is! none of scope, symbolic values, clauses, is needed! turing's machine is beautiful in its minimalism i.e. its adherence to its fundamentals: it's a sequence of bits! obviously we can generalize this easily to make it much more powerful and easy to work with: indexed numbers. the fact of having indices implies that the numbers are located in some structure. since any linear structure can be make multilinear simply by marking partition indices (e.g. a 10-element vector is a 5×2 matrix if we convert a 1D index into a 2D one by moduluo 5), so a tape is just as well. obviously all values are numbers, since that's all the hardware supports. why is anyone pretending that anything else is going on? how could anyone not be satisfied with a stack or two, a few registers, and the heap? there's so little to computing that i'm baffled by all that people have created! if someone were to simply make a language with prolog's control flow, apl's primitives, and forth's syntax, that would be better than anything that exists yet!

NB. throughout this document i refer to sets and sequences interchangeably. they are equivalent: all seqs may be represented by indexed sets, and all sets may be represented by sequences whose order and whose elements' multiplicity is ignored. ...this being said, there must be some computation to check whether each element has already been considered.... from an information-theoretic perspective, we must store a bit (or number) for each element to store whether it's already been encountered (or how many times). this requires a structure additional to the input seq. said structure would need fast finding of elements, so likely a hashmap, ordered growable array, tree (or other graph), skip list (or other probabilistic data structure). structures are internally either arrays or graphs of pointers. which of these (or what combination of them) is chosen by the system depends on the operations performed / predicates on some data. pointers are used only when a data structure must grow.

a general note: expectation of structure is doomed. we see this when people's simple life plans don't work-out, always due to unforseen events or that their plan worked but it wasn't as fulfilling as they'd expected. the same is as true in code as in life: adaptability is paramount. even fault-tolerant systems are just that: fault _tolerant_; they aren't to _avoid_ faults; they're to handle them elegantly and keep operating despite them. _design_ itself is a bad idea! go a step short of design: just the study of items (or subsystems) in systems. know how they work, so that you can use them _when you need them_. it is always a mistake to assume use of something before the very moment that it's found to be necessary.

sets/predicates are essential for refactorability; for example, changing `push` to `push-all`, or needing to use an array where there was a singleton before, or needing to change array dimensions, all suck, if you're simply adding extra elements that can be in a set! k is actually pretty nearly there already, btw. also btw, the system can simply check for e.g. conditions that were on an atom but are now on a set; the system will spot atomic predicates on on sets, and inform the programmer that they must choose a subset to satisfy the predicate e.g. any (1), all (n), or all or any of some subset. note that "any" and "all" are distinct and complementary; any means "next()" and all means "next() until exhaustion".

as you consider this document, try to keep the perspective not of "how can i structure this data?" but instead, "what queries do i want to answer?" rather than thinking about where & how data are stored, just imagine that all data are stored in some nebulous, abstract space, then let your mind naturally notice relations among data, which suggests how data should be stored.

.selling the language

_another_ lang? why? well, this isn't a new language so much as it's a new programming paradigm, and naturally i need a way to encode programs—hence the language. this language is not:

. some college nor hobby project
. a mere variant of <your favorite language here> or <obscure language here>. it's truly novel, designed by first principles and deduction, not on some arbitrary or "interesting" ideas, nor is it a mere collection of features
. an experiment that might ultimately go no where and be abandoned. it may go around a bit before it's finished & stable, but as long as i'm alive i'll be creating it in pursuit of coding perfection.

the language is designed to be as close to ideal as possible: terse, readable, elegant, refactorable, efficient (cpu & memory). if you think that "a newcomer can't best the greats" or that "one language can't have it all" then leave here and go back to c, python, lisp, haskell, or whatever. by the way, if that's you, then you are not a hacker; you're just someone incapable who latched-on to someone popular and found safety there, takes their prescriptions/suggestions, and feels threatened by anyone who questions this. a _hacker_ knows that ultimately all is just them and the object of their hacking, and that nothing changes that—not even (and perhaps especially not) any manuals that came with it, nor the opinions of anyone dead nor alive.

not only will the language be documented with a formal specification, but its design and all the reasoning that led to it will be completely and well documented, too, including things that did not make it into the design. this coding system / language is the natural result of studying _coding_ generally; thus the "documentation" is not actually documentation, but a study in its own right, and this language is a product that simply reflects conclusions from that study about what is needed or elegant to code various patterns.

== lessons [learned] from other languages

depth should correspond to text direction, not nest. e.g. rather than `[ [ ] map ] map`, it should be `[ ] [ ]` with the fact of the quotations mapping being implicit. this is just a desire, and a shoddy sketch, certainly leaving much to be fleshed-out. off the top of my head: is this a concern at all for array or set languages? what about `curry` naturally consuming from the top of the stack into part of a quotation? what about when we have `[ [ ] map stuff ] map`? i'd think that all of these questions are irrelevant since set langs don't concern depth at all and that any "nesting" is really not nesting but rather multidimensional indices, which are treated no differently from unidimensional indices in code. i suppose that this concern really begs the questions:

* use of curry—common in factor, where quotations are built-up, though fried quotations are the applicative version where we don't need curry; all quotations are expressed as inline code, which is still natural, but is it sometimes less elegant? also curry in factor can be used instead of \: & /: in k, but shaped arrays & rank in rectangular apls generalize each-left & each-right nicely, but is rank a useful generalization? and how does it practically vs theoretically compare with sql or prolog?
* what can we do with notation? this is a text concern, and text is unidirectional, so it supports concatenation, which makes it a natural notation for concatenative programs, which are elegant; but can we achieve even better elegance by going from text to some graphics or using multidimensional syntax?
* what's an elegant way to conceptualize depth? is depth a foolish concept, an illusion, that we should ignore? depth obviously conjures thoughts of scope/subset and "for all", but aren't both of these obviously ubiquitous thus making "depth" a redundant term? what's more, putting "for all" and subset right next to each other is a funny thing since a subset is either the whole (indeed, _all_) or a proper subset (just all the things but filtered!)
* the question of concatenative/stack vs applicative is stupid. the answer is "obviously neither": if you don't immediately know what to name a variable, then it probably shouldn't be named and should instead be an anonymous [read: tacit] datum in a combinator pipeline. conversely, if you have complex data relationships, then you'll likely want to reference variables by name, and naming them is likely easy since they'll likely reflect clear ideas. combinators are nice because they dispense with what's commonly needless: naming variables. furthermore, combinators are nice because they reflect symmetries, which has the bonus of making code factorable i.e. when code is represented in terms of common patterns then those patterns can be expressed in terms of common patterns and so on until the code is compressed ultimately. as i think i already mentioned elsewhere, there's no excuse for not using a stack + variable model since during any program's execution, never are 16 or more data needed at once, which means that either a stack with max size 16, or 16 registers, is ample, let alone having both together. actually using a stack isn't even helpful; the utility of a stack language is not the stack, but tacit combinators. the stack is a very good way to tacitly represent data, though i wonder if there's something better. j's `x` & `y`, and clojurescript's `%n` arg syntax allows us to reference data by position, without naming them.
* variadic functions aren't needed, and so neither are function argument delimiters (no parentheses nor commas in function calling syntax)
* i totally forgot about erroneous looping forever. `while` is rarely needed! traversals through finite spaces are exceedingly common. they're always through data structures or numbers. usually they consider subsets until null or a short-circuit flag. sometimes, however, some may loop forever. an example of one that shouldn't is a fn followed by loop state transition fn of right bitshift, since that definitely approaches 0. right shift does not. similarly, subtraction of unsigned integers w/o overflow for converges to 0. addition of non-zero size_t's always eventually satisfies `[ x >= ]` for some `x`. as this language is index-centric and traversals are just fns applied to tacit indices, there should be safety checks for halting code.
* lexical scoping is a mistake, but scoping to a subroutine is good. also i like, particularly enabled by a stack machine model, once a datum is off the stack, it's gone, implicitly marking its availability for reuse. we could have a function that uses 4 variables, but only one at a time, so we could store each of the 4 in 1 register.
* arrays are very useful in small sizes e.g. using `u/` on a 2-vector instead of needing to do `first2 u`. if using simd then the vector version is faster! yet duples are very common (e.g. alists, at least in non-array langs). still, the point is that simd affords us, despite being array (and thus perhaps supposedly more complex), even more-efficient operations than memory (struct or c array) access.

the stack model elegantly updates multiple values simultaneously for computations that don't change stack height (such as would be used in loops) but store "last n elements" e.g. in

[source,factor]
---------------------------------------------------------------------------------------
: 3-clump-mask-map ( seq q: ( a b c -- ? ) -- idxs )
  [ [ first2 ] keep 2 tail-slice ] dip over length 2/ <vector>
  [ '[ [ rot 2over @ ] dip swap [ 2 + _ push ] [ drop ] if ] each-index ] keep ; inline
---------------------------------------------------------------------------------------

`rot 2over` is `( a b c -- b c a b c )` so the last 2 elements `a` & `b` become `b` & `c` while we still perform a fn on `a b c`. the stack makes some updates nice. often, though, it does not, or the elegance isn't considerably better than expressing it applicatively, especially when we must calculate how to arrange the data on the stack to attain such elegance.

honestly i've been away from lang design awhile rn, and i've probably already answered these questions simply by designing designs that don't even allow many of these concerns.

''''

many langs have useful limitations:

* picolisp uses only dynamic vars
* factor uses the stack primarily, though has dynvars and locals
* apls don't nest scopes
* apl verbs support only at most dyadic verbs
* ngn/k verbs support no more than 8 args
* by its stack nature, factor doesn't support variadic fns

there can be the tendency to be overly concerned about limitation, e.g. "what about if i want a ternary fn, or a fn with more than 8 args, or to use locals instead of the stack" etc, but this is a slippery slope. enjoy the limitations':

. simplicity
. compression & exploitation thereof that it allows e.g. j's grammar is context-sensitive b/c it exploits its contrived grammar and thus achieves an extremely small grammar with many ad-hoc rules and very terse code
. efficient implementation, speed, and resource usage

the caveat is that either your code will rarely-but-sometimes be inelegant, or you'll have to learn how to structure your code to be elegant. i find great relief in that so much factor & apl code is so readable, terse, elegant, yet uses limited systems. they demonstrate that code does not have to be complex, though complex code is commonly written in languages that allow it e.g. java, ruby, python, c++, lisp.

in much of my personal coding, i rely on fitting as much information into a structure as possible. frequently i use e.g. all combinations of 3 booleans which i store as a number 0~7, or i map 2 booleans for each of n values where n<64 which i can store in a 64-bit dword. these coding schemes that _just get by_ are similar to having at-most-binary functions. indeed, they don't accomodate all, but they do most and their limitation enables great things, such as j's fork syntax.

some simplest designs are best, but others not. consider delimited continuations vs goto. they're both _branching_. consider exceptions in c vs java vs factor or haskell. though exceptions are commonly thought of as a unique concept, it's again just branching, no different from an `if` [factor] or thread mailboxes! all are just conditional jumps, though mailboxes feature an `await` call, but that's really hardly a conceptual adjunct!

elegant design is tricky. to achieve it, at least one must very strongly know their architecture's primitives (namely arithmetic, set & get @ idx, conditionality as arithmetic or, for io, jumps, or non-conditional jumps for forever-looping), and to begin designs that are easiest to implement for that architecture, then note when symmetries are needlessly powerful and thus can be broken into asymmetries so that each asymmetry has a separate behavior.

any time that you ask, "without X we can't do Y?", first ask: 1. do we actually need Y? can Y elegantly, effectively be done by things that we already have? 2. is Y even a good design? for example, pil did w/o macros, instead using allowing common lambdas to be used as fexprs. this is good design!

''''

* picolisp demonstrates that linked lists can often be used efficiently b/c they're either short and so O(n)≈O(1) for small n, or are being traversed in order anyway
* factor, sql, & prolog demonstrate that data can be neatly stored in one scope: the stack, tables or relations
  ** apls do not domenstrate this; their lack of being able to nest scopes is often inelegant
* sql demonstrates that indexed data can be both simultaneously efficiently & simply traversed by using some few declarative relational primitives, and that custom traversals usually don't need to be specified
* sql & prolog demonstrate that one variable can elegantly refer to a set. apls aren't so elegant, since they impose restrictions on variables' shapes.
* factor demonstrates that a mix of stack combinators & shuffle words and locals elegantly expresses relations among (few data or relations thereof)
* multi-dispatch (generics) is the same as predicate or pattern matching which is the same as case/cond/if. a single "match" form should be a language's only expression of this device.

the essential quality of arrays is that elements are considered independently of each other; for example, `map` acts on one element without regard to others. this is different if we use `reduce` or `clump-map` (as it's called in factor; in k it's called `':`). of course, the independent case is simpler than the non-independent one, so it's easier to express the former in any language than the latter. though array langs are much better for the former, they're not so much better than non-arrays langs for the latter. my sql solution is `select distinct y from x join y where y>=x group by x`, which i assume is efficient b/c sql's semantics are extremely constrained and thus can be made very efficient. my k solution was `(?x{*(~x>)#y}\:y)^0N`, which is inefficient.

NOTE: after i wrote this whole section, and spent like 1 or 2 days on it (it was so harrowing that i don't remember how long it took), cameron came-up with a solution in a minute that used bins. i assume that bins is efficient since both inputs must be monotonically increasing and the single bins operation considers both arrays altogether. the solution is not only more efficient, but terser: `y@&~=':x'y`! an apl solution would use nub sieve, which, btw, in k is `{(x?x)=!#x}`. now i'm starting to think that bins is as powerful as sql's fundamental use of ranges to efficiently answer queries. the fact of using the result of bins to nub sieve then use as a mask on y amazed me. perhaps bins alone makes apls sufficiently efficient...! and i've been told that bins specifically takes advantage of its argument being ordered. certainly it is interval index, not necessarily executed as binary search, despite the name-pun "bins".

TODO: iirc somewhere in this section i say that my cpp/factor solution has the inefficiency that x is incremented once but that i should increment `i` multiple indices at a time. this is wrong; b/c idk x[i+1], i can't just increment i beyond 1.

[source,factor]
-------------------------------------------------------------------------------------------------------------------------------
: first-following ( x y -- z )
  [ 0 0 ] 2dip over length <vector> ! 0 & 0 are last idx & val
  [ '[ pick ! gotta check that last idx is truthy
       [ 2dup >
         [ drop ]
         [ swapd [ >= ] curry _ swap find-from dup
           [ nipd dup _ push ]
           [ drop swap ] ! (A). leaves a false "last idx" value.
           if
         ] if
       ] [ drop ] if
     ] each
  ] keep 2nip ;
-------------------------------------------------------------------------------------------------------------------------------

* it's stupidly (though not necessarily considerably) inefficient in that `each` does not short-circuit in the line marked (A); once (A) is first reached, it's necessarily taken for all subsequent elements of `x`.
* needing to check whether the last index is truthy or not is dumb. maybe i could use `or` or something to make dealing with it more elegant, but then the fact of needing to do _that_ is dumb.

since i haven't learned zig yet, cpp offers the most expressive & efficient solution of all:

[source,cpp]
------------
#include<vector>
#include<iostream>
std::vector<uint> v;
uint x[] = {2,3,4,5,6,8,9,10,11,12,16,17,18,19,20,40,60,80};
uint y[] = {5,8,9,10,11,12,19,26,27,28,29,32,43,49};

int main(){
  for(uint lv,Ly1=sizeof(y)/sizeof(uint)-1,i=0,j=0;i<sizeof(x)/sizeof(uint);i++){
    uint a=x[i],b; // a & b are labels for x[i] & y[j]
    while(j<Ly1 & a>(b=y[j]))j++;
    if(a>b)break;
    if(a>lv)v.push_back(lv=b);
  } for(uint x:v)std::cout<<x<<' '; std::cout<<'\n';
}
------------

* it'd be nicer to express `j` as "the next in `y` greater than `a`". (B) can pretty easily be understood to mean "`b` is earliest in `y` greater than `x[i]` and `j` is `b`'s index" but it could be more clearly expressed.
* i didn't catch the #y-1 off-by-one error at first because i used `&&` instead of `&` so `y[j]` never got executed after `j` increased to #y.
  ** needing to consider order of operations sucks. i just want j to go through `y`. i wish that i could just say "b in y" as a constraint and conditionally call `next()` as necessary. i cannot simply null-terminating `x` & `y`, and omitting `i` & `j`, using `*a` & `*b` pointers directly to iterate through `x` & `y`, because null-terminating breaks comparisions such as `a>b`; i must iterate through `y` until i get to its end—its maximum value—and retain that value for comparisions. of course i could add an extra variable to maintain the last iteration's `b` (`lv` won't work; it's the last _pushed_ `b`), but at that point i may as well use the code that i already have. at least the code that i have minimizes the number of dereferences anyway. still, this demonstrates the strong desire for not looping combinators, but traversal primitives, specified as predicates, which combine commutatively e.g. "b in y" and "next b" which does nothing if `b` has no next. keeping indices as variables is legitimate if they're in arithmetic operations, but that's not the case here. `next()` is nearly the only needed iteration primitve needed. sometimes iterations return to their start. rarely is an iteration variable reset to any other prior value. similarly, rarely is an ivar set forward more than one position at a time. binary search is a familiar counterexample: "next" is a fn not only of the usual `i` & `#x`, but also `x[i]`. the advancement of the index fn is not the usual increment, but a complex one, even involving conditionality: next(i,j)=mi=(i+j)/2;m=x[mi];m<Q?(mi,j):m>Q?(i,mi):(mi,mi). considering `(i,j)` as a range (which i assume is a sensible primitive given its fundamentality in sql searches), `(a,a)` is a particular termination case; obviously we cannot shrink it, so we must be done
    *** this is actually a particular case of the convergence/termination/fix-point pattern, next(i)=i. we may try to shrink, but we'll certainly be left with the same interval which doesn't support shrinking, and thus the interval remains the same from one iteration to the next. we'd definitely loop forever, so we may as well consider this a loop termination case.
      **** a seeming example of fix point termination is when next() fails because we're already at the end, and thus returns the same value. however, this is indistinguishible from merely encountering the same value twice, which is a common occurrence. the truely sensibly convergent version is when next() fails and its _index_ remains the same. *for this reason, ivars should be accessible as either their index alone, their value alone, or both altogether as a whole object.*

so, i wrote the factor version first, then i wrote the cpp version, then i tested the factor version on different inputs, and i found that it's actually incorrect! given how much better the cpp version is, i did't even try to fix the factor version; instead i opted to take far too long & arduous a time translating the cpp version into factor:

[source,factor]
------------------------------------------------------------------------------------------------
: first-following ( x y -- z )
  [ dup length [ <vector> tuck ] keep ] dip -rot [ [ length 1 - ] keep ] 2dip ! v x #y-1 y v #x
  '[ [ _ nth
      [ [ < pick _ < and ] curry [ dup _ nth dup ] prepose ! ( j -- j b ? )
        [ drop 1 + ] while
      ] keep ! lv j b a
      ! if(a>b) don't loop again; else{if(a>lv)...; loop again;}
      2dup < [ 2drop f ] [ reach > [ dup _ push -rot nip ] [ drop ] if t ] if
    ] [ _ < and ] [ 1 + swap ] tri
  ] [ 0 0 0 ] dip ! lv/b j i
  loop 3drop ;
------------------------------------------------------------------------------------------------

NOTE: there's an off-by-1 error similar to the aforementioned one. it may oob of x b/c, whereas cpp's `for` checks the condition before executing the loop, factor's `loop` checks at the end and is hence a do-while. thus the error is that i increment `i` _after_ testing it against #x. it should be before. as before, it'd be ideal to just say "for i,a in x". this syntax exists in lua and is nice, and it'd be perfect here as-is, but again, ideally this form wouldn't necessarily increment `i` & update `a` on each iteration of the loop; it'd happen only when i manually call `next()`.

the cpp version was easy & natural to write, but i admit that i understood the algorithm better by being forced to know where each variable was used, leading to the `tri` on `i`. as simple as i'd intended the cpp algorithm to be, i realize that someone who isn't familiar with the algorithm would have to figure out the total meaning (use) of all seven variables, despite `i` & `j` corresponding to `a=x[i]` & `b=y[j]`—an obvious symmetry that cpp can't well express (and in this case, factor can't well express, either, i think) or that `Ly` is a constant that i saved to a name so that it'd be calculated only once. complex as the factor code is, we see that, in the first `tri` block, `i` is converted to `a` immediately and thus thereafter `i` is not present in that block's computation; it's off the stack and thus out of scope. ``i``'s use in ``tri``'s other 2 blocks is obvious because the blocks are so simple.

if you look at the factor version calmly and don't let your eyes instinctively cross, then you can easily pick-out the 3 cpp statements:

. `x[i]`
. increment-j loop
. conditional push & tell whether to loop again

i wonder how elegant a comparably efficient array version would be. the main efficiency gain is in accounting that search space `y` shrinks over looping through `x`. a secondary, much more minor efficiency gain is to simply not collect redundant `x` rather than to do so but account for it after looping by taking the nub.

unlike factor or algol-like langs, array langs don't have stateful iterators except for reduce, which is a very particular form and often less elegant than `map` that uses state i.e. the row-polymorphic part of its stack signature, `( ... xs q: ( ... x -- ... y ) -- ... ys )`.

also i found that the factor code doesn't even work correctly! for inputs { 2 3 4 5 6 8 9 10 11 12 16 17 18 19 } & { 5 8 9 10 11 12 19 26 27 28 29 32 43 49 } it gives { 5 5 8 8 9 10 11 12 19 19 }.

also i realized an inefficiency in my method that might not present in the sql version (i've to think about it): consider the new example inputs; even though `y`'s 1st elt is 5, and it's pushed on the 1st loop, the next 3 loops iterate over `x` as 3, 4, & 5, despite already knowing that `y` is already 5. ideally rather than `i++` we'd increment i until x[i]>lv.

= index[ed set]-centric computing

tl;dr: make a lang whose only data types are 2: 1. set of index intervals (at their smallest, set may be empty and intervals may contain 1 elt); and 2. thing efficiently indexable by such. _shape_ is nothing more than the index fn : Idx -> Elt.

i cringe when imagining how a list of duples of numbers might be stored, especially when i already know the max size of either value. anything other than one allocation for a contiguous memory region is cringy. i already know the size, so i should be able to specify it elegantly, inline, and the fact of it being one way vs its transpose is arbitrary! and there should be no need for nested indexing e.g. `cadr`, nor for creating a named union (tuple/struct types) just for easy access! apls get it right: packed, homogenous storage with virtual shape (assuming that they have O(1) transpose). and after collecting data into a particular shape, there's the concern of needing to reshape it for various places where it'll be used.

one troublesome aspect of apls is shape. all arrays have shape, and must conform to particular shapes for use in operations. sure, it's easy to create a new array of equal shape to another and populate it with a default value then start applying operations to it, but: 1. this is crufty and annoying to do often; 2. arrays cannot be infinite; 3. perhaps not in this case, but in some cases, as the shape changes, so must your code. were we only to specify relations, then the shapes would be implied, rather than us explicitly dealing with them. what's more, if our code is all relations/constraints anyway, then it's natural to specify some optimization constraints (e.g. x∈[0,1]) inline amongst the rest. the system would use such constraints to know the tersest encoding scheme that it can safely use. e.g. `(x<200,#x<16)` would satisfy `ceil(log2(max(x)))*#x<=128` allow it to use an avx (or w/e other vector) register.

the "computational properties" system would be used for sigfig-centric arithmetic, too. reals can be stored as literals (e.g "π" which is considered like in a CAS), or fixed-point, or rationals whose num & denom are of given sizes in bits. i recommend fixed-point sigfigs b/c they are easy & apparent, and make sigfigs easy. you'd store a number and its number of sigfigs (up to 8 for efficiency). sigfigs would be appropriate for e.g. storing stock prices (for trading, rather than record-keeping, reasons); we don't care to store more than 4, sig figs; we don't care about any amount less than 10 cents once we're dealing with prices of $10 or more, since that's at most a negligible 1% difference. this "up to 1% error" is a proprety of 4 sigfigs, and so applies regardless of the value; it applies for $20k just as much as $1. furthermore, it permits the same binary encoding for all values: say a signed nibble for the decimal place (accomodates up to 10^±7^), and 14 bits for the 4 decimal digits. sadly, in this case, 18 bits is just shy of byte-alignment, but oh well. at least the encoding remains the same for all values of 4 sigfigs.

some common reasons i've heard for why c is faster than other langs are manual memory management instead of garbage collection, or that there's no language runtime, or that it uses machine code instead of bytecode. these are all true, but they should be framed commonly: that the language forces few things between the machine and the programmer. c code makes it easy to have total control over the _computation_. most people only talk about _programs_—instruction sequences or relations that reduce to them—not the actual physical computations that occur. in c the fact of allocating memory is explicit in the code, and is thus just another instruction that the programmer specifies. this forces the programmer to be more considerate of where & how the data are stored, manipulated, & accessed, and naturally also gives the programmer control over all these aspects. giving the programmer this ability means giving responsibility & control, and thus power.

rather than nested iterators, i want to iterate once over the cartesian product of indices, except generally not the full cartesian product; i may choose during the iteration to not try any further multidimensional indices matching a given predicate. this is the same as the prolog thing of having iteration naturally continue so long as next() returns non-NULL. this system should, when used for such a simple scheme, require no more computation than a loop in assembly: next() would plainly be, in x86, the duple ([cmp ecx len; jlt], inc ecx). we don't need an else; if a predicate fails, then of course we'll try other predicates, if any are left. and any predicate may modify the set of remaining predicates; this design is like a mix of a `for` loop and a `while` loop, but of course is generally represented by `if` & conditional jump. another concern is managing which data must be preserved vs may be overwritten/repurposed throughout computations. again, like in <align-seqs>, to be clear, this is for control flow only, unlike prolog which has a global fact db. my technique works in any language (incl. asm) and is for variably local control flow. it may be global, but there are no special affordances for that. this is meant to be as inline as a loop block.

TIP: multidimensional rectangular arrays' index sets are given by cartesian products i.e. mixed radix numbers e.g. a 3×5×7 is {(i,j,k)|i∈[0,3),j∈[0,5),k∈[0,7)} i.e. i∈[0,<largest 3-digit base-3,5,7 number>).

TIP: the "in" predicate is really intersection. rather than returning a boolean, it should return the input if found. really, then, this is `find`, or more generally, `find-integer-from`, since `a b q find-integer-from` is `find` (which is the same as intersection which is the same as inclusion after lifting the element into a singleton set) but on an interval i.e. after intersecting some set or slice with another slice `[a,b)`.

specifying iteration as anything other than "next step" is a great design flaw. common examples are `for` loops, or especially iterative combinators without early exit, such as `reduce`, or words commonly implemented in terms thereof: `map` or `filter`. this immediately leads to antipatterns such as expressing "first non-negative" as `[ 0 >= ] filter first` which is inefficient. of course now you, the reader, say, "obviously they should use `find`!" but `find` is just a common combinator that happens to exist and fit this common, simple situation well. what if i want to search over two lists and perform `find`? a factor user might suggest `2 nfind`, but that still trims the input lists to be equal length. and what if i don't want pointwise traversal? what if my search space is irregular or modifies itself as it's iterated over? the variety of traversals is obviously so great that it cannot be represented elegantly by combination of any finite set of iterative combinators! what makes `next()` different is that _it is exactly iteration itself_. indeed, this reveals iteration to be no different from computation, just as a program is a sequence of instructions that execute, or a traversal is a search over some space. most generally it may be parallelized, thus being not a mere sequence, but generally a directed acyclic graph. the acyclic part is ironic, since one could earily argue that computations ubiquitously have cycles which we call "loops" but any cycle can be "unrolled" into a non-cycle, except infinite loops, which purposefully exist only in daemons. loops are noteworthy for their symmetry, but this thought is always observed when one imagines a simple loop, such as a `for` loop or even a `while`, but never does not imagine some various nested loops with complex state manipulation across the loops. at such point of complexity, one starts to call it "control flow" instead of "loops." so if "loops" are just some naive simplified representation of computation, then let's forget it and focus on the general problem, "control flow." it seems that hardly anyone has tried to actually identify the intrensic trouble in most elegantly expressing complex control flow, and have instead resorted to arbitrary pretty models such as functional programming and iterative combinators, or array programming, which are just as pretty as they are incapable of expressing complex control flow; they express it at the cost of being syntactically long, computationally redundant, and awkward—awkward because doing so requires you to identify the symmetries and asymmetries rather explicitly (the symmetries being expressed by loop combinators) but when the asymmetries are many, and are complex, then one feels dissatisfied by the loop combinators not living up to their promise of making the code look elegant. to be clear, loop combinators, like `map`, are good for expressing common, simple control flows that they are designed for, so they themselves are not bad, but rather they are insufficient to elegantly express arbitrary control flow, and a model that does so elegantly express obviates the need for such combinators. while it's good to know your code and the problem that it solves—and identifying a/symmetries is a part of that—there is no need to partition into a|symmetries; better is to specify a set of facts and let the traversal be ever implicit, like in prolog.

NB. false/empty values are represent by a `next()` which returns immediately. if it will be considered algebraically, then it has obvious properties & algebraic values.

the actual source code should read like english, and wrt symmetries, with w/o using the word "per" b/c ALL RELATIONS MUST BE IMPLICIT. especially, there are often multiple ways to express relations e.g. x per y or y per x being equivalent b/c it's just a cartesian product either way. e.g. "n-clumps of sessions, times of candles whose v or n accumulated since the session's start exceeds the prior 3 session's total v or n respectively."

an index-centric model would avoid bad design such as factor's `2each-from`, which obviously generalizes to parallel traversal of n seqs from a given offset. still, however, this function, is even poorer design because it is a particular traversal. traversals should not be in functions; they should be virtual sequences, e.g. `zipped` instead of `zip` and `cord` instead of `append`. all would be virtual; the programmer would have no ability to override this. like in k, computations would have naturally elegant information reductions e.g. reducing to `0N` which is truthy but propogates and can be converted to a falsy value by `^:`. it should not be defined, and though its definition, `-roll 2length-operator each-integer-from`, is efficient, clean, and short, it is better expressed as `[ tail-slice ] curry [ bi@ ] curry dip 2each`. the latter version generalizes to any virtual sequences of any argument seq. furthermore, i say the same for `each-index`; prefer `each` over a virtual zipping of a seq w/its corresponding iota.

features/example capabilities:

* if you specify a map operation whose output is passed only to `length` then, because it's known that map does not affect length, the map operation is not actually performed
* any train of iterations is combined into one. this is because `filter`, `map`, etc are actually traversal modifiers, not traversals themselves

case studies for implicit iteration:

* join/intersperse can be expressed well by a virtual sequence: if the index's lsb (parity) is set, then return the join element; else return seq[i/2] (i.e. seq @ i >> 1).
  ** ofc if you're going to take many arbitrary subseqs, then you'd want a literal join, since accessing contiguous memory is faster than going through index de-virtualization functions then accessing multiple unrelated sectors of memory.
* inner or outer join
* asymmetric relations, especially those that change during iteration
* consider how bins solves the general "find greatest x less than y" problem provided that it always returns either the element before or after the query. a good version would return 2 data: the value, and a bool indicating whether it's an exact match. this being said, that doesn't work well for the array method, and we can easily look-up the element at the index then test equality
* combinations or permutations
* matching the elegance of `: converge ( ... x q: ( ... x -- ... y ) -- ... y ) [ keep dupd = not ] curry loop ; inline`
* enable a hashtable to retain insertion order. this is a stupidly simple operation: add an extra integer field, and modify insert to insert size() (evaluated before adding the key) along with the key. in black-box programming, this would need to be done by adding structure [read: "relation"] around an underlying hashtable that relates the underlying elements to this seq of integers. with white-box programming, there are no scopes, and...well, no black boxes! rather than subroutines, which are sequences of instructions, we use predicates, which are inherently non-hierarchical (though ofc they incidentally permit hierarchies by various traversals) and rather than support concatenation, support union, intersection, subtraction, etc. the problem is how to conveniently retain only certain relations through refactoring. catlangs make this trivial, and stack langs have good data sharing via the global state: the stack. (i suppose that stack langs w/row polymorphic word effects are arguably "gray-box", then.)
* if n elts of a relation are have a particular attr be nil, then print those items, then prompt the user to enter a list of values; validate that there n values and that all are valid, then set each of the ith attr to the ith user-provided input.
* parsers, which are the sensible, powerful stateful combination of find & replace or otherwise just any general computation on sequences. primitives are slice, find substr, and concat. snip is defined in terms of slice. insertion at idx n is defined as snip[i:i] then 3append; this obviously generalizes to replacement: snip[i:j] then 3append. removal, like insert, changes length; therefore, as replacement, it's defined as "replace snip[i:j] by the empty seq". is defined this same, and so can be naturally expressed as "replacing. there should be 2 separate functions, insert and replace, where the former changes the length and the latter does not. substr considerations generalize to subseqs, which generalize to permutations which generalize to indices some of which may appear multiple times.
  ** anything that changes seq length is just as well done for many elements as for one. only replacement does not change length, and should be done via the primitive `set-nth`, which is either done in a `for` loop or not.
  ** subseq operations commonly do such stateful things as generalizing "replace first occurrence" to "replace all", which is just "replace first" but done iteratively until exhaustion, where each iteration has a state: the index whence to start searching.
  ** i still really like the "append under rotate" idiom, though this probably isn't appropriate for the model that i'll use
  ** it should be just as easy to replace the nth occurrence by the nth element of some other sequence.
* subsequence-and-not-substring operations (and why can't these be done with factor folds (generally expressed by `each` and selective pushing into a collection vector)?)
  ** regrouping (the version of apls' en/decode that we actually want; we don't want a number of a given radix e.g. to convert to h:m:s, we want hours to be variable, i.e. for there to be any number of hours)
  ** in factor's `tzinfo.private` vocab, effectively `[ find-transition ] map` but that efficiently operates over an ascending-ordered input seq
* consider cameron's fizzbuzz in j: +++;@((+:/,])@(0=3 5&|)#Fizz`Buzz;~":)"0>:i.100+++
* deep nesting e.g. `(activity-spike)` below

[source,factor]
----------------------------------------------------------------------------------------------------
: (activity-spike) ( cs -- masks f )
  [ d>> ] group-by
  [ second-unsafe [ s>> ] group-by ] map ! V{ V{ { AM V{ c ... } } ... } }
  [ 4 <iota>
    [ tuck of [ [ [ v>> ] map-sum ] [ [ n>> ] map-sum ] bi 2array 2array ] [ f 2array ] if*
    ] cartesian-map ! V{ V{ { AM V{ ?{ Σv Σn } } } ... } }. ? here means "or f"
  ] keep 3 [ <clumps> ] [ tail-slice ] bi-curry bi* ! 2map over days[i-k:i-1] & days[i] for k-slices
  ! q passed to map over sessions: ( session# clump current-day -- ? )
  [ [ overd at
      [ spin [ of ] curry map
        dup [ empty? ] any? ! is this session in all of the clump's days or not?
        [ 2drop f ]
        [ unzip [ mean 3/2 * ] bi@ [ swapd [ > ] 2bi@ or ] 2curry
          [ [ v>> ] [ n>> ] bi swapd [ + ] 2bi@ 2dup ] prepose [ 0 0 ] 2dip find 3nip
        ] if
      ] [ 2drop f ] if*
    ] 2curry 4 <iota> swap map
  ] 2map V{ } concat-as sift! [ c>t ] map! f ;
----------------------------------------------------------------------------------------------------

* replace all numbers in a string by a unary fn of each. solution in factor:

[source,factor]
-----------------------------------------------------------------------------
USING: unicode math.parser ;
: decrement-numbers ( s -- s' )
  SBUF" " clone tuck SBUF" " clone -rot
  '[ dup digit?
     [ suffix! ]
     [ [ [ f ] [ string>number -1 + >dec ] if-empty _ [ push-all ] keep ] dip
       swap push SBUF" " clone ] if ] each
  append! >string ;
-----------------------------------------------------------------------------

* empty sbuf occurs only once, so having empty checking in a loop is not ideal
* creating a new string buffer is dump; the current should be retained & cleared. this would be easy to code in applicatively.

applicative version:

[source,factor]
---------------------------------------------------------------------------
USING: unicode math.parser ;
: decrement-numbers ( s -- s' )
  [let SBUF" " dup [ clone ] bi@ :> ( acc b )
    [| x | x digit?
      [ x b push ]
      [ b [ string>number -1 + >dec acc push-all 0 b shorten ] unless-empty
        x acc push ] if ] each
  acc b append! >string ]
---------------------------------------------------------------------------

notice that the applicative version is, surprisingly to me, actually not terser! it's less symmetric, too! i'm able to apply effects (io) more selectively, which means that my conditional branches differ more than in the stackier version wherein i push `f` then `push-all`. the terseness and refactorability of stacky code is not only due to being tacit, but also due to being more symmetric! this "forced symmetry" is basically to keep all branches the same length (measurable by stack height, or, in functional langs, taking a fixed-arity fn param) or otherwise, more generally, require equality of some attribute(s) across multiple choices of data (where the data may be executable, quoted programs or branches (`if` in factor accepts two quoted program args, but `if` in haskell accepts two clauses of inline source code)). *in other words, it is to pad all choices to be the largest of their shapes.* this is how "spaghetti code" is avoided. of course, usually the padding element is the empty element e.g. returning `false`, `0`, `""`, etc in a functional language, or in a stack lang, pushing `f` to the stack as a dummy return value, as seen in e.g. factor's `find`, which returns either `idx elt` or `f f`. the aforementioned "choice padding" (or "alignment" is an appropriate term) is clearly seen as the presence of redundant information—here namely that `idx` nand `elt` <=> `idx` nor `elt` . expressing all branches by the same shape obviously makes factoring easy. sometimes this seems to be an inconvenience that we'd rather do without, e.g. factor's `loop` requiring its arg quot to preserve stack height. one might say that `loop` is inadequate at expressing what a recursive function can, where the recursive fn can return more outputs than it takes inputs, but simply return them only in base cases, and in the recursive branches not even return them; we'd either implicitly discard or preserve them by their inclusion in the recursive call. it is easier to do that, but we should appreciate that `loop` bluntly reveals such asymmetries. we may think of `loop` as a tiling of rectangles, and more general recursive functions as tesselations of less-regular shapes. another example is how both of haskell's `if` branches must return the same data type, which is either a product type i.e. a vector of a fixed length, or the union of those, which is an ad-hoc (asymmetrical) combination the choice of which must be resolved via a `case` clause. `loop` which does cannot change stack height is more efficient than recursion, just as mutating a fixed-size buffer is more efficient than shrinking or growing it. in such a literal language as c, loops cannot create new variables; in c, loops cannot vary the namespace. however, recursion can, and indeed does, as each recursive call has its own scope, shadowing scopes higher up the call stack; and the cost of retaining all these scopes is that the call stack grows. as always, generally: the more constrained a thing is, the less info is needed to en/de-code it, and the less capable it is. i discourage the term "flexible" because it is only one variety of capability. a 4-bit scheme isn't capable of representing 25 choices, just as `loop` can't represent arbitrary function chains. in the case of source code, "flexible" is commonly used, but this suggests that code be treated differently from other data, though it certainly should not be! each computation is capable of expressing some class of computation (im)practically, and the smaller the class, the more efficiently it can compute. this index-centric model achieves easy, flexible specification of constraint by stating as sets of algebraic rules. the algebra is done of a hierarchy of algebraic classes: either seq or multiset (permutation which may feature multiple copies of elements, which is useful only if their order or count matters) > set (permutation whose order is irrelevant) > permutation > subseq (monotonic inc seq) > substr (interval). each class supports its own sensible variety of product & coproduct (e.g. interval intersection/union (including: appending, which is just a non-disjoint union—a specific variety of what's generally disjoint union (clearly seen if you express a seq as a map from idx to elt; ofc you can union two maps and their key set may be continuous or not); and substring matching & removal, which naturally leaves the seqs leading to & away from the substr) vs set intersection/union) btw, note that i didn't say "unordered permutation"; a permutation always has order; it's only a question of whether its order has meaning or is arbitrary/incidental. no one will ever quite "call a `sort` word"; instead they'll mark a datum's constraint of needing to be sorted. the solver will handle sorting on a "need-to" basis.

similarly, array langs encourage users to code in terms of arrays, which are symmetric structures. homogenous, rectangular arrays are stifilingly symmetric, but hetrogeneous, ragged arrays are flexible while still being easy to reason about in terms of array symmetries. so array code is much more prone to being fewer, simpler, though less-efficient operations than a typical solution coded in a non-array lang.

also, when writing in applicative style, it's easy to forget to account for certain data, whereas usually in stack code if you forget to account for data, then it's just still sitting on the stack, yet to be consumed, which appears as a stack checker error; thus stack code is more suggestive in development. the lack of constraint among of local variables is freeing, but completely not suggestive. the lack of constraint means that any code runs, so the errors found in debugging applicative code will be much more frequently run-time errors than compile-time.

consider the following:

[source,factor]
----------------------------------------
: true-ticks ( v h l t n -- v h l t )
  [ <groups> ] curry 4 napply
  { [ [ sum     ] uint-array{ } map-as ]
    [ [ maximum ] uint-array{ } map-as ]
    [ [ minimum ] uint-array{ } map-as ]
    [ [ first   ] long-array{ } map-as ]
  } spread ;
----------------------------------------

of course, syntax should be terser; but my point here is that this code should represent one traversal which, rather than breaking into groups represented by tuples, should do one traversal over the 4 sequences simultaneously (with one index variable) which resets states. "<groups>" should mean "every n"; *it's a fn of traversal index*. i've never seen a language consider traversal indices as a special linguistic element, but perhaps it's time that one did. words should have the ability to modify tacit indices (and thus traversals)! this example's 4 traversals should instead be 4 words that all modify one traversal and compositionally specify how 4 distinct data should update through the traversal and when they should store intermediate values! this suffices as a universal model for computation: traversal (most generally a `while` loop) with storing intermediate states (most generally, with the ability to remove states, too).

characteristics:

. no nesting/scope
. index/virtual-sequence-based. allows multiple simultaneous multidimensional indices/subsets (generalizes partitions in that they may have non-null intersections) of any structure.
. trivial factoring e.g. sums of two seqs of equal length becomes expressed in terms of one index variable.
. non-black-box traversals. e.g. one should be able to define binary search as its own idea, but effortlessly augment it AT AN ARBITRARY INVOCATION POINT to terminate with a given error value if it compares the target to a prime number. this could be achieved by mandating that each traversal expose its loop condition (i.e. next()) so that it can be modified.

implementation: system like prolog, but computation like factor. we want the stack so that we can do row-polymorphic stuff. code will be expressed by inline combinators. the stack will be used (and will use the cpu's stack literally), but locals will be available, too, and those values will be stored in general purpose cpu registers. it'll be automatic; when something is saved by a variable name (like in a `let` block), then it'll be automatically stored in whatever the next available register is. there will be manual locals management; you must unset a local, which will free its register. simd will be used whenever possible. my goal is to basically have a forth implementation with the semantics of prolog and the efficiency of apl, and all of this being done with the most succinct data structures possible so that we avoid the heap as much as possible. with modern processors, with avx and 64-bit registers, this should be very easily achievable for all code that does not need to store large collections of arbitrary data at a time (such as reading in a large list of json objects all of which must be retained in whole in memory for operations such as median). this is possible because good code never needs more than about 6 objects on the stack, nor needs more than 6 locals at a time. i'm yet unsure how the decision to store data in a local vs a stack is relevant given that we won't be nesting computations; usually i use locals in factor to avoid shuffling, which is only ever an issue for (deeply) nested quotations.

in a stack lang, when you *do* find that you've somehow made your code deeply nested, it's often easier to just put elements far down on the stack, then pull them back up as needed, rather than to try to curry & compose them into a complex tree of quotations. perhaps even better, though, is to, instead of nesting many common traversals such as `map-filter`, create your own traversal that takes n aspects [data] on the stack and uses combinators such as `n&&` to clearly specify a sequence of predicates that return data or f.

the system, like haskell's "at" pattern matching, must make delimitation something that may optionally be used if desired i.e. that we can ever ask which sets a set/obj is a subset/elt of, or for sequences, which indi(ces) a sequence/obj is at or is a substring or subsequence of. i.e. we should be able to efficiently relate data. slices correspond to substrings, index seqs correspond to permutations (which have strictly more info than subsets).

excepting non-commutative folds over ordered data, data subsetting/subsequencing and indexing should be O(1). this is a requirement for full flexability (and application of the very powerful integer arithmetic to creating selection masks) in unnested relating without worry about cost. an example is that we should be able to clump something then collect-by or group-by each clump, which gives us the new slices (for group-by) or subsequence selection vectors (for collect-by) each with their own indexing from 0 (e.g. this is the nth clump) while retaining association with the original index so that i can, without extra computation, for an arbitrary element of the original sequence, its index in the original sequence, and to which clump it belongs to, and its index, if present, in the vector returned by collect-by. that group-by pushes into a vector is terrible design: we create new memory, have more-complex code, and lose the relation between original indices and the groups' indices. the loss of relation is the worst aspect of functional languages, as is the limitation of relation the worst aspect of functions. `collect-by` has a beautifully simple definition, both in code and abstract form; however, the very simple difference of pushing objects instead of their indices loses relation! it's exactly the same as k's `=:` except that it doesn't return indices. consider this apl-like thought wrt this system's prolog form of simply being a sequence of predicates that the system then intersects naturally for one complex traversal. as always, we should explot the extreme flexibility (multiple simultaneous data representations), ubiquity, efficiency (lightweightness), and mathematical properties of integers e.g. order, partition, or arithmetic, e.g. it's easier to use a fold to compute an average by multiplying current elt by 1/n then adding it to an accumulator than to collect into an array then sum it and divide by its length! apls are excellent for using integers for everything, including preserving relation, but they lack in that they can't relate among lambdas (no shared scope, except by using globals. this is at least analagous, if not equivalent, to lacking row-polymorphic stack effects), and in that they...don't make composing relations as simple as prolog...i should study this by coding in k in practice.

at least in the meantime, it's easy to simply do array programming in factor. it might not be quite as nice as a proper array lang, but that's only to say that operations aren't fused or otherwise specialized, and the notation isn't as brief. this is efficient, flexible, and easy. just think "how would i code it in <your apl of choice>" then write that code literally in factor. granted, you start there; if it's obviously more natural to code it in a more "factor" way then just do that; the important thing is that your thinking is array-oriented.

the "changes" fn demonstrates that we should not try to be as efficient as possible; such high efficiency should be achievd only automatically by computational systems. the larger code size, let alone complexity, is not worth the marginal improvement; and such improvements should be considered relative to the hw that the code will run on, the language runtime's efficiency (if any), and other code in your program (optimize only where it makes the MOST difference).

traversals should be implied by the traversed data and their indices. the order of traversal is given by the ordinals, and the set of indices by the intersection of the index sets, plus any ad-hoc, user-specified unions or intersections, or repeats (which is just union with the infinite-sized set of integers mod n). tracking state is troublesome only ever b/c you must change state wrt traversals, and keeping those properly arranged can be difficult. however, if you simply specify variable changes as rules (i.e. "when cond, mutate in such-and-such way") then there's no trouble! binding to locals is not at all inconvenient if done apl-style. with unnested traversals, scope isn't an issue; that vars are freed automatically when scope ends might be fine.

the stack is very computationally and expressively convenient/natural for many expressions, so definitely keep using it to express computations, even if the actual computation is done by registers instead, under-the-hood. forks aren't concatenative nor as flexible as the stack. i aim to avoid using the heap, but if i do, then allocated memory won't actually be freed; it'll just be made available for new uses.

the implementation will simply compile source code directly into machine code. it may do this to produce an executable, or may do this on the fly as jit.

maybe the "find the 1st candle of each session for which each its 3 prior days' sessions of the same type [as this day's given session] has a sum volume greater than the average of the prior 3" code would be nicer to code as regrouping—like mixed-radix, but more general—where i traverse once, building-up relations & sums among days and sessions.

e.g. cs [ day+=priorday!=day; session+=priorsession!=session; f(day,session) ... ] each. then i'd just specify the RELATION OF INDICES AND THEIR CORRESPONDING SETS plainly: nth session of mth day vs nth session of days m-[1:3]. again, the traversal is implicit, or rather, it defaults to all n in sessions and all m in days. an index variable in a natural number is 0..n-1; in a slice [m:n], from [m,n], and for an array, for all its valid indices, traversed in ascending order.

using predicates (higher-order fns / quotations/lambdas) sucks b/c they break relation, but they're good in that they're efficient: they apply the quotation to each element and basically fold that result into few data, which keeps memory usage small. so let's have a system that associates computations with data (as quotations do) yet presents like apl vectors, and has a system that automatically keeps data copies rather than reducing them e.g. if i do vector expr `x*y+x`, thet corresponds to `[ [ + ] 2map ] keep [ * ] 2map`. i really should find a less-trivial example, but this demonstrates that i translate applicative code into concatenative/stack code (though i'd just explicitly code concatenatively w/optional registers anyway) so that it's obvious which data to retain. eh, this being said, i could just as easily go full-applicative by clearing all registers that were bound within a lambda, which naturally & simply implements nested scopes (though not closures, but they aren't needed anyway, as demonstrated by their total lack in stack langs. not once have i even thought about closures nor wished for a more convenient way to code anything in factor).

so my main trouble in coding is that i'll do e.g. `[ [ v>> ] [ n>> ] bi 2array ] map unzip` b/c it's clear & easy, but i totally cringe at the idea of using 2array n times (i mean can you imagine coding malloc & free for each iteration of a loop? awful! it'd be better to malloc once, set many times in each iteration, then free after the loop, but why would we even malloc at all?! of course we'd just set registers! it's only two values, and this is known statically!) then unzipping, when i could just create two n-arrays and populate them with v & n, which is common and should be its own combinator...except that it shouldn't be a combinator, because combinators are TEMPLATES, but rather a language feature for expressing such patterns elegantly by using a bit of LOGIC to convert by effective code (as in "effectively do x, but actually it's y") into literal code. to do what i actually ideally would do in factor is very bloated and unclear for how simple a concept it is:

: map-into-2 ( xs q: ( x -- a b ) -- as bs )
  [ [ length dup [ 0 <array> dup [ set-nth ] curry ] bi@ swapd
    [ [ keep ] curry ] dip compose ] dip
    [ dip ] curry prepose
  ] keepd swap each-index swap ;
{ 3 6 4 5 2 } [ [ 6 * ] [ 20 / ] bi ] map-into-2

=> { 18 36 24 30 12 } { 3/20 3/10 1/5 1/4 1/10 }

compare it to the prolog-like solution `a[i],b[i]=f(x[i])`, which implicitly binds i to RHS x; b/c x is a sequence, i corresponds to a slice, which allows creation of the default contiguous traversable, the "array" data structure; so they're created for a & b, which implictly exist by being LHS exprs. then just evaluate this expression for all i. this system depends on being built with particular consideration of indexed structures, and constraints on those indices e.g. contiguous or not, or integral or natural indices (cf hash map), and whether the indices are ordered. it can exploit these properties and knowledge of integers to make efficient code. btw, indices is the ideal solution, not having a compiler try to recognize certain code patterns then convert them to more efficient alternatives! that's ridiculously ungeneralizable and complicated!

indices are general relation. EVERY data structure should, in code, ever be useful only if its indices are used; without indices, the structure is ignored, and it's considered only as generally & vaguely as any object. data are usually dichotomized into atoms vs structures. i suggest better terminology: indexed vs non-indexed. indices may be multidimensional, and any data may support multiple indexing schemes simultaneously. even data that grows in a linukd-list fashon (e.g. ll's, rose trees) should be indexed; indexed does not imply O(1) access. and ofc, since indices are by default free variables, we may identify subsets of structures by using predicates e.g. `{x[i]|x[i]>5}` applied to a rose tree, which would simultaneously identify i & x[i]. naturally no more x[i] would simultaneously be stored in mmeory than the max arity of expressions entailing x[i].

an indexable mod n, depending on cmp(#x,n), would be clumps or repetition. because the mod n applies to indices, the "mod n" augmentation converts any O(1)-access-&-modify structure into a mutable ring buffer. the most general flexibility comes in manual indexing expressions e.g. n-groups is defined as `λi. x[i*n:i+n]`, and n-clumps as `λi. x[i:i+n]`, and repetition as `λi. x[i mod n]`. notice that there's no need to specify that `i+n<i`; the system automatically restricts the index expression `i+n` to those for which `x[i+n]` is defined, which it can do b/c n is, at the time of evalutation, fixed, as is #x. if x is growable then we'd need to flag whenever its size changes and if this flag is set at time of an evalutation, re-compute the evalution of i. i think that this may likely be insensible in practice, though. index maps are composable, so you can do e.g. n-groups of repetition.

much of these thoughts reflect(s) that i prefer tags over hierarchies; they're soupier: they don't strictly conform to hierarchies, but they may, and they may conform to multiple simultaneously, which may even overlap! consider using a set of graph nodes like i did in sql to traverse a tree; the "tree-ness" is not kept as metadata; such structure is never explicitly stated in the code, and indeed, it is NOT in the code, it is in the data! one must search for it by trying to traverse the data as a tree.

NEXT: about `changes` algorithm: suffix #x-1 only if it isn't the last elt, right?! is this check necessary in the k solution?

adjacent indices give slices all having some common property
if empty, returns the exact same input seq
this code is actually pretty simple, but this syntax is visually unapparent;
were it represented graphically like in quartz composer, its definition would be clear.
in k this is simply {&~=':f x}, which reads 100% literally: "(indices) where f(x) changes."

* k's primitives are natural, so i don't need to add 0 nor (#x)-1; and i don't need to account for x being empty, because i don't have to break x into first & rest, because ': handles that already. this compounds b/c i must apply q to both first and rest.
* i also have to implement where and each-pair myself, though this is done very implicitly by my simple use of each-index and using the stack, and storing the current f(x) as the new prior. then i must drop it afterward.

tl;dr: not natural primitives, and compounding complexity in explicitly coding it as a single traversal, instead of composing ideas then having the single traversal be computed of them.
granted, i could just do the most literal translation of k into factor—`[ map [ = not ] 2 clump-map [ 1 + and ] map-index sift 0 prefix ] curry [ length 1 - ] bi suffix`—but this isn't as efficient. granted, perhaps k actually doesn't do nearly as much optimizing as i'd think, in which case it's just briefer factor with better primitives, and no row-polymorphic stack ops. granted, this is factor, not forth, and i'm running on x86, not a 320MHz risc-v Soc evalution cpu w/16kb data sram, so such optimization is a waste, despite being a good theoretical exercise to develop an ideal stack language. still, said language would be declarative and all would be defined in terms of relations. that solution would be...well, firstly we note that it's necessarily a computation of a sequence rather than a set i.e. indices are implicit in the sequence order. next we consider predicates, starting with what we want: 0,{i|f(x[i])<>f(i[i-1])},#x-1. using i-1 as an index for x implies the domain of i: [1..#x). in this ideal language, we are done. however, unless we somehow cleverly memoize, this computes f 2n-1 times. see the two examples below. the system would need to use induction to infer that it can compute f only n times and store only the prior f(x). how would this generalize to storing multiple data? well, actually it'd be easier, though perhaps more limited, to just have a rule for clumps; obviously per-element computations need to be computed only once per element, so for n-clumps, use a ring buffer for the prior n-1 elts then apply f(x) to the nth elt.

: changes ( seq init-vec-len q: ( elt -- prop ) -- idxs )
  pick empty?
  [ 2drop ]
  [ [ <vector> 0 suffix!
      tuck ! store vec for retval
      [ push ] curry [ [ drop ] if ] curry ] dip ! part of each's quot
    [ [ tuck = not ] compose [ dip 1 + swap ] curry prepose [ each-index drop ] curry
      [ rest-slice ] prepose ]
    [ [ first ] prepose ] bi swap
    [ length 1 - ] tri suffix! ] if ; inline

applicative version:

:: changes ( s #v0 q: ( elt -- prop ) -- idxs )
  s empty? [ s ]
  [ #v0 <vector> 0 suffix! :> V s
    [ first q ]
    [ rest-slice [ q tuck = not [ V push ] [ drop ] if ] each-index drop V ]
    [ length 1 - ] tri suffix! ] if ; inline

derived from the prolog-like solution given above: v.push(0); for i in 1..len(x) if (f(x[i])<>f(x[i-1])) v.push(i); v.push(len(x))}
this computes f 2n-1 times, but is otherwise perfectly efficient:

: changes ( s q -- idxs )
  dupd [ 1 swap [ length ] keep [ nth ] curry ] dip compose
  [ bi@ = not ] curry [ dup dup 1 - ] prepose
  V{ 0 } clone [ [ push ] curry [ [ drop ] if ] curry compose each-integer-from ] keep
  swap length suffix! ;

same, but computes f n times, but traverses twice. O(2n).
it's the same definition except that there's a `map` after `dupd`, and there's no `dip` & `compose`:

: changes ( s q -- idxs )
  dupd map 1 swap [ length ] keep [ nth ] curry
  [ bi@ = not ] curry [ dup dup 1 - ] prepose
  V{ 0 } clone [ [ push ] curry [ [ drop ] if ] curry compose each-integer-from ] keep
  swap length suffix! ;


that one stock problem: "given a seq [(time,val1,val2)], partition into days, then partition each day into hours, then, for each hour, find the first time, if any, that that hour's cumulative val1 or val2 was at least 3 times the average val1 or val2 of the 3 prior days."

* that i must be particular about which array i pass to `find` (as opposed to which arrays i curry into some traversals within find's quotation) is a total pain. i must do it b/c find returns an object from the array that it traverses over.

NB. with all traversal being implicit, and all implicitly being set (like array) so map/filter/produce are implicit, we effectively get traversal fusion for free.

== stack stuff

stack programs' execution is beautifully simple, which makes debugging very simple, nice, easy. granted, having watches on registers is just as clear as watching the stack. the system would know which registers it's tracking, much like how any system would be able to show all the variables per (nested) scope and their values. why they don't do that—why one must _add_ watches on variables—i don't know, but it's a bit of a pain to have to do so.

if i do stack stuff, ofc it doesn't actually need to use a stack. it can be a virtual stack; all that makes a stack is...well, actually nothing! stack langs aren't stacks! they're just tacit buffers! i mean, it's stack-like that evalutation occurs from the top, but we can `ndip`, which is to evaluate _not_ on the top. the stack is merely ordered, in-scope data. shuffle words merely permute the stack. we can easily have a virtual stack of max 8 elements that uses 8 general-purpose registers. i don't even need a return register if i use the shuffle definitions given by allisio in mlatu below. one central design that i won't compromise on even 1mm ever: everything is virtual & algebraic, never literal—so nothing like `compose` not actually composing but instead wrapping data together into a tuple whose class method for `call` just calls one item then the next.

=== mlatu

it's a term rewriting system, which is not reductionist^*^; for terms of referentially transparent rewriting rules, the rewriting is invertable. indeed, terms seem exactly appropriate for some problems, such as solving a rubik's cube—namely where *the string of terms reflects the string of mutations*. this is quite a different scenario from real analysis! indeed, numbers have no place in solving the cube! all aspects of the cube are arbitrary up to its group structure. think that sucks because "term" isn't "number"; it's not specific enough. however, it does invite some questions for me to answer:

^*^by _reduction_ i mean β-reduction [λ calculus], which contrasts with deduction [prolog, term-rewriting systems]. the differn between these two classes is that the former has no algebra but the latter does. algebras support solving for efficient programs. non-algebraic code systems are only shorthands e.g. a _function_ for squaring a number `: sq ( x -- x ) dup x * ;` [forth/factor] is just as well as writing its definition. this is equally true of applicative-style codes after relating variables by name. this is no different from assembly language macros. almost all programming languages are basically this; however, each one tends to add some unique bullshit features that only ever somewhat add semantics to data, namely in the form of type systems, though rust uniquely has its borrow checker. anyway, despite some type systems themselves being algebraic (e.g. those used by haskell), the type systems used in programming are not purely type systems; they're type systems applied to code. therefore the code is restricted by the "almost" type system, and the type system is only nearly complete, which creates edge cases where the type system simply does not make sense. it's a "lose-lose" situation. the only way for a type system to work well in code is for it to be complete, which requires that we have dependent types such that every value definitely is considered as all of the types that it can be, and thus implementing such a type system requires a strong type algebra and typing every value. in practice, type systems have not been used to deduce nor produce programs, but rather are merely stated constraints that can at best avoid _coding_ mistakes that have little to do with the structure of the abstract system being coded, but rather avoid mistaking a variable by its name/scope, or forgetting an operation e.g. trying to pass a number to a function defined only for non-negative inputs, but forgetting to `0 max` first. useful as they are cumbersome, such type systems trade flexibility & ease for correctness & stability.

. which operations are not of numbers? are there any? i must consider this to enable the language to account for even non-numeric values. consider that apls are entirely sequences of numbers/codepoints. this obviously means that anything else is not needed _in apl_, but we know that apl does not handle io or perhaps other special resources.
. my system is algebraic. for numbers, the algebra is understood. but what algebras can i use for arbitrary values? term rewriting systems exactly deal with arbitrary values (symbols i.e. terms), so i should consider such a calculus, provided that i may need to account for non-numbers.
  .. while considering algebras, ensure that you don't get distracted by them; remember that all is seqs or maps. everything else is just unnecessary semantics there-atop which may represent a real idea, but which may be expressed more exactly/plainly directly by the actual underlying info i.e. seqs/sets. all crufty semantics are just concepts (basically notations) to represent relations (of particular subsets.) remember that a common synonym for "relate" is "group" (though relation is usually meant to totally preserve structure, whereas grouping not always is), and relations are expanded (e.g. (a,[b,c]) expands to [(a,b),(a,c)], or are duplicated, or are removed (such as in filter, which is just group/partition but discards (at least) one of the subsets). many languages make the mistake of structuring data as distinct objects; this has the consequence of being treated specially, which means that for each unique type, one must define words that work with that type. ofc that'd be too limiting, so type structures such as ad-hoc polymorphism are created so that certain operations can work for multiple types. other "semantic" structures are created to cope with the limitation imposed by distinct types. the natural solution is to use the only necessary type—relation (which may have an attribute whose order is used for sequencing, or may not be present in which case the relation (n attrs) generalizes an assoc (2 attrs), which generalizes a set (1 attr))—in which case one expresses relations merely by index (which may be any symbol including a lexiographic name or number). aside from relation are actual computations—namely arithmetic—which can be expressed as relations, too. rather than creating data structures, we have _relation templates_: notations that express relations e.g. "pointwise" which could be used to define e.g. a dot product. there's no reason to create a vector class then define an insteance method called "dot". and as anyone who's used sql knows, pointwise is join on equality, which is an efficient operation if the join expression is an indexed value.
. which calculi work for distributed or multithreaded systems? it'd be nice to have the language naturally work for such systems, too. referential transparency might be considerable here. for example, if my language is not only concatenative, then what other evaluation strategies does it permit? referentially transparent concatenative programs may be broken at arbitrary points, each computed in parallel, then their results stuck together into a new concatenative program, which may then be evalutated (although this is probably not quite true and probably has many caveats).
. term rewriting is obviously reductionist. most langs are. even prolog is implemented ultimately by assembly, which is...reductionist, right? so perhaps the "logical vs reductionist" dichotomy that i supposed is actually nonsense, and that "computing" is a better lens. we have data in registers and on the stack and in memory or whatever, and we do stuff with it, and that stuff either replaces the data or keeps it, and then our programs are just traversals, and efficient traversals are chosen for given circumstances. there's nothing more to say. these are the basic, unavoidable facts of the computers that i'm writing for.

of course, all these questions will foremost regard the architecture, which i'll assume to primarily be risc-v, though it'd be little effort to make it work on x86, arm, or any risc or cisc.

mlatu has 6 primitives:

[options="header"]
|====================
| mlatu | factor
| `+`   | `dup`
| `-`   | `drop`
| `>`   | `[ ] curry`
| `~`   | `swap`
| `<`   | `call`
| `,`   | `compose`
|====================

ofc many convenient shuffle words may be defined in terms of these, though i'm unsure yet how universal this primitve set is. certainly keep & dip are important primitives, which leverage a return stack, which mlatu seems to lack. this is likely a worthless consideration since mlatu is made to be referentially transparent and a term rewriting system, which sounds more like a theoretical thing rather than something practical and efficient, tailored for computers.

.common words defined in mlatu, given by user "allisio"

it's easy to accept a challenge of "how would i do a given some constraints/designs?" but this is not a helpful challenge. here we have some commonly-known words defined in mlatu. but why? to show that it can be done in mlatu? surely it can, and one might even feel prideful about having completed the challenge, and might even consider it progress, and might want to continue so satisfying themselves by continuing to define many other words in mlatu, in terms of the words that they've yet defined. one may delude themselves into thinking that solving problems or conquering challenges is anything more than entertainment or a waste of time. the very premise, "how do define these in mlatu" is arbitrary; why should we do it in this way, by the constraints of mlatu? naturally one tends to fiddle around until they get a solution, and then they use that solution to solve other problems. this is hardly a strategy! as one would expect, one ends-up with very many words rather than a single, small, simple, elegant system for expressing arbitrary computation.

this thinking is dangerous; it creates thought pollution, which is distracting! many "new tools" are developed to "solve prior tools' problems", but there was hardly any solving going on. instead, it was just "hey it occurs to me that we could do something in this other, less painful way" and so they do, but that's not design; it's just occurence, and worse, it modifies an arbitrary part of the original design, rather than questioning why this problematic part existed at all in the original design; perhaps a greater subset of the original design is inherently problematic!

a common quality of mastery is doing much with little. conversely, a common quality of being ungraceful is to do much in order to succeed, and be proud of the success, without questioning how you could do it better or what got in the way of making it more graceful, or why you chose a given design over some other alternatives.

many of these words match the antipattern of defining a thing then using said definition to define a 2-thing for it, which generalizes uglyly to n-thing. the good design is to define an n-thing which then you might consider aliasing to some k-things for (m)any k. the n-design is natural in an algebraic system; the "repeat a basic computation" design is natural to a literal, reductive execution system.

[source,mlatu]
----------------------------------------------------------------------
nip: swap drop ;
2nip: nip nip ;
dip: swap quote compose call ;
2dip: swap `dip dip
swapd: `swap dip ;
over: swap dup swapd ;
2dup: over over ;
curry: `quote dip compose ;
2quote: quote curry ;
rot: 2quote swap `call dip ;
rotd: `rot dip ;
roll: rotd swap ;
pick: 2quote over `call dip ;
reach: `pick dip swap ;
uncurry: dup () `call dip uncurry' ; // same as uncons
uncurry': 2dup curry reach over = (drop `nip dip) (2nip uncurry') if ;
conj: quote compose ;
map: () map' ;
map': pick () = `2nip (`uncurry 2dip roll pick call conj map')
----------------------------------------------------------------------

== implementation from first principles

=== assembly

.primitives

* data storage. in total, "state"
  ** registers. multiple, possibly-overlapping, data can be stored in 1+ registers. O(1) access & set.
    *** special e.g. `add` stores sum in `eax` always
    *** general
    *** vector
  ** stack: assembly's only fast data structure whose data are explicitly separate. it's growable and lifo-only access.
  ** heap. slow; avoid.
* programs are opcode seqs. for fast code, avoid jumps.

we must store in a temp buffer if:

. multiple invocations of an opcode that uses special registers e.g. to keep multiple sums, we must copy from eax to elsewhere
. nested fn calls. any nested fns that don't use the same registers can be expressed as one fn. by _function_ i mean a cpu state transition contract.

.stack machines

* stack machine programs rarely put more than 7 data on a stack. given that so few objects must be stored during runtime, implementing a stack machine in assembly should be easy at least *if all word definitions are inlined*. dynamic evaluation (quote & eval) might complicate things, too.
* a stack program is homoiconic: a sequence of _words_ pushed to the stack. words are un/quoted (sequences of) symbols, or datum literals. words are executed when encountered, or pushed as literals if quoted. 
* the de facto elegant turing-complete set of 6 stack primitives is: `eval`, `quote`; `drop`, `dup`; `compose`, `swap` (or `dip` seemingly would be just as well, and would utilize a return stack). there is the basis {`cake`,`k`}, which i haven't looked at; see von thun's paper.
* concatenative: stack programs—the sequence (composition) of subprograms (word sequences)—is associative; all referentially-transparent subprograms can be evaluated in parallel, then their results replacing where they were in the original sequence, and that sequence, when evaluated, produces one result.

=== natural code

* as we know from biology and its _de facto_ so-inspired data structure, neural networks, we can store multiple information across multiple data cells, where any subset of information may span any subset of cells e.g. we may have two 16-bit registers A & B which store 3 data

          3 greater bits of a number    4 boolean values
                          |     sign bit      |
                      |-------|    |       |-----| |--- char ----|
0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1    0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1
|------------ A --------------|    |------------ B --------------|
                                     |---|
                                       |
                             3 lesser bits of a number

this is a way to pack as much data together as possible. because register operations are so fast, alignment isn't much a big deal. that being said, in practice we'd use a system that calculates best storage for cells of memory for registers, stack, static or dynamic allocation given certain value information—namely valid value range and operations that will be performed on it, and what collection of data any composite datum may have e.g. if it's one value, then store an index into a statically-allocated enumeration; but if it's multiple simultaneous elements of an enumeration, then perhaps store them as a bit set / masks. data can be accessed just as easily as by name in a struct, only now instead of each element of a struct getting its own memory cell, they may share, and the auto-implemented methods that a `struct` macro produce will handle this for us. it's hardly different from factor's `TUPLE:` associating slot numbers with "attribute" words.

aside from inexpensive data compresion for simultaneous data storage, there's the case reusing memory that is known to no longer be needed; rather than always blindly allociate memory for each datum (each corresponding to some chunk of information, usually about a concept), use allocated memory, minimizing the number of de/allocations. this is noteworthy for memory that persists through loops, recursion, or generally multiple function calls or arbitrary subprogram. rather than fns named "alloc" & "free", prefer "need" & "done"; this is a _mix_ (which i'm baffled that i'ven't seen yet) of manual memory management and a gc that marks cells for reuse (repurposing).

again, as always (and i should have a document that says all this once, then i should link to it here and anywhere else as appropriate), functions suck—black box / knobular / blocked-off from each other rather than sharing (inflexible, not adaptable), and have prologues & epilogues, which entail pushing & jumps, which are slow & unnecessary. fns are a bad design: just another form of unnecessary cordoning (raising walls) then creating the need to (selectively) relate (create doors). ofc, inline fns aren't problematic. consider apls, though, where individual functions are rare; instead, we mostly see long strings of code, binding intermediate computation values as necessary. aside from the apl primitives, whose implementations can be tailor-optimized into the compiler, programmers' programs don't suffer much from jumps nor saving & restoring state.

== TODO

* how to minimalistically express `p q ?` in computations without redundant computation. e.g. in a loop i don't want to check a condition that doesn't vary with the loop; i prefer to, before the loop, conditionally determine, at runtime, a loop quotation.
* check-out factor's `bit-set`: apparently, efficient storage of intervals [0..n] minus some few arbitrary elts.
* an essential test of the system is that loops are always implicit, and to the degree that they're apparent in code, the loops must be absolutely freely permutable effortlessly; there must be no calculus to rearranging traversals! this implies that there cannot be a `curry` nor an "each right/left". this is fortunate, as it eliminates all "left & right" duplication e.g. join left & join right, each left & each right, etc. left & right are folly anyway, since they assume only 2 args, and again, binary is the basis for reductive code, whereas seqs of arbitrary length is the basis for elegant code.
* consider join and "factor", a form of info compression: `[ \ ] [ ∩ ] [ swap \ ] 2tri`. one datum in many exprs implicitly relates those exprs; their intersection is non-null. "intersect" can be described as "largest common form". certainly all code is merely data being together vs apart. union is just another way of specifying a subset (of all). `AND` is application & merging whereas `OR` maintains distinctness/separation.

note about parsers: we can impose models, such as the array model, which does not affect the basic case e.g. `1+2` is `3` regardless of whether the array model is imposed or not. yet things that would otherwise be nonsense (uninterperable) are interpreted sensibly by it e.g. `1 2 3 + 4` produces `5 6 7`. we can freely union additional orthogonal parsers (orthogonal meaning here that each parser's parsing expressions do not overlap) without worry about changing the interpretation (meaning) of our code. we're also free to install new non-orthogonal models and compute the overlap then choose the order in which sets of overlapping rules are tried, and we can run it on code to identify which subsets of our code's meaning may change by installing the new parser. obviously <installing a new parser whose rules are tried only after the prior parser's overlapping rule fails> will affect only if the original parser fails, which may or may not be expected in your code, depending on how you wrote it.

== final note

idk if my lang will be stack-based or concatenative, but i can't deny that regardless, i will continue to code in a tacit, forth/factor-like language because its aesthetic is one that i personally find wonderful—namely the cleanness of syntax: that only whitespace separates tokens. this is natural with concatenativity. however, factor programs, by use of quotations, can become nested (forth has not this). nested code is nice, too, but comes at some moderate cost that i think i've discussed elsewhere.
