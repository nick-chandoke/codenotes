== best paradigms lang

this document is supposed to be a terse spec of the ideal programming language.

NEXT: the difference between reductionist programs and logic programming is that the latter retains implication rather than just testing a predicate, which is synonymous to computing a result because they're both reductionist, both analagous to a logic circuit. what logic programming does that a logic circuit does not is identify other circuits given a set of constraints and facts about making circuits!

_programs_ are 1. data, 2. structure, 3. traversal. _data_ is encoded _information_ [information theory]. data is structured/encoded; the structure suggests a _traversal_. data should be stored as mere relations—the least-constrained encoding. code syntax should be, for every thing being encoded, encoded by an appropriate encoding e.g. use a list syntax to represent lists rather than a general graph syntax, since the list syntax is simpler and extra generality is not needed. however, all code encodings are just syntactic sugar for a graph encoding; an optimizer will determine the most constrained traversal/encoding/structure of a given program's concerned subset of the data graph, thus keeping data separate from limiting structuring which permits more efficient traversals/execution. indeed, there's an inherent tradeoff between flexibility and constraint! constrained programs are more determined and so more efficient.

* homoiconic langs must not be langs! being that they're only _data_, their syntax communicates only _expressions_ (relations of relations or primitives) not _linguistic_ objects!
* languages like racket or other lisps, which are supposedly metaprogrammable or dynamic, are actually stupidly greatly constrained by namespace rules or static things. no language can appropriately be called "dynamic" unless it can modify itself while running and load/eval code (including from a file, regardless of whether that's called a "module" or not) during runtime. if the program supports a repl (which it should), then running code must be exactly equal to loading/evaluating it during runtime which must exactly equal evaluating the code in the repl. finally, if the program halts/crashes, then it must enable the user to modify the program while running then retry the failed instruction. furthermore, the user should be able to modify the program as it runs even if it has not halted. obviously the language must be metaprogrammable (not necessarily homoiconic, though). anything less must be inherently limited, bound to some eventual, frustrating limitation.
  ** tacit programs are especially good for live loading since they don't accumulate binds in a namespace and so they avoid scoping problems. that being said, it appears that picolisp hasn't trouble with that, despite being an applang.
* like unison [lang], no function names; instead only hashes (though they can be known my many names). other things proven are like stored e.g. stack effect or type. we must do much better than this, but it's an improvement on names. caching results of pure fns is nice, too. storing words &al data in a db is obviously good. we can think of sql triggers for keeping code consistent after renamings. consider that hashes are different from uids: they're effectively alternate encodings, as opposed to an associated arbitrary unique datum.
  ** hashes are not too good; they're still arbitrary; they do not enable us things that we should have: similarity measures, orders, and good searches for expressions.
  ** still the unison model of hashes demonstrates how stupid names-as-identifiers are. these observations should be applied to filesystems and anything else that uses names!
* again, we must balance optimization vs dynamicism
* macros have the problem that they hide and expand to ugly code; that is the very purpose of a macro! this makes live debugging difficult! so avoid macros; prefer elegant code, and if you must make the language elegant (and perhaps data-only), then do so!
* the erlang model is correct for concurrency. it apparently is good for dynamicism, too! that's not a coincidence!
* the ability to fix something as it's running is usually better than trying to ensure that it's perfect then shipping it without the ability to modify it. obviously there's nothing easier to modify than a db.
* _all_ programs should be interactive & live-modifiable!
* all code should have visualizer(s).
* *we must get do better than text code*
* you should never to deal with details beyond a program's spec; that's the optimizer's business!
* haskell functions whose paramaters are still free is like prolog vars still free, but how different b/c only beta reduction? compare with factor, too.
* also, should we use quoting & eval (first-class program data), or e.g. apl style hook/filter? what about forth? consider flat structures vs recursive [nested] ones. cf metaprogramming
* after considering prolog, i see all languages as severely limited by their doing only beta reduction as opposed to algebraic consideration of vars. can such ability be done easily in factor? it ought to be, seeing how dynamic & small factor is.
* programming is divided not into "code & data." that is inappropriate because code is data (as are all things) and code can connote data or instructions (as code means "data encoded by an encoding"). honestly code & data are the same. code & information is more appropriate: data is encoded information (data being used as a mass noun like information.) what people really mean is that _some data are instructions_; they would say that `[1,2,3]` is data and that `cons` is code. then is `cons 1 [1,2]` code? it _evaluates_ to a datum. ah, i suppose that people mean that code is data that can be evaluated. but what is evaluation? typically it's β-reduction: simply a re-expression. `cons 1 [1,2]` is equal to `[1,1,2]`; the only reason that they're presumed different is because non-algebraity is assumed i.e. that `cons 1 [1,2]` is an an _expression_ (relation of abstract symbols), but merely a _parameterized unidirectional map_ for rearranging data, including arithmetic. thus "code" "expressions" are treated as though they aren't data! they're treated as the result of their β-reduction! thus "functions" as commonly used in programming discussions are just syntactic sugar! they should be called _reductions_, not _functions_. the meaning of _procedure_, _subroutine_, and _function_ varies by context, but they're all the same at least up to scoping rules; namely most commonly some or all of _functions_ inputs are explicitly noted and are called _formal arguments_, and _procedures_ aka _subroutines_ may be defined of any vars in its scope, which may include any formal args or not. _state_ means any arguments outside the formals. the flaw of distinguishing functions from procedures is the assumption that there's meaningful difference between formal args and non-formal ones. such distinction exists only because functions are arbitrarily (and senselessly) defined by only one statement in one place, or in multiple places (e.g. haskell) but formals are static. remove the constraint/association between a function and a formals vector, and remove scope, and now you have functions being a set of sequences of mutations of arbitrary subsets of program state. now that functions are found to equal procedures, they're just state manipulations: changing a physical value (e.g. one in ram or filesystem), or re-expressing an abstract value (e.g. the cons example.) the physical changes cannot be expressed by abstract re-expressions, but only concurrency primitives must be expressed by physical changes/values. because most programs are re-expressions of abstractions, using only abstractions that emulate physical changes is quite stupid, needlessly limiting. if you have symbolic expressions, then _use them as such_ like a computer algebra system would! _reduce & rearrange_ the symbolic expressions, and only after that should you bind free vars to values for re-expression! we see this as programs re-expressing to simplified equivalents; then seeing those programs becoming less abstract when given non-abstract inputs e.g. a datum literal; in this case the program is an abstract re-expression that is _applied_ to a non-abstract value to an outputs that is _related_ by the program to the inputs (the program is seen as a relation of inputs & outputs.) however, if the inputs or outputs are abstract, then the program is just a system of constraints that is further constrained by the inputs (the inputs, being abstract, must be constraints/predicates: qualified/constrained symbols); thus specifying the abstract inputs or outputs is a constraint system endomorphism; the new system can be reduced and and solved for a set of constrained relations of inputs & outputs. we see programs then as constrained systems of variables, where freeness or boundedness of any vars is a matter of representation. the system may contain non-abstract values called _constants_ e.g. `2`; a constant is formally defined as a constraint that, when joined with any other constraints, has the same value e.g. `∀p 2 : p => 2`.
  ** stack langs, which haven't λs, degenerate λ-calculus and procedures into a single model: stack programs are just sequences of mutations of implicit state. by not having variables they appropriately describe mutation sequences without misleadingly resembling symbolic expressions.
    *** reductions & procedures will be henceforth known by the single concept called _mutation sequences_
  ** mutation sequences are often useful but are a poor universal model of computation! they should be thought of as macros [text editors, microsoft excel]: playing back a mutation sequence. symbolic expression endomorphisms is the only sensible way to write programs, as it's the only sensible way to reason about math or logic. why does programming, which is strongly associated with logic, math, and grammar, not enjoy such an old & obvious technique, instead having only reductions/mutations? i suspect that the popularity of turing's machine encouraged reasoning about mutation sequences rather than programs as abstract algebraic expressions. functional programming seems to try to take the turing model in the direction of algebra, but of course the two models cannot be simultaneously used, though a turing machine can be used just fine to implement an programmatic algebraic system (pas.) *bottom line: decent programs must be constraints/relations of symbols or constraints/relations*—`type Rel := Pred({Symbol | Rel})`, which sees predicates as n-ary, parameterized by a set of symbols or relations; however, note that, as with any recursive structure, it may be flattened, here into a set of constraints e.g. `and(even?(Y),>(X,4))` to `{even?(Y), X > 4}` (or flattening a recursively defined tree into a list of branches.) this is what prolog does; the program execution then, as with reductions, is done simply by specifying inputs (or, unlike reductions, any subset of inputs or outputs may be given.)

TODO: revise this document's comments about indices; an index is just an instantiation of a variable [prolog], which is an arbitrarily specific variety of *constraint* on a variable, generalized to a set of indices. `thing(i,v)` means a value `v` of thing at index `i`; then `thing(i,v), in(i,0,4)` is a subset of `thing` determined by predicate i∈[0,4], and `?- thing(4,v)` is `thing` at index 4: `{v = <some value>}`. this generalizes to `thing(index,...,output)`. we see that inputs, outputs, and indexes are all the same: just specifications of a relation's variables.

TODO: cf sql notes & _best paradigms_

BUZZWORD SUMMARY: information soup (data are not particularly partitioned; "all matter is interaction"), [equivalent under] information content, parser, select, index (all non-sets must be indexed!), bound | free symbols, dynamic implementation, implicit definition/relation, inherit from context, virtual relations [sql], factor oop (tuples to store needed information and generic computations).

* all structures being virtual, operations on them are cheap; they affect only how the structure is accessed or modified.
* a fact's a/symmetry about an axis [index, property] is whether its truth varies with subsets of the axis.
* facts are defined of things free or bound and thus have some degree of abstraction. for a thing to be useful it must eventually be totally bound.
  ** interpretation does not change abstraction e.g. the pair [0,b] as a sequence still consists of only two things, though we must associate it with a state to identify the seq's current elt/index. factor's `sequence` protocol is good.
* a program being a continual modification of stack, registers (map from reg name to value), or fact set makes easy serialization, metaprogramming, and live modification (pausing, changing, then resuming the program as it runs.)
* plurality is assumed. aggregates are the exception. a word is defined as an aggregate or not. aggregates return a singleton set. this allows word composition e.g. `[ # f ] [ g ] bi ×` where `#` is an aggregate and `f` is a non-aggregate will apply f to the length of a relation then cartesian prod with the application of `g` to that set. all words here are assumedly selections without puts.
* `x [ f ] map` is expressed [conceptually] as ∀i.f(x[i]). because we're using an array lang, there's no need for `map`; it's implicit. however, this demonstrates the role of [free] indices: they're logical objects that implicitly affect other expressions. like all linguistic objects in this paradigm, they do not reduce to data literals, though data literals can, in any contexts, be calculated from the context and rules concerning indices. cartesian products are expressed as `{ x[i] y[j] }` and are as `lzip` in factor; rather than thinking of × as a set, think of it as an accessor function, because that's literally what it is. it's a virtual sequence: just a map from an index to a value.
  ** setting at given indices is just like sql: `set prop of rel where pred`, where pred can be `index in <set>`.

currently known best tech:

* prolog (logic programming)
* factor (stack (implicit, single, uniform, plurality-agnostic state), concatenative, dynamic vars, duck typing (*virtual* things), quotes generalize functions as data, currying, and composition; therefore there's no need for `apply` [lisp] in factor. also `call` generalizes `unquote-splice` and `eval`)
* relational algebra (plurality agnostic, set theoretic, indexed)
* information (bit twiddling, encoding schemes, everything is select & put, no cells [assumed separation of data])

the best ideal that i currently know is:

* code:
  ** virtual: all code is logical/algebraic symbols supporting more than mere symbol substitution
  ** flat
  ** metaprogrammable: code is data computable by the underlying model
  ** for knowledge, facts
  ** for dataflow, concatenative, stack with locals
  ** both knowledge and dataflow should be supported, with arguable interoperability, because each is commonly useful and neither elegantly substitutes for the other.
* model: deductive, array, relational/set-theoretic (e.g. supporting multisets)

the currently known nearest implemenation of such an ideal is . apl/stack hybrids like lang5, xy &c are not good; the apl array model is far worse than the relational or prolog models: it is not set-theoretic, has not joins, intersections, &c, and so is not logical; instead it's ordered & anonymous data. that being said, one could say that logical programming is better than stack, as the stack is ordinal & anonymous whereas logical is unordered and everything is named. so _are_ relations better than apl arrays? why? and how does metaprogrammability of a stack+array language work, considering that's separate models for code and data? *consider this wrt indexes as described elsewhere in this document.*

programming must be concatenative or better, namely unions (unordered concatenations) like in prolog. programs must also be *flat*. say `y = x + 2; except y=2*x when x=4` instead of `y = case x [4 2*x] [else x+2]`. the flat version decouples definition from exception, and makes both refactoring and metaprogramming easier. in fact, it makes storing the program in a database easier, too. *code facts, not structures; queries, not computations (words, functions).* another example: it's more sensible to break `[if even? i then f x else g x | x <- xs]` into `even? i => f x; odd? i => g x`, which describes separate *rules* for evaluating x by its elements instead of merging the two rules into a conditional branch inside a loop.

context is a set of named relations (e.g. prepositions, which, btw, are usually binary) each of whose arguments are variably free or bound. meaning exists only in context; therefore to discuss forms is inherently foolish. again, *the only defining questions are how we select & put data from & to a thing.* this is all that programs virtualaly are: selecting & putting. all the rest of programming concerns efficiency of these operations, which is the perogative of a solver, not a programmer.

GUIDING PRINCIPLE: if there are multiple ways to encode an expression, then either 1. your computation model is too complex, or 2. your code model is too complex and the choice of implemenation should be deferred to a solver, hidden from the programmer. this is too true of structure; if you use ad-hoc polymorphism, do not use a code model that allows the programmer to select a class instance that's inefficient for their use, e.g. accessing a linked list by positional index. allowing the programmer to choose a type class's instance is the primary failure of ad-hoc polymorphism. good ad-hoc polymorphism makes choosing implementing types available only to the engine, not the programmer. then again, structures should not be defined; they should be calculated in each instance to most efficiently satisfy each object's usage (select, put).

_encodings_ are maps to/from bitstrings.
use constraint solving to encode programs.
make fn : information content -> encoding. e.g. ci english characters -> 5-bits.
the expr 2x+3y+4z can be expressed <[2,],X> (`2 lfrom { x y z } >list lzip list>array v.`) which generalizes & simplifies the original expression, and can be said to have 4 pieces of information: 2, <,>, X, and [,]. associativity, commutativity, &c affect amount of information, too.

SYMBOL RELATIONS: `xᵢ f` means `x f` ∀i, where "has subscript i" is a single-argument relation between a variable symbol and the symbol i, i.e. (_,i,subscript). this is the general relational model like sql. _indices_ are exactly things by which a thing is uniquely referenced. _queries_ generalize indices; they're more powerful but require more computation. generally a program is just a collection of relations—effectively a metaprogrammable relational database. programs are thus queries & puts.

generalize & specify asymmetries e.g. all follow this rule except x. this is encoded as x -> { ... [ t ] x } cond.

the register/asm model, when ideally done, isn't much better than the stack model. in practice <someplace on the stack> is easier & more regular than trying to assign registers to hold all sorts of particular kinds of values. identifying an elegant set of kinds of values would be difficult.
should have an auto code reducer e.g. `b a - b a - 2/ +` reduces to `b a - dup 2/ +`, though frankly that should be pretty easy for a programmer in a stack lang. that being said, it'd be nice, especially to see super-compressed expressions.
stack langs free us from many decisions, e.g. about scoping or iteration vs recursion, or jumping to a function, or functions vs data. sometimes one forgets about how _easy & simple_ stacks are. loops are practically identical to folds in a stack lang.

stack has ordinal args with evaluated elts lower and to-process elts higher. we can nest or otherwise encode particular relations, but these should be visualized as above, below, prior, and successor in an arg graph (and back & forward in 3D), or organic chemistry notation, so that programs look like organic compounds. we can use various symbols or line styles to denote variations of data or fns. math notation makes much use of subscripts & superscripts, and notations therein e.g. in superscript n is exponent but (n) is derivative. these are natural generalizations of a stack: seeing a stack as a linked list, we're just adding pointers from a cell to other things, generally forming a graph, so the program looks like a stack with small loops on its sides occasionally.

aspects of information manipulation:
* precision (probabilities of a proposition at varying degrees of generality of all of the proposition's axes)
* information theory: entropy (e.g. sigfigs), mutual information, compression
* a/symmetry on axes over data/operations
  ** vector/[SM]IMD operations exploit symmetry across set membership i.e. no (fn,x)ᵢ shares information with (fn,x)ⱼ, so multiple applications can be performed simultaneously
* encoding and transforms [re-expressions] (e.g. lossy or lossless compression, expansion, probabalistic reconstruction)

solvers should ultimately output assembly, which means that the solver uses bits and a t.b.d. set of assembly instructions with fallbacks e.g. ADDMUL if available, else ADD then MUL.

will target risc-v, for its simple set of 40/38 integer instructions; its abstraction from execution environment; and its freedom from permission. there are already purchasable systems with 1GHz+ risc-v cpus and 1GB+ main memory, which should suffice for non-professional computing (though this is insufficient for satisfying, modern graphics; that is accomplished by a gpu, and i'm yet unsure the extent to which any risc-v-compatible boards support reasonably capable gpus.)

the risc-v add instruction accepts a 12-bit immediate: a max value of 4096u or 2047s. that should be large enough. remember that generally we only need capacity for what we're _currently simultaneously considering_ at any time in the program; of course programs and data may be arbitrarily large, but how much of it do we need to compute on at any time?

one must be careful to consider never only a single construct, but only a composition of them that forms a complete program; this sees whether, ultimately, any subset of cooperative constructs has redundancy to reduce (i.e. compress.)

avoid ram; use registers & stack, except for streams/seqs. TODO: how do stack programs compare to register analogues? how can we transform between the two? remember to exploit bit twiddling tech. also encodings should be computed per the program e.g. if we create [EFFECTIVELY] a "matrix" but mostly compute its trace, then its internal representation will be optimized for trace! an m×n matrix is isomorphic with an m×n-length sequence with modality vector <m>. the modality vector generalizes to k-dimensional arrays. much of programming is mere mutation of some few registers or places on the stack or, worst-case, in memory. by flattening programs (loops, not recursion), going loopless as much as possible (preferring arithmetic instead), inlining/composing functions, using extremely compressed encodings, using both registers & stack paradigms, we can greatly reduce jumps! of course, at the heart of such optimization is not using functions and having the code express only ideas, not implementation! that is to say that the code only _virtually_ describes a program, much like how `[2..]` virtually describes the naturals from 2 onward. the implementation may be quite transformed away from the idea. the most extreme case is that effectively meaningless code is not even considered.
i want self-modifying and bootstrapping code, but maybe not at the price of introducing more jumps. i expect to see [assembly] programs that compute their next steps then load them into the cpu for evaluation, rather than loading machine code from disk.

groupings will generally be implicit, encoded as a set of edges e.g. `x∈G, y∈G` for `G:=(x y | r)`. free variables represent axes, bound variables indices.

TODO: consider fns under transforms

everything being virtual is true of the code only before it's compiled; the executable is plain machine code, no run-time dynamicism, unless you choose to have the program interpreted.

.THE DESIGN

* data types: relations [relalg] of integers, strings, or symbols. no nesting, all flat, like sql. strings like factor: encoding-agnostic lists. dynbind symbols like picolisp.
  ** quotes are not special; they're ordinary relations that probably contain symbols. in most langs featuring quotes, quotes can be evaluated. in this lang, the language does not track whether a relation can be evaluated; it just tries to evaluate any as the programmer asks.
  ** attribute names may be referenced, or attributes may be referenced by ordinal position, or all will be used if none are specified (this case most common for when the relation is a single attribute whose name is irrelevant)
  ** btw relations generalize alists: whereas alists are indexed by head (or not indexed at all), relations may be indexed by any sequence of attributes. though alists are commonly thought of as key-value pairs, they may instead of thought of as lists (seeing as list is a specific variety of pair in lisp) with indexed heads. json can encode relations: attribute names are keys and attribute values are [lists of] values.
* use stack style combinators. no syntax.

TODO: check whether this model supports bottom propogation

TODO: can i make the parser basis, {qq, β, λ}, tacit?

.DATA STRUCTURE

of course one would generally prefer a graph, which obviously generalizes at no cost and can even be pure. that being said, purity sucks. literal list ops like split, zip, etc suck, too; the better versions are virtual ones—non-strict, possibly lazy ones that _describe_ a get or put operation in terms of an index transform, or in the case of `zip`, an index relation, namely joining on a given index. it's so much more efficient to insert a translation fn (int -> int) than to modify a structure! furthermore it's more powerful! i can virtually make any linear structure circular simply by using a modular index function. it now has an infinite number of indices.

state isn't a problem in stack langs because return args are indistinguishable from loop state; they'll both just things on a stack. so if you want to change what your "function" returns, you could just not prune the stack of the function's state.

but of course we're talking about data structures, not control flow. but we've decided that all structures are just optimized virtual ones, which means that they're routines that act like structures. all structures must have length, index. we know that any index may be expressed by any sufficiently preserving encoding of so much information. one such encoding is an integer, given that the structure's shape is associated with it.

each [instance of a] structure should be automatically calculated for whatever information it needs to encode and arranged in such a way that it's efficient for the operations used on it during its time in the program. obviously this requires an information engine that knows about encodings and basic places where information is preserved or not, e.g. associativity, in order to know the space of acceptable transforms. really transforms will be sequences of [non-]preserving transforms.

generic oop is relational; it's mere association of types with implemenations: a generic word switches on tuple name: that's just an alist.

rather than cons pairs, generalize to a structure with arrows car & cdr. defining such a structure makes it virtual; `car` & `cdr` may compute return value upon access rather than strictly storing data. factor tuples/oop generalizes this to any number of slots needed for internal information keeping and generic methods to compute virtual values in terms of stored information. with everything being virtual values computed of tuples, everything is "lazy" if it needs to be, but we really see that laziness is not a thunk but instead a generator function. consider the "lazy list" [1..] as a [virtual] sequence defined simply as `[ 1 + ]`. it takes state from the top of the stack. the "sequence" is then merely the idea "recursive +1 with state." it _could_ be considered as a scan, thus actually producing a list, but this is foolish; the sequence, if it's useful, must be consumed, so it may as well be consumed as it's produced, never allocating memory. this is obviously feasable if the list is processed one element at a time. if multiple elements are needed, then it's exactly the same, because only one element at a time is ever processed; the only difference between "processing one at a time" vs "multiple at a time" is the amount of state/information needed. consider `min`: it processes one elem at a time and has a 1-datum state. why? because it uses a _binary_ function `<` to compute its value; one value is the current elt in the seq; the other must then be from state. an n-ary fn requires so many elts, and at least one must come from a seq. many being of a seq is expressable as a curried, now-unary fn whose information content is greatened by the number of curries that've bulit it up. a good currying model sees a curried fn reduce _as_ it's curried e.g. `3 2 [ + curry ] swap curry` --> `[ 5 + ]` instead of `[ 3 2 + ]`.

.antiexample

[source,scm]
----
(define (trace xss) (let R ([s xss]) (if (null? s) 0 (+ (caar s) (R (cdr (map cdr s)))))))
----

this example uses only lists, not keeping separate state. the information needed to write the fn is contained in the single argument. that's not considerable! do not think of this as clever; it's just an encoding that does not explicitly bind to part of the information that we need; whether the information is given an identifier or not, or whether that information is only selected from the list or copied to another section of memory (e.g. a varibale or in a stack) is irrelevant!

this example is here to encourage you to stop thinking about _data structures_ per se, but instead think in terms of selecting information patterns from a given object without regard to "cells" like data structures are usually assumed to have: a collection of relations of cells, like vertices in a graph (incl list, tree, &c.) instead, reason about information _parsers_—things that select information from a thing. *the information used should determine the structure; the structure should not be decided first.* this necessitates that all code is virtual, with implementation computed thereof. "cells" are a terrible woe; they encourage one to distinguish between `(values 5 6)`, `'(5 6)`, and `'(5 . 6)`, or an identifier varying over scope, which is utter nonsense, since *their information is equivalent*. information is all; all else is linguistic cruft. coders do only two things: 1. import & store information and 2. select information, both unambiguously (i.e. without information loss.) _information_ consists of: 1. literal data; 2. orientation/relation [patterns]. e.g. `6` is a literal data (not datum, because there's no plurality. consider it as a bitstring; now it's suddenly multiple! gasp, what magic!?), and _sequence_ is a relation pattern (namely orderd by an index permitting an order or by pred & succ pointers.)
