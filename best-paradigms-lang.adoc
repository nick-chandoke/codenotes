== best paradigms lang

this document is supposed to be a terse spec of the ideal programming language.

_program_ [noun] is not a useful term/concept. forget the term _language_, too. say _coding_ instead; `code` entails encoding [of information] and denies the fallacious code vs data dichotomy. whether a relation is about a universally quantified symbol or a particular literal value determines the ad-hoc or symmetric aspects of the relation e.g. `thing(i,v), in(i,0,4)` to mean `{(i,v) | i∈[0,4]}`. remember that these are predicates, though! the predicates set of implied values are not computed until queried. relations are like edges in a graph (symbols <-> vertices.) computations are traversals of (paths through) these graphs. btw, predicates are the most general abstract structure, and graphs are the most general data structure. *coders should never deal with data structures; they should deal exclusively with abstract structures.* this rule does not apply to those implementing logic engines. coders should never to deal with details beyond a program's spec; that's the optimizer's business!

predicates directly correspond to sets; however, predicates are terser encodings than sets.

to code:

. identify information, aka: relations, structure, constraints, predicates, queries, rules
  .. _joins_ are relations of relations
  .. each structure suggests at least one efficient traversal
  .. a relation may be thought of as a set of tuples all of the same tuple type. they may be used like factor oop.
  .. relations will [a filter of] pointwise pairing or cartesian prod. this all is accounted for by `join` [relalg]
. identify convenient encodings/notations which have equal information. e.g. `2` which can be interpreted as [2..], and [a,b] encoded by { a b }, which may represent 2 ordered or unordered things, or a sequence from a to b, or the cartesian product [0,a]×[0,b], or a sequence of `a` repeated `b` times. think not in terms of cardinality, but in information content. another example is encodings of geometric space: cartesian, polar, cylindrical, spherical; relations exist among these spaces (with free vars for converting from 2D to 3D); considering relations as graphs, we may say that there are cliques, and staying within one clique is most efficient, but going from one clique to another may be useful, most commonly going from clique A to clique B, then traversing a path in B, then going back to A, analagous to an operation under a transform.
  .. try to identify encodings that permit multiple interpretations of that information
  .. using interpretations allows pure structure to remain alone, open to study and interpretation. it's equivalent to duck typing but considering it as _information extraction_ biases the coder's mind better.
. specify constraints then see implications. this is enabled b/c the empty predicate [logic/symbolic programming] is everything whereas the empty value [reductionism] is nothing.

programs must be virtual/abstract, not reductions. think logic programming: symbols are literal math symbols, not just placeholders for literal values. everything is relations. a relation can have multiple indexes. transforms should be preserved, not applied; programs must be accumulations of constraints, not sequenced mutations on values e.g.

* `x=x` is immediately known as tautology and reduced to `t` because that's a terser, lossless encoding of equivalent information
* `swap swap` reduces to the identify function
* `swap` changes indices; this is a swap on two ints
* predicates may be unioned (a commutative operation/relation)/joined or had a subset extracted from it &c, then used in `filter` or `map` &c

keeping a rolling system of constraints enables, dynamicism, optimization, and implication. the idea of a self-modifying program here is silly; we just modify the fact set kept in primary and/or secondary storage. seeing as we're just determining implications of constraints, running in the repl will be exactly the same as compiling a program. here _program_ usefully refers to a binary executable loadable by a hardware processor.

.coding primitives

* `join` [relalg] is a subset of a cartesian product of relations and generalizes zip[with] (on a commonly satisfied predicate of attributes) 
  ** we're concerned both with relations of sets and relations of subsets; therefore relations & subset selection are our bread & butter.
  ** sql `where` can be rephrased as `for`
* predicate unification
* relations generalize functions

like in relalg, this model uses sets; therefore map & traversal are implied by subset selection. what is usually expressed in reductionist langs as complex, nested traversals, is here expressed by relation. use sql or stack models to avoid input/symbol ambiguity, imperatively no further namespacing tech be used!

wrt stack vs app prog, sql's local bind, `with` is acceptable because sql statements rarely nest, and practically never nest more than 1 level deep. sql programs are pretty much just a _sequence_ of statements, not a tree. thus there's no _confounding_ of variables / scopes. on that note, scoping is kept clean by being chunked into tables. this allows having single-character "variable" [attribute] names because they're of a given table, again not confounding with other tables' attributes.

TODO: consider how a pure expression can be more efficiently expressed by mutation e.g. rather than mapping `case when p then select f(x) from t [else null]`, use `update t where x=f(x) where p`. the obvious symptom is that we mutate instead of return an empty nonce value. consider this too in factor, though perhaps in factor it's not as easy, due to the constraint that both branches of `if` must have the same stack effect. btw, conditional branches, even in sql, are a good place to use a stack, as a preferable alternative to `let ... in cond ...`. recall that any thing must be eventually used in i/o in order to be useful; and that there are purely stateful, non-functional models, though these are _mostly_ better done by the stack paradigm; there are still places where mutation is useful, though this is usually best done in an array/set manner a la apl or sql.

TOOD: how does sql, relalg, or logic/predicate programming do `let-values`, namely when sets of values are cods of cond blocks? think about conds as associative maps encoded as relations or predicates and that `let` ``set``s for a context, and that contexts are representable as relations. even having the term `let-values` horribly confuses me, distracting me from the elegance of relations that would be obvious without the distraction. i feel the same about sql, which unnaturally shoehorns thought into its limited set of arbitrary constructs, such as triggers; in prolog, _everything_ is triggers, right? isn't A->B is equivalent to when A then B? _actions_ are just i/o or state changes that persist regardless of scope. even prolog has scoping rules: 1. named rules are defined top-level and are then in scope of following rules; and 2. the symbols bound by the LHS of :- are in the RHS' scope. hell, when things are not named, scope is no matter, as demonstrated by stack langs!

.comparison of predicate forms

* `intersect` & `except`, `union` & `union all`, `where` & `having`, and `join`; and `case`

NOTE: `put` is called `replace` in sqlite (and some other sql engines). see `upsert`, too.

in sql, any recursive expressions must contain `union [all]`, `intersect`, `except`, or `join`. the base case is when either argument of those is 1. not a table created for `with recursive`; or 2. a select statement that evaluates to an empty table. empty tables are produced by predicate forms. a statement _s_ prefaced by `with recursive` loops then evaluates _s_ once, so the recursion logic the same regardless of the type of statement (get, put, or del) that the loop parameterizes. <somethings> are always bulit on get, never on del or put! this is fine because del & put are just persistent get's; del should not even exist as an algebraic primitive; there should only be put; instead of `delete from x where p`, say `x = x\*∈x:p`. the same is true of put: instead of `insert into x t`, say `x=x∪t` and instead of `update x set (a,...) = e from t`, say `x = x\*∈x:p∪e`. that last one is invonvenient because it requires redundant code just to keep things the same, and isn't even appropriate, because it considers rows differently from any row's values even though they're all just related data. therefore modifying a row should be no different from modifying any of its values.`x y join on p` is equivalent to:

[source,factor]
----
x y × ! cartesian product (unconstrained inner join)
dup p filter -- `select` can be rephrased as `filter`
\
----

this can be generalized to left & right join and beyond by making `p` return either a row (include function of input row (of x×y) in result) or `f` (do not include row in result). that generalized join is equivalent to `x y × p map-filter`. this can be generalized to a fold over a set on an order, which can be further generalized to a loop permitting action execution & short-circuiting over a set on an order. that is, however, firmly outside relation algebra, and should be properly separated into 1. the [virtual] generalized join (henceforth _goin_) and 2. the action loop that may short-circuit.

let's define a sequence for each attribute and name each attribute. table schema are then specified as a set of attribute names. goin is then easily defined:

[source,factor]
----
SYMBOL: attrs -- global map from attribute name (as string) to sequence of values
-- the goin is a virtual array defined by strictly unioning both tables attrs then to `select f(a),b` is just `seq-index { "a" "b" } { f [ ] } [ attrs at call ] 2map`
: goin ( x y -- t ) [ attrs>> ] bi@ union ; inline -- unfinished definition
----

this model does not need to especially consider indexes [sql] because a sql index is just a map (int→int) from virtual to literal index.

NOTE: prolog uses comma to denote `AND` (boolean product) and sql uses comma to denote `join`. the set product is intersection, so you'd think that comma would be intersection. but intersection is quite similar (and probably somehow expressible in terms of) `where` i.e. "such that."

''''

TODO: how does sql bias the coder to sequenced mutations vs queries over data? prolog may or may not feature mutation, and sql supports it by write ops like `delete`, `update`, `insert`. there's no true difference between mutative or "pure" programming, and the stack is a nice meet between the two. the only considerable difference between stateful or not mutation is the set of contexts in which we want the mutations to persist. the pure stack model would see no definite tables, but all transient tables on a stack. then statements would not reference tables by name, but instead implicitly by position e.g. `select a,b,g(c) from t join s on p` would be `p join [ first3 g 3array ] select` except that instead of `first3` & `3array` we'd use a words that work on attribute sets rather than sequences. attribute sets would implement the `sequence` protocol and are really just arrays of attribute names (as strings); the model would interpret them as attribute names, using each as indices in the relation's attribute set, and `g` here would be implicitly applied to all `c` up to constraint.

also, why does sql have triggers? sql has many built-in features & syntaxes. prolog does not, and prolog is all data. how would sql triggers be expressed in a logical language?

.looping

in sql, to express a loop that may need to short-circuit, we must use the sole looping device, `with recursive`. consider a set partitioned into sets, T∈S|p, t∈T. for no more than one `t` per `T`, `p(t)` => execute effect `f(t)`. `∀T #{t∈T|p(t)}<=1` is incorrect; more than one `t` _may_ satisfy `p`; it's just that we won't perform an action for more than one.

the mapping over one t generally affects other t; we must use a fold and not a map if using iterative method.

if using logical method then we'd iterate not by map nor fold, but by backtracking, which generalizes fold to include `fold` & `if`. as such the logical iteration/evaluation (they're unified under the logical model!) method is like an `if` inside a `while` loop. the loop would better short-circuit than loop over all `t` but do nop for `{t|p(t)}\τ` where `τ` is the set of elements for which f(t) has been already performed, unlike the set of `t` (implicitly given merely by the symbol `t`), `#τ(T)<=1` is correct! the expression `τ(T)` means `τ` at a given `T` i.e. `τ` parameterized by `T` or `τ` such that `T` i.e. the intersection of `τ` & `T`'s contexts. we thus see the program modifying itself while it runs: as part of unifiing the predicate (analagous to traversing a stack or ast for stack or applicative langs) side effects are performed which both perform `f` _and_ modify the system being unified (by changing `τ`,) which affects the unification itself, making `#τ<=1` make the program correct. thus the whole program is:

----
Ω∈π(S), ∀Ω α∈Ω.
∀Ω∃τ. τ←∅. ! every Ω is uniquely associated with a τ. this is a bad notation b/c it's a commutative relation yet the use of two separate symbols, ∃ & ∀. really there should be only non-commutative 1:n and commutative n:n. n:1 is omitted like right join is omitted in sqlite
∀x∈{α|p(α)} #τ≤1∧f(x)∧τ<-τ∪{x}.
----

or, better expressed by a stack notation:

[source,factor]
----
Ω π(S) ∈
α Ω n:1
Ω τ n:n τ ∅ set
 x
 α [ p ] s.t.
∈
{ [ τ # 1 ≤ ] [ x f ] [ τ { x } ∪ τ set ] } 0&&
----

or, exploiting the stack:

[source,factor]
----
Ω
 [ π(S) ∈ ]
 [ α 1:n ]
 [ τ n:n τ ∅ set ]
tri
 x
 α [ p ] s.t.
∈
{ [ τ # 1 ≤ ] [ x f ] [ τ { x } ∪ τ set ] } 0&&
----

and this has equal program semantics.

TODO: correctly express by replacing `1:n` & `n:n` by ⊗.

.primitives

predicate unification engine. predicates are preferred over sets because they're just a terser encoding. the λ calculus is not considered because it's verbose and accepts ordered arguments. prediactes are just symbols that become increasingly constrained. that's a very simple & flexible model. also predicates are a single construct that generalize both literal data insertions (by predicates without bodies) and potentially recursive queries (by predicates with bodies.)

|========================================
| symbol   | description
| ∧        | and
| ∨        | or
| ¬        | not
| #        | cardinality
| →        | implication
| (a ... ) | group literal
| ⊗        | cartesian product (group)^*^
|========================================

^*^initially we considered pointwise relation (`join using`) vs one-to-many relation. however, one-to-many (1:n) is actually unconstrained `join` i.e. ⊗. the "1" in "1:n" can be considered as a datum in an array variable; then this 1:n relation is true of all in the variable, which means that 1:n is re-expressable as n:m so that the whole space is n×m associations. this is equal to ⊗ because `join using` is a specific variety of predicated `join`, and because predication is already a separate concept, 1:n & n:n shall be reduced to just ⊗, with optional filtering (predicate application/endowment) being understood to be implicit. also predicated join generally associates each `x` with a unique number of `y`, connoting a ragged array.

[TODO]
* cartprod is the largest set derivable from two sets that does not have set elements (cf powerset.) how to express powerset? shouldn't there be something more powerful/general than cardprodin order to
  ** should be plainly expressable by a looped application of ⊗
* below i say that join is nonsense, that it's just a scoping mechanism that shouldn't be needed because scope should never be limited in the first place! so how can i justify keeping ⊗?
  ** ⊗ is the grouping operator; it or its subsets relate things by grouping them.
* what's ⊗ in predicate logic?

.obviated concepts

|======================================================
| concept                  | obviating generalization
| if/cond/case/loops/folds | unification & backtracking
| x∈S                      | S(x)^*^
| x∈S                      | {x}⊆S^†^
|======================================================

^*^ sets correspond to predicates, which generalize the particular `∈` relation.
^†^ set theory only. in set theory it's appropriate to use _sets_ rather than elements.

.preferred concepts

these concepts make everything consistently predicate logic rather than set theory.

|====================================
| concept     | preferred alternative
| {x∈S:p(x)}  | x∈S ∧ p(x)
| ∩, ∪, , x\y | ∧, ∨, & x∧¬y
| ∅           | ⊥
|====================================

subset selection (`where`) is obviated by predicates & free symbols; `π(x)` constrains symbol `x` to predicate `π`. this corresponds to `{x|π(x)}` i.e. `x where π(x)`. in prolog predicates are clearly parameterized; in sql clauses' expressions are syntax that does not make apparent which variables are being related under the same relation/predicate. for example, i may `select x,y from t where (select z from t2 where z%2)`, and it's not so obvious that this is an impractical query since the nested `select` does not reference either `x` nor `y`! `π` must be a function of `x` for this to be practical; were `π` an expression not in terms of `x`, then `x` would have no meaning: if `π` then `x` gains no new information/constraint; if not `π` then `x` is ⊥/∅ and is therefore useless or insensible.

i favor predicate logic over set theory at least because:

. its _such that_ (`where`) is the primary object and is implicit i.e. sqls statements may not feature `where` clauses but a prolog program must have them, since that's literally all that a prolog program is.
. membership/subsets are not special in predicate logic; predicate logic features fewer primitives. then again, if it's not a primitive, is there an alternative? what's it even needed for anyway? i really must identify the common information basis underlying set theory and predicate logic.

i'll avoid ∀ & ∃ because they distinguish plural vs singular vs none. they're more clearly communicated as ∧ & ∨ anyway.

i may prefer (+,×,-,0) as notation instead of (∪,∩,\,∅) or (∨,∧,¬,⊥) since they're familiar, a little easier to type [input], and are not commonly used, thus explicitly connoting ring algebra, removing preconceptions of set theory or predicate logic.

NOTE: binary logical operators work on variables, not data, and are actually relations, not operators. thus we never fold them over sets.

what if all of the lang's syntax were strings, the above primitives, or numeric literals? there'd be no need to quote strings, and strings would be used as commonly as relalg attributes. quoting would be needed only if one of the primitives were used. ah, that's right! picolisp does this! and with a programs being so simply & canonically expressed, querying the code as a db would allow e.g. selecting all of the code related to other code, or tracing paths through code relations.

all binary operations are generalized to sets e.g. logical and is _all_, logical or is _any_, logical xor is _any one_. these all are further generalized to a range that #p(x) must satisfy (_any_ is `#p(x)>0`, _all_ is `#p(x)=#`, xor is `#p(x)=1`), which is even further generalized to a predicate on count, thus supporting e.g. `#p(x)∈[3.5]` or `#|2`. TODO: what is predicate logic's analogue of `#`? this should generally be stated as "how are aggregates expressed by predicates?"


NOTE: x∈S is generalized to {x)⊆S, and the latter is preferable in set theory or relational algebra because it uses two sets rather than one set and one "naked" element (two different types.) however, in predicate logic x⊆S is effectively `x t [ S ∈ ∧ ] reduce`.

NOTE: ideal lang should be tacit, stack logic lang e.g. supporting `π1 ∧ π2` instead of just `π1(x) ∧ π2(x)`.

.non-primitive convenience forms

* `if`. built-in abbreviation rule: `if(x,y,z) <-> (x -> y) ∧ (¬x -> z)`.
* `<->`, bidirectional implication. `a<->b` abbreviates `a->b. b->a`.

.total nonsense

* relations. relations are arbitrary grouping [coupling] of array/set data. have only "columns" (really just (non-nested) arrays.) this agrees with having a relation then, rather than adding one or more columns, adding a new relation that contains an attribute joinable with the original relation. more relations/attributes means better encoding: one large relation would have many nulls but the same encoding on multiple tables would see few nulls; the nulls would be present only when the relations are outer-joined. this is obvious when any attribute is plural e.g. `location:={name : string, hours : [(day-of-week,open,close)]}`.
* join
  ** `join` includes both `x`'s & `y`'s attributes in the query's scope. you may think of it as creating or identfiying a table whose attribute set is the union of `x`'s & `y`'s, but that's a needlessly complicated interpretation because it suggests that we needed to union; we did not; we will not be mutating the transient table, and `x` & `y` retain their attributes; we already can access them plainly; the only thing that changed was the query's scope. with only columns and no tables of course we would be able to access any column at any time. tables are sql's only scoping device. without tables we'd be able to access all attributes, so there'd be no need for join! therefore join really is not an algebraic operation, but instead is a linguistic device that exists solely to get around an arbitrary language limitation!
  ** the coder should not be able to choose nested select vs join; there should be a single include-in-scope device.

''''

* the stack accumulates context. applicative langs do too, but with the asymmetry of needing local bind clauses or anonymous nesting of data to function inputs.
* _scoping_ is a poor design for relating things. `{a b}` (`a` & `b` together) and `a b` (`b` applied to `a`) are fine notations for relation. scope _enables_ things to be related. ...what is that about? why would not everything be able to be related? if i say that alice is related to bob, then they are. why would any mechanism prevent me from relating those things? the answer is that scope is used in reductionist languages; scope limits _data_ (of which there are many and no datum is an idea, being only a literal value) to be _passed_ to _functions_; rather than _symbols_ (which are abstract and implicitly define ideas) being _related_ to other ideas by a _predicate_.
  ** though factor is reductionist, at least it's without scope; all things are in the stack, which is manageable because they're related positionally, which suggests the present program state, as opposed to applangs, which throw around identifiers in confusing ways: we assume (sometimes incorrectly) that a bound identifier is used within the scope of its binding clause, but we do not know where nor how frequently. we do not know when the program is done considering using it, unless coders particularly limit all binds' clauses to the smallest valid scope. but that would be a huge pain. that would mean no `let*` clauses unless all of its binds are used in exactly the same statement—unlikely! to approach a stack's datum/scope relations elegance would require a glut of bind clauses, making the code unreadable and severely bloated. in a stack, you _know_ when some datum is no longer used; it's not used if it's not in the stack! it's used soon if its near the top, and used later if near the bottom!
* use ragged relations (equivalent to document stores)—relations whose attribute set is not static, and is implied by facts e.g. `v@n∈r.` to declare a value at a given name in a relation. `v@p∈r?` asks rather than declares. being that code will be computable, messy or redundant ragged relations will be consolidated so we get the decoupling—declaration/definition from usage; frankly, declaration/definition statements are foolish; we never know what something is, or whether it even exists at all; we may suppose that it does but find that that conception was fallacious or redundant. in fact, _things_ is not even proper; _stuff_ is. all partitions of stuff into things is arbitrary and any rigidity of those partitions creates needless coupling and coding trouble!
  ** note that in relalg `v@p∈r` would mean `select v from r where n`. there would be no reason for imperative `.` nor interrogative `?`; in relalg the expression just refers to a set, and the contents of the set would need to be literally specified rather than abstractly specified by a prediacte.
* any [relation] declarations/definitions should be assertions—facts to be included in the fact set, checked for correctness or implication, just like any other facts!
* *just as humans query a logical fact db, the canonicalizer should tell the coder the implications or better interpretations of what they're saying! the computer should not wait for a human to ask; it should tell, inform the coder of what they're saying _as they're saying it_, so that they can update their specs with realtime wisdom!*
* relations allow us an always-valid empty value: simply omit the item from a list. a function on any relation returns a relation. unless the function uses `union`, an empty relation input guarantees an empty relation output. this is _conditional insert_ and is not supported by sql; in sql one either hardcodedly inserts a row or not; only the row values may change, and the only empty row value is `null`. a `null` that propogates is always valid, though one may need to convert it to or immediately use another empty value like `0` or `""`, to correctly make it a coproduct identity or a product identity. in sqlite, `null` itself is a product identity; its inclusion in any [non-comparison] function guarantees that the function outputs `null`. this is akin to multiplying by 0. basically you must choose whether to error, halting the program; or ignore the lack of valid value *while still maintaining the functional-set model* by having the return value be ∅; or have the error invalidate the expression that, without especially accounting for `null`, uses it. appreciate how the set/relational model naturally handles a lack of valid values: `nth` usually works on a list and halts on the empty list. in the relational model we must say `v@i=n`, which will return ∅ for the empty relation or otherwise when index is out of bounds. certainly one should be able to insert an assertion thereafter to, only for debugging reasons, halt or print an error if e.g. `v [ i n = ] @ # 0 =`, but that is not _natural_, as is obvious when considering that i/o & errors do not exist in the relational algebra, or in pretty much any algebra! errors are usually due to our implementing model not exactly fitting the spec of the abstract model (there are exceptions, e.g. div by 0, which can be considered an error in some abstract models.)
* forget first-class functions, or really functions altogether; like factor, prefer quoted programs. in applicative langs functions maps parameter [values] to their locations within expressions. in factor programs are functions because this mapping is moot & implicit.
* one must importantly consider sql's scope! `select x,sum(y) from x group by x%2=0 having sum(y) > 20;` works and this is nice b/c `y` is in the scope of `having`'s expression! sql would be much better if we could manage scopes across expressions so that we'dn't need to have redundancies across `select` statements or other expressions!
* forget _sequences_; consider only sets which may permit multiple orders
* sql views are not parameterized and so are not functions; were sql to have `eval`, they'd be able to read parameters from a mutable table whose name would be hardcoded into the view
  ** at least prepared statements support variables in their predicates, though not supporting variable tables or select statements. with relations corresponding to functions (at least in a logical language, wherein that's appropriate because the empty value is `∀x.x` rather than `∅`, unlike in sql) a view with variable tables should correspond to a higher order function. 
* sql generalizes apl by removing the ordering constraint. but often apl uses that constraint for succinctness; this wip lang should allow forms succinct like apl for convenience. these expressions should generalize [refactor] easily. succinct expressions do not necessarily require special _syntax_; they merely must make implicit or assume symmetries. one example is sequence literal notation, which is a syntax, and the terseness over sets is that text [syntax] and the sequence itself share order; the syntax is joined [relalg] with semantics on this linear ordering symmetry. however, one could use a set literal then pass that set to word `seq` which would endow the set with the fact that its order is irrelevant, which would, for example benefit, enable an optimizer to sort the set by an order that makes its traversal efficient, given its relation to other expressions entailed in a query.
* why do a _nested loop_ when you can do a single loop over a lazily computed filtered cartesian product? a-haaaa~
* remember that you can get around stack ordering difficulties by using tuple accessor & putter words, which are implicit in the context of any given table.
* sql syntax `select a from S where p` is subset selection. `a` takes subset of columns and `where p` takes subset of rows
* stack model & syntax should be used to encode programs. that prevents nesting/ordering creep at least in syntax alone.
  ** quotation & eval should be used as often as in factor? except that this is all constraints, so how can such a thing exist, right?
* like unison [lang], no function names; instead only hashes (though they can be known my many names). other things proven are like stored e.g. stack effect or type. we must do much better than this, but it's an improvement on names. caching results of pure fns is nice, too. storing words &al data in a db is obviously good. we can think of sql triggers for keeping code consistent after renamings. consider that hashes are different from uids: they're effectively alternate encodings, as opposed to an associated arbitrary unique datum.
  ** hashes are not too good; they're still arbitrary; they do not enable us things that we should have: similarity measures, orders, and good searches for expressions.
  ** still the unison model of hashes demonstrates how stupid names-as-identifiers are. these observations should be applied to filesystems and anything else that uses names!
* the erlang model is correct for concurrency. it apparently is good for dynamicism, too! that's not a coincidence!
* the ability to fix something as it's running is usually better than trying to ensure that it's perfect then shipping it without the ability to modify it. obviously there's nothing easier to modify than a db.
* all code should have visualizer(s)
* we should do better than using text to represent relations

* all structures being virtual, operations on them are cheap; they affect only how the structure is accessed or modified.
* a fact's a/symmetry about an axis [index, property] is whether its truth varies with subsets of the axis.
* plurality is assumed. aggregates are the exception. a word is defined as an aggregate or not. aggregates return a singleton set. this allows word composition e.g. `[ # f ] [ g ] bi ×` where `#` is an aggregate and `f` is a non-aggregate will apply f to the length of a relation then cartesian prod with the application of `g` to that set. all words here are assumedly selections without puts.
* `x [ f ] map` is expressed [conceptually] as ∀i.f(x[i]). because we're using an array lang, there's no need for `map`; it's implicit. however, this demonstrates the role of [free] indices: they're logical objects that implicitly affect other expressions. like all linguistic objects in this paradigm, they do not reduce to data literals, though data literals can, in any contexts, be calculated from the context and rules concerning indices. cartesian products are expressed as `{ x[i] y[j] }` and are as `lzip` in factor; rather than thinking of × as a set, think of it as an accessor function, because that's literally what it is. it's a virtual sequence: just a map from an index to a value.
  ** setting at given indices is just like sql: `set prop of rel where pred`, where pred can be `index in <set>`.

* virtual: all code is logical/algebraic symbols supporting more than mere symbol substitution
* avoid ordering
  ** nesting is a variety of ordering. prefer flat structures, namely sets, which may permit multiple orders.
* both knowledge and dataflow should be supported, with arguable interoperability, because each is commonly useful and neither elegantly substitutes for the other.
* aside from being terse for its own elegance, there's the particular need for selective relation or separation/exclusion of codes, so that code can be considered in manageably-sized subsets, then, after being understood, allowing oneself to consider more code, starting with the most-related code. being confronted with a large mass of code can be dispiriting! even if code is neatly formatted, things like nesting & indentation systemically create extra work for the parser (human (both mind & eyes. it's even worse for blind people!) or computer.) comments can be about the spec or implementation, and it'd be very good to have those clearly separated! also comments take the most characters, and should be hidden when not needed. in fact, all code that isn't being presently concerned should be hidden. most importantly, though, code must be automatically related, reduced, & otherwise organized, as enabled by the code being computable.
  ** in our organization (incl categorization) of code, the code must be able to belong implicitly to multiple (predicate-defined) sets.
  ** the automatic canonicalization & reduction of code is imperative, and likely the single most important code operation.
  ** specifying a predicate _on_ the program is done just as predicates are specified _in_ the program
  ** current tech: put comments like ;1 above fndefs. in kak, select then pass to a script that looks-up that id in a comment db, then opens that in a new kitty window.

.flatness

`y(x,v). y(4,8). y(x,x+2).` is better than `y = case x [4 2*x] [else x+2]`. the flat version decouples definition from exception, and makes both refactoring and metaprogramming easier. in fact, it makes storing the program in a database easier, too. another example: it's more sensible to break `[if even? i then f x else g x | x <- xs]` into `select x call from xs join values(even?,f),(odd?,g) as p,f on x p`, which describes separate *rules* for evaluating `x` by its elements instead of merging the two rules into a conditional branch inside a loop. NEXT: translate sql into predicates. i used a mix of factor & sql syntax. we can reduce the syntax for 1-column relations: `select x from x` becomes simply `x`, and `select x f` is just `x f`, which is really just `f` since it's pointfree when not applied to `x`, and applying to `x` is implicitly applying to all in `x`. if functions (predicates/relations) were supported like this is sql, that would enable joins on applying them to columns selected from other tables being joined.

context is a set of named relations (e.g. prepositions, which, btw, are usually binary) each of whose arguments are variably free or bound. meaning exists only in context; therefore to discuss forms is inherently foolish. again, *the only defining questions are how we select & put data from & to a thing.* this is all that programs virtualaly are: selecting & putting. all the rest of programming concerns efficiency of these operations, which is the perogative of a solver, not a programmer.

GUIDING PRINCIPLE: if there are multiple ways to encode an expression, then either 1. your computation model is too complex, or 2. your code model is too complex and the choice of implemenation should be deferred to a solver, hidden from the programmer. this is also true of structure; if you use ad-hoc polymorphism, do not use a code model that allows the programmer to select a class instance that's inefficient for their use, e.g. accessing a linked list by positional index. allowing the programmer to choose a type class's instance is the primary failure of ad-hoc polymorphism. good ad-hoc polymorphism makes choosing implementing types available only to the engine, not the programmer. then again, structures should not be defined; they should be calculated in each instance to most efficiently satisfy each object's usage (select, put).

_encodings_ are maps to/from bitstrings.
use constraint solving to encode programs.
make fn : information content -> encoding. e.g. ci english characters -> 5-bits.
the expr 2x+3y+4z can be expressed <[2,],X> (`2 lfrom { x y z } >list lzip list>array v.`) which generalizes & simplifies the original expression, and can be said to have 4 pieces of information: 2, <,>, X, and [,]. associativity, commutativity, &c affect amount of information, too.

generalize & specify asymmetries e.g. all follow this rule except x. this is encoded as x -> { ... [ t ] x } cond.

the register/asm model, when ideally done, isn't much better than the stack model. in practice <someplace on the stack> is easier & more regular than trying to assign registers to hold all sorts of particular kinds of values. identifying an elegant set of kinds of values would be difficult.
should have an auto code reducer e.g. `b a - b a - 2/ +` reduces to `b a - dup 2/ +`, though frankly that should be pretty easy for a programmer in a stack lang. that being said, it'd be nice, especially to see super-compressed expressions.
stack langs free us from many decisions, e.g. about scoping or iteration vs recursion, or jumping to a function, or functions vs data. sometimes one forgets about how _easy & simple_ stacks are. loops are practically identical to folds in a stack lang.

stack has ordinal args with evaluated elts lower and to-process elts higher. we can nest or otherwise encode particular relations, but these should be visualized as above, below, prior, and successor in an arg graph (and back & forward in 3D), or organic chemistry notation, so that programs look like organic compounds. we can use various symbols or line styles to denote variations of data or fns. math notation makes much use of subscripts & superscripts, and notations therein e.g. in superscript n is exponent but (n) is derivative. these are natural generalizations of a stack: seeing a stack as a linked list, we're just adding pointers from a cell to other things, generally forming a graph, so the program looks like a stack with small loops on its sides occasionally.

aspects of information manipulation:
* precision (probabilities of a proposition at varying degrees of generality of all of the proposition's axes)
* information theory: entropy (e.g. sigfigs), mutual information, compression
* a/symmetry on axes over data/operations
  ** vector/[SM]IMD operations exploit symmetry across set membership i.e. no (fn,x)ᵢ shares information with (fn,x)ⱼ, so multiple applications can be performed simultaneously
* encoding and transforms [re-expressions] (e.g. lossy or lossless compression, expansion, probabalistic reconstruction)

solvers should ultimately output assembly, which means that the solver uses bits and a t.b.d. set of assembly instructions with fallbacks e.g. ADDMUL if available, else ADD then MUL.

will target risc-v, for its simple set of 40/38 integer instructions; its abstraction from execution environment; and its freedom from permission. there are already purchasable systems with 1GHz+ risc-v cpus and 1GB+ main memory, which should suffice for non-professional computing (though this is insufficient for satisfying, modern graphics; that is accomplished by a gpu, and i'm yet unsure the extent to which any risc-v-compatible boards support reasonably capable gpus.)

the risc-v add instruction accepts a 12-bit immediate: a max value of 4096u or 2047s. that should be large enough. remember that generally we only need capacity for what we're _currently simultaneously considering_ at any time in the program; of course programs and data may be arbitrarily large, but how much of it do we need to compute on at any time?

one must be careful to consider never only a single construct, but only a composition of them that forms a complete program; this sees whether, ultimately, any subset of cooperative constructs has redundancy to reduce (i.e. compress.)

avoid ram; use registers & stack, except for streams/seqs. TODO: how do stack programs compare to register analogues? how can we transform between the two? remember to exploit bit twiddling tech. also encodings should be computed per the program e.g. if we create [EFFECTIVELY] a "matrix" but mostly compute its trace, then its internal representation will be optimized for trace! an m×n matrix is isomorphic with an m×n-length sequence with modality vector <m>. the modality vector generalizes to k-dimensional arrays. much of programming is mere mutation of some few registers or places on the stack or, worst-case, in memory. by flattening programs (loops, not recursion), going loopless as much as possible (preferring arithmetic instead), inlining/composing functions, using extremely compressed encodings, using both registers & stack paradigms, we can greatly reduce jumps! of course, at the heart of such optimization is not using functions and having the code express only ideas, not implementation! that is to say that the code only _virtually_ describes a program, much like how `[2..]` virtually describes the naturals from 2 onward. the implementation may be quite transformed away from the idea. the most extreme case is that effectively meaningless code is not even considered.
i want self-modifying and bootstrapping code, but maybe not at the price of introducing more jumps. i expect to see [assembly] programs that compute their next steps then load them into the cpu for evaluation, rather than loading machine code from disk.

groupings will generally be implicit, encoded as a set of edges e.g. `x∈G, y∈G` for `G:=(x y | r)`. free variables represent axes, bound variables indices.

TODO: consider fns under transforms

everything being virtual is true of the code only before it's compiled; the executable is plain machine code, no run-time dynamicism, unless you choose to have the program interpreted.

.THE DESIGN

* data types: relations [relalg] of integers, strings, or symbols. no nesting, all flat, like sql. strings like factor: encoding-agnostic lists. dynbind symbols like picolisp.
  ** quotes are not special; they're ordinary relations that probably contain symbols. in most langs featuring quotes, quotes can be evaluated. in this lang, the language does not track whether a relation can be evaluated; it just tries to evaluate any as the programmer asks.
  ** attribute names may be referenced, or attributes may be referenced by ordinal position, or all will be used if none are specified (this case most common for when the relation is a single attribute whose name is irrelevant)
  ** btw relations generalize alists: whereas alists are indexed by head (or not indexed at all), relations may be indexed by any sequence of attributes. though alists are commonly thought of as key-value pairs, they may instead of thought of as lists (seeing as list is a specific variety of pair in lisp) with indexed heads. json can encode relations: attribute names are keys and attribute values are [lists of] values.
* use stack style combinators. no syntax.

TODO: check whether this model supports bottom propogation

TODO: can i make the parser basis, {qq, β, λ}, tacit?

.DATA STRUCTURE

of course one would generally prefer a graph, which obviously generalizes at no cost and can even be pure. that being said, purity sucks. literal list ops like split, zip, etc suck, too; the better versions are virtual ones—non-strict, possibly lazy ones that _describe_ a get or put operation in terms of an index transform, or in the case of `zip`, an index relation, namely joining on a given index. it's so much more efficient to insert a translation fn (int -> int) than to modify a structure! furthermore it's more powerful! i can virtually make any linear structure circular simply by using a modular index function. it now has an infinite number of indices.

state isn't a problem in stack langs because return args are indistinguishable from loop state; they'll both just things on a stack. so if you want to change what your "function" returns, you could just not prune the stack of the function's state.

but of course we're talking about data structures, not control flow. but we've decided that all structures are just optimized virtual ones, which means that they're routines that act like structures. all structures must have length, index. we know that any index may be expressed by any sufficiently preserving encoding of so much information. one such encoding is an integer, given that the structure's shape is associated with it.

each [instance of a] structure should be automatically calculated for whatever information it needs to encode and arranged in such a way that it's efficient for the operations used on it during its time in the program. obviously this requires an information engine that knows about encodings and basic places where information is preserved or not, e.g. associativity, in order to know the space of acceptable transforms. really transforms will be sequences of [non-]preserving transforms.

generic oop is relational; it's mere association of types with implemenations: a generic word switches on tuple name: that's just an alist.

rather than cons pairs, generalize to a structure with arrows car & cdr. defining such a structure makes it virtual; `car` & `cdr` may compute return value upon access rather than strictly storing data. factor tuples/oop generalizes this to any number of slots needed for internal information keeping and generic methods to compute virtual values in terms of stored information. with everything being virtual values computed of tuples, everything is "lazy" if it needs to be, but we really see that laziness is not a thunk but instead a generator function. consider the "lazy list" [1..] as a [virtual] sequence defined simply as `[ 1 + ]`. it takes state from the top of the stack. the "sequence" is then merely the idea "recursive +1 with state." it _could_ be considered as a scan, thus actually producing a list, but this is foolish; the sequence, if it's useful, must be consumed, so it may as well be consumed as it's produced, never allocating memory. this is obviously feasable if the list is processed one element at a time. if multiple elements are needed, then it's exactly the same, because only one element at a time is ever processed; the only difference between "processing one at a time" vs "multiple at a time" is the amount of state/information needed. consider `min`: it processes one elem at a time and has a 1-datum state. why? because it uses a _binary_ function `<` to compute its value; one value is the current elt in the seq; the other must then be from state. an n-ary fn requires so many elts, and at least one must come from a seq. many being of a seq is expressable as a curried, now-unary fn whose information content is greatened by the number of curries that've bulit it up. a good currying model sees a curried fn reduce _as_ it's curried e.g. `3 2 [ + curry ] swap curry` --> `[ 5 + ]` instead of `[ 3 2 + ]`.

.antiexample

[source,scm]
----
(define (trace xss) (let R ([s xss]) (if (null? s) 0 (+ (caar s) (R (cdr (map cdr s)))))))
----

this example uses only lists, not keeping separate state. the information needed to write the fn is contained in the single argument. that's not considerable! do not think of this as clever; it's just an encoding that does not explicitly bind to part of the information that we need; whether the information is given an identifier or not, or whether that information is only selected from the list or copied to another section of memory (e.g. a varibale or in a stack) is irrelevant!

this example is here to encourage you to stop thinking about _data structures_ per se, but instead think in terms of selecting information patterns from a given object without regard to "cells" like data structures are usually assumed to have: a collection of relations of cells, like vertices in a graph (incl list, tree, &c.) instead, reason about information _parsers_—things that select information from a thing. *the information used should determine the structure; the structure should not be decided first.* this necessitates that all code is virtual, with implementation computed thereof. "cells" are a terrible woe; they encourage one to distinguish between `(values 5 6)`, `'(5 6)`, and `'(5 . 6)`, or an identifier varying over scope, which is utter nonsense, since *their information is equivalent*. information is all; all else is linguistic cruft. coders do only two things: 1. import & store information and 2. select information, both unambiguously (i.e. without information loss.) _information_ consists of: 1. literal data; 2. orientation/relation [patterns]. e.g. `6` is a literal data (not datum, because there's no plurality. consider it as a bitstring; now it's suddenly multiple! gasp, what magic!?), and _sequence_ is a relation pattern (namely orderd by an index permitting an order or by pred & succ pointers.)

.repl

a stack state repl would be very nice with relations.
