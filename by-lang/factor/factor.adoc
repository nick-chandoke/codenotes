= the factor programming language
notes by nicholas chandoke
:toc:

== factor vs other programming languages / code systems

factor is best b/c it minimizes any stuff that gets between you and code, and maximizes access & ease of understanding the code; and its code is simple, so there's minimal distance between the abstract thought and its code representation, and code is easy to refactor, which makes it a highly-interactive system, which means detailed, easy, immediate feedback and hackabilty. i've found it so for small scripts to polished, robust, end-user products. it's like coding elisp in emacs (rather, a usable variant of it, e.g. spacemacs) except that it doesn't malfunction each time that you sneeze or that a glacier falls off the antarctic ice shelf. oh, and without the parentheses.

so yeah, very simple, terse, readable, efficient, refactorable, hackable, and comes with tons of functionality. in fact, its only shortcoming is that the executable sizes of compiled factor programs can be absurdly large (hundreds of megabytes) (also the factor binary itself (including the required `factor.image` file) is large: 130M). i would say that it being infeasably slow to use as an interpreted language is a shortcoming, but it is not: factor code is best compiled, which produces a fast executable that anyone can run. the need to compile it (and the compilation time needed) are no more undesirable than needing to have an end-user install a program just to run yours.

this being said, if you want to use factor for many small personal scripts, then...perhaps i'll fork picolisp then make it concatenative and have good error messages and debugging tools or something ^^; i know of no good scripting language. dyalog apl is prorietary, so bleh. j & k have sucky os interfaces. ...maybe link:https://dt.plumbing/user-guide/lang/stdlib.html[dt]? i don't remember why i disliked it...i'll need to reconsider.

pro's:

* hackable to the hilt
  ** primitives are _really_ primitive.
  ** definitions marked as "private" are just as accessable as any non-private ones; only they're _marked_ as private, as a _hint_—not a _restriction_—to the programmer
* mostly all the capability anyone could ask for
* dead-simple semantics. b/c syntax is homoiconic, syntax is also dead-simple.
  ** no order of operations, so no need for parentheses. the only nested expressions are quoted programs or nested data structure literals.
* syntax: all tokens are whitespace-separated so considering arbitrary subprograms is easy. this is more significant than you may think; in practice, the code being so clean is quite less stressful than using an applicative language. there is a latent burden that creeps in and that applicative programmers become used to. the required whitespace does make the code less terse, but easier to read and less stressful. i prefer it over apls.
* concatenative (TODO: make this word a link to description of concatenativity in a more-general code note document. lookin' for those notes i took that one day on a walk): refactorable, easy to debug b/c vm's design is simple: just 2 stacks and a sequence of hashtables (the sequence corresponding to scope nesting).
  ** incremental
  ** stack
    *** [actually a property of any linear computation sequence, so applies to ] the one global state. such a simple model is a degenerative case of functional vs stateful/mutative.
    *** no scope—just position in the stack.
    *** multi-valued functions act exactly as single-valued ones. all programs are function composition.
      **** factor also supports both lexically and dynamically scoped symbols
* awesome debugger ("the walker")
* documentation & source code browsing system: navigable, thorough, offline, and dynamically populated, like in emacs: loading your code automatically makes it searchable in the browser just like any other library.
* dynamic. though lisp-like macros which evaluate before runtime are available, factor effectively uses quote & eval _instead_ of lambdas. therefore there's rarely a need for macros in factor. factor is so data-oriented that a programmer practically never desires custom syntaxes. indeed, factor macros are more _needed_ when manipulating a number of stack data where the number is given at parse time.
* virtual sequences make sequence operations efficient
* code as high-level or low-level as you like. factor feels truly unique, like a blend of c and scheme. idk, maybe that's what common lisp is like.
  ** code is compiled on the fly into highly optimized single static assignment (SSA IR). such a simple lang supports extreme optimization.
    *** use `optimized.` (instead of `.`) to see optimization details of some code
* ffi can call c, fortran, obj-c, python, js, lua, and c++. ffi is dead-simple (or, for c, about as close to that as possible): just declare the library and its function, then you can use it like normal factor function. apparently c ffi is complicated, so there are probably many edge cases for when c ffi is difficult regardless of the ffi system.
* actually does oop so well that i usually forget that it's oopy. feels like nothing more than adts & type classes, but with shorter syntax.
* as in haskell, functions are semantically (and thus syntactically) the same as data literals i.e. there would be no distinct `mySymbol` and `mySymbol()` like there would be in python, or `mySymbol` vs `(mySymbol)` in lisps.

con's:

* factor's codebase has libraries that you likely won't use, and very many functions, so discerning which are truly useful can take time, and might significantly lengthen factor's build time.
* can be hard to find libraries needed for the graphical install
* building factor takes a lot of cpu time
* using factor to compile factor programs can take a bit of time
* unpredictable executable size for your factor programs. various vocabs demand certain compilation options (namely higher "run levels"), which can drastically affect the executable size (e.g. using peg (packrat parser) takes a 2MB to 272MB), and which you cannot know in advance; practically, you must use (a lot of) trial & error to determine a vocab's required options—and the required options can change if the vocab is updated e.g. in a new factor release!
* like many vm's (lua, haskell, v8 js), factor uses "green" threads / coroutines, not os threads.
* vocabs _must_ be defined by a particular directory structure: they must be named `<name>/<name>.factor`; to load that vocab, you cannot specify the filepath; instead, <name> must be a subdirectory of a directory that you've registered with factor's vocab loader via the `add-vocab-root` word.
* in development, factor is like lisp: it's dynamic, hackable, flexible, and gives immediate feedback in its version of a repl, called the "listener". however, to run factor, you practically must compile your program; factor is far too slow to use as an interpreted language (namely b/c loading vocabs takes a long time)

fixed-arity fns isn't an issue. it's just as well to have a function by one name that takes k args and another that takes n args, as it is to have one fn that takes either k or n args. you can simply name your factor fns by the prolog convention e.g. a word called `log/1` that takes 1 argument and does the natural log, and `log/2` which accepts a base. it's still shorter than having parenthesis everywhere for all fn calls.

.REVISION

ok, so these notes are:

. overview: why factor is a good code language & system
. concatenativity
  .. stack, and coding in a cat/stack lang well
. basic description of factor's vm, language & execution model
. coding in factor (aside from aforementioned stack stuff & cat-program style): control flow, data structures, and words over those structures. factor wrt `universal-coding.adoc`, that means: the sequences & assoc protocols, and implementors: array, hashtable, avl tree, linked assocs, vectors (generally growable arrays, which we end-up using instead of linked lists (but should we use linked lists instead? i'd think it'd be more efficient than `resize-array`, but i haven't read its implementation—can't even find it!)
  .. this section is critical! one must be told the small set of useful words, as browsing through vocabs, large as they are, is unreasonable and draining, especially to find words such as `glue` which are purely for brevity, not representing any unique concept/operation.
. the listener, walker, and documentation browser
. useful vocabs, sectioned by purpose

.interesting concatenativity example

i found link:https://blog.demofox.org/2016/08/23/incremental-averaging/[some code] when looking for moving averages:

-------------------------------------------------------------------
NewAverage = OldAverage + (NewValue - OldAverage) / NewSampleCount;
// Or:
Average += (NewValue - Average) / NewSampleCount;
-------------------------------------------------------------------

at first i tried coding the second expression in factor, since it looks like it's fewer computations: `over - 5 / +`, but i didn't get the result that i expected (because of my own mistake; the expression is correct), so i looked to the first expression to see if it'd give my expected result. but when i would need to modiify this expression in factor, i found that it was already identical! thus what can be expressed only one sensible way in factor can be expressed in two different ways in applicative languages! by "sensible" i namely mean that i'm not counting `dupd swap -` instead of `over -`, since the former is just a poorer expression that we might expect from a factor novice.

'''''''''''''''''''''''

TODO: add section about how coming to factor from scheme/haskell was awful, but from j was good. factor's combination of virtual sequences, mixins, row-polymorphic looping combinators, and mutation is unique, allowing one to do things that they cannot do nearly so elegantly in c, haskell, scheme, or j.

TODO: catlangs are commonly contrasted with applicative ones, but tacit apl dsls are neither concatenative nor applicative.

notes' new structure:

. overview of factor as a code system
. intro to factor: stack, cat, and discussing how factor programs generalize fns, and how purity is no longer applicable, since the code is not applicative.
  .. i imagine that a common reason that people coming from app.prog. find factor difficult to read is that they're used to trying to hold a whole function or other block of code in their head at once; if they don't see it altogether, then they feel like they're blind. by contrast, cat.prog. is about hardly holding _any_ of the code in mind at a time; the whole point is that you don't need to care at all about anything before or after some small part of the program that you're considering, since they don't affect each other—yk, being _concatenative_ and all. so these app.prog.-minded people start at either the beginning of a word, or at the middle but then work their way backward just to trace the stack effect. that's madness!
. how to code well in factor: combinators, quotations, currying, stack shuffling (retain [somewhere], discard, or shuffle; e.g. `pick` & `rot` both bring, of `x y z`, `x` to the top, but `pick` copies whereas `rot` moves), factoring, and code structure.
  .. to interactively play with complex shuffling, if you so wish instead of using locals, you can e.g. `SYMBOLS: v n h l t ; { v n h l t v n h l t } [ [ rot ] 3dip ] with-datastack` to make the datastack print in one line for easier tracking
  .. currying is not always the most elegant, since it requires composing, which can be difficult, especially in nondeterministic loops through multiple sequences, namely when you must, inside a quotation passed to `map`, curry the current element then compose that `curried` with another quotation to be passed to an iterator such as `find`, all still inside that quotation passed to `map`. contrastingly, the stack is just the stack as the computation goes on; it's easy to shuffle it at any point in the computation.
  .. to retain something on the stack when a quotation will be called multiple times, then use `curry`, as is common in looping/iteration combinators like `loop`, `each`, `map`.
  .. to copy something, then use `over`, `pick`, etc
  .. to leave something on the stack as a return value, then depending on the data flow, you'll use either `dup` (copy atop), `tuck` (copy below), or `[ ... ] keep` (copy on top with fn of stack). `over` is equivalent to `swap tuck`, btw. your proficiency in shuffling the stack will show itself in your ability to notice such relations, by thinking in terms of datum position on the stack and duplication.
    ... `[ ] keep` is equivalent to `dup`
    ... if you're having trouble with shuffling, just use nkeep or ndup then drop or nip as needed. keep & dup are the simplest forms of retention, and though words like `over` efficiently shuffle with duplication, sometimes it's easier to decouple shuffling & duplication. for example, consider the case where you have `i x xs` on the stack, and you want to branch on whether xs@i > x, then use `i`, `xs`, and `x` in the branch computation. you would probably think to use `over` first, but then you want the result of `>` to top the stack for `if`, while retaining all 3 data for its branch. it's easier to just `3dup swapd nth [ ... ] [ ... ] if`. in fact, this code is perfect, since it consumes all 3 data and leaves them exactly where they must be.
  .. to apply a symmetry to multiple data on the stack, use "cleave", "spread", or `apply` combinators e.g. `2bi`. `tri*`, or `bi@`.
  .. otherwise sometimes you'll want to use general shuffle words. don't worry about making it beautiful; as you code, over time you'll naturally begin to notice more elegant ways to manage the stacks
  .. a funny shuffle hack: with n elements on the stack, to break them on the ith then swap those partitions: `n-i ncurry i ndip`. of course, you could always use the `shuffle` vocab's `shuffle-effect` which allows you to plainly, explicitly permute elements.
. vocab reference (useful stuff, eliding unnecessary things, as most things are)
. common troubles & solutions
. tuning: using numbers, bytes, avoiding: reflective objects, pp, and certain vocabs like peg, if you can. otherwise you'll have to deal with longer compilation times and (much) larger executables. a good deal of effective tuning technique is to use the array paradigm in factor, which is easily accomplished by virtual sequences.

a lot of working with the stack is not imagining the stack, tracking items in order. rather, it's reading concatenative code as relations e.g. "dup" to mean "retain original below" or "keep" to mean "retain original atop". you should read stack programs' code just as sequences of transforms littered with (ordered by frequency of occurrence) retentions, permutation, or removals, which we collectively call "shuffle words".

factor's fried quotations used to use local namespaces and dictionaries (like `[let` &al locals still do), but at some point the fried quotations became converted into proper programs. idk how it does it, but i know that the algorithm would basically be:

. to insert a datum into a quotation e.g. `[ x y _ z ]`, simply split at the `_`, then `curry` & `prepose`: `[ z ] curry [ x y ] prepose`.
. to insert a quotation inline (akin to scheme's `,@`), e.g. `[ x y _ z ]`, it's basically the same except that we use `compose` instead of `curry`: `[ z ] compose [ x y ] prepose`

back when factor used hashtables, i'd recommend to avoid fried quotations because they're difficult to follow in the walker; however, now that the quotation is fully built before being executed, you can just prettyprint it to see what it is before execution, but that's unnecessary since you already know what it is because you coded it as a fried quotation. thus fried quotations are now both easy to understand in code, and easy to follow in the walker. so freely use `B` in fried quotations to debug!

factor is not the most lightweight nor easiest to install (due to dependencise and long compilation times), but it's simple, cross-os (though iirc, only x86), has all the functionality that you'll ever need, and makes going from nothing to complete programs exceptionally fast.

.deducing stack effects & shuffle words example

we'd do `now [ swap time- duration>seconds 30 < ] curry filter`, but were we to not use `curry`, it'd be `now swap [ [ time- ... ] keepd ] filter nip`. i deduced this thus: assuming that we use `filter`,

. filter mandates that its quotation:
  .. take something atop the stack
  .. leave an output (assumed as boolean) atop the stack
  .. not change the stack height
. the timestamp returned by `now`:
  .. must be retained for upcoming iterations
    ... under the boolean output, as per (2a)
  .. and in such a way that `time-`'s argument are properly ordered
. to satisfy all of (3), we think, "which retain word do we use?", and answer the question by considering:
  .. which datum we want to retain
  .. where we want to retain it.
  .. b/c we want to do `<now> _ time-` and retain `<now>` under a value, `keepd` is the natural choice
. finally, we don't want to retain the timestamp, so we nip

.stack management tips

* the more frequently data is used, the nearer the top of the stack you should put it

"ahh! i'm stack shuffling too much!"
> are you using `keep`?

"ahh! there're `keep`'s everywhere!"
> use a `cleave` combinator such as `bi` or `tri`

remember: your code should clearly associate data with operations. `cleave` associates one datum with many ops. `napply` associates multiple data with one op. `spread` associates with each of many data an op. then there are variants, such as `2tri`, which associates two data with 3 binary ops.

stack management is always simple and organic:

. put together fragments of code as they occur to you. notice which data they consume. 
. produce those data and put them on the stack
. if those same data are consumed in other places, too, then keep the data by using [variants of] `dup` or `keep` depending on whether you want to keep the data under some others vs above others.
. if there are symmetries among data & functions expressible by a combinator, then so express

if some symmetries are only _almost_ obeyed, then you may express in terms of a combinator and a variant of `drop`.

.come from apl, not scheme

in other words, know how to use sequences in factor. factor is not a pure language, but purity does not help factor like purity helps applicative languages, because there's no namespace to maintain/track, mutation isn't troublesome.

.array solution
[source,factor]
----------------------------------------------------------------------------------------------------
: factor-prefix ( x y -- prefix rem-x rem-y )
  [ ]
  [ [ = ] { } 2map-as [ not ] find drop ] ! find is used equivalently to "filter first" here, or "where first" [apl]
  [ [ length ] bi@ min ]
  2tri or [ cut ] curry bi@ nip ;

"catamaran" "cats, the play" factor-prefix [ . ] tri@
"cat"
"amaran"
"s, the play"

"cats" "cats" factor-prefix [ . ] tri@
"cats"
""
""
----------------------------------------------------------------------------------------------------

.same, in k
[source,k]
-----------------------------------------------------
factorprefix:{l:(#y)&#x;i:*&~(l#x)=l#y;(i#x;i_x;i_y)}
factorprefix["catamaran";"cats,the play"]
("cat"
 "amaran"
 "s,the play")

factorprefix["cats";"cats"]
("cats"
 ""
 "")
-----------------------------------------------------

the k solution is less elegant in that i need to bind to local variables, but more elegant in that k's verbs are designed to be symmetric, so i don't need to account for the case of equal strings especially. also factor's `2map` trims the iteration to the shortest of its two input sequences, whereas in k, due to its use of shapes, we must manually trim both sequences before using `=`.

in scheme (or haskell, erlang, etc), we use linked lists, so we'd uncons, using a zipper-style iteration, which naturally leaves the prefix and the suffixes.

.scheme-style solution
[source,factor]
---------------------------------------------------------------------------------------------------------------------------
V{ } clone -rot pick '[ 2dup [ empty? ] either? [ f ] [ [ unclip-slice ] bi@ pick = [ swap _ push t ] [ nip f ] if ] if ] loop
[ >string ] tri@
---------------------------------------------------------------------------------------------------------------------------

this solution is surprisingly elegant; we don't need to track the index, and this zipper-like approach is very well suited to this task.

in c, we'd use a simple `for` loop with `break`—the most efficient and simplest solution. we can do that in factor, too, by creating a quotation via `bi-curry@` and `find-integer`—or rather, we would, but thankfully we have `sequences.generalizations`, so we don't have to:

.c-like version (`nfind`)
[source,factor]
------------------------------------------------------------------------------
USE: sequences.generalizations
2dup [ = not ] 2 nfind 2drop 2over [ length ] bi@ min or [ cut ] curry bi@ nip
------------------------------------------------------------------------------

.c-like version (`find`)
[source,factor]
----------------------------------------------------------------------------------------------
[ ] [ [ length ] bi@ min ]
[ [ nth ] bi-curry@ [ bi = not ] 2curry [ find-integer ] keepd or [ cut ] curry bi@ nip ] 2tri
----------------------------------------------------------------------------------------------

TIP: the most fundamental form of `find` is called `find-integer-from`. whereas `each-integer-from` corresponds to a `for` loop from i to n, `find-integer-from` corresponds to the same but enabling `break`. in other words, `find-integer-from` is like `until` with an index under the quotation except that now the quotation is curried to the quotation.

consider the task of, given two strings, return their common prefix and the remainder of each string.

.row-polymorphic iterators

that `map` is row-polymorphic practically makes it magical: it's no longer constrained to that the mapping function must be of only one element! instead, it's that the function must handle _at least_ the current iteration's element! this is described by the row-polymorphic type signature `{r|x}`, using purescript's notation. thus now map, fold, and each are basically the same, since `map` is just `each` with pushing into a vector or setting an array's ith element. thus `map` should really be called `collect`. this means that we can use `map` to trivially implement scan: `-10 { 6 2 9 } [ + dup ] map nip` returns `{ -4 -2 7 }`. so if you want to make your own looping combinator, it's trivially easy to do so: just use `push` and `each` (or `find` if you want short-circuiting). for example, let's push 5* the sum of the current & prior elements only when the distance between the current & prior elements is at least 7, and stop once their sum is at least 20:

[source,factor]
--------------------------------------------------------------------------------------------------------
{ 10 6 4 20 9 16 12 }       ! input seq
[ first ] [ rest-slice ] bi ! 1st elt is init elt, on the stack below seq to iterate over
! idiom: make new output vec, curry to each's quotation, and keep it on stack after `each` finishes
V{ } clone
[ '[ [ [ + [ 20 >= ] keep ] [ - abs 7 > ] 2bi [ 5 * _ push ] [ drop ] if ] keep swap ] find 2drop ] keep
--------------------------------------------------------------------------------------------------------

`find 2drop` is common when using `find` only for a short-circuiting fold. the stack's data order must be considered a bit, but it's simple here: it's reverse order; the `if` condition tops the stack, with the data to act on below it, and finally the boolean telling whether to stop the loop is below, but must be swapped with the current element which we kept so that it becomes the new loop state, stored properly under the boolean.

j's `F:` & `Z:` verbs can handle this, but whereas in factor you can have multiple data on the stack below, in j, like in haskell, you must have exactly one accumulator, so you'd have to join your multiple data into one tuple of them, and pattern-match them out inside the loop lambda.

also, note that in this particular case of the loop state being the prior element, we could better express this in terms of clumps, without a persistent loop state:

[source,factor]
--------------------------------------------------------------------------------------------------------
{ 10 6 4 20 9 16 12 } 2 <clumps> V{ } clone
[ '[ first2-unsafe [ + [ 20 >= ] keep ] [ - abs 7 > ] 2bi [ 5 * _ push ] [ drop ] if ] find 2drop ] keep
--------------------------------------------------------------------------------------------------------

the loop state is useful only if you want something to change incrementally as you encounter new elements in a loop.

.tricky loop example

this takes two sorted arrays; it maps over one, and for each element `x` in that array, it returns `y`, the greatest element less than `x` in the other array:

[source,factor]
---------------------------------------------------------------------------------------------------------------
0                                         ! initial index (index into array B)
{ 1.0 1.1 1.2 1.3 2.1 2.4 2.6 3.4 4 5 6 } ! array to map over
{ 0 1 3 5 7 }                             ! array B

! build-up a quotation

! two data used in each of the next two quotations
[ length        ]
[ [ nth ] curry ] bi

! two quotations to be composed together
[ [ < ] compose [ dupd ] prepose ! retain x on the stack for successive iterations of find-integer-from
  [ find-integer-from ] 2curry ]
[ [ [ or 1 [-] dup ] curry ] dip ! curry the length with `or`; if no element is found, use the last (rightmost)
  compose [ swapd ] compose ! x i y -> i x y
] 2bi compose

[ 2array ] compose ! ( x y -- z )
[ swap ] prepose   ! map puts x atop i. find-integer-from needs i above.
map
---------------------------------------------------------------------------------------------------------------

anyone used to reading applicative code would probably feel a bit in their gut just looking at this. however, this is concatenative code! whereas applicative code's topology can be approximately understood upon a glance—namely by its nesting & indentation—concatenative code should be read linearly (since concatenative code is necessarily linear), one word at a time. the comments for `bi` & `2bi` would not be in production code, but the rest would be. after all, this is an unusually complex traversal. the actual code as i have it in my `util` vocab:

[source,factor]
---------------------------------------------------------------------------------------------------------
! precondition: A & B are sorted number sequences
! scaffolds for iteration over A that matches each a in A with y, the
! greatest element in B less than a; then performs f(x,y).
! you'll want to nip after using the quotation to iterate
! example:
! { 1.0 1.1 1.2 1.3 2.1 2.4 2.6 3.4 4 5 6 }
! { 0 1 3 5 7 } [ 2array ] align-seqs-by-value map nip .
! { { 1.0  1 }
!   { 1.1  1 }
!   { 1.2  1 }
!   { 1.3  1 }
!   { 2.1  1 }
!   { 2.4  1 }
!   { 2.6  1 }
!   { 3.4  3 }
!   {   4  3 }
!   {   5  5 }
!   {   6  5 } }
: align-seqs-by-value ( A B f: ( x y -- z ) -- 0 A q: ( i x -- j z ) )
[ 0 -rot [ length ] [ [ nth ] curry ] bi
  [ [ <=> +lt+ = ] compose [ dupd ] prepose ! retain x on the stack for successive iterations of find-integer-from
    [ find-integer-from ] 2curry ]
  [ [ [ or 1 [-] dup ] curry ] dip ! curry len(B) to or; if find ret f, use B's last elt
    compose [ swapd ] compose
  ] 2bi compose
] dip compose [ swap ] prepose ; ! swap b/c map puts x atop i. find-integer-from needs i above.
---------------------------------------------------------------------------------------------------------

for comparison, here it is in applicative style (javascript-like):

--------------------------------------------------------------
f(A,B){
  i=0
  L=length(B)
  g(x){
    gtIdx=findIntegerFrom(i, L, (i) => {x < B[i]}) or L
    2array(x, B[max(0, gtIdx - 1)]) // [-] is - 0 max
  }
  map(A,g)
}
--------------------------------------------------------------

notice the nested scopes! i shadow `i`, use `L` & `i` in `g`, and `x` in the lambda passed to `findIntegerFrom`! this is why the factor code is so hairy. maybe i should have just written it in applicative style in factor by using locals. then again, the tricky part was just thinking about how best to code such a complex relation. also, this applicative translation might not be perfectly accurate; i have no way to test it. seems about right, at least.

another important note about this example: it demonstrates that, like such words as `length-operator`, it's common in factor to modify some items on the stack but leave the stack the same height and of mostly the same form; this style of programming is to take multiple arguments and return multiple, so that they are prepared for certain combinators such as `map`, but now the map traversal is modified. this is unheard of in non-stack languages. instead, what's common in applicative functional languages is a slew of combinators that take specific varieties of arguments (e.g. some take binary functions with or without some extra arguments that affect how the function executes, and there may be ternary variations or whatever other variations), so you have very many functions that are mostly the same, and hopefully one of them is just what you want; god forbid you need even the slightest variation, since then you'd have to define your own function! the stack is much more flexible since words/combinators hardly require the stack to be particularly constrained. building-up or modifying quotations is common practice in factor. very rarely do applicative languages have higher-order functions from functions to functions, and those that do exist are typically very much less flexible than corresponding factor versions.

btw, this array style solution is worse than an incremental-style one (and i strongly predict that generally array solutions are inferior to incremental ones). arrays require us to consider the whole array at once, which is needless coupling. more flexible, and easier, is to write a function that acts on a single timestamp. here's a better alternative to aligning sequences:

[source,factor]
---------------------------------------------------------------------------------------------------------------------------------------
! when the output quotation is called, finds the index, j, of xs's least item greater than a, then returns j, a, & ys[j].
! returns a quotation so that xs & ys are curried-in, and the quotation can be used in traversals over monotonically increasing inputs.
: aligner ( xs ys -- q: ( i a -- j a y ) ) ! pass 0 for i for 1st call, then let j be used for further calls
  [ [ 3dup swapd nth >
      [ over [ > ] curry [ swapd ] dip [ find-from drop ] keepd length or 1 - swap ]
      [ drop ]
      if over
    ] curry
  ] dip [ nth ] curry compose ;
---------------------------------------------------------------------------------------------------------------------------------------

whereas aligning sequences strictly [eval] produced two sequences, this can be used to produce such sequences, but there's no need. it could just as well be used to update values in-place, or be used in any general traversal, to be applied over any arbitrary subset of values (though it would be easiest if applied to inputs given in ascending order, so that we could leave i/j on the stack as-is through the traversal).

btw, this whole section came from me writing an efficient version of `find-transition` from `tzinfo.private` to act on a sequence of monotonically increasing unix timestamps. i wrote `aligner` as an improved solution, but i see how it is similar to, yet differs from the sequence aligning method; i'll later add to this section an expression of sequence alignment in terms of `aligner`.

.practical factorability and reuse of code

using many small words is easy in catlangs because we merely copy/cut & paste. many small words means more granular control over computation. many of these words are inline, so there's no cost to invoking them. consider the following code:

[source,factor]
-----------------------------------------------------------------
: timestamp>sec ( t -- ms ) timestamp>millis 1000 / ; inline

: dm>timestamp ( d m -- timestamp )
  [ julian-day-number>date <date> ] dip
  60 /mod [ >>hour ] dip >>minute ; inline

: c>t ( c -- timestamp ) [ d>> ] [ m>> ] bi dm>timestamp ; inline

: some-fn ( c -- x ) c>t timestamp>sec "time" 2array 2map ;
-----------------------------------------------------------------

i defined `c>t` in terms of `dm>timestamp`, and `some-fn` in terms of `timestamp>sec` & `c>t`. true, in factor i must put the stack signature whenever i define a word, but that's little refactoring. to define such granulatiry in an applicative language is cumbersome:

-----------------------------------------------------------------
fn timestamp>sec(t) int { timestamp>millis(t)/1000 }
#pragma inline timestamp>sec

fn dm>timestamp(d,m) timestamp {
  y,m,d=julian-day-number>date(d)
  <date>()
  60 /mod [ >>hour ] dip >>minute
}
#pragma inline dm>tmiestamp

fn c>t(c) timestamp { [ d>> ] [ m>> ] bi dm>timestamp }
#pragma inline c>t

fn some-fn(c) x { c>t timestamp>sec "time" 2array 2map }
-----------------------------------------------------------------

notice that in the applicative version we must explicitly code that `julian-day-number>date` returns 3 data, and we must give those data nonce names just to use them in our computation, and their natural names shadow others in the namespace. the ability to return multiple data also gives us finer granularity over computation; we may retain more aspects of a computation and use them if we like; or, as is here, we may return multiple parts, reducing (un)wrapping; for the code in this example, someone using an applicative language would likely prefer to package the year, month, and date together into a timestamp object just to avoid returning multiple data! then the user of that fn would do

---------
y=t.year
m=t.month
d=t.day
---------

which is some severe bloat, pollutes the namespace, and is a pain to refactor. by the way, the stack makes default arguments easy: e.g. `<date>` is defined as `0 0 0 0 0 0 gmt-offset <duration> <timestamp>`; the first 3 zeroes represent the timestamp's hours, minutes, seconds; the next 3 are the duration's year, month, and day. `gmt-offset-duration` pushes hour, minute, & second for `<duration>`, which, like `timestamp`, takes 6 args: ymdhms.

the stack is a very simple structure yet is very apt at describing programs (sequences of one-day data transforms). naming things is a very powerful tool, but is useful only relatively infrequently, and is crufty to use. namely, the one circumstance in which it is elegant and the stack is not, is nested, complex computations: when the computation refers to arbitrary elements of other structures at arbitrary nesting depth. another way to phrase this is "many irregularly overlapping contexts" where "context" means a continuation with a data and/or return stack, and/or namespace. the more irregular relations of data across structures, and the less symmetric your control flow, the better off you'd be to code applicatively i.e. the better off you'd be to refer to data by index, regardless of whether the index is a name or number), and being able to insert short-circuits at arbitrary places may be elegant.

most computation is simple, and regular, so it's nicest to use simple, symmetric models such as a stack (which is like functional programming but simpler & more elegant). however, not all computation is, in which case you'll want less-constrained coding models, such as c, which has most permissive control flow and data manipulation.

btw, what makes applicative code better is none of

. having infix binary operators
. inlining in-order function args

what makes it better is merely that we store variables individually without regard to other variables i.e. that we use locals instead of on-stack, b/c being on stack means needing to maintain their existence/position in the stack as the stack is modified, whereas if a is bound to a fixed address (again, usually indexed/referred to by a symbol (i.e. a variable name)) then it retains its state unless something explicitly modifies _it_ per se. the advantage is that it exists regardless of context, which expresses itself in two ways: 1. it exists for all scopes; 2. the affairs of other data are independent. what makes this advantageous is that the data are freely available as we last left them.

actually, this has nothing to do with being concatenative vs applicative. concatenativity does not imply referential transparency. thus we may have a purely concatenative program which has words that bind to variables which defines those variables as words which refer to the variables' values e.g.

[source,factor]
-----------------------------
4 "i" set
"i" inc
"i" get ! puts 5 on the stack
-----------------------------

is a factor program and is concatenative, and still has all the advantages of concatenative code; only now it is context-sensitive, where the context is the values of the bound variables. this is semantically equivalent to passing these variables as arguments to the function and retaining them on the stack (if using a stack) throughout the computation as necessary.

NOTE: though _locals vs stack_ is unrelated to _concatenative vs applicative_, the stack is a concatenative structure—it supports a `concat` operation—and so it's a natural data structure to use to encode concatenative programs. generally a designer may base an evaluation model on any data structure and exploit its algebraic properties. for example, prolog is based on predicates/sets, which support the product/intersect operation. thus (referentially transparent) prolog programs are commutative intersections of predicates.

it's all just a question of whether/how modifying one thing affects others, and whether you want it to (in that way).

.more "thinking stack" verbiage

nb. i use the term "cat" here to mean "concatenative language" or "concatenative program"

firstly, note that anyone coming to a cat/tacit lang from an applicative lang is going to try to reason about catlang programming in terms of applangs, which is failure. it's inevitable and to be overcome in time, but overcome one must in order to actually program cats well. anyway, to mentally assign a name or label to data on the stack, and think about how its stack position changes, is bad; a good catter does not much monitor the stack; instead, they think about what sequences of operations they want. at each step, they must know what the input is, and so there's some stack tracking, but it's local stack tracking; they ever care only about part of the stack. this reflects cats themselves: arbitrary subcats don't affect others! certainly there is still some considerable stack consideration, such as when using `tri*`, `over`, or `rot`. it's still not much, though; rarely is complex shuffling done, and rarely does one ever care about more than 3 items on the stack. you'll know that you've become true to habitually thinking in cat/stack-way when everything seems like currying to you; when you see `swap 10 + *` as "2 things on stack. add 10 to 1 then multiply them", and it should be immediately obvious that you could instead do `[ 10 + ] dip *`, or if you wanted to leave an item on the stack, then `over 10 + *` because you know `over` to be equivalent to "swap but leave one." thinking in factor _feels_ very easy once you unlearn other perceptions of programming. stack programming is very natural to humans! that it reads left to right, is simple, and pretty with little syntax, is quite obvious to anyone who hasn't thought of "what code should look like." link:https://codeberg.org/ngn/k/src/branch/master/j.c[whitney-style c code] is 18% parenthesis! isn't it amazing that nearly 1/5 of the tersest c is just needless syntax? that's before we even consider the bloat due to variable names, type declarations, or other what-have-you.

pay attention to these facts! they reduce the complexity of the programming & language model, which makes easier and more efficient: reasoning about how to code programs, refactoring, coding it in the first place, debugging. boilerplate is rare and can be easily avoided by defining a macro in a few words.

it's been suggested that the stack is not suitable for some coding; comomnly the quadratic formula is suggested. however, i defeat that argument here:

[source,factor]
------------------------------------------------
: solve-quadratic ( a b c -- x1 x2 )
  [ neg ] dip rot
  [ -4 * * over sq + sqrt [ + ] [ - ] 2bi ] keep
  2 * [ / ] curry bi@ ;
------------------------------------------------

i consider this code more elegant than applicative versions. coming-up with the code wasn't a struggle; in fact, it was quite simple & plain: just identify that there are some computations with `f(b)`, `g(b,c,a)`, then `a`, so naturally we want `b` on the bottom of the stack, `c` & `a` in the middle, and `a` on top.

of course, the solutions that you see on link:https://rosettacode.org/wiki/Roots_of_a_quadratic_function#Factor[rosetta code] are a bit more complex because they have special consideration for numerical precision.

=== primitives

`word` & `tuple` are the most general objects in factor. tuples are basically product types (ml-based langs e.g. haskell) or structs (c) or classes (oop langs). words are, as the docs say, "the fundamental unit of code"; a factor program is a sequence of words. the word which executes words, `(execute)`, is a primitve. macros (`MACRO:`) or syntax (`SYNTAX:`) are functions from words to words. words are most commonly functions defined by the `:` syntax, but may also be _symbols_—words which evaluate to themselves, just like symbols in lisp, except that the factor parser requires the programmer to explicitly declare words as symbols via the `SYMBOL:` syntax, rather than there being a literal symbol syntax—or classes, or maybe other things, too. words are not only functions in factor, though; it's just that they may be treated as functions, namely if their `def` attribute is set. run the following in the listener to observe the great amount of information stored in various words:

[source,factor]
----------------------------------------------------------------------
USE: generalizations
: print-word ( word -- )
  { [ name>>          ] [ vocabulary>> ] [ def>>          ]
    [ props>>         ] [ pic-def>>    ] [ pic-tail-def>> ]
    [ sub-primitive>> ]
  } cleave [ ... ] 7 napply ;

\ fixnum+ print-word ! a primitive
\ + print-word       ! a generic word
\ reach              ! a simple, ordinary word
\ word               ! a class (which implies that it's also a symbol)
----------------------------------------------------------------------

* if you look at the definiton for symbol, you see `PREDICATE: symbol < word [ def>> ] [ [ ] curry ] bi sequence= ;` which means that `symbol` is the subclass of `word` whose members have an empty definition.
* classes are defined as `PREDICATE: class < word "class" word-prop ;` i.e. they're a subclass of word with a property called `"class"`.

summary: _words_ are parsed things which may have definitions; if so, then they're functions with optional metadata; else they're symbol literals (with optional metadata). _classes_ are words that support oop. the classes' structures & relations—e.g. hierarchy, ad-hoc polymorphism (aka "interfaces" (java), "purely abstract classes" (c++), "type classes" (haskell)), class attributes—are implicitly handled by words such as `TUPLE:` & `GENERIC:`.

you can look through the factor codebase—especially in `extras/`—to find example factor programs.

=== oop [stub]

see factor docs for:

* `Class linearization`
* `Class operations`
* `Generic words and methods`

== generally useful vocabs

* `math`, `math.functions` includes common functions like `^` (exponentiation), logarithms, and trigonometric functions.
* `math.intervals` and `math.vectors` are useful, too
  ** try to use `math.vectors.simd` wherever possible! in fact, it's the best way to use bit vectors, too, since it actually is proper vector operations on numbers, for large bases (e.g. `uchar-16` is vector ops for 16-length vectors each of whose values are 0..255). this is overkill for mere bitwise operations since there's no simd type for a bit vector, so you'd just use `math.bitwise` & `math.bits` (which are surprisingly accomodating). i have no idea why factor has each of bit arrays & vectors as their own structures, let alone whose elements are _booleans_ instead of numbers; don't use them; i see no advantage to them, and converting between boolean sequences and integers is a hassle!
* rational numbers (abbreviated as "ratios") are always used in factor for division unless floats are explicitly specified. their syntax as `a/b` or `a+b/c` is also supported e.g. `1+1/2 5 *` gives `7+1/2`.

from the factor faq's:

* factor ships with a deploy tool which creates mac os x .app packages, or as windows and unix executables bundled with an image and some .dlls. to put a factor program into a package so it can be run easily, deploying a vocabulary into an application which will run the vocabulary's main word: `USE: ui.tools.deploy "vocab-name" deploy-tool`.
* if you need two different vocabs that define synonyms but want to use only one vocab's word, then put it after the other in `USING:`. otherwise use qualified imports: `QUALIFIED: v` to load vocab `v` s.t. its words are accessible as `v:word`.
* ffi: 1. ensure that your shared object is compiled for the same architecture that factor was (most commonly 32- vs 64-bit). also, for alien code, `add-library` first. alien supports C's name mangling but not C++'s.

== introduction to stack machines

like lisp, factor's general/free model hardly suggests any idiomatic style. factor should be thought of as lisp except with a different model for relating data i.e. functions' inputs & outputs:

* a neat property of stack machines is that the stack relates all of its elements, whereas functions' arguments are distinct. we see this in functional combinators vs stack effect combinators such as `2tri`, which does not associate each of 3 functions with each of two inputs—the sequence [(f,x,y), (g,x,y), (h,x,y)]—then evaluate each triple of that sequence; instead, it performs stack effects `f`, `g`, & `h` in a given order, which means that the effect of the earlier-executed ones can affect the inputs of the latter-executed ones.
* whenever a computation is to be performed once but its output passed to multiple functions, lisp requires a binding clause. in factor we'd use `dup` or `keep`, or a combinator that uses either, such as `bi`.

link:https://toml.io/en/v1.0.0#array-of-tables[toml's array of tables syntax] is like stack langs whereas JSON is like applicative languages. in stack langs we accumulate programs imperatively then eventually execute them. applicative languages specify large program segments as _one_ complex (and deeply nested!) structure of data relations. stack programs are not nested. stack is a 1-dimensional data structure, whereas (abstract syntax) trees are two dimensional and irregular. granted, asts can be built of stack programs, too. the tradeoff is that one must maintain awareness of the stack's state at a given point in time but the syntax is nice (which makes refactoring nice), whereas applangs display the whole program all at once, which...gives the whole picture at once, but it's still complex! imperatively building programs allows us to go one step at a time. *showing the whole program at once does not make it easy to trace through.* also the mere fact of syntax being more complex is a burden. it's one usually taken for granted, but there's no reason for that.

== compilation & deployment

embeds the factor vm into the executable; no external runtime is needed to run a compiled factor program. to compile: in the listener, evaluate `"your-vocab-here" deploy-tool`. your vocabulary does not need to be already loaded. when compilation finishes, a file browser or terminal is opened to the directory where the compiled binary was left, namely in the directory in which factor is installed/cloned.

=== some words' required deploy options that i know of

failing to use sufficient run levels or deployment options will cause your program to fep-out, declaring that it's crashed and that such crash is a bug.

[options="header"]
|==============================================================================================================================================================================================================
| vocab / word(s)                          | run level / options
| `prettyprint` vocab (including `printf`) | compilation level 3+
| `mirrors` and `all-slots`                | level 5+. however, `tuple-slots` does not require anything above level 1, so if you hardcode tuple slot names then you can effectively use mirrors at run level 1.
| `peg`                                    | level 6 plus retain all word properties and definitions. it used to only require lvl 5.
|==============================================================================================================================================================================================================

== usability

.dynamic evaluation

* if you define word A in terms of word B then redefine A, then B's definition is implicitly changed.
* `with-datastack` is like scheme's `apply`, except that it returns an array

.caveats and common errors

know when to use `clone` after syntax that represents a mutable structure (e.g. vectors, string buffers, and hash tables) consider the following code:

[source,factor]
----------------------------------------------------------------
[[ >12 0 6 ; 12 4 5 ; 2 5 3
;2 19 8 ; 4 1 0  ; 4 0 1]]
V{ } clone tuck SBUF" " -rot
'[ B dup digit? [ suffix! ] [ [ _ push ] bi@ SBUF" " ] if ] each
----------------------------------------------------------------

i forgot to put `clone` after the `SBUF" "` inside the `each`. i'd meant to push the string buffer and current character to the vector, replacing the prior string buffer with a new one. however, because i didn't put `clone`, the same string buffer is pushed! what happens is that syntax `SBUF" "` creates a new string buffer object at parse time; then that object is curried into the quotation; then the quotation is executed on each iteration of `each`, though we only ever actually use the string buffer after the first one (the one before `-rot`) is pushed into the vector, since it's only then, in that `if` branch, that the `SBUF" "` within `each` is pushed to the stack. when i use `SBUF" " clone`, then still the same string buffer object is used on each iteration, but it's forever empty because i clone it before any mutations are done, and of course all mutations are done to the clone rather than the original. the first `SBUF" "` syntax literal is unrelated to the second. each literal creates a new, unique object; just be aware whether that object is part of a quotation, since that's the same as the object being being curried into the quotation, which means that it's the same object for all invocations of the quotation.

thus `V{ } [ ... ] curry` is fine—no need for `clone`.

* when using packed tuple arrays, you get "matching failed." you should use `{ } map-as` instead of `map`
* setting a dynamic variable has no effect, or dynamic variable is `f` even though you just set it. check whether you're executing it within a namespace combinator e.g. `with-file-writer` is ultimately defined in terms of `with-variables`, so any setting within its quotation will not affect the namespace outside the quotation! for example, consider `SYMBOL: myVar "~/test.txt" ascii [ 0 myVar set [ myVar get dup even? [ myVar inc ] when ] with-my-db myVar get 5 + . ] with-file-writer` where `with-my-db` is defined as described in `db.tuples` document, _Tuple database tutorial_. execution throws an error: "No suitable arithmetic method. left: f; right: 5; generic: +" `myVar` was set only within the context of the inner namespace—the one of `with-my-db`. within the namespace of `with-file-writer`, it was still unset. more precisely, after ``with-my-db``'s quotation finished, `myVar` was set back to the value that it had had before that quotation was evaluated. indeed, even when we move `0 myVar set` to the outer quotation, "5" is written to the file, not "6", because the increment occurred only within the inner quotation! `myVar` is reset to 0 after that quotation finishes!
* confounding `map` errors: `map` maps into the same type as the thing being mapped over. if you want to map into an array then use `{ } map-as`. this is especially common if you're trying to map over a string.
* for words like `set-at` which consume a structure and don't leave it on the stack, use `keep`: `H{ } [ "val" "k" rot set-at ] keep` leaves H{ { "k" "val" } } on the stack
  ** use `over adjoin`
  ** use `[ _initAssoc set-at ]` or `[ set-at ] curry` or `[ set-at ] keep`
  ** use `over [ change-at ] dip` or `_q curry [ change-at ] pick [ 3curry call ] dip`
* `inline` can make reading tracebacks more difficult e.g. with ``: a ( x -- y ) 0 / ; inline : b ( x -- y ) a ;``, evaluating `b` with any input will throw an error, and the traceback will go as deep as `b`.
* `read-contents` hangs
  ** you meant `utf8 file-contents`
* assocs: you do something like `f "key" value { } 2sequence assoc-union` and get a weird result. you meant to do `f "key" value { } 2sequence { } 1sequence assoc-union` or `f value "key" associate assoc-union`
* `call-n` doesn't work like you'd expect. did you mean `napply`?
* the stack checker sometimes fails for complex row-polymorphic functions. consider the following: `[ second length 3 > ] [ first2 dupd [ myfn ] [ 0 > ] bi 3array ] filter-map` was a mismatch, saying that the filter clause was `( x -- x )` but that the map was `( x x -- x x x )`. that's obviously wrong. the problem is that `myfn` was defined `inline` and had `map` in its definition; thus when the compiler inlined it, the composite effect was beyond its reasoning, despite `myfn` having successfully compiled with stack effect `( x -- x )`.
  ** as it turns-out, the problem was `map-filter` being too polymorphic. i learned this by changing both the filter and map clauses to `[ ]` yet i still got the error! i suppose that the lesson here is to start from the outside then specify inward as needed. the specific thing that i did is take the erroring `<quot> <seq> <quot> rot map-filter` and change it to `<quot> <seq> <quot> rot \ map-filter execute( s q: ( a -- b ) f: ( b -- ? ) -- s' )`
* no output expected to stdout: use `flush`.
* forgetting `get` after a symbol; remember that symbols are symbol literals and are not themselves dynamic variables, though they can be used as such
* mixing `set-global` & `set` or `get-global` & `get`
* using `::` but forgetting to put leading args
* using a quotation in `::` without `compose` or `call` (thus giving a larger return stack than expected)
* "cannot create slice from 1 to 0": slice on empty sequence, commonly by `unclip-slice`

inference branching:

you may be baffled to find that `[ sum ] sort-by` works but `[ 0 [ + ] reduce ] sort-by` gives an inference branch error, citing that the quotation's effect is now `( x x x -- x x x )` instead of `( x -- x )`. the trouble is caused by the ellipses in `reduce`'s declared stack effect. similar issues arise with using `map` instead of `reduce`. so just use `call(`, i suppose.

.run-time computed values

first check that you did `prepose`, not `prepend`. `prepend` is for sequences in general; `prepose` is considered specially for quotations by the compiler. `prepose` works where `prepend` gives the "cannot apply such-and-such to run time computed values" error.

the help document "Stack effect checking escape hatches". it mentions `call(` & `execute(` for quotations and words with statically-known stack effects; and `with-datastack` for general manipulation. there are some other strategies:

* the `literals` vocab is parse-time computation, like macros, except that macros result in callables whereas literals result in values.
* macros are very convenient, too, for specifying dynamically-computed values that are known before runtime.

.the call/curry trick
[source,factor]
-------------------------------------------------------------------------------------
[ [ ... ] compose ] when ! conditional prepose makes quotation run-time computed, but
[ call( x -- x ) ] curry ! call( makes its effect statically known
-------------------------------------------------------------------------------------

.macros

the following code failed b/c `ndip` (and probably `npick`, too) can't take a run-time computed value:

[source,factor]
----
: reduce-collect ( ..a seq q: ( ..a e -- ..a ?collectval ) -- ..a collection )
  dup infer in>> length 1 +
  [ [ V{ } clone ] swap ndip ] ! accumulation vector
  [ 1 + npick ] bi [ push ] curry [ when* ] curry compose each ; inline
----

so what to do? well, fortunately i expect the quotation to always be specified inline, which means that its effect can be known at parse time, before runtime. macros allow us to dynamically compute values which are, at runtime, literals, thus solving the runtime-computed value problem:

[source,factor]
----
<PRIVATE
: (reduce-collect) ( ..a seq q: ( ..a e -- ..a ?collectval ) ndip -- ..a collection )
  [ [ V{ } clone ] swap ndip ] ! accumulation vector
  [ 1 + npick ] bi [ push ] curry [ when* ] curry compose each ; inline
PRIVATE>

! i don't need to specify q's stack effect here. i do so for documentation's sake only,
! so that the user can know what kind of quotation to pass.
! same for the ..a's outside of q's effect.
MACRO: reduce-collect ( ..a seq q: ( ..a e -- ..a ?collectval ) -- ..a collection )
  dup infer in>> length 1 + [ (reduce-collect) ] 3curry ;
----

actually, a little later, when i passed a run-time-computed quotation to `reduce-collect`, i found that `npick` is then a runtime-computed value, too! so i had to amend the code:

[source,factor]
----
: (reduce-collect) ( ..a seq q: ( ..a e -- ..a ?collectval ) ndip -- ..a collection )
  [ [ V{ } clone ] swap ndip ] ! accumulation vector
  [ npick ] bi* [ push ] curry [ when* ] curry compose each ; inline

MACRO: reduce-collect ( ..a seq q: ( ..a e -- ..a ?collectval ) -- ..a collection )
  dup infer in>> length 1 + dup 1 + [ (reduce-collect) ] 4 ncurry ;
----

TODO: i found that `SOMESET get-global >hash-set [ in? ] curry map-filter` gave the "cannot apply 'call' to a run-time computed value" error but removing `>hash-set` solved that error.

=== handling conditionality

* primarily use the primitvies `if`, `if*` (akin to `maybe` with `<|>` in haskell), `and`, & `or`. i promise that if you use these alone, you'll go far.
* some words should be used just for their brevity e.g. `if-empty`, and `when` (though only 2 characters shorter than its definition, `[ ] if`). "unless" is longer than its definition, so...unless you think it's more readable, i see no use for it.
  ** `when*` is simple & useful, too; it's just `when` but its quotation acts on the predicate also. 
* forget other conditionality combinators such as `or?`, `unless*`, `smart-if*`, `?if`, etc; you don't need them and they're difficult to understand.

in general, avoid combinators except `keep`. use combinators only for their brevity; never try to _reason_ in terms of combinators. by their very definitons, they're specific templates for specific, though often-common, circumstances. they aren't primitives, so they can't accomodate all circumstances.

* whereas `if*` applies a computation to a non-false or uses an alternative value, `keep and` is like guards in haskell: if tests the value; if it passes the test, then it remains, else it becomes false.
* `myDefaultValue or` is a common idiom
* remember that "if p then a else b" is equivalent to `p and a or b". 

== continuations

a _continuation_ is a point of execution. for example, in `3 4 5`, the continuation begins before 3. after 3 is evaluated, then the continuation is at 4. the continuation is the cursor in a program (a sequence of words); it's a position to execute from. see <<_factor/j_bilateral_translation_table>> for a simple example.

=== other considerations

* the documentation is usually _astounding_, except that it _never_ features examples. some vocabs have only the technical, auto-generated docs.
  ** includes word definitions as source code
* the listener (repl) is super-capable and integrated well with the docs
* there are _many_ libs builtin (see factor handbook > libraries > vocabular index), and *they're all documented offline in the docs*
* the docs are updated realtime as vocabs are loaded
* ffi w/lua
* has python bindings

== environment

* `USE: <lib>` imports one lib. `USING: <lib> ... ;` imports many.
  ** *put space between last lib and `;`*
* `FROM: vocab => word ... ;` disambiguates imported words. it overrides `USE:`/`USING:`, and can be used in lieu of those
* see `QUALIFIED:`, `FROM:`, `EXCLUDE:`, AND `RENAME:`, too.
* `<PRIVATE code ... PRIVATE>` exports `code ...` with the suffix `.private`
* module A may use module B even if B has errors, as long as A doesn't use any of B's words in which the errors exist
  ** or maybe not? perhaps _sometimes_....
* `IN:` defines a module. *required when writing any module*
* you must import `kernel` when running scripts. yeah, even `drop` must be imported.
* _quotation's stack effect does not match call site_ is an inconsiderable runtime error displayed when a script finishes with a non-empty stack. even `MAIN:` is hard-coded to check against `( -- )`. either put `clear` at the end of your script or make your script have stack effect `( -- )`. this is probably the most idiotic thing i've seen factor do yet.
* `save` saves the entire program state to a file. this is useful for scripts, since they're usually re-evaluated on each run. of course, for programs that do not need re-evaluation, it's best to use the ui deployment tool (`deploy-tool`) to make native, speedy executables.
* command line args: `USE: command-line command-line get-global`. *arg0 (program name) is not included!*
  ** it's a bit easier to get parameters from the environment than from the command line, as long as you know that it won't badly affect any subprocesses. this is convenient for storing default parameters, too. where `SYMBOLS` is a sequence of symbols that you want to set, the code is: `USE: env SYMBOLS [ dup name>> ] env [ at [ swap set ] [ drop ] if* ] curry compose each`. it can be easily modified to set in an assoc rather than setting dynamic variables.
* envars: `USE: env`; then singleton `env` is an assoc

see factor handbook > the language > vocabulary loader > vocabulary roots. you can get there by searching for `vocab-roots`. `"resource-path" get` gives the factor install directory.

vocabularies have metadata. this is encoded by directories: each vocabulary has its own directory e.g. `foo`, and inside it contains at least `foo.factor`, among any special metadata files (e.g. docs, author) or other files. any of the 3 methods in _working with code outside of the factor source tree_ are good for making directories available for use with `USE:` &c. otherwise you can use `add-vocab-root` *with an absolute path* (leading homedir tilde is supported.) *this are supported only in the listener.* in a source file, `USING:` is processed before the rest of the source file regardless of the order of words. this means that you can't set `FACTOR_ROOTS` in `env`, either. so `FACTOR_ROOTS` is useless for scripts, unless you're fine with wrapping every executable factor script in a single-line shell script that sets `FACTOR_ROOTS` before running the script. using `add-vocab-root` in `~/.factor-rc` is the best solution.

NEXT: try `require` after `add-vocab-root`, just to see how it works

.example

suppose i'm keeping a `util` module at `~/programming/util/util.factor`, and i want to use it in the listener.

[source,factor]
----
"~/programming" add-vocab-root
USE: util
----

`util` here refers to the directory; that's why it's `util` and not `programming.util`. however, even if i name the module as `IN: programming.util`, i still can only `USE: util`, not `USE: programming.util`. that's unexpected. anyway, declaring names without periods is simpler anyway. still, TODO: explore how module (and corresponding directory) hierarchies correspond to `USE:` statements.

.no transient imports of generic words

because generic words are potentially many (and can often collide) the module system requires that you, at least in the listener, `USE:` providing vocabs despite having already `USE:`'d a module which itself `USE:`'d that same module. e.g. if my `util` module uses `io` for `stream-contents` (which is not generic but is defined in terms of `stream-contents*` which _is_ generic), then if you `USE: util` in the listener, you'll be prompted to `USE: io` so that `stream-contents` can be resolved. this affects only generic words. this is a price of dynamicism.

== invoking factor in the terminal

* if envar `DISPLAY` is not set then factor will run in a text repl
* there's no man nor info page, and `factor -h` sets the global var `h` to `t`, which is definitely not what we'd expect. to learn about invoking the factor interpreter, see "command line arguments" in the docs.
* see "scripting cookbook" in the docs for more info
* when you run factor, you'll probably want to put in `~/.local/bin` a script that `cd`'s to the factor install location then runs `./factor -i=factor.image "$@"`. because there's a gnu coreutil called `factor` (which factors prime numbers) ensure that `$HOME/.local/bin` is one of the foremost entries in your `PATH`.
* programs don't need `MAIN:`; the program is executed like most scripting langs

== exploring code & learning factor

nb. i use _cuc_ to mean "code under cursor in input field."

* `#concatenative` on irc.libera.chat (or irc.freenode.net? i'm seeing more ppl on libera)
* start with the factor repl's `help` menu item
  ** see _developer tools_
  ** see _all tips of the day_ (factor handbook > developer tools > help system > tips of the day)
* read the factor source code
* ^i: see the stack effect of cuc
* ^w: step through cuc
* ^t: time execution of cuc 
* `apropos` e.g. `"group" apropos`. equivalent to searching in the factor handbook [help] search box, except that the elements are put inline in the listener >:O that is rad!
* familiarize yourself with word naming conventions (handbook > the language > conventions § word naming conventions)
* `:error` gives most recent error. `:c` to see its callstack

== semantics

* see `DEFER:` for mutual recursion
* scope is not often a consideration. however, `set` is scoped only within a source file (b/c files are parsed with `with-scope`)
* strings are sequences of unicode code points, not of bytes. factor supports encodings well. writing bytes is merely a matter of using the correct encoding (namely the `binary` encoding)
* pushing quotations does not use memory
* `f` is the false value; all others are truthy
  ** `t` is the canonical truthy value
* `{ 1 2 3 } dup [ [ 1 + ] map! ] dip . .` prints `{ 2 3 4 } { 2 3 4 }`. therefore `dup` duplicates, at least for non-primitives, a pointer, and arrays are mutable...? this seems to suggest so, but `{ } 3 suffix!` confoundingly fails with _sequence index out of bounds_. this example fails when i use `3 [0,b]` instead because ranges are immutable.

.concurrency & parallelism

see vocab `threads`, vocabs tagged with `concurrency`. parallelism words are in `concurrency.combinators`.

== special builtins

these are contrasted with non-special builtins; these builtins are not useful in writing programs, but are used to examine programs or otherwise concern the vm or language itself.

* `call`: lisp's `eval`. runs a quotation, curried fn, or fry expression.
* `\ f`: pushes `f` onto the stack. `f` is then callable via `execute`
  ** `execute` cannot be used with dynamically bound variables; in that case you must use `execute(`

== the repl (the "listener") and the (documentation) browser

* browser keybinds: //note: mac uses use command key instead of alt
  ** alt-f: focus search bar
  ** ctrl-k: open "jump to" dialog
* *just because a program runs in the listener does not mean that it is correct*. e.g. `f [ 1 ] unless` runs but trying to get its stack effect produces a stack effect mismatch error! replacing it by `unless*` runs the same as `unless` but has a correct stack effect.
* set font: e.g. `"monospace" 20 set-listener-font`. you can `save` the image or put in `~/.factor-rc`
  ** btw the browser font size is *not* adjusted by using ctrl-- & ctrl-+, despite what's been said in the mailing list
* press `shift+return` to start a new line in an expression; press `return` to evaluate.
* when the cursor is left in a word for 1s, its stack effect is displayed in the status bar
* the `refresh-all` word reloads all loaded source files. unlike clojure/cider, reloading the file does not merely execute statements; suppose that a file defines a word; then that file is loaded, modified to have the word definition removed, then reloaded; the word is no longer defined in the listener.
  ** TODO: determine when/how/why `refresh-all` fails. never trust it too much.
* supports tab completion
* supports ^p & ^n but not up & down arrows
* runs as a gui rather than cli program
* is a client that connects to a repl server
* tracks the stack for you, which makes easy both working with state and debugging

== stack evaluation model

NOTE: the _retain stack_ stores values to push back later. it's used by words like `dip` (or `keep`, which is defined in terms of `dip`). see it in action in the walker (`^w` instead of `return` in the listener)

there is no function _composition_. there are only combinators (higher order functions) and application (β-reduction.) combinators are obvious because they always use qutations. unlike functional languages, words are always applied unless quoted (i.e. in a quotation); unquoted words are always applied. this differs from scheme, where `f` is different from `(f)` and `f` may be passed as an argument. factor is different from haskell, where `f x` evaluates to a result but `f` may still be passed as an argument to a higher-order function. in factor `f` is always applied to the stack below it. furthermore there is no distinguishment between data and functions; like haskell, words are all the same and each has variable natural number arity. `+ = 1 -1 ?` uses neither higher order functions nor composition _per se_; it is equivalent to composition, though composition exists only in a functional model and has no meaning in a stack model, since there composition is equivalent to application which are/is always implicit. binary `+` is applied, then binary `=` is applied. notice that i did not say "applied to `+`'s result." there are no function outputs in the stack model! the only input and output is the stack. any word may affect the stack in any way. here `+` is applied to the top two stack elements, then `=` is applied to the top two stack elements. therefore the stack effect of `+ =` is `( x x x -- x)`; `1 2 3 + =` is `1 == 2 + 3` in common pseudocode, and `+ = 1 -1 ?` is `λx y z. if x == y + z then 1 else -1`.

* `[ + = 1 0 ? ]` has stack effect `( -- x)` i.e. it's just a datum; but `[ + = 1 0 ? ] curry` has stack effect `( x -- x)`.
* non-higher order functions cannot be variadic, though higher order functions can be; their arity is a function of their argument function(s)'.

NOTE: fns are curried. e.g. `{ { 0 1 } } at` is illegal if the stack is empty; however, `: X ( x -- x ) { { 0 1 } } at ;` is fine b/c it defines but not evaluates `X`. functions may be defined in terms of other [curried] functions, which in turn are curried. you can tell that a function is curried by using an unquoted function that would usually cause stack underflow if applied to an empty stack.

== syntax

the only true syntax of the language itself, rather than a syntax implemented in factor itself, is that words are whitespace-delimited. defining words is a user-definable syntax, as are definition suffixes like `flushable`; consider the definition `: pp ( a -- ) . ; flushable`. here we're pushing each word to the stack. `:`, `(`, `--`, `)`, `;` are all just words. after `;` is pushed & evaluated, a definition is left atop the stack. that definition is an argument to `flushable`. one beautiful benefit of such uniform design is that the documentation for _all_ parts of the factor language is uniform and equally accessible by simply clicking on the word in the help docs.

furthermore factor beats lisp(s except picolisp and possibly some other uncommon, simple lisps) at its own game: factor actually does not distinguish between code & data; all language objects are _words_, which are just strings associated with properties. the only truly core parts of the language are hashtables, tuples, and other primitive data structures. this means that the language is not at its core a language, but instead a simple system of data manipulations i.e. creating & re/moving data and elementary arithmetic; the only other unique aspect of the language that makes it factor is the implicit & simple fact of how the stack is evaluated, viz β-reduction, and its static stack effect checking.

NOTE: primitive words are marked by featuring the `PRIMITIVE:` word in their definitions e.g. `datastack-for` in `kernel.private` vocab.

the _continuation implementation details_ page is very refreshingly overtly simple: "a continuation is simply a tuple holding the contents of the five stacks: [... each of which] can be read and written." no black box. no trepidation about internal complexity, and certainly no external complexity. maybe i've been scarred by racket's docs on continuations, but i know that all languages besides factor that i've encountered have even attempted to be so clean.

* bitstring literals are enterable by `B{`, the byte array literal syntax. you can use `B{` with `write` e.g. `path binary [ B{ 96 0xa 65 } write ] with-file-writer`
* `0x` syntax is directly supported by factor. no need for even number of hex digits, btw.
* `0/0.` is (positive) floating-point NaN. useful when you want all ordinal comparisons to fail. it is syntax recognized by the parser, not a word. see "Float syntax" in the docs.

== oop / generics / ad-hoc polymorphism

if you aren't using generic methods or other oopy things, then prefer hash tables over collections of tuples because 1. they support the whole `assocs` vocabulary, and 2. they don't require special syntax; keys can be dynamically generated easily, and can be any value.

TODO: discuss _protocols_ e.g. `assoc`

probably the easiest & most flexible oop ever:

[source,factor]
----
TUPLE: circle r ;
TUPLE: rect l w ;
GENERIC: area ( shape -- area )
M: circle area r>> dup * pi * ;
M: rect area [ l>> ] [ w>> ] bi * ;
----

NOTE: `>>foo` writes, `foo>>` reads. i guess that words [functions] are used because, if true, as class hierarchies are built, mere accesses become arbitrarily or greatly augmented. such degree of augmentation seems unlikely, though. i would expect, especially in a language like factor that touts its dynamicism, that hash keys would be preferred over accessor & setter words, as it's done in clojure. it seems that factor is perhaps not so flexible or dynamic as picolisp. TODO: how are tuples advantageous over mere hash maps? actually, they cannot be, since maps are the plainest general structure.

these are called _tuple_ classes. `r`, `l`, & `w` are called _instance variables_, so named for the interpretation of these named tuples as _classes_ and a constructed tuple (rather than its type/spec/shape) being seen as an _instance_ [object] of the tuple class. a _method call_ is a generic function that applies to a tuple e.g. `r>>` or `area`, both of which apply to any object that supports them (viz any tuple instantiated of a class having an `r` instance variable and a class that supports `area` respectively, where support is determined dynamically.

ways to instance a tuple: `boa`, `new`, `T{`, or by using the `constructors vocab.

i know not of classes other than tuples. tuples are considered as sets of attributes.

_derived classes_:

* _predicate classes_ are subclasses satisfying a predicate.
  ** is a subclass not merely a union? e.g. `TUPLE: a a b c ; subclass b a d ;` sees `b` as a's attributes ∪ {d}, yeah?
* _union & intersection classes_ are the union or intersection of classes.
  ** _mixins_ are a variety of union class. i have no idea what they add to union classes.

* _primitive_ classes represent data primitives and cannot be subclassed
* what are
  ** multiple dispatch (planned inclusion in factor, but currently implemented by a library)
  ** predicate classes

three functions from class to class:

* derivation
* union (n-ary)
* intersection (n-ary)

three types of classes:

* primitive
* tuple
* derived
* predicate (subclass B of A where A consists of instances satisfying a predicate)

primitive & tuple classes use >> & << (but not derived ones?)

== common words

.`sequence` vocab

* `nth`: elem at index or error. `nths` is like mapping curried `nth`
* `set-nth`. mutative, so whereas `CHAR: c 1 "-s" set-nth` leaves the stack empty, `"-s" CHAR: c 1 pick set-nth` leaves "-c" atop
  ** `change-nth` may be preferable. like `set-nth`, it's mutative, so you need some odd `dup`'s e.g. `{ "CAT" } dup 0 swap [ dup CHAR: c 1 rot set-nth ] change-nth` leaves `{ "CcT" }` on the stack.
    *** `swap over` ( a b -- b a b ) may be useful here
* `?nth`: elem at index or `f`
* `prefix`, `suffix`: adjoin at head or tail
  ** `prefix?` & `suffix?` are not defined; instead use `subseq-start 0 =` for `prefix?` and `[ subseq-start ] [ [ length ] bi@ swap - = ] 2bi` for `suffix?`
    *** regarding `subseq-start` &al, the factor docs use _subsequence_ to mean _substring_
* `insert-nth`: insert at provided index, moving latter elements rightward by one index
* `prepend`, `append`: concatenate 2 topmost sequences
* `concat`: concatenate elements of a sequence of sequences
* `join`: intercalate then concat

there's no complement of n-array; however, `2array` &c has complements `first2` &c. `nths` pushes 1 sequence, not n elements, to the stack.

example: find 1st element matching some predicates: `[ preds 1&& ] find nip` e.g. `{ "kak" "file" 36 41 } [ { [ number? ] [ even? ] } 1&& ] find nip` returns `36`.

=== pattern matching

there's a primitive built-in pattern matcher, but you're better-off rolling something better, or using PEG.

[source,factor]
----
USE: match
MATCH-VARS: ?x ?y ;
: my-match ( seq -- )
{ { [ _ "2" ?y ] [ 14 number>string write ?y print ] }     ! case 1
  { [ ?x _  ?y ] [ ?x 7 * number>string write ?y print ] } ! case 2
  { [ _ ] [ "<no match>" print ] } }                       ! else
match-cond ;
{ "1" "2" " is the number" } my-match ! writes 14 is the number
{  6  "6" " is a number"   } my-match ! writes 42 is a number
----

case 1 is more specific than case 2; were case 2 earlier, it would match even if case 1 were a better match.

== stack tech

.tips

* if `seq q map` is used as per usual, then `seq q each` pushes the results of the map to the stack rather than collecting them into a seq. sadly, this is a hack; it works only in the listener, which does not stack check thoroughly; `each` requires its quotation to have effect `( ... x -- ... )`. therefore we must use `with-datastack` e.g. to perform a 4-ary fn `f: ( a b c d -- x )` on data from an assoc: `[ at ] curry { "a" "b" "c" "d" } swap map [ f ] with-datastack first`. you can instead use `firstn` in `sequences.generalizations`.

=== good stack words

* `preserving` (of the very useful `combinators.smart` vocab): when running a word, don't consume its args from the stack e.g. `1 2 [ + ] preserving` leaves `1 2 3` atop the stack.
* `?if` is a seemingly particular one: it's `a -> (a -> Maybe b) -> (b -> c) -> (a -> c) -> c`. it's the same functionality as haskell's `either`.
* `[ x ] 2dip` is clearer than `x -rot`. you should rarely use `[-]rot`; there's usually a better way to structure your code!

.impure `cond`

`cond` performs stack effects in order until the top is truthy. prior conditional predicate quotations affect later ones. this example demonstrates it, as does the following one:

[source,factor]
----
{ { [ dup empty? ]              [ drop 1000 ] }
  { [ dup first 6 * dup 50 <= ] [ ] }
  { [ drop t ]                  [ drop "none" ] }
} cond
----

[options="header"]
|=============================
| argument   | resultant stack
| `{ }`      | 1000
| `{ 5 3 }`  | 25
| `{ 15 3 }` | "none"
|=============================

note its ``dup``s & ``drop``s. the 1st condition must `dup` so that, if not empty, the sequence will remain on the stack for the 2nd condition to test, and so on. consequently, each branch replaces the sequence by some other value. factoring-out the ``dup``s to before the `cond` assoc is incorrect; that'd be the same as moving the first `dup` and removing the second. `dup` must be performed before each of `empty?` and `first`; a sequence must be atop the stack before each of those predicates is performed, and each predicate must ensure that it keeps [that] sequence atop the stack for the next predicate to evaluate, unless the assoc is designed to mutate the stack as it goes through the predicates. admittedly, though mutating state while going through predicates is _generally_ useful, it's _commonly_ not, and a pure version of `cond` would be nice to have additionally.

stateful `cond` is especially useful in writing parsers e.g.

TODO: rewrite this in relational style

[source,factor]
----
USING: kernel namespaces system command-line ;
SYMBOL: PARAM1 PARAM1 off
command-line get-global
[ [ f ]
  [ unclip-slice { { [ dup "--param1" = ] [ drop PARAM1 swap set-global t ] }
                   { [ dup "--help" = ] [ print-help 0 exit ] }
                   { [ drop t ] [ write " is an invalid arg" print -1 exit f ] } }
                 cond ]
  if-empty ]
loop
----

=== sequence & looping

* `collector-as` (guard is filter) & `selector-as` (guard is short-circuit) are the most general looping functions that collect into a sequence. they do not require input sequences; they use whatever state the stack has as input.
  ** `q collector` leaves a quotation that applies `q` then pushes that result to a resizable seq, and that resizable seq (to keep it in scope)
    *** `collector` is more convenient than `loop`: less shuffling and terser.
* `seq [ ] each` pushes each elt of seq to the stack

`reduce` with stack modification example: test whether all items in a sequence equal. ``reduce``'s identity starts at `t` and is a boolean of whether all elements so far are equal. the part left on the stack for the reduction quotation to implicitly use is the previous element encountered, initializing to the first.
 
[source,factor]
----
: all-eq? ( seq -- ? ) [ first ] keep t [ pick = and ] reduce nip ;
{ 1 0 3 } all-eq? ! f
{ 1 1 1 } all-eq? ! t
----

* `reduce` accepts only one `identity`, so we need to have the other part(s) of our accumulator already on the stack before the input sequence.
* `nip` to remove the non-output part of the accumulator. generally you'd `[ drop ... drop ] dip`

actually, though, this particular example is more elegantly expressed as:

[source,factor]
----
: all-eq? ( seq -- ? ) dup unclip-slice suffix = ; inline
----

.deep-each example

`{ { { 1 2 { 3 4 } 5 6 } { 7 8 } } } [ . ] deep-each` outputs:

----
{ { { 1 2 { 3 4 } 5 6 } { 7 8 } } }
{ { 1 2 { 3 4 } 5 6 } { 7 8 } }
{ 1 2 { 3 4 } 5 6 }
1
2
{ 3 4 }
3
4
5
6
{ 7 8 }
7
8
----

the `sequences.squish` vocab defines `squish` which takes a function of `{ 1 2 { 3 4 } 5 6 }` & `{ 7 8 }` whereas `deep-map` tries applying a quotation to `{ 3 4 }` & `5`, probably b/c `{ 3 4 }` is the deepest sequence and `5` follows it. idk what the general pattern is; i'll explore that when i have nothing better to do. idk what "preorder" means.

.`mnmap`

you'll eventually want to map over a sequence once but produce two different outputs. you can do `[ ... 2array ] map unzip` or `[ [ ... ] map ] [ [ ... ] map ] bi`, and you might even consider pushing into multiple vectors. that would be decently elegant in an applicative lang, e.g.

---------------
vec<int> v1, v2;
loop(x in s){
  v1.push(f(x))
  v2.push(g(x))
}
---------------

but the equivalent in factor is ugly compared to usual factor code. fortunately, `sequences.generalizations` has us covered with `mnmap`:

[source,factor]
----------------------------------------------------
{ 1 2 3 } { 10 20 30 } ! 2 input seqs (m)
[ [ + ] [ * ] 2bi ]    ! a computation ( x y -- s p ). it has m inputs & n outputs.
2 ! m
2 ! n
mnmap
----------------------------------------------------

leaves 2 sequences on the stack:

------------
{ 11 22 33 }
{ 10 40 90 }
------------

and the same, but explicitly making the 1st output an array and the 2nd output a vector by `mnmap-as`:

[source,factor]
-----------------------------------------------------------------------------------
{ 1 2 3 } { 10 20 30 } ! 2 input seqs (m)
[ [ + ] [ * ] 2bi ]    ! a computation ( x y -- s p ). it has m inputs & n outputs.
{ } V{ }               ! "n*exemplar" i.e. n output types
2 ! m
2 ! n
mnmap-as
-----------------------------------------------------------------------------------

produces:

-------------
{ 11 22 33 }
V{ 10 40 90 }
-------------

==== folds with short-circuiting

stack langs are extremely powerfully flexible in that the whole stack is available to loop bodies. thus the whole `map` vs `2map` problem is not really a problem, once those are recognized as convenience functions, not essential combinators. generally we use `while`, or `loop` if the continuation condition is of the iteration's output, for non-sequences, and `each` for sequences. although `map` is optimized a bit (using `nth-unsafe`), `collector` with `each` is just about as good. still, note that ``map``'s definition is not in terms of `unclip-slice`! *factor does not use linked lists.* `map` is defined in terms of `map-integers-as`, which accepts only an integer—not a sequence—as its input! rather than linked lists, factor uses growable sequences, which grow from the _end_ in O(1) time and have O(1) lookup. these are much more natural. of course a sequence is added to at the end, not the beginning! any non-coder would suppose so, just as they'd suppose that left folds are natural, not right ones.

all this to say: _never_ use `loop` and `unclip-slice` together. this isn't haskell or lisp, and thank god. well, ok, you _can_ use _unclip-slice_ and it's still natural in some cases, probably, but `unclip-slice` is just a shorthand for `[ 1 index-to-tail <slice> ] [ 0 swap nth ] bi` which obviously generalizes when we use numbers other than 1 & 0. furthermore, `nth` is random access, as is slicing eventually. i suppose that the motivation for looping with `unclip-slice` is that we check `empty?` which is easier than checking whether an index is less than length. regardless, there are looping combinators for:

. looping through sequences
. looping until a predicate yields `f`
. short-circuiting
. collecting loop iteration results

and it's better to use direct access than sequential access because it considers elements independently of others, enables getting multiple elements at once (array programming) and not tracking context. consider zippers (data structure). they represent the context at one and only one index, and they need a whole data structure for that! contrast this with a set of indices, which represents any number of contexts simply. the obviousness of it is supreme.

an example of "augh! this looping control flow is too complex. let's just modify the stack." is, given two lists A & B, generating `{ { a bs } ... }` where a∈A and bs(a) is the substring of `B` all of whose values are greater than or equal to `a`, when `A` & `B` are both sorted ascending. your first idea may be to use `accumulate*` because B progressively becomes a substring of itself. scans/folds are associated with iterative mutation. however, we're also mapping over `A`! `2reduce` doesn't help b/c we aren't _mapping_ over `B`; we're progressively modifying it in total. this is not a 1:n map. it's a 1:n reduction. it's worth mentioning that an efficient solution (enabled by `sorted-index`) is given by arrays; see the definition of `join<` in _§using the stack well_. such solutions should always be preferred. however, suppose that we use an alternative method which is not sensible for this scenario, but similar scenarios would entail these kinds of control flow concerns: at each iteration, return (a,{b|b∈B,a>=b}), removing all b<a from B for the next iteration.

TODO: write this code when i have time
[source,factor]
----
! : join< ( B A -- joined ) ! precond: A is ordered ascending. each of A & B is `values` of their pk->val assocs
{ 2 3 6 10 12 18 24 36 42 83 91 102 }
{ 10 12 34 56 87 } ! next: test when a>sup(B)
[ [ swap >= ] curry find drop [ tail-slice ] [ { } ] if* ] ! ( B' a -- B' )
accumulate*
! ; inline
----

`combinators.short-circuit` is a helpful vocab. example: `[ { [ sequence? ] [ integer? not ] } 1&&`, meaning scheme `(λ (x) (and (sequence? x) (not (integer? x))))`.

==== other general looping stuff

here's an interesting pattern:

[source,factor]
----
V{ } clone dup [ last . ] [ push ] bi-curry
[ 400 ] dip [ call ] keep ! push 400 into the vec
over call ! prints 400
[ 2 ] dip [ call ] keep ! push 2 into the vec
over call ! prints 2
2drop . ! prints V{ 400 2 }
----

== namespaces

=== globals

like lua's `_G`, factor has a global namespace called `global`. namespaces instance the `assoc` class.

[source,factor]
----
SYMBOL: x      ! declare
4 x set-global ! set
x get-global   ! access
----

=== locals

[source,factor]
----
60 [let 2 5 + :> x 49 x / * ] ! pushes 420
60 [let :> x x x * ] ! pushes 64. :> binds the top of the stack to an identifier while dropping it
----

remember that quotations are not special in factor. factor, being a stack lang, does not have local variables, and so it does not have scopes, so quotations don't introduce new, nested scopes. thus expressions like the following are allowed:

[source,factor]
----
[let 40 :> x x even? [ x 2 * :> y y 2 * ] [ ] if ] ! pushes 160 to the stack
----

indeed, factor's locals support scope nesting & shadowing. this occurs if e.g. you have nested `[let`'s, or if you have a `[let` inside a `::` definition. factor's implementation of nested scopes is very simple: it's a stack of hash tables from symbols to values; the topmost (innermost) table is checked for a local name, and if it's not there, then the next highest hash table is checked, and so on.

==== mutable vars

[source,factor]
----
USE: locals
! 3 f => 11
:: f ( x! -- t ) ! x! makes x mutable by enabling x! to set x (see below)
  x 2 * x! ! x<-2x
  5 x + ;  ! return 5+x
----

this syntax can be used in `[let` also e.g. `[let 24 :> x! x x * x! x 400 - ]` which outputs 176. then again, you could instead just use `:>` again: `[let 24 :> x x x * :> x x 400 - ]`. exclamation marks are a tad shorter, i guess.

=== multiple simultaneous output streams

NOTE: see <<_logging>> in this document if that's your particular use case.

[source,factor]
----
USING: io.encodings.ascii destructors ;
SYMBOL: extra-out
: ./eo ( x -- ) extra-out get [ . ] with-output-stream* ; ! note the asterisk! i don't want to close the output stream in ./eo!
"extra.log" ascii <file-writer> [ extra-out [ 10 . 20 ./eo ] with-variable ] with-disposal
----

writes 10 to stdout and 20 to ./extra.log.

this can easily be extended to more output streams, though for arbitrarily many you'll probably want to make your own version of `with-disposal`.

== caveats

lines like `f number>string drop` cause scripts to exit silently WITH EXIT CODE 0 nonetheless! also i spent quite a bit of time trying to debug a script, only to find that `{ "systemctl suspend" } run-detached` was the issue; it should've been `{ "systemctl" "suspend" }`! it silently tried executing the invalid program name. even the resultant `process` object did not have anything indicating an issue. however, the resultant `process` object of `{ "systemctl suspend" } run-process` had `{ status 255 }`.

code in the listener that uses `if` may successfully or unsuccessfully run despite having improper stack effects. `ctrl+i` recognizes the mismatched stack effects. were i to put this in a function in a vocab then try to load the vocab, i'd get a stack mismatch error. thus this issue really exists only when running code directly in the listener.

== libs & specific words

* for graphics, use cairo; it has bindings to factor
* see factor documentation > libraries. it's a wealth of functionality in one big listing!

== tricks

* if you have e.g. a stack of `a b c w x y z C` and you want to compute on `a b c C`, then you can just `curry 4dip`
* `USE: math.ranges CHAR: a CHAR: z [a,b]` works b/c characters are integers
* `USING: math.parser random ; "(ddd) ddd-dddd" [ { { CHAR: d [ 10 random number>string ] } [ 1string ] } case ] { } map-as concat`
* `USING: calendar calendar.format ; now 1 months time+ { YYYY " " MONTH " " DD " " hh ":" mm ":" ss "\n" } formatted`. `formatted` is a macro.
  ** `millis>timestamp`, and its complement, `unix-1970 time- duration>milliseconds >integer`

`io.styles` vocab e.g.

[source,factor]
----
USING: colors.gray io.styles hashtables sequences kernel math ;
10 <iota> [
    "Hello world\n"
    swap 10 / 1 <gray> foreground associate format
] each
----

== `math`

* `bitxor`, `bitand` &c. see the docs for related fns like `2/` (right shift by 1 bit), `bitcount`, and `even-parity?`

== os

=== subprocesses (`io.launcher` vocab)

generally one may make a `process` tuple then modify its properties then run it. however, usually we want the default process behavior. notice that words like `run`, `run-detached`, and `<process-reader>` accept "desc"s, not process object _per se_. indeed, these words use generic word `>process`. to convert an object into a process. therefore we commonly do e.g. `{ "echo" "hello, there!" } run-process` rather than `<process> { "echo" "hello, there!" } >>command run-process`.

.shell expressions

factor does not have a word that uses the shell at environment variable `SHELL` to evaluate a string. for example, `"ls \*" process-contents print` will print nothing unless you happen to have a file called '\*' in your directory. this string is not passed to e.g. bash for the asterisk to be evaluated as a blob expression. the very fact that factor runs the expression at all is a bit misleading; if you give a single string (instead of a sequence of strings) as a command, what factor actually does is (at least on unix-based oses) calls `tokenize` to split basically on whitespace, then calls `execvp` (for `fork-process` if you use `run-process` on a \*nix os; or `posix_spawnp` if you call `spawn-process`) (with the environment) on the head & tail.

regardless, if you want to execute something by the default shell, then do:

[source,factor]
----------------------------------------------------------------------------------------
: exec-shell ( cmd -- ) tokenize { "/usr/bin/sh" "-c" } prepend process-contents print ;
"ls *" exec-shell ! as though i'd typed "ls *" in a terminal
----------------------------------------------------------------------------------------

at least this works for bash. i haven't cared to check whether the `-c` flag is standard posix.

.read a process into a string

[source,factor]
-------------------------------------------------
USING: io.launcher ;
{ "echo" "hello, there!" } process-contents print
-------------------------------------------------

`<process-reader>` (used by `process-contents`) starts a process asynchronously (via `run-detached`) and returns a stream, which may be partially read from whenever. compare with `[ read-contents ] with-process-reader* 2drop`, which allows you to do stuff with the process object and exit status, but *waits for the process to finish*. also note that `<process-reader>` launches the process. btw, `with-process-reader*` generalizes to `with-process-stream*`, which binds both the input & output streams. see the following examples.

[source,factor]
-----------------------------------------
USING: io io.launcher io.encodings.utf8 ;
<process>
{ "/usr/bin/date" } >>command
utf8 [ readln ] with-process-stream*
-----------------------------------------

NOTE: there are no `process` slots whose values you can set to make the process's stdout redirect to factor's stdout. this being said, `process-contents print` is just as well since it uses pipes and `stream-contents`.

.stdin & stdout redirection

to pipe to a process's stdin, use `with-process-writer*`; within the quotation that you pass to it, `output-stream` is bound to the process's stdin. you can also set a process's `stdout` slot to a string which it'll interpret as a file path to write to.

.effectively posix shell `tr lt ' d' <<< 'but what the hell, forget about it!' > msg.txt`
[source,factor]
--------------------------------------------------------------------------
<process> { "tr" "lt" " d" } >>command "msg.txt" >>stdout
ascii [ "but what the hell, forget about it!" write ] with-process-writer*
--------------------------------------------------------------------------

use `with-process-writer` (no asterisk) if you don't need the process object nor its status. to stream one process's output as input to another process, use `run-pipeline` of the `io.pipes` vocab.

NOTE: `with-process-writer*` is (at least effectively) just `with-process-stream*` but without binding `input-stream`.

in unpredictable, asynchronous cases, you may want to set the `process`'s `timeout` attribute. note, however, that if a timeout is met, then the process is killed, then a `process-was-killed` error is thrown! if you want to simply wait some time before checking the exit status, then i suppose that you must quote some call to `with-process-stream*` then pass it to `with-timeout` (or use `with-timeout` inside the quotation).

i don't understand why `with-process-stream*` returns the process object in addition to the status, since the process is guaranteed to be terminated by the time that `with-process-stream*` returns, which would seemingly make the process object useless. indeed, both `wait-for-process` and `(wait-for-process)` return only the exit status. similarly, i don't know why either of these two words can be run any number of times on a process.

you can do duplex, asynchronous communication with a process:

[source,factor]
------------------------------------------------------
<process> { "cat" } >>command
ascii [ "hi!" print flush readln ] with-process-stream
------------------------------------------------------

this is good but is dangerous since it's prone to livelock. certainly always you should `flush` the output stream before trying to read the subprocess's output, but beyond that, you must know how your subprocess handles buffering and when it writes to its stdout (which factor can read from by e.g. `readln`). if you don't, then the subprocess may indefinitely await input, leaving the program hanging! some programs, such as `asciidoctor`, apparently don't write until their input stream is closed, so we must not only flush, but `dispose` the output stream to close it:

[source,factor]
---------------------------------------------------------------------------------
<process> { "asciidoctor" "-e" "-" } >>command ascii
[ "hi!" print flush output-stream get dispose read-contents ] with-process-stream
---------------------------------------------------------------------------------

without `output-stream get dispose` this program will hang forever! also idk if you have to `flush` if you're going to dispose afterward anyway.

you can replace `cat` by a curses program e.g. `w3m` and see that curses programs work fine, too.

.example: invoking k from factor
[source,factor]
-------------------------------------------------------------------
<process> { "/home/nic/.local/bin/k" } >>command
ascii [ "`0:`j@3+!9" print flush readln ] with-process-stream json>
-------------------------------------------------------------------

pushes `{ 3 4 5 6 7 8 9 10 11 }` to the stack. `k` is a repl without support for multiline statements, so we know that it writes something each time that we stream a newline-terminated string to it, so `flush` alone is safe here. in fact, if we want to keep the repl open and interact with it many times, then of course we can't close its pipes! we want to retain full duplex communication. here's an example where `k` & factor communicate many times:

[source,factor]
---------------------------------------------------------------------------------
<process> { "/home/nic/.local/bin/k" } >>command ascii
[ "`0:`j@3+!9" print flush readln json>
  4 head-slice [ >dec ] map " " join "`0:`j@2\\" prepend print flush readln json>
] with-process-stream ...
---------------------------------------------------------------------------------

prints `{ { 0 1 1 1 } { 1 0 0 1 } { 1 0 1 0 } }`.

so you can use any repl effectively as a stateful server.

NOTE: `binary` encoding does not work with `with-process-writer` nor `with-file-writer`! it gives some odd error: `element-size` does not define a method for the <such-and-such> class (class depends on what you're writing) dispatching on <item of that class>.

.check that process didn't immediately crash

sometimes you can't be confident that your command line is correct or that the desired executables exist at the paths that you specify. in these cases, you may want to check for this almost immediately, namely if you expect that the proccess runs in the background and communicates with factor asynchronously.

[source,factor]
------------------------------------------------------------------------------------------
! invalid executable path means immediate crash, so status is 255 (on arch linux at least)
{ "/usr/bin/soleep" "5" } run-detached 1 seconds sleep status>>

! when status>> is executed, process is running; status is f
{ "/usr/bin/sleep" "5" } run-detached 1 seconds sleep status>>

! when status>> is executed, process had exited with exit code 0, so status is 0
{ "/usr/bin/sleep" "1" } run-detached 5 seconds sleep status>>
------------------------------------------------------------------------------------------

thus `status>> not` tells whether the process is still running.

things can get a little nuanced. in the following example, we expect the process to run for much more than 2 seconds, but if it instead immediately crashes, then we print its stderr contents then throw a factor error.

[source,factor]
-------------------------------------------------------------------------------------------
USING: io.launcher.private ;
<process> { "/usr/bin/sudo" "./althttpsd" [ ... ] } >>command
+stdout+ >>stderr ascii (process-reader)
2 seconds sleep dup process-running? [ 2drop ] [ swap stream-contents . process-failed ] if
-------------------------------------------------------------------------------------------

* i don't think that `(process-reader)` deserves to be private.
* `process-running?` kept returning true when i sleept only 1 second when i made the server crash (ok, i didn't make it crash; i just misspelled the path to the server executable)
* this demonstrates that we can read from a process after it terminates
* if you put an invalid path for the first element of the command, then `process-running?` returns `t`, on linux, at least.

''''

.exec

a common use of factor is as a powerful alternative to bash, often simply preparing command lines then executing them, replacing itself by that child process (exec). this is done by using `exec-args-with-path` (of the `unix.process` vocab) instead of `run-process`.

=== filesystem

* vocabs: `io.files`, `io.directories`, `io.encodings`
* load files as streams: `with-file-[reader|writer]`
* load whole file: `[set-]file-[contents|lines]`
* `current-directory` dynvar

examples:

* `"filepath.txt" utf8 [set-]file-contents` to read or write to a file.
* `"writeme" utf8 [ "readme" mac-roman [ [ print ] each-line ] with-file-reader ] with-file-writer`

== peg

factor's `peg` vocab is a link:https://bford.info/packrat/[packrat parser].

peg is like regex but makes extracting substrings and implicitly putting them in an ast much easier. peg also works on sequences of any type:

.intro demo
[source,factor]
----
USING: peg peg.search ;
{ { -47 4 } { 2 34 } { -1 6.6 } { 3 766 } }
[ first 0 < ] satisfy [ second ] action repeat1 search .
----

NOTE: `search` is defined in terms of `any-char-parser`—a special parser defined in `peg.search.private` which, despite its name, parses _any thing_ not just _any character_. `any-char-parser` is, at least in factor v0.99, equivalently defined as ``peg.parsers``'s `any-char`.

prints V{ V{ 4 } V{ 6.6 } }. pattern matching on number sequences can be extremely useful for e.g. technical stock trading, finding subsequences of blobs.

* `parse ( input parser -- ast )` where input may be a string
* common parsers (e.g. `any-char`) are in `peg.parsers`
* `hide`
* `satisfy` matches a character against a predicate quotation
* `token` is a parser that tries to match a string literal
* `sp` modifies a parser to accept & ignore leading whitespace e.g. `"  hi" "hi" token sp parse .` prints "hi"

primitive (by definition—not theoretical canonical basis) peg parser words:

* `action`
* `range`
* `satisfy`. accepts only a sequence as input. tests only a single element of the input sequence.
* `sp`
* combinators (they take 1+ parser(s) as inputs):
  ** `seq`
  ** `choice` (or). *not commutative!* tries the 2nd only if the 1st fails!
  ** `optional` (or hide)
  ** `repeat0` (kleene star)
  ** `repeat1` (kleene plus)
  ** `semantic` (define a parser's validity in terms of its output. for a parser composed by `seq` or other combinators, `semantic` enables you to decide whether a group of parsers altogether are valid)
    *** `semantic` is `satisfy` except that it accepts another parser as input

.tips & examples

* parse 3-substrings of increasing value: `any-char 3 exactly-n [ [ < ] monotonic? ] semantic`
* you can use `[ ... ] satisfy [ ... ] action` to perform a function on a matched value, or you can do `[ ... ] action [ ] semantic` to perform a function then check whether its output is valid. this pattern is helpful when your `satisfy` & `action` quotations would share a lot of code; you can write the code only once as an argument to `action` then use `[ ] semantic` to filter the results.

* there's no "and" to complement "or" (`choice`). this is because pegs parse-out values. and & or complements naturally exist for testing whether a parser suceeds, but not for combining results.
* `any-char` is regex `.`. idk why it's in `peg.parsers` instead of in `peg`, what that implies. for a particular character, just make it a singleton string then pass to `token` e.g. `"0" token` to parse a zero. `peg.parsers` has a word, `1token`, which does exactly that.

special parsers that affect not what's parsed, but the parsing itself:

* `box`
* `hide`
* `check-parse-result`
* `delay`
* `ensure`

.regex as peg words

these are in `peg`:

[options="header"]
|================================================================
| regex    | peg word(s)
| [A-Za-z] | `range` & <and AND combinator to be defined>; or `range-pattern` (in `peg.parsers`)
| ab       | `seq`, `token` (`token` is `seq` on string literals)
| a?       | `optional`
| a*       | `repeat0`
| a+       | `repeat1`
| (a\|b)    | `choice`
|================================================================

the following are in `peg.parsers`:

[options="header"]
|=====================================================================================
| regex            | peg word(s)
| a                | `1token`
| .                | `any-char`
| {m,n}            | `at-least-n`, `at-most-n`, `from-m-to-n`, `exactly-n`
|                  | `epsilon` (empty sequence)
| `[0-9]`          | `digit-parser`
| `[0-9]+`         | `integer-parser`
| ((<pat>)<sep>?)* | `list-of` e.g. `"2,32,64" integer-parser "," token list-of parse`
| "([^"])"         | `string-parser`
|=====================================================================================

.ideas sensible only in peg, not regex

* `ensure[-not]`
* `satisfy`
* `semantic`
* `hide`
* `action`
* `surrounded-by`
* `add-error`

[TODO]
* how to run a parser just to see if it succeeded or not?
* how to combine a parser `p` with `satisfy` as `[ p quot and ] satisfy`?

* `satisfy repeat[0|1]` returns a vector of characters
* `1token`, defined in terms of `1string`, returns a singleton string

`ensure-not` allows us to check whether we're at the end of input:

* `"X" any-char any-char ensure-not 2seq parse` pushes `V{ 88 }`
* `"" any-char ensure-not parse` pushes `ignore`

* it seems that adding `ensure[-not]` to `choice` makes a `cond`-like parser

examples:

[source,factor]
----
! COMMON PARSERS
: any ( q -- parser ) satisfy repeat0 [ >string ] action ; inline
! to is to-end if predicate is never hit
: to ( q -- parser ) [ not ] compose any ; inline
: to/c ( c -- parser ) [ = not ] curry any ; inline
! BUG: to-end fails on empty string; it should then return the empty string
: to-end ( -- parser ) any-char repeat1 [ >string ] action ; inline
: many ( q -- parser ) satisfy repeat1 [ >string ] action ; inline
: a* ( c -- parser ) [ = ] curry any ; inline
: a+ ( c -- parser ) [ = ] curry many ; inline
! sp is probably more efficient when you can use it; ws* & ws+ are
! intended to be used at least for list-of.
: ws* ( -- parser ) CHAR: space a* hide ; inline
: ws+ ( -- parser ) CHAR: space a+ hide ; inline
: WORD ( -- parser ) [ CHAR: space = not ] many ; inline
: words ( -- parser ) WORD ws+ list-of ; inline

! EXAMPLE COMPOUND PARSER
: my-clause-parser ( -- parser )
  f ! empty seq
  CHAR: - to/c [ [ CHAR: space = ] trim ] action suffix
  "->" token sp hide                             suffix
  WORD sp                                        suffix
  CHAR: : to/c [ words sp parse ] action sp      suffix
      [ CHAR: : = ] satisfy ensure
      ":" token sp hide
      to-end sp
    3seq
    any-char ensure-not
  2choice                                        suffix
seq ; inline
----

[options="header"]
|===========================================================================================================================
| input                                              | output
| "expr -> mytbl apple   booty cow  dargon : x >= 5" | V{ "expr" "mytbl" V{ "apple" "booty" "cow" "dargon" } V{ "x >= 5" } }
| "expr -> mytbl apple   booty cow  dargon"          | V{ "expr" "mytbl" V{ "apple" "booty" "cow" "dargon" } }
|===========================================================================================================================

* "x >= 5" is in a vector because of `3seq`; e.g. `"A" any-char parse .` returns 65 as expected, but `"A" any-char 1array seq parse .` returns `V{ 65 }`.
* how to parse recursive syntaxes? there should be a peg json parser example on the web for an example.

caveats & mistakes:

* `"thing horo nee" any-char repeat1 ws+ list-of parse` returns a singleton vector of a vector! this is because `list-of` calls `any-char repeat1` which matches the whole string; then `list-of` tries to break on spaces, but there's no more input, so it returns that single vector of characters in a vector.
* error about gensym: then check to see if you forgot `suffix` after your parser
* error about `length` not having method for `parser`: you probably put 2+ parsers on the stack but forgot to put them into a sequence. especially with `ensure`, ensure that you do `<q> ensure <parser> 2seq`

.search & replace

[source,factor]
----
USING: peg peg.search kernel make sequences strings ;
! simple modification of string-parser in peg.parsers vocab
:: delimited ( start end -- parser )
    [ [ start = ] satisfy hide , [ end = not ] satisfy repeat1 ,
      [ end = ] satisfy hide ,
    ] seq* [ first >string ] action ;

CONSTANT: props H{ } clone
"TSLA" "INST" props set-at
"i feel like trading {INST} today. {INST} is a fine stock."
CHAR: { CHAR: } delimited [ props at ] action replace .

"queueing either makes one happy or not."
[ "aeiou" member? ] satisfy repeat1 [ first ] action ! parse the 1st of a string of vowels
any-char 2 at-most-n 2seq [ first2 >upper 2array ] action ! (A). capitalize the 2 (or fewer if end of input) characters following the last vowel
replace .
----

prints "i feel like trading TSLA today. TSLA is a fine stock." and "quNG eTHeR maKEs oNE haPPy oR noT.". strange how we need `first` before `>string`. somewhy the vector of characters matched by `repeat1` is itself wrapped in a vector.

notice that action (A) returns an array of a character and another array. `[ first2 [ 1array ] [ >upper ] bi* 2array 1array ]` has the same effect; arrays are effectively flattened; `replace` is defined in terms of `tree-write`.

NOTE: `replace` works only on strings! if you want to work on non-strings, just use the majority of ``replace``'s definition inline: `any-char 2choice repeat0 parse`

.generalized `replace` example

[source,factor]
----
TUPLE: myt fst snd ;
"eixayz"
[ "aeiou" member? ] satisfy repeat1 [ first ] action
any-char 2seq [ first2 [ 1string ] bi@ myt boa ] action
any-char 2choice repeat0 parse .
----

prints

----
V{ T{ myt { fst "e" } { snd "x" } }
   T{ myt { fst "a" } { snd "y" } }
   122 }
----

=== EBNF

basically, unless i'm given a correct, formal description of `peg.ebnf`'s ebnf's grammar, then it's unusable. use manual parsers instead.

peg's ebnf syntax produces a parser that you could've written by hand, but i'm unsure that ebnf can describe all that manual parser combiniation can. i'm not even sure when ebnf is really more convenient than manually writing a parser. for example, can ebnf elegantly describe tokens delimited by `/[[:space:]]+/` or a group of tokens delimited by commas with optional space?

* `EBNF:` in `peg.ebnf`

syntax is like regex:

* `|`
* `[abc]` & `[^abc]` (don't quote characters)
* use double-quotes for literals
* `?`, `*`, `+`
* `EBNF[[ y=[W-Z] x=[T-X] ]]` creates rules `y` & `x` and is a quotation that applies a parser that checks `y AND x` i.e. a single character in `[W-X]`.
* need to use `<tokenizer-name>=`; no unnamed tokenizers.

in trying to learn the ebnf grammar by reading source, i'm learning about using non-ebnf parser( combinators) e.g. `choice*`, and i'm finding those easy to use though more verbose and less readable than ebnf.

the errors can be astonishingly stupid: `"A" EBNF[[ aa = "A" aa|"B" ]]` errors with "Expected 'A' or 'B'. Got 'A'", though it parses `"B"` just fine. however, after some poking around, i see that `|` does not mean "or": `"AAAAB" EBNF[[ aa = "A" aa|"B" ]] .` prints `V{ "A" V{ "A" V{ "A" V{ "A" "B" } } } }`. with such complexity, i decide to no longer try to try to learn the ebnf grammar by looking through source code.

.lookahead

`"a ∈ mytbl -> t(b,c,d)" EBNF[[ y= .+ => " -> " .+ ]]` fails b/c `.+` matches whole string before required token `" -> "` is attempted to be parsed; b/c there's no more input, `" -> "` fails to match, causing the whole parser to fail. `ensure[-not]` can be used for lookahead. then again, we usually want something more specific than `.+`; for example, here "a ∈ mytbl" should be matched against some parser that chooses from multiple valid expressions; the expression should be terminated by its own grammar rather than `" -> "` terminating that expression; therefore the expression should match without worry about accidentally parsing `" -> "` before the appropriate occasion. that `.+` may match `" -> "` and more is not a defect of ebnf; it's no easier to manually write a parser that has not that problem.

this being said, it probably is sometimes reasonable to want to parse until a given string. TODO: how to do that?

.decoding ebnf grammar

terminal: blank or ∈ ["'|{}=()[].!&*+?:~<>]

== debugging

there's no ^C (interrupt) on linux (but iirc there is on windows). to get around this, you can run `listener-window` to spawn a new listener window that shares global variables with the original. create a global variable that you'll set to `t` or `f` to choose whether to exit a loop e.g. you may `SYMBOL: stop? [ ... stop? get-global not ] loop` then you can run the loop in the original listener window and run `t stop? set-global` in the new window whenever you like to stop the loop.

firstly, using `prettyprint` is not apt for debugging a running program, though it's fine in the listener to print values that you don't want kept on the stack. to inspect values during runtime, use `B` (an alias for `break` in `tools.continuations` and the inspector (context-click an object then select "Inspector") or factor's logging framework. to produce values for logging, use `unparse` or `unparse-short` of the `prettyprint` vocab.

to set a breakpoint for a word—especially useful for words that you don't use directly, but are ultimately entailed in the definitions of words that you use—use `breakpoint` for example, to break at the start of executing generic word `call-responder*` for the `file-responder` type, do `M\ file-responder call-responder* breakpoint`. to remove the breakpoint hook, do `M\ file-responder call-responder* reset`. `M\ file-responder call-responder*` satisfies `word?`, so it supports having a breakpoint set for it. *do not use `breakpoint` on common words* like `map` or `append`! because the listener or walker employs such words, setting a breakpoint upon them will cause the listener or walker to infinite loop.

one trick is to push some values before a breakpoint, then drop them sometime later. that way they'll be on the stack in the walker. this is easier than making & setting dynamic variables, and besides, the "variables" dialog shows only symbols declared in `scratchpad`. `IN:` does not change this fact.

* see factor docs: "Watching variables in the listener". namely you'll want `show-vars`. this, however, prints the variables after every listener statement's execution finishes, rather than upon the vars' change(s).
* `^w` in listener to walk through a quotation
* see the doc "Watching variables in the listener"
* if using a higher order fn, mimic it by running its argument at the top level e.g. if `[ f ] each` isn't working, test `f` with the arguments that you expect
  ** if `each`, `map`, `reduce`, or any other traversal over a sequence, is failing, then the easiest way & most direct way to debug it is to stick a `1 head` after the input sequence.
* check the stack signature (ctrl+i)

consider the following code which *incorrectly* tries to implement j's key (`/.`) (it accumulates into a hash set instead of a vector):

[source,factor]
----
: groupby ( vals keys -- groups ) ! like /. in j or `group by` in sql
  H{ } over [ swap [ HS{ } ] 2dip [ set-at ] keep ] each spin ! h ks vs
  [ swapd [ over adjoin ] curry [ change-at ] pick [ curry call ] dip ] 2each ; inline

{ 0 2 4 0 7 1 100 56 35 } { 0 2 4 0 2 4 0 2 4 } groupby
----

i kept getting the output:

----
H{
    { 0 HS{ 0 1 2 35 4 100 7 56 } }
    { 2 HS{ 0 1 2 35 4 100 7 56 } }
    { 4 HS{ 0 1 2 35 4 100 7 56 } }
}
----

i took a couple of hours to realize that it was because the `HS{ }` was one object, used as all values for the hash map! using `HS{ } clone` fixed the problem, giving the correct output:

----
H{ { 0 HS{ 0 100 } }
   { 2 HS{ 56 2 7 } }
   { 4 HS{ 1 35 4 } } }
----

btw, yes, i'd later discover that this exact functionality is implemented by `collect-by` of the `assocs` vocab. and look at its definition—how much better it was written! clearly i still have much to learn about how to write good factor code.

=== logging

vocabs `logging`, `logging.server`; and less importantly: `logging.analysis`, `logging.insomniac`, `logging.parser`.

factor comes with a framework for logging to log files.

.example
[source,factor]
----
"myApp" ! here we name the log
[ 10 0 [ / ] [ \ / log-error ] recover ] ! log-error logs a traceback. notice that log error takes a word as its top input.
with-logging ! with-logging is needed to actually write to a log
"myApp" log-path . ! where the log was written
----

NOTE: use `path q with-log-root`, not `log-root path q with-variable`; the latter does not work, and the former has a specialized definition to accomodate multithreading.

to disable logging temporarily,...huh, i thought there was some simple built-in functionality for that, such as setting off a dynamic variable. well, you can just replace `with-logging` by `log? get-global [ with-logging ] [ nip call ] if` after you've created the `log?` symbol.

=== reading tracebacks

my comments are prefixed with a `!`.

.example

the error was "sequence index out of bounds" where the index was `1` and the seq was `f`.

----
(U) [ set-namestack init-catchstack self quot>> call => stop ]
! i ran the code in the listener
(O) listener-thread
(O) listener
(O) listener-loop
(O) listener-step
(U) [
        [ ~quotation~ dip swap ~quotation~ dip ] dip swap
        [ call get-datastack ] dip => swap [ set-datastack ] dip
    ]
(U) [ call => get-datastack ]
! the error was thrown inside execute-parser which was called by `perform-parse` which was called by `parse`, " `safe-search` " `parse-signals` " `backtest`
(O) backtest
(O) parse-signals
(O) safe-search
(O) parse
(O) perform-parse
(O) execute-parser
(O) ( gensym )
(O) execute-parser
(O) ( gensym )
(O) execute-parser
(O) ( gensym )
(O) execute-parser
(O) check-action
! the error was thrown inside this quotation (which was defined using lexical variables)
! this quotation was called from `check-action` which is used in the word `action` of the peg vocab
(U) [
        2 load-locals first2 2 load-locals 0 -1 get-local 1 - 0 max
        -3 get-local <slice> <reversed> 0 get-local [
            load-local second l>> 0 get-local l>> -
            abs w-tolerance get-global < 1 drop-locals
        ] curry find => nip -1 get-local -2 get-local [
            2 load-locals first -1 get-local 0 get-local <slice>
            dup ~quotation~ keep ~quotation~ keep drop
            ~quotation~ dip drop ~quotation~ ~quotation~ if
            2 drop-locals
        ] curry curry and* 4 drop-locals
    ]
! the error was thrown inside `find`
(U) [ [ 0 ] 2dip do-find-from => index/element ]
    find
! more specifically (i think), it was thrown in the following stack shuffling soup:
(U) [ 2keep => drop ]
    keepd
(U) [ swap [ dip ] dip => ]
    2dip
(U) [ swap [ call ] dip => ]
    dip
(U) [
        [ nip call ] 3keep => roll
        [ 2drop ] [ ~quotation~ 2dip find-integer-from ] if
    ]
(U) [ swap [ 2dip ] dip => ]
    3dip
(U) [ swap [ dip ] dip => ]
    2dip
(U) [ swap [ call ] dip => ]
    dip
! ah! wait, i recognize this part! i wrote this code!
! it's the quotation that i passed to `find`.
! and right after this quotation is the description of the
! bounds error. using a little inference, i see that `second`,
! which is the same as `1 nth`, was apparently called on `f`
! where a non-empty sequence was expected.
(U) [
        load-local second => l>> 0 get-local l>> -
        abs w-tolerance get-global < 1 drop-locals
    ]
(O) M\ sequence nth
(O) bounds-error
(O) M\ object throw
! this last quotation is printed for all tracebacks of `break`. ignore it.
(U) [
        OBJ-CURRENT-THREAD special-object error-thread set-global
        current-continuation => error-continuation set-global
        [ original-error set-global ] [ rethrow ] bi
    ]
----

.codes

* `M\ x y` means that `y` is generic and indicates ``x``'s implementation of it
* `(O)` seemingly indicates an ordinary word
* `(U)` seemingly indicates a quotation

=== the walker

if you create a mutable structure (e.g. hashtable) then, in the walker, then context-click it and select "push", it will open a new listener window with the object on the stack. notice that it is the _same object (same reference)_ in the walker, in the original program (if it's using the stack of the original listener window), and in the new listener window. if you modify it in one listener, execute `.s` in the other listener to see its new value.

this is very useful for stepping through words of long definitions and computationally expensive code, to preserve state so that you can resume conmputation from a saved state.

TIP: assumedly the mutable structure is retained on the stack throughout the whole computation after it's pushed; if the computation fails for some reason, then you can find the structure in the traceback window's stack view, then push it.

also, i thought that i could create a `SYMBOL:` then `set-global` in the walker, but `get-global` only returned `f`, except for the first time that i tested it. anyway, this does not seem like a good method for saving & loading items between your source code and the listener!

== using the stack well

* small programs defined as words are common both for modularity but also b/c if a word evaluates then it's verified to be correct, assumedly for its functionality, but certainly for its stack effect. breaking a program into many word definitions makes tracking the stack easy. calling a word is like a checkpoint.
* despite aptly being called "factor", coupling is important, too! however data are always used together, couple them into a single item on the stack, as early as you can.
  ** curry when you can. this reduces the number on items on the stack and appropriately couples a fn with its arg. build-up programs incrementally as early as you can. an example is `1 xs [ / ] with map` or the equivalent `xs [ 1 swap / ] map`. the naive way is to think of `/` as a binary operation which takes two args, and assume that those two args should be provided as arguments on the stack. however, here one arg is an atom and the other a sequence, so we can't simply apply it nor can we use `2map`. a novice might put `1` on the stack then retain it beyond its consumption by `/`: `1 xs [ over [ / ] dip swap ] map`. that's unnecessary hell. or they may do a less elegant thing: creating a sequence of a constant: `xs dup length 1 <repetition> swap [ / ] 2map`. these just make stack langs look bad.
    *** i expect that this is why words like `nth` take the index then the sequence, or why `set-at` takes the hash map off the top of the stack, or why `member?` takes the sequence off the top of the stack.
    *** exploit that programs are sequences e.g. `[ "v1" "v2" ] [ "k1" "k2" ] [ H{ } set-at ] [ 2each ] keep first .` set-at is effectful, consuming our hash table. `keep` leaves the program `[ H{ } set-at ]` atop the stack; the hash table is thus still on the stack, contained in the sequence; `first .` prints it.
* any time that data have an attribute in common, where that attribute is relevant to some fn that the data are passed to, put the data in a sequence then iterate over it.
* if you're having trouble orienting composition/currying of fns throughout a stack process, then just define a helper fn or combinator. stack langs work best when each word has a short definition.
* manual recursion really is rare! surprisingly, even `(split)`'s definition is non-recursive!
* if you're nesting combinators and getting stack effect mismatch errors, then check the stack effects (ctrl+i in the graphical listener) of each part individually and algebraically reason through what outer quotations mandate vs what your inner quotations effects are.

basically: express symmetries as sequences and asymmetries as relations of sequences. these relations are commonly themselves sequences. `[ H{ } set-at ]` is a sequence/relation of two distinct things paired particularly, and this relation is passed to `2each`, which relates a matrix (having two axes [of symmetry]) to an operation (program).

if you don't immediately find obvious how to tacitly write a program, then code it in applicative style, then factor it. reading factored code is easy, but identifying it without first seeing the distributed code is often difficult, and is a variety of optimizing too early. indeed, factoring is a function of some code altogether. a good exercise for this is making `sorted-index` like j's `I.`.

.deriving tacit I.

i start with the following non-tacit form:

[source,factor]
----
:: I. ( elt seq -- i )
  elt seq natural-search :> i v
  { { [ v not          ] [ i ] }
    { [ v elt <=> +lt+ ] [ i 1 + seq length min ] }
    ! if v>=elt then we've already the correct index.
    { [ t              ] [ i ] } } cond ;
----

factoring:

. the longest common substring of ``cond``'s bodies is `i`; therefore i should leave i on the stack before calling `cond`
. all of ``cond``'s heads (can) start with `v` (once i rewrite `t` to `v drop t`)
. because ``cond``'s heads are used before their bodies, `v` should be atop the stack; then because `i` is used after, it should be beneath `v`
. the main branch of `cond` requires both `seq` and `elt`, which the other branches do not entail. for the sake of this one branch, i must put them on the stack sometime before calling `cond`. if it's before calling `cond`, then that implies that they must be taken off the stack by the other two branches. this expresses all of the branches in terms of common factor `seq elt`.
. combining points (3) & (4), the stack must be `i seq elt v` before calling `cond`
  .. or we can do the relation of `i` & `seq` ahead of time once we decide that `v` is not `f`, but that'd mean breaking the `cond` into two ``if``'s

[source,factor]
----
: I. ( elt seq -- i )
  2dup [ swap ] 2dip natural-search -rotd ! i seq elt v
  { { [ dup not         ] [ 4drop 0                    ] } ! replace i (f) by 0
    { [ swap <=> +lt+ = ] [ [ 1 + ] [ length ] bi* min ] }
    ! if v>=elt then we've already the correct index.
    { [ drop t          ] [                            ] } } cond ; inline
----

test:

----
A=10 12 34 56 87 500
B=2 3 6 10 12 18 24 36 42 83 91 102 

   B I. A NB. j
3 4 7 9 10 12

   A B [ sorted-index ] curry map . ! factor
{ 3 4 6 8 9 } ! incorrect

   A B [ I. ] curry map . ! factor
{ 3 4 7 9 10 12 } ! yay! success!
----

== examples of good programming

.state, currying, a/symmetry obvious in code
[source,factor]
----
{ HS{ } HS{ } } ! list 1
{ "hs1" "hs2" } ! list 2
[ over [ H{ } set-at ] dip ] [ 2map ] keep ! accumulating into a list of hash sets, to keep them in scope for later modification
                                           ! running 2map under keep leaves the hash table quotation atop the stack
second first ! extract hash map from quotation
dup . ! print hash table
[ { "beans" "jeans" } [ over adjoin ] 2map . ] dip ! insert into, then print, each hash set
. ! print hash table
----

this code generalizes easily to any number of key/value pairs. this elegant encoding keeps few elements on the stack, even if it's "strange" that we retain the hash table within the quotation passed to `2each`. after all, we're always setting values of hash table, so why not couple it with `set-at`? it's so simple that one may fail to realize that it's a variety of metaprogramming that macros cannot enable. for this, one truly must use call/eval and be able to extract subprograms from a (quoted) program. it's even easier in factor because the hash map is not a program that we need to evaluate; it's a datum itself!

.a quintessential example of factor coding

[source,factor]
----
: collector-as ( quot exemplar -- quot' vec )
    dup new-resizable-like [ [ push ] curry compose ] keep ;
    inline
----

this is from the `sequences` vocab. it uses `curry`, `compose`, `keep`, `dup`, and state (`push` is effectful only.)

reading & reasoning about the code:

. don't read `dup` by itself. read `dup new-resizable-like`. `quot exemplar` becomes `quot exemplar new-resizable-like-exemplar`. `dup f` (at least when `f` is unary) should be read as "push f(top of stack) to stack"
. next we hit a quotation; we immediately look to see of which word this quotation is an argument. it's `keep`. `keep` should be read as "under pop" in the sense of "pop, do stuff, push back." if the stack were a list, then it's like saying "apply quotation to `init`" where `init` is from haskell, meaning "the subsequence defined as all but the last element."
  .. now we're picturing the stack `( quot exemplar [ push ] curry compose ) new-resizable-like-exemplar` where the parenthesis has its usual meaning: "result of this expression goes here"
. we hit a quotation again. we see that it matches `curry`. we apply it, resulting in: `( quot [ exemplar  push ] compose ) new-resizable-like-exemplar`
. we hit a quotation again. we see that it matches `compose`. we apply it, resulting in: `( [ quot exemplar push ] ) new-resizable-like-exemplar`
. the parenthesis surround a quotation; they've done their job, so we remove them, leaving the result of `collector-as`: `[ quot exemplar push ] new-resizable-like-exemplar`

tacit data structure combinators tend to have definitions with many uses of `curry` & `compose`. the use of `push` is clever: it has stack signature `( elt seq -- )`. by currying _a particular_ sequence with it, we've created a quotation that takes an element and pushes it to a sequence. `push` is purely effectful, which means that we can pass around this "push to sequence" quotation around anywhere without needing to also pass the sequence, _and_ we don't need to consider how to manipulate the stack in order to keep the sequence in the right position, since it's not on the stack at all anymore! the first benefit is accomplishable simply by currying, which can be done in many languages. the second point is a consideration only in stack langs. however, factor is the only language that i know that supports _uncurrying_! `quot first` leaves the sequence on the stack! we can say "'sequence' becomes 'push to sequence', and later becomes 'sequence' again." true, haskell has `uncurry` but it: 1. works only on duples (a structure of only 2 data); 2. the data must be structured in that particular way—as a duple—which is not the natural way to express computations in haskell. fortunately lisp generalizes lists & cons pairs to one structure, and has `apply`, which is similar to `curry`, but even lisp does not support uncurrying, because a currying a function with an argument results in a function, and functions support only application (β-reduction.) however, technically lisp does support uncurrying just like factor does: we can build an argument vector then `cons` a function name to it and `eval` it, while retaining the input arguments. this is, however, inelegant (clunky) in lisp and very unidiomatic! idiomatic lisp uses macros instead of `eval`, and pure rather than mutative operations (whereas factor supports both equally), and even the macros must consider macro hygiene because they use identifiers whereas factor is totally tacit and thus has no analagous concerns. factor is just like lisp but better.

.more practice reading factor

this code is from the `calendar` vocab:

[source,factor]
----
: weekdays-between ( date1 date2 -- n )
    [
        [ swap time- duration>days 5 * ]
        [ [ day-of-week ] bi@ - 2 * ] 2bi - 7 /i 1 +
    ] 2keep day-of-week 6 = [ [ 1 - ] dip ] when day-of-week 0 =
    [ 1 - ] when ;
----

. first we encounter a quotation; thus we look to see of what word the quotation is an argument. here, it's `2keep`. ok, so we're computing over `date1` & `date2` and storing the result under them on the stack for later.
  .. to know what that computation is:
    ... we see a quotation, so we look to see of which word it's an argument. oh! the quotation is followed by another quotation! ok, so we'll see of which word these 2 quotations are arguments. (generally, repeat, collecting quotations as arguments, until a word is encountered.) ah, it's `2bi`.

if you followed that, then you can certainly parse the rest of `weekdays-between`.

== generators

`USE: generators`

installs `return` in a quotation such that the repeatedly.

generators are tuples built on continuations/coroutines. `<generator>` accepts a nullary quotation that loops, but the loop does not actually evaluate completely because sometime within the loop, `yield` is called. `yield` stops evaluation of the quotation like `return` does in many programming languages.

idk how efficient generators are. i know that they're like a non-strict-evaluation version of `make`, and that `make`, while strict-eval, is still slower than looping combinators like `reduce`. one may suggest that virtual sequences are non-strict, but this is not correct: virtual sequences are not computations! they're _representations_! virtual sequences may be used in strict or non-strict computations.

all generator quotations:

* "return" comes in two varieties:
  ** `yield`: return value.
  ** `stop-generator` return the fact that there's nothing to be returned.
* usually have some looping construct
  ** or else just generate intermediate values of a computation

words that run generators: `?next`, `skip`, `take`, `take-all`.

.wrapping `produce` trivially
[source,factor]
----
[ 1337 [ dup 0 > ] [ 2/ dup yield* ] produce ] <generator> 6 take
----

produces the same as `produce 6 head` but non-strictly; only the first 6 values are computed. `produce` does not fully evaluate!

.wrapping `produce` helpfully
[source,factor]
----
: time ( -- s ) now [ hour>> ] [ minute>> ] [ second>> floor ] tri [ "%02d:%02d:%02d" printf ] with-string-writer ; inline
[ [ t ] [ time yield* 4 seconds sleep ] produce ] <generator> 5 take .
----

prints `{ "23:23:54" "23:23:58" "23:24:02" "23:24:06" "23:24:10" }`.

this is easier than `produce` because we don't have to keep a loop counter. it's very probably less efficient, though! of course, an unpredicated reduce is better expressed by `loop`: `[ [ time yield 4 seconds sleep t ] loop ] <generator>`.

.a finite generator: pop off a (resizeable) sequence
[source,factor]
----
[ V{ 1 2 3 4 } [ dup empty? ] [ dup pop yield ] until ]
<generator> take-all
----

produces `{ 4 3 2 1 }`.

.an infinite generator: natural numbers
[source,factor]
----
[ 0 [ [ 1 + ] keep yield t ] loop ] <generator>
----

.generator composition: call one generator to yield values, from within another
[source,factor]
----
[ 0 [ dup 7 = [ [ V{ 1 2 3 4 } [ dup empty? ] [ dup pop yield ] until ] <generator> yield-from ]
              [ [ 1 + ] keep yield ]
              if t ]
    loop ] <generator>
----

`11 take` from this produces { 0 ... 6 4 ... 1 }. `12 take` loops forever! the problem is that i never incremented past 7! the incrementer increments to 7, then the if's true branch is taken, which yields the 4 values, but then hsa no more values to yield, but keeps trying to! the solution is to add an increment operation after `yield-from`:

[source,factor]
----
[ 0 [ dup 7 = [ [ V{ 1 2 3 4 } [ dup empty? ] [ dup pop yield ] until ] <generator> yield-from 1 + ]
              [ [ 1 + ] keep yield ]
              if t ]
    loop ] <generator>
----

`20 take` on this produces `{ 0 1 2 3 4 5 6 4 3 2 1 8 9 10 11 12 13 14 15 16 }`.

`[ next ] [ skip ] [ next ] tri { } 2sequence` on it leaves { 0 2 } on the stack.

== suffix arrays

[source,factor]
----
"cat" "con" "mar"
[ dup SA{ "cats" "catamaran" "concat" "cafe con leche" "mary" } query { } 2sequence ]
tri@ { } 3sequence [ ... ] each
----

prints

----
{ "cat" { "concat" "catamaran" "cats" } }
{ "con" { "cafe con leche" "concat" } }
{ "mar" { "catamaran" "mary" } }
----

== html

=== parsing

[source,factor]
----
USE: http.client
"https://github.com/" http-get nip
----

on this data, ``modern.html``'s `string>html` fails with a malformed html error, whereas ``html.parser``'s `parse-html` succeeds. however, the former supports `string>html` while the latter does not, though i could write such a function easily. anyway, ``modern.html``'s parser seems intolerant of some common varieties of html. this is a shame considering that `modern.html` features words that would be useful for "html in the wild", such as `walk-html` and `find-links`.

i find the internal representation of `modern.html` odd—namely that it includes separate open & close tags, and fields `open` & `close` for "<" & ">", as if they could ever differ. idk why anyone would ever not prefer html as represented by sexps (what would be sequences and assocs in factor, like how it represents json) in racket scheme.

the `html5` vocab is yet unfinished. idk why it was started; html5 validation software already exists, and for parsing, an html soup parser would work.

== http client

firstly, be aware! if you construct a `url` object yourself, you *must* check its validity! to check validity, you cannot merely pretty-print the url! the pretty-printed url may be perfect, but the url object may not be! namely, i've gotten 400 Bad Request errors from servers because the `path` slot of the url did not begin with a slash! thus i presume that the request was e.g. "GET path/to/resource" instead of "GET /path/to/resource"!

=== POST data

see "HTTP client post data" in the factor docs. summary: may be `f`, bytes are sent as-is, strings and assocs are url-encoded (via `assoc>query`), or for the finest specification, pass a `post-data` tuple.

on that note, a few things about the `post-data` tuple:

* `content-encoding` isn't used yet anywhere in the factor codebase. do not confuse with the `content-encoding` slot of a `response` object, which is used in many places.
* despite what docs say, `data` (of `post-data`) cannot be a string in that it cannot be a `string` object; however, encoded bytes works e.g. `"mystring" ascii encode "text/text" f post-data boa`. again, note that `content-encoding` is `f`. it could be `123456` and no one would care. `data` is the post *body*.

the request's `data` slot may be any object that implements generic word `>post-data`.

`write-request` uses `unparse-post-data` which uses `normalize-post-data` which is where we actually see the code that sets `data` from `params` if `params` is set. we also see the—afaik—undocumented ability to put a `pathname` into `data` to send a file as a "measured" stream i.e. a stream that gives a content length.

TIP: words like `append-path` *do not return pathnames!* they return strings, which can be converted to `pathname` objects by the word `<pathname>`.

if `params` is not `f` then it is sent alone, regardless of `data`.

however, apparently bytes just don't work, and strings are not url-encoded. see the following example.

pass data as a string w/o url encoding (same as cURL's -d option):

.simple version
[source,factor]
-----------------------------------------------------
"a=b&c=+*&d=e" URL" http://localhost:3000" http-post*
-----------------------------------------------------

performing `ascii encode` on the string makes `http-post` error, and the string is not url-encoded.

.fully-specified request
[source,factor]
-------------------------------------------------------------------------------------------------
"POST" URL" http://localhost:3000" f "1.1" H{ } clone ! begin filling params for request boa
"a=b&c=+*&d=e" ascii encode f "application/x-www-form-urlencoded" f post-data boa ! post data
f 1 request boa http-request*
-------------------------------------------------------------------------------------------------

pass dict of post params, which factor encodes before sending:

.simple version
[source,factor]
--------------------------------------------------------------------------------
H{ { "a" "b" } { "c" "+*" } { "d" "e" } } URL" http://localhost:3000" http-post*
--------------------------------------------------------------------------------

sends the following request:

-----------------------------------------------
POST / HTTP/1.1
Connection: close
Host: localhost:3000
User-Agent: Factor http.client
content-length: 16
content-type: application/x-www-form-urlencoded

a=b&c=%2B%2A&d=e
-----------------------------------------------

note that it sets the `content-type: application/x-www-form-urlencoded` header sends the params as a url-encodes query string in the body. to accomplish cURL's `-d` behavior, you must manually specify the assoc as a query string, which generally may be not url-encoded. the following does not work because setting the `content-type` header is ignored; apparently the content type is inferred from the post data type (namely a string here) sometime within `http-request*`:

[source,factor]
---------------------------------------------------------------------------
"a=b&c=+*&d=e" URL" http://localhost:3000" <post-request>
"application/x-www-form-urlencoded" "content-type" set-header http-request*
---------------------------------------------------------------------------

the solution is to use `post-data`:

[source,factor]
------------------------------------------------------------------------------------------------------------------------
"a=b&c=+*&d=e" ascii encode f "application/x-www-form-urlencoded" f post-data boa URL" http://localhost:3000" http-post*
------------------------------------------------------------------------------------------------------------------------

note that in this case, simply passing a string does not work! we must pass a bytestring only! whattttt the hell. idk what the supposed utility is of specifying the encoding post-param.

== http servers

the core functionality of a server—that which produces a response from a request—is called a _responder_ in factor. the factor http server basically maps urls to responders—commonly called "routing" in general http server jargon, but called _dispatcing_ in factor. naturally, in code, this is done by a `dispatcher` tuple object.

.a quick note about frameworks

personally, i dislike frameworks on principle, so i haven't learned about them nor will i, so you won't find notes about them here. instead, i'll just discuss the workings of responders, then give server examples. both of actually starting the server, and using dispatchers, are very simple, so i discuss them briefly, mostly by comments inside the code example. however, before doing that, here're a few tips and a small sketch of the templating frameworks:

factor has a web framework called "furnace", for handling forms and various other things, and "chloe", a system for producing xhtml. there's also an alternative to chloe called "fhtml" which is like php server pages for factor: factor expressions are inlined in text; when generating a response, the server evaluates the expressions, replaces the expressions with their output, then responds with the new text. furnace and chloe or fhtml are commonly used together. chloe is contrived but more certain. fhtml is freer and more liable to error. `USE:` `html.templates.<chloe|fhtml>` then view their docs. both implement the `template` interface of the `html.templates` vocab. chloe templates support conversion to responses via `<chloe-content>`. this being said, one can simply do `<fhtml> [ call-template* ] with-string-writer <html-content>`

TIP: if you poke around factor's http vocabs, you'll see `action` a lot. it's a furnace thing. with regard to furnace actions, `display` is the main property for GET requests, analagously: `submit` for POSTs, `replace` for PUTs, `update` for PATCHes; set it to a quotation that takes a request and returns a response.

=== responders

the thing that defines a responder is that it implements the generic word `call-responder*`. this word is where the logic of producing a response from a request goes.

. the dynamic variable `request` is set (along with some redundant, other variables: `url` and `params` (url params)) before `call-responder*` is called, so they're available for use within Implementations of `call-responder*`
  .. load `http.server` then search "HTTP request variables" in the help browser for more info.
. rather than a responder being `( req -- resp )`, it's `( -- )` where we load the request by `request get`, and write the response to `output-stream` (see note about `callable` below)
. the `trivial-responder` tuple is all that you need if you want to write just a custom function from request to response. to actually have the server use your responder, set the global variable `main-responder` to your responder object. 

TIP: if, for whatever reason, you want to define your own class that implements `call-responder*`, then know that `call-responder*`'s `path` argument is given as an array e.g. path `"/hi/there"` is given as `{ "hi" "there" }`.

. the `response` class has obvious fields. `body` is the interesting part, since it's the actual payload. `body` may be a string, or anything else that implements `call-template*` (of the `html-templates` vocab).
  .. `callable` is my preferred type for `body`. of course the `callable` that i use is just an inline quotation; even if i define a word e.g. `: my-responder ( -- ) ... ;`, then i use `[ my-responder ]` as the value for the response's `body`. as i mentioned recently, its effect must be `( -- )`. `callable` is it's so general and has smaller code and runtime complexities/costs than collecting into, then outputting, a string.
. usually i do not directly set the `body` field of a `response`. instead, i pass the body value to `<html-content>` or `<text-content>` (of the `http.server.responses` vocab); both accept a body value, which is directly put into the `response` object
  .. yes, any class that implements `call-template*` may be passed to `<html-content>` or `<text-content>`!
  .. sometimes i may create a `response` myself, but it's usually easier to create one via `<html-content>` or `<text-content>`, then modify whatever few slots i want of the resultant `response`.
. finally, i must pass the `response` to `<trivial-responder>`

=== example servers

.the simplest server

if you want to just generate a response from a request, then this is the simplest, shortest way to do it. this one simply prettyprints the request. to generalize the functionality, replace `.` with some function of the request that prints a response to `output-stream`.

[source,factor]
------------------------------------------------------------------------------
USING: http http.server http.server.responses prettyprint ;
[ request get . ] <text-content> <trivial-responder> main-responder set-global
8080 httpd ! start the server on port 8080
------------------------------------------------------------------------------

run `stop-server: ( server -- )` whenever you're ready to shutdown & cleanup the server. see the `io.servers` vocab for more useful words, namely `stop-all-servers`, which is useful if you've accidentally lost the server object from the stack (or if you wanted to avoid ever needing to retain it on the stack!)

.server with pages at /headers and /query

here i introduce dispatchers; it routes /headers and /query to the associated quotations.

[source,factor]
-------------------------------------------------------------------------------------------------------------------------------
USING: http http.server http.server.responses http.server.dispatchers io.servers sequences.extras ;

: resp ( quot -- responder ) <text-content> <trivial-responder> ; inline
: print-dict ( dict -- ) [ ": " glue print ] assoc-each ; inline

<dispatcher> ! create a new dispatcher object

! ( req -- resp )                             ! route   ! add responder to dispatcher, leaving updated dispatcher on stack
[ request get header>>      print-dict ] resp "headers" add-responder
[ request get url>> query>> print-dict ] resp "query"   add-responder

! constant response string instead of quotation. works b/c string implements call-template*
"c'mon bro, ain't no page there!"        resp >>default ! optional. if omitted, then a generic 404 is given

main-responder set-global 8080 httpd
-------------------------------------------------------------------------------------------------------------------------------

the responder paths' are split on slashes, and the first one is tested against the given url, so if you want to serve local files, you'll need a special initial path segment to specify that.

[source,factor]
-----------------------------------------------------------------------------
! localhost/fs/path/to/file will respond with the file at myroot/path/to/file
<dispatcher> "myroot" <static> "fs" add-responder
-----------------------------------------------------------------------------

if you're wondering how it is that it doesn't try to find the file at `/fs/path/to/file`, let me explain: the `path` input supplied to `file-responder`'s `call-responder*` method is a slice that doesn't include the head; this is so because the `path` parameter is supplied by the `dispatcher` class' `call-responder*` method which employs `find-responder`, which employs `rest-slice`. so this changes the `path` given to `call-responder*`; however, you'll find that `request get url>> path>>` is the full path including the leading `/fs`. the request object is never modified.

this is by far the most elegant, simple, capable server design that i've seen. by this dispatch design, the url's path's 1st segment is the function, the rest of the path segments are the command line arguments, and parameters are given by the url's query. e.g. the command line `git commit -am 'my commit'` could be represented by the url `/git/commit?a&m=my20%commit`.

NOTE: if you're running the server from the listener, and your http client gives a "server reset connection" error, then that's because there's a dependent vocab that isn't compliing. e.g. if my `my-server` vocab depends on `my-utils`, then if i edit `my-utils` such that it fails to compile, then i get the "server reset connection" error.

* `httpd` is a convenience word for `<http-server> 8080 >>insecure f >>secure start-server` creates then starts a new http server on port 8080, leaving the http-server object on the stack. you *must* specify both `insecure` and `secure`; otherwise the server will fail to run, citing error "permission denied."
* i suppose that `wait-for-server` is useful only if we want to block until the server terminates itself, or receives such a signal e.g. by someone requesting URL path `/stop` of the server.
* `http-server` subclasses `threaded-server`, which has attrs `max-connections`, `encoding`, `timeout`.

TIP: the global `development?` variable controls whether to respond with either a trace or a code 500 if your responder has an error

.simple calculator app
[source,factor]
-------------------------------------------------------------------------------------
USING: assocs http.server http.server.responses io io.servers
kernel math.parser multiline namespaces prettyprint sequences ;
IN: calcy
: page ( -- )
[[ <!doctype html>
<html>
  <head>
    <title>test</title>
    <style>
      label{padding-right:0.5em}
      form{display:flex;flex-direction:column}
      form>*{padding-bottom:1em}
    </style>
  </head>
  <body>
    <form action="/" method="get">
      <div><label for="x">number 1:</label><input type="text" name="x"></input></div>
      <div><label for="y">number 2:</label><input type="text" name="y"></input></div>
      <input type="submit" value="calc"></input>
    </form>
  <p id="result">]] print
  params get values
  [ string>number ] map sift dup empty?
  [ drop "NOOOOOO! bad inputs! >:(" ]
  [ sum >dec ] if print
  [[ </p>
  </body>
</html>]] print ;

: start ( -- )
  [ page ] <html-content> <trivial-responder> main-responder set-global
  t development? set-global
  8080 httpd drop ;
-------------------------------------------------------------------------------------

i literally threw this calculator app together in 5 minutes, including the time that i needed to consult w3schools for the form element 'cause i haven't written html or used forms in a long while. so it goes to show that you sure don't need furnace or other frameworks. sure some people will cringe at it, and i cringe back at them when they use frameworks. it's simplicity vs modularity, hackiness vs oop.

=== other http-related vocabs

.useful vocabs
[options="header"]
|==================================================================================
| vocab                               | description
| `io.servers`                        | tcp/ip. http is a protocol built atop tcp, so `io.server`'s words can be useful for http servers, too.
| `http`                              | provides the `request` & `response` classes
| `http.server`                       | http server
| `http.server.static`                | ftp-like behavior (create responder that translates url to local pathname to serve files)
| `http2.server`                      | subclass of `http-server`. handles http2 clients.
| `logging.server`                    | logging
|==================================================================================

there are also vocabs `http.server.cgi`, `cgi`, and `fastcgi`. cgi abbreviates "common gateway interface"—a standard for invoking subprocesses to generate responses from requests. i don't know why factor makes cgi available; i imagine that factor's server would be used for cgi with a more robust and efficient reverse proxy such as link:https://sqlite.org/althttpd/doc/trunk/althttpd.md[althttpd], link:https://redmine.lighttpd.net/projects/lighttpd/wiki/Docs_Configuration[lighttpd], or link:https://github.com/caddyserver/caddy[caddy]. and so i have not read about cgi in factor.

.convenience vocabs
[options="header"]
|========================================================
| vocab                     | description
| `http.server.responses`   | response bodies
|========================================================

== websockets

TODO: see `handle-discord-websocket` in `programming/factor/extra/discord/discord.factor` for example ws client usage.

websockets are made in the design of unix sockets; they're lightweight, full-duplex realtime communication. once established, the connection remains open and both client & server may send or receive any bytes at any time. the websocket protocol is extremely small & simple.

though websockets are not http, `ws:` & `wss:` use ports 80 & 443 respectively, and link:https://en.wikipedia.org/wiki/WebSocket#Opening_handshake[http is used to establish websocket connections]; the websocket server responds with http code 101 ("switching protocols"). this is why factor's `http.websockets` vocab does not have any words for establishing nor managing connections.

as usual in factor, there are no websockets handles to pass around; a connection is opened which sets `output-stream` & `input-stream`. in fact, the whole websockets vocabulary is just `add-websocket-upgrade-headers ( request -- request )`, constants, and words for de/encoding bytes per the websockets protocol.

=== ws client

words on the right are defined in terms of those on the left. words that you should use are in bold:

* `get-read-payload-length` > `read-payload` > *`read-websocket`* > *`read-websocket-loop`*
* `send-websocket-bytes` > `send-websocket-text` > *`send-masked-message`* & `send-unmasked-message`

btw, the `final?` parameter of `send-websocket-bytes` seems to refer to whether the message is fragmented.

so *it seems that* generally you should use only `read-websocket[-loop]`, and inside its quotation, `send-masked-message`. `read-websocket-loop` loops until either you say to stop or the ws protocol or connection mandates stopping. in `read-websocket ( -- obj opcode loop? )`, `obj` is the payload.

NOTE: according to wikipedia, a client must mask all frames sent to the server, and the server must not mask any of its frames; so i suppose that `send-unmasked-message` exists for implementing websocket servers.

we've yet to turn a url into a context in which the input & output streams are set so that we can actually use these websocket words! enter the common `http.client` vocab, which already has builtin, automatic support for websockets! observe:

[source,factor]
--------------------------------------------------------------
"wss://delayed.polygon.io/stocks" <get-request> ...
T{ request
    { method "GET" }
    { url URL" https://delayed.polygon.io/stocks" }
    { proxy-url URL" " }
    { version "1.1" }
    { header
        H{
            {
                "Sec-WebSocket-Extensions"
                "permessage-deflate; client_max_window_bits"
            }
            { "Host" "delayed.polygon.io" }
            { "Upgrade" "websocket" }
            { "Sec-WebSocket-Version" "13" }
            { "Cache-Control" "no-cache" }
            { "Pragma" "no-cache" }
            { "Sec-WebSocket-Key" "01K2CcLS2FBLWYkMzFm/eQ==" }
            { "User-Agent" "Factor http.client" }
            { "Connection" "Upgrade" }
        }
    }
    { cookies V{ } }
    { redirects 10 }
}
--------------------------------------------------------------

what we'll be doing differently from normal http requests is that rather than reading a whole body into a byte array at once, we must leave the connection open and handle data as it streams-in. this is accomplished by the lowest-level http word, `do-http-request`. looking at its definition, it's unclear how to use it, so let's see how a familiar word, `http-request*`, is defined in terms of it:

[source,factor]
--------------------------------------------------------------
: http-request* ( request -- response data )
    BV{ } clone [ '[ _ push-all ] do-http-request ] keep
    B{ } like over content-encoding>> decode [ >>body ] keep ;
--------------------------------------------------------------

for http requests, this executes as follows:

. `BV{ }` on stack
. `do-http-request` pushes all chunks into said vector and returns a `response`. the `BV{ }` is kept on the stack.
. convert `BV{ }` into `B{ }`
. decode it per the response's `content-encoding`
. set the response's `body` to that decoded string, keeping the `response`

for ws, it executes as follows:

. `BV{ }` on stack
. `do-http-request` pushes all of nothing into said vector and returns a `duplex-stream`. the `BV{ }` is kept on the stack.
. convert `BV{ }` into `B{ }`
. execution halts at `content-encoding>>` b/c duplex streams don't support that.

so clearly `http-request*` is inappropriate for ws, but `do-http-request` is still instrumental. if you carefully observe the definition of `do-http-request`, you'll see that the `( chunk -- )` quotation is not even used in the ws case (to be precise, when `upgrade-to-websocket?` returns `t`), so naturally for ws we'll do `[ drop ] do-http-request` as a dummy quotation that still stack-checks.

it pushes all into a byte vector then converts it into a decoded byte array. this implies that `chunk` is bytes. generally you must assume that chunk is not a fully parsable string, so you must incrementally parse & act on bytes as they come in. if your ws server sends json, then this is fine, since `read-json` blocks until enough input comes in to return a well-formed object.

TIP: if you want to read ``do-http-request``'s code, then read its source code, not the parsed definition as shown in the help browser, because it's defined in terms of a fried quotation.

.example
[source,factor]
----------------------------------------------------------
USING: http.client http.websockets io.streams.duplex ;
"wss://my-endpoint.com/or/whatever" <get-request>
[ drop ] do-http-request ! returns duplex stream
[ binary decode-input ! (1)
  binary encode-output
  read-websocket      ! (2)
  "idk some stuff" send-masked-message
  ! (3)
  [ [ [ . ] bi@ ] with-global ] ! print to stdout, not web socket
  now 5 minutes time+ [ now <=> +gt+ = ] curry compose
  read-websocket-loop
  f t 8 t send-websocket-bytes ! formally close connection
] with-stream
----------------------------------------------------------

. used in `read-response-body` to, of course, read the http body which may be arbitrary binary data. in `do-http-request`, the duplex stream is created via `<request-socket>`, which uses `ascii` for both input & output. ascii is just the binary encoding but, if it recieves a byte whose value is greater than 127, then it throws an error and/or replaces the byte with the unicode replacement character, code point 65533, which is ironic, seeing as that's a 2-byte value. by the way, `latin1` is basically the same as `ascii` but its threshold is 255, which is odd considering that that's the max byte value. see their implementations of `decode-char` for details.
  .. likewise, we need to be able to send binary over websockets.
. commonly, a websocket server will require authentication or provide metadata which you'll read via `read-websocket` then account for, before beginning the main streaming loop.
. this simple loop just outputs the object and opcode to a file and reads from the websocket for no more than 5 minutes.

as it's currently written, `read-websocket-loop` executes the quotation that you pass regardless of whether `read-websocket` returns `t`, though only if it _and_ your quotation return `t` does it continue to loop. i'd've thought that it would execute your quotation only if `read-websocket` returned `t`. this leads to some odd patterns such as you needing to check the opcode even though `read-websocket` has kinda already handled it e.g. i must check whether a payload is of opcode 8 ("close connection") or not, so that i can handle the payload differently. however, because it's opcode 8, i know to not continue the loop, so naturally i'd handle that myself, but `read-websocket` has already guaranteed that the loop will stop here anyway! so the awkwardness is the redundant need for both `read-websocket` and my argument quotation to `read-websocket-loop` to `case` on the opcode. this being said, it's not too bad in practice because `read-websocket` reads all payloads (except maybe for opcode 9, "ping") into a utf8-encoded string, so the argument quotation to `read-websocket-loop` really only needs an `if` clause to check whether it's opcode 8 or not. btw i don't see how `read-websocket` handles closing correctly; i see some ``be>``'s in it and `read-payload`, but no `utf8 decode`; the payload is, if present, supposed to be a 2-byte/be reason code, followed by a utf8-encoded string.

== websocket (server)

TODO: link:https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers[implement]

== filesystem

vocab `io.files.info` provides word `file-info` which returns a `file-info-tuple` tuple (yes, it has "tuple" in the name) which has attributes: `type`, `size`, `permissions`, `created`, `modified`, and `accessed`, and `size-on-disk`, if that's ever useful.

== statistics

* `USE: math.statistics`
* `math.statistics.running` enables efficient access to the current 1st 4 statistical moments, count, range, & sum, and simple computations thereof (e.g. variance) at any given time during traversal of data point inputs

.about quantiles

from the `math.statistics` module source code:

[source,factor]
-----------------------------------------------------------------------------------------------
! quantile can be any n-tile. quartile is n = 4, percentile is n = 100
! a,b,c,d parameters, N - number of samples, q is quantile (1/2 for median, 1/4 for 1st quartile)
! <https://mathworld.wolfram.com/Quantile.html>
-----------------------------------------------------------------------------------------------

so the various 8 builtin quantile functions (notice that `quantile2` is missing!) all do the same thing, but are different implementations, giving slightly different values due to how they round, interpolate, or resolve real-valued indices. only `quantile1` & `quantile3` always return values from the input distribution.

therefore if you simply want the element at the xth percentile: `percentile ( seq p -- elt ) 1array quantile1 first` *where `p` is a rational*.

== database

NOTE: i have no documentation on reading from result sets which are generated by manual sql queries such as `simple-statament`'s. `query-map` & `query-each` should make it often easy. i have nothing on `+foreign-id+ either. in fact, does factor's db vocab do _any_ table joining?!

TODO: revise all notes where the priamry key is manually specified! despite the factor docs for `+primary-key+`, multiple columns may have primary keys! observe:

[source,factor]
----------------------------------------------
TUPLE: person name age phone ;
person "ppl"
{ { "name"  "namae" TEXT    +db-assigned-id+ }
  { "age"   "sai"   INTEGER +db-assigned-id+ }
  { "phone" "denwa" TEXT    }
} define-persistent

person create-table ! CREATE TABLE ppl(namae text, sai integer, denwa text, primary key(namae,sai));
----------------------------------------------

if you want to never have your primary keys auto-generated, then you can still use `+db-assigned-id+` or `+random-id+`, but just never insert a tuple whose primary key fields are `f`; then you can use `ensure-table` instead of manually specifying that sql statement for `sql-command`. 

.foreign key example (adapted from `resource:basis/db/tuples/tuples-tests.factor`)
[source,factor]
---------------
TUPLE: artist name ;
artist "artists" { { name "name" TEXT +db-assigned-id+ } } define-persistent

TUPLE: song title artist album ;
song "songs"
{ { title  "title"  TEXT }
  { artist "artist" TEXT { +foreign-id+
                           artist ! tuple class
                           "name" ! sql attribute name
                         } }
  { album  "album"  TEXT }
} define-persistent
---------------

remember: if something's undocumented, then read the source code! even though definitions showed in the help viewer can be helpful, they don't have the exact same form as source; e.g. fried quotations don't appear in their original form, and neither comments nor indentation are preserved. additionally, having multiple definitons all strung together, defined in order (because, like in c, factor requires that a word be defined before it's used in another word's definition).

query tuple slots accept values for which `where` (of the `db.queries` vocab) has a method. you can read the method definitions to see how the data are transformed into query clauses.

[source,factor]
----
USING: db.sqlite db db.tuples db.types ;
[ f                         ! any value
  { "vietnamese" "bingsu" } ! any of these multiple values. the `sequence` instance of `where`
  NULL                      ! attr is null
  8 [a,inf]                 ! attr>=8. the `interval` instance of `where`. you can see inside the method's definition that infinite intervals are handled specially.
  mytuple boa
  >query "genre" >>order select-tuples . ] with-my-db
----

TODO: document `person new "(202)%" likeexp boa >>phone [ select-tuples ] with-my-db .`

you can view a representation of the select, update, or delete statement, computed of the tuple, that will be passed to the db by using `<select-by-slots-statement>`, `<update-tuple-statement>`, or `<delete-tuples-statement>`.

if your schema has a primary key, then the update statement uses the primary key exclusively in the `where` clause; only the non-pk attributes may be updated. if you want to have a single-attribute table which acts only as a set, then you may have a primary key (which is nice b/c it's indexed), but you must manually generate a statement e.g: 

[source,factor]
-------------------------------------------------------------
TUPLE: x a ;
x "x" { { "a" "a" TEXT +db-assigned-id+ } } define-persistent
[ x ensure-table ] with-my-db
-------------------------------------------------------------

now i want to prepare a statement for "insert or ignore into x values(('cat'))". factor's `db` vocab does not provide a mechanism for "insert or ignore" statements, so we'll have to make one ourselves, by modifying a normal insert statement. `[ x <insert-user-assigned-statement> . ] with-my-db` gives a description for the `sqlite-statement` tuple, which consists of a template string stored in `sql`, and `in-params` to populate that string, after they're automatically escaped. as expected, `out-params` is `{ }` because insert statements don't have output. thus i execute:

.custom "insert or ignore" statement
[source,factor]
-------------------------------------------------------------
: put-x ( x -- )
  x db-connection get insert-statements>>
  [ drop
    "insert or ignore into x(a) values(:a);"
    x "a" "a" TEXT { +user-assigned-id+ } <sql-spec> 1array
    { } <prepared-statement> ] cache
  [ bind-tuple ] keep execute-statement ;

"SAMPLE" x boa [ put-x ] with-my-db
-------------------------------------------------------------

this code was adapted from `insert-user-assigned-statement`. note:

* i specify the `x` class (which is a key in the db connection's `insert-statements`) manually, since i'm also manually specifying an insert statement specific to the `x(a)` schema.
* the prepared `statement` is disposable. using `cache` ensures that we create an insert statement only once per class per invocation of `with-my-db`
  ** like `insert-user-assigned-statement`, i still use `cache` for consistency, but my quotation starts with `drop` because whereas `<insert-user-assigned-statement>` takes a class as input, i'm specifying a statement literally, not as a function of any input.
* omitting `[ bind-tuple ] keep` inserts a blank into the db
* if you don't store the statement in the db connection's `insert-statements` slot, then at the end of `with-my-db`, when `dispose` is called on the `sqlite-db-connection`, then "the database file is locked" error is thrown. this error is itself erroneous; the actual issue is not the db being locked, but rather something else; basically we tried some manual, low-level stuff, and did it incorrectly.

`sql-command` just generates a `<simple-statement>`, which exactly equals a `<prepared-statement>`, then calls `[ execute-statement ] with-disposal`. if a statement will be called multiple times, then it's more efficient to prepare it once, then execute that prepared statement many times. at the core of it all is `execute-statement*`—namely its method for `object` (and namely when that object is a `statement`), and the core of _that_ is `query-results`.

(because sql's syntax is the same for selecting from tables as it is from views) you can select from views just as well:

[source,sql]
----
CREATE TABLE x(a,b);
CREATE TABLE y(b,c);
CREATE VIEW z(a,b,c) as select * from x natural join y;
insert into x values(1,10),(35,50);
insert into y values(10,103),(50,90),(1024,2048);
----

[source,factor]
----
: wdb ( q -- ) "~/test.db" <sqlite-db> swap with-db ; inline
TUPLE: z a b c ;
z "z" { { "a" "a" INTEGER } { "b" "b" INTEGER } { "c" "c" INTEGER } } define-persistent
[ z new select-tuples ] wdb .
----

----
{ T{ z { a 1  } { b 10 } { c 103 } }
  T{ z { a 35 } { b 50 } { c 90  } } }
----

you can't create intervals of timestamps, which is bad design, since timestamps implement `<=>`; for intervals to use only numbers instead of comparables is limiting without benefit (afaict). the only good work-around is to get all results then filter them via `filter`. `sql-query` not only returns an array of strings. even `[ mytuple slots>tuple ] map` isn't smart enough to parse the right types.

NOTE: if some attribute values may be greater than 2^32^, then use either `SIGNED-BIG-INTEGER` or `UNSIGNED-BIG-INTEGER`; you'll get incorrect values if you use `INTEGER` and try to select a tuple whose attribute has too large a value.

of course, we can instance `where` as we like e.g:

[source,factor]
----
USE: strings.parser
TUPLE: like str ;
M: like where str>> over column-name>> 0% " like " 0% bind# ;
SYNTAX: LIKE" lexer get skip-blank parse-string like boa suffix! ;
[ f f f f 8 LIKE" %belt%" NULL entry boa select-tuples . ] with-my-db
----

the code that really makes `select-tuples` work is the `<select-by-slots-statement>` hook. it specifies how a tuple is converted into a sqlite statement. we don't need to bother understanding `make-query*`; just do `>query` to make your usual query tuple into a `query`, then set its attributes e.g. `[ my-query-tuple >query "column4" >>group select-tuples . ] with-my-db`.

TODO: how to use binds? e.g. how can i "insert or ignore into tbl values($1,$2,$3)" with a tuple `{ "bats" now f }`?

=== `NULL` values

[source,factor]
----
: tdb ( q -- ) "test.db" <sqlite-db> swap with-db ; inline
TUPLE: tt f1 f2 ;
tt "t" { { "f1" "f1" INTEGER }
         { "f2" "f2" INTEGER } } define-persistent
[ 10 NULL tt boa insert-tuple
  100 200 tt boa insert-tuple
  tt new select-tuples .
] tdb
----

prints `{ T{ tt { f1 10 } } T{ tt { f1 100 } { f2 200 } } }`. thus `tt new select-tuples [ f1>> ] map` returns `{ f 200 }`.

=== updating tuples

the `+primary-key+` modifier cannot be specified in `define-persistent` e.g.:

[source,factor]
----
TUPLE: mytuple a b c ;
mytuple "mytable" { { "a" "a" INTEGER +primary-key+ } { "b" "b" VARCHAR } { "c" "c" INTEGER } } define-persistent
----

is incorrect despite no error being thrown. the trouble is that +primary-key+ is a union class, not a singleton class. as such, it represents a set of things. the way that `define-persistent` works is that any primary key—any one of `+primary-key+`'s singleton classes—may be specified in any one column e.g. `mytuple "mytable" { { "a" "a" INTEGER +user-defined-id+ } { "b" "b" VARCHAR } { "c" "c" INTEGER } } define-persistent` would be correct.

anyway, incorrectly using `+primary-key+` allows `insert-tuples` to work as expected: trying to insert a tuple with an extant primary key appropriately raises the "constraint violation" error. however, `update-tuple` fails with a strange error: "incomplete input."

aside: as it turns-out, this is an error provided by sqlite. the sqlite c library sets an error pointer to that string, which is then read by factor's sqlite alien binding. searching the net for "sqlite incomplete input" mostly shows people with syntax errors trying to execute sql statements. understanding why factor is throwing the error for such a simple case clearly requires in-depth knowledge of its db vocab, potentially the sqlite-specific part.

also, unlike the sql `UPDATE` statement itself, `update-tuples` requires a primary key to be set.

[source,factor]
----
[ "create table if not exists mytable(a integer, b text, c integer, primary key (a,b))" sql-command
  T{ T f 10 "hi" 70 } insert-tuple
  T{ T f 10 "hi" 700 } update-tuple
] with-my-db
----

if you try updating a tuple whose primary key does not exist, e.g. `T{ T f 20 "hi" 700 } update-tuple`, then nothing happens.

.the three primary key modifiers

`+user-assigned-id+`:: you created a table with a primary key by some method other than `db.tuples`, and you're specifying to `db.tuples` that your table has a primary key. you may specify `+user-assigned-id+` in any one column; it doesn't matter which.
`+db-assigned-id+`:: if your tuple has a `f` in the primary key slot(s) then the sql db chooses a value
`+random-id+`:: if your tuple has a `f` in the primary key slot(s) then factor chooses an arbitrary value

also, apparently you can totally do the following in sqlite:

[source,sql]
----
create table dat(x text, y integer, w text, primary key (x,y));
insert into dat values('HOLLA',NULL,NULL);
insert into dat values('HOLLA',NULL,NULL);
insert into dat(x) values(null);
select * from dat;
┌───────┬───┬───┐
│   x   │ y │ w │
├───────┼───┼───┤
│ HOLLA │   │   │
│ HOLLA │   │   │
└───────┴───┴───┘
----

this surprises me for two reasons:

. `y`, despite being part of the primary key, is allowed to be null
. the same (x,y) are allowed, when i guess _any_ one or more of the primary key values is null?

but then

[source,sql]
----
insert into dat values('cat',10,'bat');
insert into dat values('cat',10,null);
----

but _now_ we get a constraint violation error! anyway, weird.

NOTE: on one occasion, `update-tuple` simply would not update the last column, which i'd created by using `alter table ... add column`. i have no idea why, and i could not recreate the behavior in a fresh test db.

== audio

uses OpenAL/FFI. supports raw pcm, vorbis, aiff, wav. you can also programatically generate pcm. 3D audio such as one would desire for games is built-in. if you are only concerned with playing audio such as a music player does, then ignore the spatial audio tuples and assume their default values.

=== tuples

* `audio`: represents pcm. fields: `channels` (e.g. 2 for stereo), `sample-bits` (e.g. 16), `sample-rate` (e.g. 48000), `size` (length of data) `data`.
* `audio-engine`: like a server responsible for playing audio clips. unless you want to specify an output device yourself, you'll create an audio engine by `f n <audio-engine>` where `n` is the number of tracks that the engine may need to play simultaneously.
* `audio-clip`: actual audio data. it's an opaque type; don't try to construct it yourself nor should you be concerned with its attributes.
  ** `static-audio-clip`: audio which can be loaded at once, such as a song file.
  ** `streaming-audio-clip`: audio generated dynamically (in realtime), such as procedurally-generated audio
* spatial audio:
  ** `audio-listener`: represents volume and location for 3D spatial audio. fields: `position` (a 3D vector), `gain` (volume), `velocity` (for doppler effect; useful for games or simulations), `orientation` (see below; just use the default).
  ** `audio-orientation-state`: 3D, not necessarily normalized vectors that represent the user's position for 3D spatial audio. fields: `forward` `up`. idk why one would ever not use the default unit vectors.
  ** `audio-source`: firstly, "source" does *not* mean "i/o source!" "source" refers to spatial audio. a trivial implementation of the _audio source protocol_—a collection of generic words which control spatial audio: those implemented trivially by `audio-listener`: `position`, `gain`, `velocity`; and the rest: `audio-relative?` (whether velocity & position are coordinates relative to listener vs absolute coordinates), `audio-distance` (distance below which a listener hears a sound at full volume), `audio-rolloff` (degree to which audio lessens as listener is away from the source).

.`audio` vs `audio-clip`

these are two different types! `<static-audio-clip>` & `<streaming-audio-clip>` convert ``audio``s into ``audio-clip``s, which are then actually usable by the `play-clip` &al words. *remember that `start-audio` does not start playing audio objects!* `audio` objects cannot be played; only ``audio-clip``s can be played! i suppose that the `audio` tuple is revealed to the coder only so that they can implement their own audio types, e.g. enabling loading ape's or mp3's. for the most part, you will not concern the `audio` type! furthermore, i assume that it's only for technical reasons that it's ever possible to produce an `audio` object rather than directly producing the actually usable corresponding `audio-clip` object!

`<static-audio-clip>` & `<streaming-audio-clip>` both take an engine [read: audio server] and source [read: location].

where they're different:

[options="header"]
|======================================================
| type      | 3rd param       | 4th param
| static    | `audio` object  | whether to loop forever
| streaming | audio generator | number of buffers
|======================================================

.wav & aiff vs vorbis

wav & aiff are static audio. vorbis is a stream. thus you load wav or aiff objects as ``audio``s by `read-audio` then convert them into ``audio-clip``s by `<static-audio-clip>`. however, the ogg/vorbis loader reads into a stream, which implements an audio generator, even if you're reading from a file! idk why it's designed like that. `<vorbis-stream>` reads from a binary input stream, in case you really are streaming some vorbis-encoded audio. for files, use `read-vorbis-stream` then convert into an audio clip by `<streaming-audio-clip>`.

=== words

engine:

* `start-audio` (a convenient version of `start-audio*`) starts [read: enables] an audio engine. it must be started before any audio can be played.
* `update-audio` applies any new values of spatial audio dynamic variables and refills `streaming-audio-clip` buffers. you'll call `update-audio` only if you used `start-audio*`, not `start-audio`.
* `stop-audio` stops an audio engine. i do not know why someone would want to do this.

the clip words are self-explanatory.

=== spatial audio

the audio engine automatically reads dynamic spatial audio variables each time that it updates, so just update your spatial audio attributes and they're automatically practically immediately reflected in the audio that's playing.

`<streaming-audio-clip>` & `<static-audio-clip>` return `audio-clip` objects. aside from calling `play-clip` &al on them, you can access their `source` attributes then update their source values to change the volume, position, &al.

=== loading & playing audio

* `read-audio` (of the `audio.loader`) vocabulary is the easy way to load wav or aiff audio from filepaths. examine dynamic variable `audio-types` to see exactly which file extensions it supports and which words it uses to load each type of audio file.
* `start-audio*` is useful only if you want to synchronize the audio with some other timer e.g. a game loop: updating the audio buffer in sync with updating game world state and painting to the screen.

.simple example

just load & play some audio. no spatial audio nor manual timers or anything:

[source,factor]
----
USING: audio.engine audio.vorbis destructors ;
f 1 <audio-engine> dup                                          ! engine, which i've started. dup again to retain on the stack for disposal later
dup start-audio                                                 ! gotta start the engine before doing anything else!
audio-source new                                                ! default audio source, because we aren't using spatial audio
"/path/to/song.ogg"
stream-buffer-size                                              ! a constant defined to 4096. guess that's a good buffer size.
read-vorbis-stream                                              ! vorbis, so it's an audio generator
3                                                               ! (1)
play-streaming-audio-clip                                       ! convert the audio generator into a `streaming-audio-clip` then play it
drop                                                            ! drop the audio clip. the engine remains on the stack. i can call dispose on it before or after the song is done playing
----

note (1): i don't know how to determine in advance how many buffers to use. the audio didn't play with 1 buffer, and i got really confused about why it wasn't playing, since i was just starting-out trying to use the audio vocab. at 2 buffers just a second or so of the song played before i got silence. 3 worked fine. what also worked was using 2 with doubling the buffer size.

.the audio.engine.test code, annotated
[source,factor]
----
! Copyright (C) 2009 Joe Groff.
! See https://factorcode.org/license.txt for BSD license.
USING: accessors alien.c-types audio.engine audio.loader
calendar destructors io kernel math math.functions math.vectors
random ranges sequences specialized-arrays timers ;
SPECIALIZED-ARRAY: short
IN: audio.engine.test

TUPLE: noise-generator ;

M: noise-generator generator-audio-format
    drop 1 16 8000 ;
M: noise-generator generate-audio
    drop
    4096 [ -4096 4096 [a..b] random ] short-array{ } replicate-as
    8192 ;
M: noise-generator dispose
    drop ;

CONSTANT: location1 T{ audio-source f {  1.0 0.0 0.0 } 1.0 { 0.0 0.0 0.0 } f }
CONSTANT: location2 T{ audio-source f { -1.0 0.0 0.0 } 1.0 { 0.0 0.0 0.0 } f }
CONSTANT: location3 T{ audio-source f {  0.0 0.0 0.0 } 1.0 { 0.0 0.0 0.0 } f }

:: audio-engine-test ( -- )
    ! read data from files into `audio` objects
    "vocab:audio/engine/test/loop.aiff" read-audio :> loop-sound
    "vocab:audio/engine/test/once.wav" read-audio :> once-sound
    0 :> i                       ! ! time variable. used later to vary locations with time.
    f 4 <audio-engine> :> engine ! an engine which can play up to 4 clips simultaneously
    engine start-audio*          ! start the engine before doing anything else!

    ! convert the audio objects into clips
    engine location1 loop-sound          t play-static-audio-clip    :> loop-clip
    engine location2 noise-generator new 2 play-streaming-audio-clip :> noise-clip

    [ ! every 20ms: update spatial locations then play audio
        i 1 + i! ! increment time variable
        ! compute new locations as functions of time
        i 0.05 * [ sin ] [ cos ] bi :> ( s c )
        loop-clip  source>> { c 0.0 s }          >>position drop
        noise-clip source>> { c 0.0 s } -2.0 v*n >>position drop

        i 50 mod 0 = [ engine location3 once-sound f play-static-audio-clip drop ] when

        engine update-audio ! we must call update-audio b/c we used start-audio*, not start-audio
    ] 20 milliseconds every :> timer
    "Press Enter to stop the test." print
    readln drop
    timer stop-timer
    engine dispose ; ! always dispose of your engine to avoid memory leaks!
----

TODO: refactor to not use locals (so that there're fewer words, which makes groking the code easier when the user is new to the vocab; e.g. one may easily mistake "engine" for a word definition rather than a local), and use `with-dispose`, since calling `dispose` is generally bad practice.

if you don't dispose your engine, then not only is it a memory leak, but also it'll remain in your operating system's list of audio streams (as seen in e.g. pavucontrol on linux, or, in windows, probably in the list of applications with sound streams), polluting that list.

== efficiency/tuning

aspects: speed, storage. you can use efficient factor code, or interface with an efficient `alien` [ffi] library. if you must write the code, then you'll likely prefer a stack/cat lang, so writing c, fortran, or asm would be kinda uncomfortable.

* you can write assembly inline in factor via `alien-assembly`

=== specifying how data are stored

* firstly, you may read the factor doc, "manual memory management".
* if you know your data types & sizes (e.g. u32, i64) then it's more efficient to use:
  ** `SPECIALIZED-ARRAY:` instead of a factor `array`
  ** `STRUCT:` instead of `TUPLE:`
  ** `SPECIALIZED-ARRAY:` instead of `TUPLE-ARRAY:`.
* there's `SPECIALIZED-VECTOR:`, too. creating a specialized vector for a given type implicitly creates a specialized array for that type, too.

tuple( arrays) are better when you either:

. store factor types in tuple slots, e.g. `timestamp`, `bignum`
. store potentially multiple types in any given slot

for c ffi: structs (`classes.struct` vocab), byte arrays, specialized arrays or vectors. these all are unboxed machine values. a non-ffi-compatible, but more general structure, is tuple arrays: effectively arrays of tuples, but stored _unpacked_ (TODO: clarify exactly what this means). you don't have to type the tuple's values, and the types don't have to be c types e.g. you can store factor ``timestamp``s in tuple slots.

TIP: the types that can be used for specialized arrays' are c types: primitve types given in `alien.c-types` vocabulary's documentation (see the factor doc, "Primitive C Types"), or struct types, which are made of those primitves. many of the words are undocumented and seem redundant, e.g. `u16`, which i can only guess is equivalent to `ushort`. then there are endian-aware types (`alien.endian` vocab) if you must use a particular endianness, such as network protocols.

.array of structs (AoS) vs struct of arrays (SoA)

depending on your coding situation and favored coding style, you might want to store your code a la apl, as vectors; instead of a struct each of whose fields stores one value (aka an "atom" or "scalar"), have it represent an array of values. if you have multiple variables each of which represents an array, then it's just called "array programming". if, however, you group multiple variables together, e.g. by putting them together as fields of a struct, then this representation is called a "struct of arrays" or "SoA", as opposed to an array of structs, "AoS". in apl it's called "inverted tables". or it can be described as a _column-major_ db table. this design is inherently more memory- & cpu-performant, regardless of programming language / runtime/vm, and, being of the array programming paradigm, enables all the wonders of that paradigm, provided that your language supports it.

NOTE: factor structs don't support specialized array fields. rather, they do, but only in a roundabout way if you type them as `void*`. however, you can store them as a tuple, which is just as well, since the actual efficiency is in using specialized arrays, and we're just storing pointers to them anyway.

instead of `i arr nth [ a>> ] [ b>> ] bi`, you'd do `a b i [ swap nth ] curry bi@` (or w/e, depending on where your data are on the stack). really i like the apl method better. aside from enabling the usual apl magic (iykyk), it's just nicer e.g. if you've a struct with 5 attrs then you do `a b c d e` [ ... ] 5 nmap` which is nicer than `[ [ a>> ] [ b>> ] [ c>> ] [ d>> ] [ e>> ] cleave ... ] map`.

example from my automated stock trader code:

[source,factor]
-------------------------------------------------------------------------------------------------
USING: specialized-vectors alien.c-types ;
SPECIALIZED-VECTORS: uint ushort uchar ;

! each slot is to store a specialized vector
TUPLE: ts { d read-only } { m read-only } { s read-only } { p read-only } { v read-only } ; final

: <ts> ( n -- d m s p v )
  { [ <uint-vector> ]
    [ <ushort-vector> ]
    [ <uchar-vector> ]
    [ <uint-vector> ]
    [ <uint-vector> ]
  } cleave ; inline

: (push-ts) ( ts -- q: ( d m s p v -- ) )
  { [ v>> [ push ] curry ]
    [ p>> [ push ] curry ]
    [ s>> [ push ] curry ]
    [ m>> [ push ] curry ]
    [ d>> [ push ] curry ] } cleave
  compose compose compose compose ; inline

: (push-all-ts) ( ts -- q: ( d m s p v -- ) )
  { [ v>> [ push-all ] curry ]
    [ p>> [ push-all ] curry ]
    [ s>> [ push-all ] curry ]
    [ m>> [ push-all ] curry ]
    [ d>> [ push-all ] curry ] } cleave
  compose compose compose compose ; inline

-------------------------------------------------------------------------------------------------

where `: (trades) ( ... ticker time-range q: ( results -- ... ) -- ... ) ... ;`

.using specialized arrays

* they support the following methods: `clone`, `length`, `like`, `new-sequence`, `resize`, `nth`, `set-nth`.
* the fact of them being sequences means that they can be used with `map-as` or any word that uses `like` and/or `new-sequence`.
* see document "Specialized array words" for other useful words.

.using tuple arrays

* `0 <myclass-array> map-as` produces a "cannot call run-time computed value" error. you should do `T{ myclass-array } map-as`. `T{ myclass-array }` is an empty tuple array. it's things like this that remind me that `{ }` is not syntax for "array type" but rather is a new array object pushed onto the stack for use by `map-as`.
* if you `TUPLE-ARRAY: x` then words `>x-array` and `<x-array>` are generated which respectively convert a sequence to, or initialize with given capacity, a tuple array.

.serializing structs & specialized vectors

structs and specialized vectors don't need serialization; they can be written & read raw:

[source,factor]
-------------------------------------------------------
USING: specialized-arrays alien.c-types io io.streams.c
       classes.struct ;
SPECIALIZED-ARRAY: uint
STRUCT: my-struct { foo uint } { baz uchar } ;

uint-array{ 24 1 56 0 4 3 }
6 10 my-struct boa
"/home/nic/test" "w" fopen <c-writer> [ [ write ] dip write ] with-output-stream
-------------------------------------------------------

note that we cannot have specialized array types (e.g. `uint-array`) in structs; they aren't "c types", so the `STRUCT:` syntax disallows it. we can store pointers to these arrays and use `void*` for the struct type slot, and we get usable code from that, but then we can't just write the raw struct to disk, since the pointer would be written, and its value isn't generally meaningful. also you'd need to write the length of the array, which, i suppose you'd specify manually depending on its length; you'd write an endian-encoded length of a given number of bytes. most generally you could consult `sequences.private:array-capacity` to know how many bytes are needed (8 bytes currently on a 64 bit system).

there's a word, `read-struct`, which reads a struct from `input-stream`, and thus would support reading from a file, so that's done. but there's no extant, elegant way to read bytes into a specialized array. it's not hard, though: `: read-uint-array ( n -- ) uint-array [ element-size * malloc ] 2keep boa`, though honestly i'm unsure whether, by this method, the underlying memory is freed properly, or if i must `free` it sometime....

==== encoding numbers

the `endian` vocabulary has efficient words for coding (un)signed integers and floats as byte arrays of either endianness. `>le` &al's 2nd parameter is the number of bytes needed to encode the number. it truncates or pads as appropriate: padding: `10 2 >le` => `B{ 10 0 }`; truncation: `260 1 >le` => `B{ 4 }`; normal: `2000 2 >le` => `B{ 208 7 }`. notice how negative numbers are encoded: `-5 2 >le` => `B{ 251 255 }`. it's 2's complement of the positive version. `bitnot` is equivalent to `neg 1 -`. because factor fixnums are not stored as fixed-size bit arrays, if you want to flip the kth bit of some length n representation, then you must convert them to a length n bit array or byte array via the `integer>bit-array` (which determines number of bits from  `bit-length` i.e. basically `log2 1 +`) or `>le` &c words, then flip bits either by using boolean words for bit sets, or manually by using words like `bitxor` (or, more conveniently, words of the `math.bitwise` vocab), possibly using a masks such as `255 bitand` as necessary.

TIP: `math.intervals:fixnum-interval` gives `fixnum`'s min & max values

NOTE: remember that factor supports syntax like `0b1001`. also, `.b`, `.o`, `.h` print in binary, octal, or hexadecimal, and that you can set dynamic variable `number-base` to make all pretty-printed numbers display in a given base. because they use dynamic variables, they work for byte arrays, too: `2024 03 28 julian-day-number 3 >le .h` prints "B{ 0xee 0x8a 0x25 }". even values shown in the walker are in the set base!

==== writing binary files

being that binary files are so particular, it'll often be useful to overwrite only some bytes in a file. to do this, open the file in append mode then seek to the position that you want to begin overwriting, then write. "append" also means "overwrite", here. if you must replace a subsequence of bytes, starting at a position _p_, by a different-length one, then you must rewrite all the bytes of the file starting at _p_ e.g. the most efficient way to replace "c" in "abcde" by "xy", resulting in "abxyde", is to leave "ab" as it is, seek to "c", load "de" into a buffer, write "xy", then write the buffer contents.

in factor, we use `seek-input` & `seek-output`. the seek types that they take are `seek-relative`, `seek-absolute`, and `seek-end`, as found in the doc for `stream-seek`. if you give a position past the end then the between subsequence is zeroes. *to use seek, you must use the `binary` encoding class, and you must write a sequence of class that implements `element-size` e.g. `byte-array` or `uint-array`, but not strings!* see the example:

.replace last n characters of a file
[source,factor]
-----
: overwrite-file-end ( str path -- )
  binary [ >byte-array dup element-size 1 + neg seek-end seek-output write ] with-file-appender ;
"ZE" "/home/nic/samplefile" overwrite-file-end ! initial file content was "stuff"; now it's "stuZE"
-----

important notes:

* the file did not end with a newline. if you're getting off-by-1 errors, check whether anything writing the file is writing a newline
* i seeked to one byte before the string's length because `0 seek-end` sets the position to append to the file's end
* seeking is sensible only in append mode / with a file appender, not in write mode / with a file writer. `<file-writer>` opens a file with mode `"wb"` (see `fopen(3)`) which truncates the file (discards its entire content then seeks to 0). `<file-appender>` uses mode `"ab"` which opens in append mode, which really should be called "overwrite mode", even though it does start the stream position at the file's end. the `b` is for _binary_, btw. append/overwrite overwrites anything already existing or else appends, expanding the file size.

tl;dr: think of `with-file-writer` as "set file content" and `with-file-appender` as "modify file content."

==== reading binary files

think of the integer argument passed to `seek-absolute` as "skip this number of bytes, then read." the same is true of writing, naturally. with a file called `TEST` whose content is `12345` followed by a newline, the following makes it `123ZZ` followed by a newline:

[source,factor]
---------
"TEST" binary [ 3 seek-absolute seek-output "ZZ" B{ } like write ] with-file-appender
---------

==== inspecting binary files

this isn't a factor-specific section, but it accomodates the other sections on working with binary in factor. it's good to have external tools to double-check your binary files whenever you doubt that your factor code is working correctly.

i prefer link:https://justine.lol/braille/[bd aka braille dump] for hex dumping (until i write my own), or hexyl if i want to print only part of a file. like other hex dumpers, bd.com _squeezes_ its output: it puts asterisks to abbreviate series of duplicate lines (most commonly when each of the lines is all zeroes). maybe this started with hexdump(1) or something earlier. in this example, i write 201 bytes of zeroes.

.`"newfile.txt" binary [ 200 seek-absolute seek-output B{ 0 } write ] with-file-writer`
------------------------------------------------------------------------------
00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  │▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁│
*
000000c9
------------------------------------------------------------------------------

.`"newfile.txt" binary [ 200 255 randoms B{ } like write ] with-file-writer`
------------------------------------------------------------------------------
00000000  c4 27 79 97 4e 1b 3c cb  7d f1 ee 17 75 63 b2 17  │⠪'y⣡N←<⣎}⡛⢾↨uc⢉↨│
00000010  97 81 4e 24 ea 59 b8 40  fc f2 05 b4 80 48 d1 c2  │⣡⡀N$⢞Y⠍@⠿⢛♣⠩█H⡑⢊│
00000020  7b 48 02 1d 67 1b 3b f8  7b ec 7d 59 63 57 be 3e  │{H☻↔g←;⠟{⠾}YcW⢭>│
00000030  ce 70 43 f1 ba e8 68 f1  c8 31 0f 26 bf 7e 57 0a  │⢮pC⡛⢍⠞h⡛⠎1☼&⣭~W◙│
00000040  d8 5c 40 96 0c 3c e6 cf  c9 e7 c4 f7 ea da d4 f0  │⠕\@⢡♀<⢺⣮⡎⣺⠪⣻⢞⢕⠱⠛│
00000050  ba 9b 23 d5 28 7e e9 14  45 b0 24 59 09 cc be db  │⢍⣅#⡱(~⡞¶E⠉$Y○⠮⢭⣕│
00000060  39 7e 7f 6e 81 59 f9 6f  43 41 6b d5 c3 8e 23 a5  │9~⌂n⡀Y⡟oCAk⡱⣊⢤#⡣│
00000070  1d 98 d4 cc 08 0e 5e 63  08 94 87 70 95 f3 8f 86  │↔⠅⠱⠮◘♫^c◘⠡⣠p⡡⣛⣤⢠│
00000080  65 da 2d 90 88 8e b4 e5  ca 45 83 f5 6e 9e 77 c3  │e⢕-⠁⠄⢤⠩⡺⢎E⣀⡻n⢥w⣊│
00000090  56 79 23 09 b9 3e 1b 8a  cc 82 2e a4 93 14 3e 14  │Vy#○⡍>←⢄⠮⢀.⠣⣁¶>¶│
000000a0  7d 90 75 3c f1 48 38 6d  6b f9 27 40 c4 a5 6c da  │}⠁u<⡛H8mk⡟'@⠪⡣l⢕│
000000b0  88 8b 29 dd 4c ba e5 bd  ea 3c 09 57 d4 d1 bf c6  │⠄⣄)⡵L⢍⡺⡭⢞<○W⠱⡑⣭⢪│
000000c0  3f f5 4d 01 bf 59 ca ae                           │?⡻M☺⣭Y⢎⢧│
000000c8
------------------------------------------------------------------------------

== sockets

.server
[source,factor]
--------------------------------------------------------------------
USING: io.sockets io.encodings.ascii io.streams.duplex destructors ;
"127.0.0.1" 3000 <inet4> ascii <server> ! (1)
[ accept drop
  [ readln                              ! (2)
    "hello to you!" print               ! (3)
  ] with-stream                         ! (4)
  print                                 ! (5)
] with-disposal                         ! (6)
--------------------------------------------------------------------

run this code, then run `netcat localhost 3000` in a terminal shell and type "hello" or something, then press return.

. see ``<server>``'s documentation about choosing & specifying host descriptions
. read message from client (here, netcat)
. send a message to the client. `with-stream` binds the duplex socket to the default input & output streams, so `print` sends to the client
. `with-stream` closes the stream, so we don't call `dispose` separately on the socket returned by `accept`
. now that `with-stream` has returned the default input stream to stdout, we print the client's message
. close the server socket and any connected sockets (of which there are none here; when `with-stream`'s block finishes, that connection's socket is closed, leaving none left in this example

TIP: factor also supports unix sockets in particular, which particularly & only facilitate interprocess communication on one machine. however, these cannot be used with netcat (to my limited knowledge), so i don't document that here.

to be a client, sending messages to a server:

run `netcat -l -p 3000`, then run the following:

[source,factor]
--------------------------------------------------
"127.0.0.1" 3000 <inet4> ascii
[ "sending a msg!" >byte-array print ] with-client
--------------------------------------------------

you'll notice that i used neither of the `io.sockets` vocab's `send` nor `receive`. i have no idea what they are used for.

== feature ideas to implement personally, or perhaps with...your?...collaboration, dear reader~? :3c

* make a `SECRET:` syntax that is like `CONSTANT:` for strings but stores the value in a way that obscures it from the standard `strings(1)` unix program. ideally, when the vm starts, it'll overwrite the encoded string by its decoded version of the same length, keeping the same location in memory. if it's non-cryptographic (or effectively so by using a fixed symmetric cipher key) then i suppose that a dedicated hacker might decompile the factor binary, then identify the places where strings would be stored even if as a seq of obscured bytes, then just reverse the encoding algorithm,...but that would take some effort, and, at least currently and somewhat sadly, with factor being unpopular, unlikely someone would take time to release such a hack.
* http2 client. link:https://www.rfc-editor.org/rfc/rfc9113.html[http/2] is a version of http that particularly handles what in http/1.1 is keep-alive connections. keep-alive is just plan sensible for rest api calls, especially for frequent calls.
  ** reference <https://github.com/httpwg/http2-spec/wiki/Implementations>—perhaps the haskell `http2-client` package.
* when loading a program, list all errors (namely unfound symbol errors) at once, and suggest import locations as the listener does, outputting an import string like when you try _running_ a program. my example is loading `.factor-rc`; i was told many times that a word was "found in the current vocabulary search path."
* make a word to reload `.factor-rc` without restarting the listener (actually, i think that i encountered this already but forgot what the word is called)
* document `sequences.generalizations`. a small bit of documentation goes a long way.

== factor hackery

=== (custom) images

TODO: see "Bootstap initialization file"

bootstrap:: create an image from source rather than as a memory (heap) snapshot of a running factor process. see "Bootstrapping new images" doc.

NOTE: when saving images for quickly resuming state, i advise that you never overwrite the original image file; generally, it's good to preserve all canonical versions, and modding only copies.

the image saves only the heap, not the stack. afaict yet, custom images' priamry use is saving time by effectively caching precompiled preloaded vocabularies. i haven't learned anything about the bootstrapping process yet, though.

you can't load an image in the listener; you must invoke factor with the `-i` parameter.

a neat thing: when you load from an image, it sets `image-path` which is used by `save`, so factor tracks which session you're in, so that `save-image-and-exit` is akin to hibernating.

TIP: `save` saves the ui theme, so you can associate a theme to each session/project so that when you start factor, you can immediately know which projcet image is loaded. fonts aren't saved, unfortunately.

NOTE: `image-path` is a word that gets the value of dynamic variable `\ image-path`! note the preceeding backslash to treat `image-path` as a symbol instead of invoking the word!

TODO: somehow only my saved image cannot find the `trees.avl` nor the `aws` vocabularies. why? why are some vocabs present but others not? i suppose that i should learn more about exactly what images are and how vocabularies relate thereto, and generally work, before i expect to be able to answer. maybe it had to do with me deleting the original image then rebuilding it....

=== modding the listener

see `ui.tools.listener` vocab.

=== general

* open vocabs in your text editor; see a section on text editor integration, and TODO: write that section if i haven't already
* poke around the following vocabs:
  ** inspector
  ** ui.theme
  ** help.vocabs

=== theme-change hack

in the listener, evaluate:

[source,factor]
---------------------------
SYMBOL: S_
get-datastack S_ set-global
dark-mode listener-window
---------------------------

then, in the new window: evaluate `S_ get-global set-datastack`.

=== bugs

=== stack effect checking defects

TODO: merge with cameron's note about bad stack inference

given this dated definition for `read-websocket-loop`:

[source,factor]
----------------------------------------------------------
: read-websocket-loop ( quot: ( obj opcode -- loop? ) -- )
    '[ read-websocket _ dip and ] loop ; inline
----------------------------------------------------------

`com-stack-effect` gives `( -- )` for this expression:

[source,factor]
--------------------------------------------
[ drop [ drop ] each t ] read-websocket-loop
--------------------------------------------

however, add a couple objects before it...

[source,factor]
--------------------------------------------
f t 6 ! 3 dummy elements
[ drop [ drop ] each t ] read-websocket-loop
--------------------------------------------

and we get:

-----------------------------------------------------------------------------------
The input quotation to 'read-websocket-loop' doesn't match its expected effect
Input                              Expected                Got
[ 8 = ~quotation~ ~quotation~ if ] ( obj opcode -- loop? ) ( x x x x x -- x x x x )
-----------------------------------------------------------------------------------

poking around reveals that the "Got" effect is just the declared/expected `( x x -- x )` except that the in & out both have number of dummy elements extra.

if i replace `[ drop ] each` by `drop` which have the same stack effect, `( x -- )`, then it works:

[source,factor]
-----------------------------------------
t t f [ drop drop t ] read-websocket-loop
-----------------------------------------

in fact, if i take the `each`-y expression but replace `read-websocket-loop` by its definition, then the stack checking is fine:

[source,factor]
---------------------------------------------------------
f t 6 ! 3 dummy elements
[ drop [ drop ] each t ]
[ [ read-websocket ] ] dip [ dip and ] curry compose loop
---------------------------------------------------------

correctly is said to have effect `( -- x x x )`. and indeed, if i redefine `read-websocket-loop` to have its most general stack effect, then it works:

[source,factor]
--------------------------------------------------------------------------------------------
: rwsl ( ... q: ( ... obj opcode -- ... loop? ) -- ... )
  [ [ read-websocket ] ] dip [ dip and ] curry compose loop ; inline

f t 6 ! 3 dummy elements [ drop [ drop ] each t ] rwsl ! com-stack-effect gives ( -- x x x )
--------------------------------------------------------------------------------------------

always specify combinators stack effects as their most general, row-polymorphic forms (if applicable), so that you can use other row-polymorphic combinators inside of them without getting incorrect stack check errors. note that the stack check errors can occur even when it's insensible; namely i guess that that case is when one of the combinators is recursive & inline.

another stack checking defect:

`"" <get-request> [ drop ] do-http-request [ ] with-stream` works fine but if we re-express by changing argument order and using swap—`[ ] "" <get-request> [ drop ] do-http-request swap with-stream`—then we get the "call/run-time computed" error. if we use `dip` instead of `swap`, then there's no error: `[ ] [ "" <get-request> [ drop ] do-http-request ] dip with-stream`. this seems to be due to the combination of `do-http-request` being inline recursive and with-stream using continuations (and being inline). i guess this b/c `"" ascii [ ] "" <get-request> [ drop ] do-http-request swap with-file-writer` (or file reader) errors but `"" ascii [ ] "" <get-request> [ drop ] do-http-request swap 0 "" with-variable` works, and the difference between `with-input-stream` and `with-variable` is `with-disposal`, which uses continuation words.

=== factor as (part of) a computer interface

i don't write many programs. usually i use my programming knowledge for everyday tasks, where i write code on the fly, tailored to each situation—basically like how people use bash, except i use factor and kakoune for interactive data munging, so it's actually nice instead of a headache. these, a little bash, and the yazi file browser, and firefox are the majority of how i interact with my computer.

so! this section gives examples for how it's nice to have a factor listener always running, ready for arbitrary computations or system administration, as a preferable alternative to command line shells for sufficiently-complex computations. again, i should check-out link:https://dt.plumbing/user-guide/lang/stdlib.html[dt]. still, i bet that factor's ability to interact with data on the stack well and browse through tuples' slots is unparalleled. good for active development and interactively, simultaneously developing & running computations alike!

==== find disagreeing cue files

the situation: i have the scorpions discography, but its directory structure is kinda complex and some albums have multiple cue files. i want to know how the cue files compare so that i can decide which to use with link:https://mp3splt.sourceforge.net[`mp3splt(1)`]:

-------------------------------------------------------------------------------------------------
 .
├──  Albums
│   ├──  '1972 Lonesome Crow'
│   │   ├──  '1972 Lonesome Crow [1989 US R2 70915 Rhino]'
│   │   │   ├──  '1972 - Lonesome Crow (R2 70915).cue'
│   │   │   └──  '1972 - Lonesome Crow (R2 70915).flac'
│   │   └──  '1972 Lonesome Crow [2002 US 422 825 739-2 Hip-O]'
│   │       ├──  'Scorpions - Lonesome Crow (422 825 739-2 USA).cue'
│   │       └──  'Scorpions - Lonesome Crow (422 825 739-2 USA).flac'
│   ├──  '1974 Fly To The Rainbow'
│   │   └──  '1974 Fly To The Rainbow [1987 USA 5057-2-R RCA]'
│   │       ├──  '1974 - Fly To The Rainbow (5057-2-R).cue'
│   │       └──  '1974 - Fly To The Rainbow (5057-2-R).flac'
...
├──  'Box Set'
│   ├──  "2004 Axe Killer Warrior's Set [France AX070398]"
│   │   ├──  'CD 1'
│   │   │   ├──  'Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 1.flac'
│   │   │   ├──  'Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 1.flac.cue'
│   │   │   └──  'Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 1.wav.cue'
│   │   └──  'CD 2'
│   │       ├──  'Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 2.flac'
│   │       ├──  'Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 2.flac.cue'
│   │       └──  'Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 2.wav.cue'
│   └──  '2010 3 Original Album Classics [EU LS02361 Sony]'
│       ├──  '1975 In Trance'
│       │   ├──  'Scorpions - In Trance (flac).cue'
│       │   ├──  'Scorpions - In Trance.cue'
│       │   └──  'Scorpions - In Trance.flac'
│       ├──  '1976 Virgin Killer'
│       │   ├──  'Scorpions - Virgin Killer (flac).cue'
│       │   ├──  'Scorpions - Virgin Killer.cue'
│       │   └──  'Scorpions - Virgin Killer.flac'
│       └──  '1977 Taken By Force'
│           ├──  'Scorpions - Taken By Force [CDA] (flac).cue'
│           ├──  'Scorpions - Taken By Force [CDA].cue'
│           └──  'Scorpions - Taken By Force [CDA].flac'
...
-------------------------------------------------------------------------------------------------

so i hacked-up this factor code:

[source,factor]
----------------------------------------------------------------------------------------------
"/home/nic/music/scorpions-discography"
[ dup directory?
  [ dup directory-files [ file-extension "cue" = ] filter dup length 1 >
    [ swap dup print [ { "/usr/bin/diff" } prepend process-contents print f ] with-directory ]
    [ 2drop f ] if
  ] [ drop f ] if
] find-files drop
----------------------------------------------------------------------------------------------

it gave this output:

-----------------------------------------------------------------------------------------------------------------------------------------------------
/home/nic/music/scorpions-discography/Singles/1990 Wind Of Change (Russian & Spanish) [EU 866 265 - 2]
1,2c1
< REM ACCURATERIPID 0001a56a-00058ca7-1902f803
< REM GENRE "Rock"
---
> REM GENRE Rock
7c6
< FILE "Scorpions - Wind Of Change.flac" WAVE
---
> FILE "CDImage.ape" WAVE

/home/nic/music/scorpions-discography/Compilations/2005 The Platinum Collection/2005 The Platinum Collection [2005 Germany 00946 340 407 23 EMI]/CD 3

/home/nic/music/scorpions-discography/Compilations/2005 The Platinum Collection/2005 The Platinum Collection [2005 Germany 00946 340 407 23 EMI]/CD 2
1d0
< stuff
8c7
< FILE "Scorpions - The Platinum Collection CD 2.wav" WAVE
---
> FILE "Scorpions - The Platinum Collection CD 2.flac" WAVE

[...]

/home/nic/music/scorpions-discography/Box Set/2004 Axe Killer Warrior's Set [France AX070398]/CD 1
7c7
< FILE "Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 1.wav" WAVE
---
> FILE "Scorpions - In Trance Virgin Killer  Deluxe Collector Edition Disc 1.flac" WAVE

-----------------------------------------------------------------------------------------------------------------------------------------------------

granted, ideally i'd add some delimiters for clarity, but this is sufficiently readable.

== antipatterns & solutions

* `[ at ] curry { a b c d } swap map first4-unsafe` should be `[ a b c d ] dip [ at ] curry 4 napply`, or if you want to apply functions to the key values, then you must use `{ [ a of f ] [ b of g ] ... } cleave`.
