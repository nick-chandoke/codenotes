== bit twiddling, and encoding

using an enum/number enforces mutual exclusivity. consider `(or a b)`; the first non-falsy is chosen; however, `a ‚àß b` may be true. if you want to help ensure that `a xor b` is true, then have a single number called. why integers? most types work; we need for all elements of the type to be unique. integers are a good choice because they obey that property, and are machine words, and support bit twiddling. the point of this pattern is doing `case x v1: ...; v2 ...; ...` instead of `cond`, since `x` must have exactly one value, whereas `cond` statements, while more general (i.e. supporting general predicates instead of mere equality), must be carefully ordered, and may be not mutually exclusive.

why?

. fun like number theory, which it practically _is_, but in binary
. keeps programmers familiar with information theory, which all programmers should know intimately
. efficient:
  .. bitwise ops are single-cycle
  .. elementary arithmetic uses few cycles
  .. word is the physical unit of memory
  .. bitwise (and simd) ops parallel: `a&b` = `(map and a b)`
  .. opcodes are inline; no jumping
. flexible: boolean/integer duality
. all systems support this variety of data, including direct support by ISAs

.conventions, terms, and other notes

* 2's complement is assumed unless otherwise noted
* _word_ always means machine word, not natural language word
* for bits, 1 may be called _set_, and 0 _unset_
* _bits_ is short for _bitstring_ i.e. a bit vector. because any bits can be split into many, _bits_ means a bitstring or a string of bitstrings, recursively.
  ** _bits_ is plurality-agnostic

=== numbers, sets, and sequences (they're all the same!)

numbers are no different from sets (and thus no different from sequences, since sequences are merely a specific variety of set). both may be considered as amorphous blobs, or any number of structures may simultaneously be considered of them. for example, the number `6` has several interpretations i.e. is an encoding of several informations, some of which are:

. not zero (truthy)
. {2 3} (prime factors)
. 0b110 (a sequence of 3 booleans, or one boolean and a number on [0,3])
. a number less than 10
. the rational 12/2 (thus a compression scheme that reduces two numbers into one)
. 4√ó1+2 i.e. 4r2 (one unit of 4 with a remainder of 2 i.e. a single period of measure 4 plus one half that period i.e. 1.5√ó4.)

generalization of numbers: ùîπ ‚äÇ ‚Ñ§ ‚äÇ ‚Ñö ‚äÇ ‚Ñù ‚äÇ ‚ÑÇ ‚äÇ ‚Ñç .... it's incorrect to say that any integer is a single datum but that a complex number a 2-sequence of data, because an integer is an n-ary sequence of bits. now truly the bit _is_ the smallest unit of information. anyway, clearly if integers can encode such a variety of information, then rationals, complexes, &c can encode very much more!

these are no different from sets. both have product, coproduct, subset. breaking a number into other smaller numbers then selecting one of those numbers is an example of subset selection. numbers support many unions; bitwise `or` is a union that applies to integers (and even to IEEE754 floats, if you're a real stud like the developers of  quake [game] were (viz fast inverse square root)), just as `+` is coproduct. they're coproducts of different rings, but a number can be interpreted by many rings, simultaneously or not.

all numbers permit orders. so do all sets, at least regarding programming, since the sets are, as all things are, comprised ultimately of bitstrings, which are numbers. the pertinent question, regardless of set vs number, is whether any natural ordering is useful, or whether we instead need to introduce our own ordering scheme, independently encoded from the natual one (i.e. variance of values across the the natural one's axis does not vary values along the introduced one. this brings to mind partial derivative invariance.)

with common thought, _set_ or _sequence_ refers to a _data structure_‚Äîan encoding where each datum is stored in its own cell. these structures are far too limited and their excessive & artificial partitioning often fails to encode any natural relationships among the data. they all have their own construction & traversal, places where they're suited or not, regarding elegance or efficiency. the foolishness of them is that they're all artificial; rather than use numbers, which can naturally encode so much information, people decided to treat numbers as mere values (as though numbers or other natural values are as unrelated as pie and books) and design their own encodings of structure. spoiler alert: humans did a worse job at structuring than nature. as usual, it's better to study natural structure than try to make our own. _succinct data structures_ are a nice example of storing information without needless partitioning. i can't say how natural they are, but they're at least a step in a good direction.

==== vote strings for the de facto sequence type

every language should have a _de facto_ sequence type. in lua, it's the string or table. in sql, it's the string or relation. in lisp, it's the list [stack], but that's a bad choice because lists cannot be easily & efficiently manipulated. consider that elements of a list can be expressed as a space-delimited sequence of uids. if we can efficiently manipulate strings but not linked lists, then a string representation is better; the string representation is converted to literal data when the literal data are needed; this may be a conversion of any subset of the sequence. lisp lists support nesting, but strings, like relations, do not. again, instead of nesting, we use `join` or reference by index e.g. in sql or lua `'1 2 3'` represents a permutation of table with keys/attributes having values 1 2, & 3. many languages feature pattern matching on structures but not on strings; if ever done for strings, it's usually in some quite different parser framework. why the asymmetry? aren't strings powerful enough? they may represent numbers of any radix, and generally generalize bitstrings, being able to encode an exceptional amount of value per character. a string of characters belonging to an alphabet of cardinality _n_ contains information equivalent to a number of radix _n_. though strings of arbitrary encoding are always representable by bitstrings, the partitioning (character size i.e. number of bits per character, e.g. 64 in UTF8, 128 in UTF16. remember that the utf number is bytes, not bits) can be quite useful, just as hexadecimal (4 bits per character) is a useful interpretation of bitstrings. any hexadecimal string is guaranteed to have n√ó4 bits. this helps with string alignment for bitwise manipulations. for example, we may consider hex and octal together; each hex character is 4 bits and each octal is 3. they align at 12 bits, or 4 octal / 3 hex. i can't now see how that'd be useful; in fact, quite the opposite, since that's such poor alignment compared to e.g. 2 base-4 & 1 hex.

* c is one language where strings are represented as static arrays of characters, and characters are equivalent to integers. good symmetry! unfortunately it's a poor structure for manipulation. lua stores strings as ropes, so they support all sequence operations easily & efficiently. it also stores all strings as bytestrings. it's a very elegant bytestr/charstr model.
* one can use strings or bits to express regrouping i.e. that a counter has passed a threshold, representing that a program has reached a certain point in execution. indeed, one can have a large bitstring that represents the current control flow point (or multiple), and mutating this is easier than updating a stack to track state.
* regex is extremely powerful for strings. there is no reason to limit their use to text! use them for bytes (if your regex engine supports it, e.g. lua's), and sequences of _anything_, even if manipulating that sequence is done by manipulating a virtual string representation of the sequnece!
* using strings as an encoding for sequences, we see that `gsub` (replace substrings matching a regex) is `filter-map`, assuming that `gsub` accepts a unary string endomorphism Œª (such as in lua).
* remember that sequences can be used as [multi]sets. this is easiest if `insert` & `update` are unified into `put`

in summary: all encodings are sequenced; just ask whether we can leverage that or if we must introduce new independent information. all things support subset and adjoin. even a single bit supports subset: 1‚Üí{1}, 0‚Üí‚àÖ. obviously the bit supports adjoin: 0+1‚Üí1, 1+1‚Üí1. in the case of the bit, `+` & `&` are identical, or rather `+` is truncated/filtered from ‚Ñ§ to the set {0,1}. in this case, `min` & `max` are the filters. is this use of _filter_ different from filtering a set by a predicate? you decide, but i suggest "no." indeed, to write `min` or `max` in c, you'd use a ternary expression‚Äîa convenient encoding of `if`. the predicates' information is inside the definition of `min` & `max`, namely `<`. btw, though they've the same predicate, each chooses a different input, together corresponding to the two branches of `if`.

obviously bitstrings, being sequences, support:

. adjoin = interleave (at arbitrary indices, not necessarily every other index) (append is a specific version)
. subset = subsequence (substring is a specific version)
. mutation by bitwise ops

=== in/dependence [of information]

so why do people prefer data structures? perhaps it's that bit twiddling / information theory / the art of encoding was never popular, or fell out of popularity, and seems less obvious than data structures. certainly anyone, without knowledge of encodings, number theory, or even intermediate mathematics, can understand a list, graph, tree, &c. i think that people like to consider data separately, too. it's easier. the price paid, however, is less elegant and more verbose code‚Äîmore steps needed to relate data & keep them in scope. furthermore, independence is clean; we can consider one thing at a time. that's appreciable. indeed, dependence is terser and _can_ be more elegant, but things like factor use dependence (adding or removing an item on the stack affects other items' stack position) in a way that trades readability and ease of reasoning & writing programs for terseness, and it's not a worthwhile trade! this is not a fault of dependence, but of poor dependence. *good programming carefully relates data or not.* as prolog and relational algebra represent, programs are only relations & values, matter & form. *dependence of information is just as important as independence. code elegance is the balance of the two.*

the ideal program uses relations that either "constructively" interfere or do not interfere. for example, if i want to simultaneously consider a number as both a non-zero number and a boolean value, then i'm in luck: i can perform any operation (endomorphism) on the number without affecting whether it's a boolean or not. however, if i want to simultaneously encode a boolean value and a number which may be zero, then i need to make these data independent; otherwise an operation may take a number from non-zero to zero, but that's a truthy zero, not a falsy value. the obvious solution in the data structurist mindset is to use a cons pair where `car` is a boolean value and `cdr` a numerical one. that satisfies the need for independence, but are there other encodings that also do that, but are more elegant? for example, in haskell we may use a duple; duples have categorical type class instances, so the ordering of boolean and number can change our code's elegance!

bitstrings are usually not helpful for describing recursive structures. consider a linked list. its length is unknown and partitioning into cells is its very use. the only possible gain by concatenating bitstrings is that it _might_ make window functions easier to write, but even that is an imprementation-specific benefit; they're the same at the level of theoretical abstraction. however, bitstrings are only useful for describing _particular_ structures. this is really true of any strings whose certain intervals connote certain values, though. anyway, loops over alists can elegantly set variables. suppose that data x, y, and z are encoded in a bitstring at substring ranges [1,4],[5,7],[8,12]. then i can get the constituent parts of 0b10111,001,1010 (2970):

[source,sql]
----
with n(n) as (values(2970)), s(var,i,l) as (values('x',1,4),('y',5,3),('z',8,5)) select var,(n>>(i-1)) & ((2<<(l-1))-1) as val from s,n;
----

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ var ‚îÇ val ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ x   ‚îÇ 10  ‚îÇ
‚îÇ y   ‚îÇ 1   ‚îÇ
‚îÇ z   ‚îÇ 23  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

the `-1`'s show the difference between my preferred representation of bit substrings and the actual arithmetic. also, and i'm not a fan of tracking order of operations, but we can remove most of the parens: `n >> i-1 & (2 << l-1) - 1`. turns-out that subtraction has higher precedence than bitshit. given this example, can't say i'm disappointed with that!

NOTE: bitstrings are not used by sql! they must be converted to relations before being usable with the relational algebra! since sqlite is already efficient, just use relations directly. still, bitstrings are useful in other languages. the above sql statement easily converts to a loop that populates a hash map in any language.

=== domains & transforms

the domain of your values determines its maximum information content. the number of bits needed to encode _n_ values is ‚åàlog‚ÇÇn‚åâ. your encoding should support your program's preimage & image, and transforms on the encoding should appropriately preserve information or not.

=== exploitation of relations

remembering that functions generalize to relations, we see that, in the immediately-prior duple/ordering example, the categorical type class methods, being functions, are thus relations. they relate data to an im/proper subset of the duple. the question, as always, is which information is retained in the output, and whether it's enough for us to calculate the original inputs, if need be. *we want each particular relation/mutation to implicitly affect other information while preserving other information [invariants].* handling this manually is tedious, error-prone, and uncessary (soon). coding a constraint system to identify most efficient encodings given a set of invariants and domains should be at least as easy as coding ŒºKaren, which is only 39 lines of scheme, though it might require a bit of knowledge of number or information theory.

=== list of common encodings

per relation, here listed are transforms and their domains and which information they preserve or not. though most encodings are tailored, they're commonly composed of some common patterns.

* adding an extra bit doubles the number of values supported. this corresponds to (<<1)=(*2). the common example is where an extra bit accounts for sign

=== particular encoding examples

==== stock

this example demonstrates: 1. reducing domain to practical preimage; 2. efficient utilization of extra values by modular arithmetic. by _extra values_ i mean those describable by a bitstring of a given length but not in the set of ordinarily valid values encoded by the bitstring; to encode n values i need ‚åàlog‚ÇÇn‚åâ bits, implying 2^n^-(1+n) extra values.

say i'll invest some percent of my cash into a given stock. that's a real [number] value. however, though i may invest 50% in it, or 33%, or 25%, or maybe even 10%, i don't think that i care about investments less than 10%, and nor do i care about my investment amount being in increments any finer than 10%. thus my investment amount is an integer n ‚àà [-10,10]. that's 10 values plus a bit for sign i.e. 19 values (since 0 & -0 are equal); i need 5 bits to encode it. 5 bits gives 32 values. since i'm using only 19, i can assign special meaning to some 32-19=13 values. can i combine m & n into o such that i can break o into m & n i.e. can i losslessly compress m & n into a single value o? yes, but i need an interesting transform. `with t(a) as (select * from generate_series(0,32)) select a/19,a%19 from t` produces:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ a/19 ‚îÇ a%19 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0    ‚îÇ 0    ‚îÇ
‚îÇ 0    ‚îÇ 1    ‚îÇ
‚îÇ 0    ‚îÇ 2    ‚îÇ
‚îÇ ‚ãÆ    ‚îÇ ‚ãÆ    ‚îÇ
‚îÇ 0    ‚îÇ 18   ‚îÇ
‚îÇ 1    ‚îÇ 0    ‚îÇ
‚îÇ 1    ‚îÇ 1    ‚îÇ
‚îÇ 1    ‚îÇ 2    ‚îÇ
‚îÇ ‚ãÆ    ‚îÇ ‚ãÆ    ‚îÇ
‚îÇ 1    ‚îÇ 13   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

one column has two unique values, and the other has 19 unique ones. the combination of these two data describes all 32 values. however, i must keep these data separate in order to do that. this means using a bit for the first column, and ‚åàlog‚ÇÇ18‚åâ=5 bits for the second. that means a total of 6 bits to encode two values n‚àà[0,10] and m‚àà[0,13]. add an extra sign bit for n, and our total is 7 bits. not bad; that's 2 bits a better compression than storing the numbers separately, which would be ‚åàlog‚ÇÇ19‚åâ+‚åàlog‚ÇÇ13‚åâ=5+4=9 bits. through the magic of modular arithmetic, we made two bits disappear!

there's probably at least off-by-one error in this example, but whatever. the idea is valid regardless.

notes:

* this example would have been much easier and more boring if we'd rounded down 10 values to 7, which is neatly described by a 3-bit substring. all of the bytes' possible values would be reserved for that one variable, so we'd need to use other, independent bits for other variables. in other words, we wouldn't be overloading bits.
* when using a sign bit, you can decide whether -0=0 or not; you may choose for 0 to be 0, but interpret -0 as a flag e.g. if 0 corresponds to closing a position at market price, then -0 may connote closing by a market-on-close order.

=== bit twiddler's mindset

often bit twiddling, like assembly, is considered difficult to manage; this simply means that a good notation hasn't been chosen. bitstrings or hexstrings are not amenable to humans! (solution: blinkenlights. see below.) it may be difficult or arduous to identify compression schemes then apply them. sure, but doing either by hand is silly; automate it. the only part to do manually is the part that actually requires the programmer! namely, that task is identifying relevant information; given any na√Øve idea/concept/datum, just identify the considerable attributes and the relevant sets of which they're members; mapping any domain to {0,1}‚Åø (isomorphic with the more general form, d‚Åø where d is a digit of a given radix) is easy. after that, allow the computer to use algebraic rules (see below) to auto-compress and arrange your data conformant to the program-defining predicates (the program _spec_) that you provide.

the bit twiddle model forces programmers or designers to consider their data's properties, such as any datum's properties' e.g. mutual information or any of a datum's attribute's mathematical properties e.g. symmetry, closure, or associativity. this is very good; one must consider why they're using some assumedly useful data, or interpretations thereof, and the nature of the data [type]: how it behaves / can be manipulated, and how it can be interpreted, and, surprisingly importantly, the amount of redundant information of any interpretation.

.bits & decimals

tl;dr: floating point considered harmful unless you're using division an arbitrary number of times on values on (0,1) or need ¬±‚àû.

floating point is hardly necessary; use fixed point instead. arbitrary precision is best handled by perfect precision e.g. rationals instead of floats; rationals are perfect precision whereas floats are arbitrary but often incapable of exact representation. furthermore one must consider 1. what degree of precision is useful and 2. what degree of precision is meaningful viz sigfigs.

consider the polynomial 3.452069245x¬≥ - 6.25678x¬≤: how many digits are useful? would 3.452x¬≥ - 6.257x¬≤ produce significantly different values? if not, then we need to encode (3.452,3,6.257,2). the exponents are naturally expressed by indices (see polynomial representation of numbers in ¬ß_bits algebra_) which leaves the decimals. if we assume that 3 decimals is sufficient, then we can have 1000 decimal values expressible by 10 bits; on a 64-bit system, that leavs 54 bits to express the non-decimal, so the max value is ‚âà18 quadrillion. 4 decimal digits requires 14 bits, so the max then is ‚âà1 quadrillion. finally, the larger the whole part, the less significant the decimal; i can't imagine a context in which 10,645,245,627.2345 is significantly different from 10,645,245,627.2346. therefore you should consider bits to represent the number of sigfigs! indeed, the meaning of the decimal operator directly corresponds to the fact that there's an infinite number of values on [0,1] and [1,‚àû)! this implies that, without division, a single unit cannot express all values on [0,‚àû). for places where that property does not apply, e.g. `$120.67`, we can simply change the unit from dollars to cents, or tenths of a cent, etc; this trades the decimal place for leading zeroes, e.g. `$120.67` = `1206700` when the unit is one-hundredth of a cent. use of integers acknowledes that the choice of unit is arbitrary rather than inherently meaningful. all this said, even 32 bits should more than suffice for a polynomial; who needs a polynomial of degree greater than 5? nobody, that's who. rather than using floats for all purposes, the programmer specifying the number of decimal bits explicitly tells the expected order of magnitude for the value, which can suggest the meaning/nature of the program where that value is used.

anyway, back to the polynomial: the most needed to express an 8^th^-degree (3 bits) polynomial with maximum coefficient of 32,768 (15 bits) at 4-digit (10 bits) precision, we need 8√ó(3+15+10) = 8√ó28 bits. rather than consider that as 204 bits, it's useful to say that 28 is less than but approximately 32 bits‚Äîa halfword on common modern systems, so we need 2 words or 1 dword to express an *eigth degree* polynomial‚Äîonly 25% the size of `float[8]`, and doesn't use the heap!

for `floor`, `and` with a mask that has 1 for non-decimal indices and 0 for decimal indices. we can express this mask simply as "not decimal." no iteration nor type conversion. one cycle.

by this expression, polynomials naturally support addition and subtraction. for multiplication or division, replace the polynomial representation by its numeric output value. that requires a couple more cycles, but still is far more efficient than anything not done with bit twiddling. you'd need to define separate addition & subtraction operations for `float[8]`‚Äînot so, here!

what about marshalling between languages? no need to convert array types or throw-around pointers to allocated memory! just pass some few ints‚Äîtrivially easy in any marshalling system.

lastly, decimals occur only when dealing with continuous things. most non-scientific computing is discrete. besides, for scientific computing you'll probably use a gpu which handles floats extremely well.

decimals are also a useful grouping mechanism: we can select unique elements from a group of uids {1, 2, 3.1, 3.2, 4.1, 4.2} by using equality, or we can select groups whose elements have a common floor: {1, 2, {3.1, 3.2}, {4.1, 4.2}}. this is achieved by using a mix of integers and floats, or could be done using all-floats. remember, though: floats' precision can be relied on only when values are static! it's fine to use uids as long as they are never mutated; floating point mutation can break equality. for example, 1.1 + 1.0 may equal 2.1 or it may not; never assume that it will! you may keep it simple by encoding as fixed-point decimals or some other scheme partitionable into two discrete integers. generally this scheme is a tree structure: {3.1.0, 3.1.1, 3.2, 4} corresponds to sexp ((3 (1 (0) (1)) (2)) (4)). the sexp encoding is more compressed.

==== relation between efficiency and simplicity

the real polynomial example shows us that we get total elegance: efficiency *and* simplicity, naturally together. this is common, though it often requires a bit of tact to identify elegant encoding schemes.

bits are a data structure that lives entirely on 1+ register(s). they can be traversed and mutated incrementally, and thus support some common or uncommon algorithms.

=== intuitive programming with an improvement on blinkenlights

it'd be good to have, rather than programs as text, programs as graphics. the limitation of text is that the bits of the codepoint corresponding to each glyph is not represented in the glyph itself, excepting ascii majiscule/miniscule case, denoted by the 6th bit. contrastingly, an arbitrary graphical display of code can use any properties that it wants: geometric (2d or 3d) orientation (rotation, position, reflection, skew, size, or any other affine properties that i missed), color (each of h,s,v), size, shape (e.g. polyhedra), line thickness, &c. this is basically a symmetric version of reading blinkenlights, where every independent bit of information is displayed independently e.g. changing color does not change number of degree of a regular polygon, which contrasts with toggling _any_ bit of a codepoint resulting in a completely unrelated glyph! with a small & regular set of primitives (namely arithmetic and set & seq ops), this should make subconscious/intuition-based debugging quite easy. btw, this kind of programming should be done because it leverages the particular power of the human brain, much more efficient, liberating, and creative than trying to reason by constraint. let the computer deal with constraint and the human with play & investigation, to each their particular strengths.

i'm inspired by encrypted "garbage" like `ehJH~=SxY}^!Êòπ9},u@?’µaO}?>~#`, which looks like j for all i know, and i think about how powerful that terseness is. but by using codepoint-glyphs (ad-hoc assignment of bits to glyphs) instead of glyphs composed of in/dependent, a/symmetric information relations (symmetric relation of information to glyphs), we can encode _far_ more information in each glyph, and use "custom" glyphs which are actually just natural consequences of their latent information.

the basic idea here is that graphics are superior to text because they have more dimensions and fewer constraints. they naturally support graphics that freely represent arbitrary information, rather than unidirectional sequences and limited-size alphabets whose characters don't compose systematically.

this technique is good generally for displaying any information. for programming, this includes both reading & writing code, and debugging "dead" or "living" programs‚Äîthose that execute a sequence then halt, or those that stay alive until killed, and which you can inspect & modify as it runs. for debugging, think of that a light dimming to blackness is easier to spot in a mess of graphics changing over time than a number going to zero among a mess of numbers changing over time. for encoding programs, we can display any glyph reflected over the vertical axis rather than making a specific codepoint & glyph, and modifying a font file, and installing the font file, &c, just to connote the idea of an operator with flipped arguments. we can add a dot above and/or it or something if it supports both negative & positive values‚Äîor not, or something else; it's the coder's decision; implementing that display is just a couple graphics api calls away! your glyphs can directly represent properties about them, and glyphs be computed dynamically of their properties!

personally i want to manipulate code with my hands, with the code being abstract geometric objects, or sounds, or whatever; i want something more natural and free than keeping my hands affixed to a keyboard, using a modal text editor! sequences of key presses, sequenecs of bytes. sequences are for turing machines. sequencing text is like programming by a turing or stack machine. though better than applicative style, using sets (sql, prolog) is even better, and our interface & representation should reflect that! in fact, screw looking at a screen. rather project a world of objects around me like a planetarium to walk about in and manipulate! trade keyboards & monitors for projectors and cameras. 10 years ago such designs were a bit expensive, but they're quite affordable now, what with our vr headsets, tracking gear, inexpensive projectors, computer vision, and cameras that can caputure most of 4œÄ steradians. i want to use my two hands & arms simultaneously to manipulate data. rather than map keys to functions, map actions to functions, and let those functions be better than manipulating text; let them manipulate data. manipulating data requires fewer primitives than manipulating text, because text a more verbose represention of information than geometries & sounds. currently we're mapping 10 fingers to 104 keys. why not include more parts of the body? why have 104 keys? let's not pretend that having all latin characters is anything special. it doesn't help chinese, japanese, etc. it doesn't help entering math symbols. and there's no need for each letter to have its own key. in fact, if we're using glyphs naturally given by the information that they represent, then the idea of one key per glyph is obviously stupid. the ios kana japanese input _method already_ demonstrates more efficient and beautiful input methods. function keys? come on, we all use accelerators/modifiers instead anyway. we hardly need any keys. i can rotate my arms, hands, move them around and such, assume many body configurations. those mean more options for representing information, such as data manipulations. cameras are better than hardware because, to add/modify functionality, only new graphical patterns need to be programmed, rather than creating or modding hardware. finally, the argument that we can enter keys quickly is no good, because a sensibly efficient encoding of information would need fewer manipulations. mashing keys all the time is like using feet to drive a car rather than having gas and brake pedals. faster feet or longer legs are not what we need. it's time to break away not just from the old terminal/typewriter designs, but from text altogether, seeing text input as a special case of general input. obviously software character input can be very good, and indeed far superior to hardware text input.

perhaps ironically, these designs' importantce is proportional to the amount of time spent coding. i'm dissatisfied with my desire for coding to compete with my needs & desires of being an animal. coding is too wonderful an exploration of art & truth than to be sullied by being aggressive toward creatures that need to move and play.

of course, i haven't identified the proposed mappings from body configurations to data manipulations, but i have faith that the body supports far more configurations, and can manipulate parts of the body independently, corresponding to many values for independent data; this allows things like e.g. using the left arm to set the case of a character and the right arm to choose the character. this is a silly example because the arms support far more configurations than any that has the information content of a single bit, but the example generalizes nicely.

=== you may as well use bits

an example compound encoding scheme that does not use bits is {pairity,sign,abs(x)>=1000,v^*^}. actually, pairity is a predicate on the lsb; `x&1` discards all but x's lsb, which is 0 for even, 1 for odd. the same is true of sign (in 2's complement) too, except thet sign is determined by the msb. ^*^_v_ is x's value not including 1000; if x>1000 then we subtract 1000 from it, since the 1000 isn't really x's value, but instead connotes other information. this implies that x's value is in [0,1000). the `if(x<1000,x,x-1000)`. rather than 1000 (10^3), though, we could use x>=2^n, which is easily toggled by toggling the nth bit. this is not a general encoding for subsets, though; toggling only applies to bits e.g. you can't _toggle_ the nth digit of a base-5 digit string; you can choose one of 5 values for each digit. anyway, choosing a given digit of any any-radix number is overconstrained; we want to easily express subsets by arbitrary numbers‚Äîagain, like x>1 & x>4. btw, the use is that when `1` & `4` are arbitrarily chosen to represent given properties and one property implies the other, then that corresponds to that x>4 implies x>1.

==== branching by filters

TODO: incorporate about how e.g. abs is a piecewise fn `if x < 0 then -x else x`, also interperable as a filter (filters-out the sign). min & max are low- & high-pass filters, too. filters/piecewise fns are alternative(s) to control flow; they're both asymmetry primitives. also the type trinity is numbers, strings, and sets because they represent {mass, countable: {ordered, unordered}}. numbers are mass in that e.g. 1 + 2 yields 3, but given 3 we cannot know which addends produced it. the big point when designing an encoding scheme is which functions use which information; for example, a function defined of positive numbers is equal to a function of the absolute value of positive numbers; this allows us to adjoin the sign information without affecting the value of the function. we generally want our scheme to meaningfully affect some operations but not affect others. another example is that we may let character case encode e.g. genders per names, without affecting a case-insensitive sort. we may use bitwise operations as filters (corresponding to their electronic gate counterparts) to implement logic or control flow. another example of predicate satisfaction is whether a datum is within a given range; you can use basic comparison operators for subsets; x>4 is a subset of x>1.

min, max, & abs are all piecewise: abs(x)=if(x<0,-x,x); min(x,y)=if(x<y,x,y); max(x,y)=if(y<x,x,y). min0:=min(x,0)=if(x<0,x,0). max0:=max(x,0)=if(x>0,x,0). -x=0-x. min0/max0 is a coproduct of the identity and constant functions; it passes-through or discards its value. we don't need both min & max, nor > & <. in fact, we need only (min,max,-,+,0), notably lacking √ó; √ó is useful with `+` when predicates return 0 or 1, but i want to code more like lisp, using values if truthy, alternatives if falsy, and short-circuiting if falsy and no alternatives. `abs` is not a primitive: abs(x)=max(x,-x). -abs(x)=min(x,-x). `/` is the unit-change operator, and `%` is expressed by `/` & `-`: x%y = x-x/y, or in factor, `dupd / -`. anyway, `min`, containing the information of `id`, `const`, and `<` or `<=`, seems a good canditate, alongside `+` to choose alternatives.

using this algebra, let's re-express `if(x<n,x,x-n)` by (min,max,-,+,0). if x<n then min(n,x)=x=v and max(0,x-n)=0. else min(n,x)=n and max(0,x-n)=v. x=min(n,x)+max(0,x-n).

[source,sql]
----
with t(x) as (values(1200),(40)) select min(1000,x),max(0,x-1000) from t;
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ min(1000,x) ‚îÇ max(0,x-1000) ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1000        ‚îÇ 200           ‚îÇ
‚îÇ 40          ‚îÇ 0             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
----

...ok, so somewhat of the way there. i know that i didn't want to use predicates as logical values, but i can get to that by cheating here by using integer division: x>n = min(x/n,1) and 1-p for ¬¨p, though i could have used xor with 1 instead (i think):

[source,sql]
----
with t(x) as (values(1200),(40)) select (1-min(x/1000,1))*min(1000,x) + min(x/1000,1)*max(0,x-1000) from t;
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ (1-min(x/1000,1))*min(1000,x) + min(x/1000,1)*max(0,x-1000) ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 200                                                         ‚îÇ
‚îÇ 40                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
----

quite frankly, i don't know what i'm doing, but i feel that these things should be researched, and ultimately an algebra system that refines predicates to numeric properties and gates on them should be designed.

we can use min & max to make something a predicate, too: `x>y` is expressed as `min(1,max(x-y,0))`, derived from the observation: x<y <=> x-y<0 <=> max0(x-y)=0. we can then use that with the standard branchless form: `if(x>y,a,b)`=`(x>y)*a+(1-(x>y))*b`=`(min(1,max(x-y,0)))*a+(1-(min(1,max(x-y,0))))*b`.

with min & max, we can toggle whether a value will be discarded, by negating it. e.g. assuming x>0, max0(x)=x and max0(-x)=0. then max0(x)+max0(y) will give either x, y, or x+y. similarly min0(x) will choose 0 or -x depending on whether x<0. we can negate the output of min0 or max0, or negate their inputs.

if(x>y,a,b)=min(1,max(x-y,0))*b+a.

[TODO]
* consider min & max wrt lattices, and how prolog's predicate unification uses a lattice. obviously using min & max on reals with 0 & 1 is stupidly limited compared to using them against other reals. how can we usefully generalize the boolean ring?
* consider `sign` gives 3-valued boolean logic.

''''

=== entropy & encoding

TODO: read about encodings, e.g. huffman, hugh-tucker, and wavelet trees; and hilbert curves.

==== _succinct_ data structures

‚àÉ papers about them, but not how to use them (according to link:https://www.youtube.com/watch?v=sdHXaYCX3RE[kmett in 2015].)

* you need H = log(n choose k) + 1 bits to encode n bits where k are set.
* rank(Œ± s i) is #{1 | Œ± == s[k], k ‚â§ i}. rank(0) shares all info with rank(1). rank can be computed in O(1) by chunking s into chunks each of size log(n).
* select gives the position of the ith Œ± in s. it can be done in O(1) by recursing upward through a huffman tree.
* rank & select share information.
  ** rank(Œ±,select(Œ±,i)) = i (rank is a left-inverse of select)
  ** select(Œ±,rank(Œ±,i)) ‚â§ i (select & rank form a galois connection)

the rank of a huffman tree (which is isomorphic with a bits) can be found by recursing on rank.

rank & select work on alphabets of any size, and on all prefix-free codes, especially order-preserving compression schemes.

an important principle that this technique demonstrates: we only need to encode data. we do not need to have separate "cells" for each "separate datum." such conceptualization is na√Øve and inefficient. do not constrain yourself to keeping data separate; only care that you can _effectively_ manipulate the data as desired (namely CRUD), which may mean compressing, mixing, &c the data together, then extracting or reconstructing the actual logical data. this sees data as-manipulated and as-stored.

the _order_ of an encoding is the number of bits that each datum encodes, assuming that enough data is available. a positive order means autoregression.

==== miscellaneous little tips

* search by fewest possibilities first, e.g. lookup dates as month day year, because there are at most 12 months, 31 days, and an unbounded number of years. looking-up by 12 then 31 then n enforces a lookup complexity upper bound. 

=== bits algebra

* bitstrings can be split. e.g. a 32-bitstring can be 4 8-bitstrings i.e. a 4-vector of octal values.
* index is exponent. radix is always 2
* a length n bit vector can encode 2‚Åø values
* numbers are expressed as polynomials: Œ£d·µ¢r‚Å± where d is a digit and r is a radix
* like how the smirnov transform in statistics transforms into U[0,1], a set of values can be compressed into a set of bitstrings and a back-transform.
* for booleans/bits, complement = opposite; both are represented uniformly by `not`.
* an unordered set of bits is expressible entirely by its count of set bits.

==== symmetries

efficiency is obtained by exploiting symmetry and/or coincidence.

===== lattices

TODO: unify < & min, or explain why that's impossible.
TODO: consider how x can be split into information |x| & sgn(x)
TODO: consider complex numbers.

NB. sgn(x-y)min(z,|x-y|) (or something like it) is a terse form of iff(x>y,min(z,x-y),max(-z,x-y)).

a common problem is choosing of `(<,max,high)` or `(>,min,low)`. this is simpler encoded as `(<,max,high)` under negation or not; min(a,b) = -(max(-a,-b)). thus the `<` & `>` lattices are opposites. one's min is the other's max.

.retained info & arity
[options="header"]
|===================================
| fn      | abs | sgn | arity | #cod
| abs     | yes | no  | 1     | ‚àû
| cmp     | no  | yes | 2     | 3
| sgn     | no  | yes | 1     | 3
| <,>,=   | no  | no  | 2     | 2
| min/max | yes | yes | 2     | ‚àû
|===================================

* `sgn` is unary `cmp`
* `min`/`max` retain(s) the most information

to exploit symmetry, use only `<`, but not in its literal sense; its meaning must be relative to the usual lattice or its opposite. in program semantics this means that the re√Øfication of `<` is context dependent i.e. it'd be in a type class rather than selected from an `if` clause. if `if p then a < b else a > b` (or anything dealing with `min` or `max`) appears multiple places, then `opposite {a < b}`, where `opposite` specifies `<` & `min` to use the opposite lattice and is scoped, is easier to refactor and is less prone to code entering mistakes. as for selecting `low` or `high` (assuming that we need to track both), use only a single context-dependent identifier called `extreme`. such a context can be specified by using dynamically bound variables, e.g., in racket:

[source,scm]
----
#lang racket/base

(require (rename-in racket/base [< LT] [min MIN] [> GT] [max MAX]))

;; these zeroes are dummy initial values. low & high will be set
;; throughout the program.
(define low  (make-parameter 0))
(define high (make-parameter 0))

(define pos (vector LT MIN low))
(define neg (vector GT MAX high))

(define (set-lat x) (if (> x 0) pos neg))

(define lat (make-parameter pos))

;; unfortunately in racket i can't define things in terms
;; of memory addresses; i would instead define them as macros,
;; but they're defined in terms of `lat`, and sharing identifiers
;; between macros and non-macros is a pain. thus i define them as
;; functions so that they'll be evaluated upon each use [invocation].
(define (<)   (vector-ref (lat) 0))
(define (min) (vector-ref (lat) 1))
(define (ext) (vector-ref (lat) 2))

(low 5)
(high 20)

(printf "~a < ~a: ~a~n" 40 ((ext)) ((<) 40 ((ext))))
(set-lat #f)
(printf "~a < ~a: ~a~n" 30 ((ext)) ((<) 30 ((ext))))
----

outputs

----
40 < 5: #f
30 < 20: #t
----

this method branches often, which is inefficient. ideally we'd multiply everything by a given variable whose value is either -1 or 1. in most languages, though, this would look absolutely horrible, since that multiplication would need to be explicitly specified in syntax in many places. ideally we'd store `pos := (<,min,ext)` and `neg := (>,max,ext)` in arrays with constant memory offsets so that we can simply set a variable `lat` to either `pos` or `neg`, and use macros `<`, `max`, & `ext` to refer to `lat[0]`, `lat[1]`, & `lat[2]`.

to define this code well, we need a mechanism to select whether eval is done at definition vs use. see _best paradigms_ section on evaluation for further discussion. ideally we'd use clever bit twiddling to avoid all this.

consider whether `high` & `low` are positive fixed-point or not, and which encodings they support. recall that -x = ~x+1.

.identities

* abs(a) = max(a,-a) = -min(-a,a).
* a <= b = a < b or not -a < -b
* min(a,b) = a < b ? a : b. min is the result of folding <.

everything is defined in terms of `<`, booleans, and `-`. we know that everything can be defined by `nand`, but can we use that simplicity to enable elegant code?

=== encodings

* a binary coproduct of positive integers can be represented by a single signed integer whose sign determines the interpretation of the absolute value i.e. n is shorthand for +n which contrasts -n which is interpreted as the cons pair (sign,n) where sign‚àà{+,-}
* if two numbers are always sufficiently small and have fixed point precision, then we can fit them into a common integer, one taking the high bits, the other taking the low bits.

=== integer algebra

TODO

==== symmetries

TODO

=== exploiting bits' multiple interpretation

note the _in bitstrings_; we can encode bitstrs such that certain substrs have useful boolean/integer interpretations. 

shift for expt/log, _ for multiply/divide

TODO: explore modular arithmetic, number theory, combinatronics

the operation (when (p x) (inc x)) can be expressed x=x+p x when p returns 0 or 1.

in double dash, there's a counter for which checkpoints you've hit. just because you hit checkpoints #1 & #3 does not imply that you've hit #2. thus whether you've hit each checkpoint is an independent boolean; thus a 32-bit word can be used to store this value (assuming that a course be broken into 32 pieces, which is pretty damn reasonable.) thus to check whether someone actually _has_ played the course properly (w/o cheating), just test the word against an n-bits full of 1's.

=== what bits can't/don't accomodate

* a type that requires more than a word to encode a single datum of that type, e.g. arbitrary ad-hoc sequences e.g. arbitrary strings
* branching; branchless programming does not concern bit twiddling, though relatedly bit twiddling can often well encode combinations of conditions

one may assume that categorical values must be represented by _arbitrary_ numbers/bitstrings, but this is not true: mnemonic strings can be expressed by words: a word on a 64-bit system can represent a string of 12 ci latin characters, and 6 chars by 32 bits, since ‚åàlog‚ÇÇ26‚åâ = 5, and ‚åä64/5‚åã = 12 & ‚åä32/5‚åã = 6. thus `int[n]` is a more efficient version of `char[12][n]`.

=== recepies

set nth bit to 1, 0, or complement:

. (1 << n) | x
. ~(1 << n) & x
. (1 << n) ^ x

* trailing 0's to 1's: (x - 1) | x
* -x: ~x + 1
* lowest set bit: x & -x ; (number->string (let ([p 52]) (bitwise-and p (- p))) 2) prints "100". 52 is 110100b.
* masked copy: given bitsets A, B and a mask M, copy bits from B into A where M is set (where M is unset A we have A's value at that bit): (B & M) | (A & ~M)
* swap bits an indices i & j of x: y = ((x >> i) ^ (x >> j)) & 1; x ^= y << a; x ^= y << b
* # of set bits (POPCNT on x86): because x & (x - 1) unsets the lowest set bit, our solution is: [TODO: this solution is obviously wrong] for (c = 0; x != 0; c++) x = x & (x - 1)
* # of set substrings: (+ (& x 1) (/ (popcnt (^ x (>> x 1))) 2))
* next highest number with the same number of set bits: let t = x | (x - 1); nt = ~t in (t + 1) | (((nt & -nt) - 1) >> (bsf(x) + 1)), i.e. let t = trailing(x); nt = ~t in (t + 1) | (lsb(t) - 1)
* toggle case of ascii character (or set case by anding with 1 or 0): c^32
* not bit twiddling, but x‚àà[a,b] is well expressed by a stack grammar: `x { [ >=a ] [ <=b ] } bi and`, or even better syntax in apl: `(‚â•a‚àß‚â§b)x`.

==== square-and-multiply

TODO
