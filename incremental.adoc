== incremental vs array/set models

NEXT: consider non/dependency among data. this generalizes fold, which relates data of the same vector, to relations across data. for example, some traversals may depend on others, or actions depend on others, e.g. checking the type of a file only if the file is found to exist. it's not only that one predicate [prolog] runs if another first does, but that all predicates have access to each others' data (ofc only when the data exists; if A => B, then access to B implies that A is in scope, but this is just the same as saying "A AND B" or "A,B" where AND is understood to short-circuit when necessary. also, the ability to define a curried fn within a scope is how that obviously should NOT be a fn, but rather just a syntax convenience. _these_ are the kind of "macros" that we need! indeed, higher-order functions are beneficial simply because they mean "enforce these constraints" and do so cleanly. we don't desire them for their function semantics! we only use them for brevity (readability, refactorability) and to be definite in their form, that we don't accidentally mis-specify a form. macros do this just as well. (stack lang words w/row polymorphic effects are like this too: they're inline, encapsulated, and additively (cf exclusively) composable). in fact, this is true of all functions, not just higher-order ones! commonly people argue against macros because of hygeine, but that's its own issue, not an inherent flaw of macros. indeed, functions are often too constrained; they're inflexible. macros are less so; they tend to enforce constraints without limiting the specification of other constraints. certainly designs that say "X" instead of "only X" are better; we see this in row-polymorphic types in purescript, a feature often wanting in haskell.

TIP: notice how j's `x u/.y` uses data instead of a higher-order fn; usually we'd do `q group-by` but instead we would do `(q x) u/.y`. there's no need to associate an _operation_ with `/.`; we can just as well associate data (the result of an operation) with it. higher-order functions are just complected function calls & traversals, and can always be re-expressed as multiple traversals over data, since the only thing that functions can do anyway is be called; all logic is done on their output, not on their structure itself. functions' foolishness should now be obvious. they are encapsulation, but achieved by such immalleable forms that the only operation that they support is chaining, and they complect computations which cannot be untangled, removed, rearranged. that's quite limited! is it worth the benefit of encapsulation? regardless, certainly at least two designs best it. were functions not "black box in/out devices", could their structure actually be used arithmetically/algebraically, then functions would be justified. indeed, this is exactly the design that i advocate: such things as that `map` doesn't change length but may change type, or that `filter` cannot add length.

tl;dr: there's no way to systematically convert between the models. the best method is to first know computation's fundamental information, then recognize them in code given in language X, then code that information anew in language Y.

.computation's fundamental information
. arrays (equivalent to indexed sets; indices are defined as unique (so that they can be accessed in one operation) and may be multidimensional) are collections of data which have enough information to represent all structures: when flattened, sequences; remove the indices and you have multisets; nub that to get a set; the set's [min,max] is its interval; and sorting the set gives a partitioned interval (frets). we can exploit uniqueness of indices and/or of values in order to relate data across arrays (a la sql's join). sometimes we may order solely for the sake of efficiency b/c order is extra info which can be exploited, e.g. testing set membership via binary search.
  .. an atom is a singleton array or set, and is an interval s.t. inf=sup. this implies that `i.` & `I.` are the same operation but accept different input forms, and obviously the indices are relative to different spaces (de facto indices vs intervals) and `i.` is invertable whereas `I.` is not.
  .. after you achieve code elegance & brevity by using array operations & combinators, some unique complexity in the form of specifying relations among subsets (e.g. `+/` or `x u\y` or `x u/y`). relations may be coded in many different ways but all may as well use arrays [apl] of indices.
. inf & sup reflect sufficiency (`find`) and totality (`map`). inc & dec reflect traversals through intervals. people's care about data is to find n sufficient/desired elements—comomnly 1 needed (minimize search), or to apply an operation to all possible (maximize application).
. predicates are meaning i.e. signal; thus they're filters; they partition signal vs non-signal (noise or another signal). therefore predicates are all. j does not describe everything in terms of predicates. for example, `,` is union i.e. `OR`, and -. is "without" i.e. AND NOT (or just "NOT", since AND is the natural relation among multiple things.) predicates give partitions e.g. frets, or groups (read: equivalence classes) as used in `/.`.
  .. btw, union of n sets is sensibly done by creating an n vector of their pointers, rather than allocating for the sum of their lengths then copying memory.

also think in terms of transforms' invertability; if its dom is plural, does the map preserve elements' distinction? is it 1-to-1, or is it ambiguous? does it lose information or introduce noise? dropping unnecessary information (and compressing) should always be done unless there's cleanup cost. preserving maps should be avoided as much as possible since they aren't theoretically necessary. additive maps should also generally be avoided because they risk adding the need to parse-out info that you already had. generally they're bad b/c the only kind of info that a map can add is noise!
  .. some operations cannot be done except by isomorphism e.g. pretending that `23 b.` doesn't exist, bitwise "or" would be ++++./&.#:+++. yet even here, why use `&.` instead of `@`? is there any benefit to converting back to decimal? likewise, any time that you use `I./1` or its inverse, ask both what benefit conversion earns, and why your data weren't already in that form.
  .. remember that all non-io computing is just information extraction. programs don't begin with data; they get it from somewhere. then they have it; what more could one want? ah, to make obvious that information which was only before latent in the data. btw, all io computing is just sequencing.

''''

* instead of `minimum x >`, prefer `[ x > ] all?` (`char b = 0; for(char i = 0; i < len(A); i++) { b&=A[i]>x; if (!b) break; }`) since it short-circuits. then again, it has a jump inside a loop; i wonder how processors handle that compared to having a jump only after the loop.

"do you know that feeling of having to hold too many things in your head at once, such taht if someone talks to you, you lose everything? fp removes that, by definition" is true because functions are units of computation. they have particular *_form_—constraints aka invariants*—and thus they compose easily, unlike manually arranging data, trying to manually compose all the constraints altogether. however, predicates are a far-superior specification design than are functions! even array operations best functions. unlike functions, array ops are binary or unary, they are few actually basic ones, and each one corresponds to a fundamental property of computing e.g. position, subset, order. *a system of composing constraints is all that we need for easy programming!* by "predicate" i mean it in the prolog sense: something returns a set; a false predicate returns the empty set, but a truthy one returns a non-empty set. this is very k-like: implicit plurality with intelligent support for null. apls best prolog by considering indices and specializing about primitves. if we were to simply have a prolog with those designs, it would be amazing, though we can still do a bit better than that. also, i currently wonder whether indices should be naturals or if any uids would suffice. indices are useful for relation a la sql; this allows us lightweight operations on relations without having to relate everything together; for example, commonly arrays of structs/tuples are used so that we retain all of a record's info in one place, but this makes operations on them crufty since we _must_ consider all data at once; any operations on part of the data implies an operation on all of the data, since the tuple itself is what retains the relation; if we break the tuple, we lose the relation. maintaining stuff sucks. furthermore this design implies that this relation is the _only_ relation. we know that this is far from being true, though, as we know from using e.g., in sql, `x join y on a>=b`.

.why are some operations easier to express by array ops, and how do we convert between array exprs and incremental ones?

incrementals work on streams, which are effectively scans/folds; we consider one datum at a time and, at each iteration, have access to all prior elements. some operations must be different from their array versions: for example, where we may grade an array, we cannot afford to do that for each iteration. we'd use a treeset instead—likewise for bins, which both requires that an the search space be sorted, and which can be more efficient when we apply it at once to a vector of queries, namely when some queries are repeated or all queries are ordered.

* when i say "fold" i mean "loop", which is just as well expressed as `while` or `for`, since they're just variants of a comomn syntax. scan is just `map : ( ... xs q: ( ... x -- ... y ) -- ... ys )` (or its generalization, `nmap`) that has non-empty `...`.
* using `reverse` with streams: either push onto a stack then traverse, or collect into a vector then traverse it in reverse order, or, depending on the computation, invert some operations or swap arguments.
* streams use one big loop instead of multiple array operations each of which loops (or in a good pluralistic lang, which merge into one traversal). to convert between these models: folds don't need translation; else it's expressible by some sequence of filters and/or maps. use the rule `map f . map g` = `map (f.g)`. express filters as masks, then combine them and select by the result.
* shift/rotate is just to offset an index, with optional modulus
* `append` becomes `push` or `push-all`
* note that bins & grade both concern order, and maintaining order of all elements (except for sequence order, of course) is a non-trivial problem.
  ** to keep things sorted, keep a tree set in the loop state. to efficiently give the grade as each new element is entered, idk
* `nub` in a stream loop is best done by retaining a set (hash or tree) as part of the loop state. i'm curious how nub & nub sieve are implemented in j, similarly to `i.`.
* atomic operations (e.g. arithmetic) doesn't need translation
* a verb on infix (clumps) is easily accomplished by using a ring buffer in the loop, performing the verb on each iteration; for verbs on n-groups, just perform the operation each time that the loop index divides n.
* outfix: TODO
* general verbs, wrt rank, is a simple translation: just iterate through two streams simultaneously (`2each`), retaining whatever state you want. use `curry` to effectively broadcast. for example, vector `=` is either `x [ = ] curry map` or `[ = ] 2map`
* of course, we can't do tally (length) until we finish consuming the stream
* iota is trivial and ubiquitous: it's just the iteration number
* cut/intervals (`I_A` in k) is elegantly expressed in a loop: just collect into a vector on each loop; and upon meeting a condition (e.g. current elt equals spilt character, or loop iteration number divides n, or loop iteration number equals the head of a queue of split indices), push that collection vector into another collection vector. to split on a string or other predicate (e.g. `E.`) quickly becomes the question of how to write a parser. at this point, just use a packrat/peg parser. every language should (as in, that would be good; not as in i expect it currently) come with one.
  ** head, tail, take, & drop are all just particular varieties of cut/intervals. "take n" is expressed in a loop as modifying the index variable's limit to be max(n,prior_max)
* `#.` & `#:` probably wouldn't be expressed as a loop, but were it: collect into an output value (shift left/right or divide/multiply, then add or bitor). mixed radix might require regrouping; i don't recall.
* for key [dyad], just use a hash map in the loop state
* agenda becomes switch/case
* index of (`i.`) of course just returns the loop number upon meeting a predicate of the loop state
* `e.` is linear or binary search

NOTE: the whole following `E.` section is actually `E.~`; `x` is the search space and `y` the query.

`E.` can be implemented as "match each y-sized substring of x against y", `{((#y)(y~)':x)}`. this is usually nearly optimal, except for when you want to search for a long string most of whose initial characters repeat e.g. `'ccccccccccd'E.'cccccccccxdcccccccccceccccccccccc'`. the truly optimal version, in c++, is:

[source,cpp]
-----------------------------------------------------------------------------------------------------------
for(char e,i=0,k=0,n=sizeof(y)-1;i<sizeof(x)-1;i++)if((e=y[k]==x[i])&((k=e*(k+e)%n)==0))v.push_back(i+1-n);
-----------------------------------------------------------------------------------------------------------

btw, this method isn't designed to work when `1=#y`; that special case can be computed more efficiently (namely by `e.` or `i.`) and is a degenerate case of `E.`.

TIP: the minus one's of the length are b/c c strs are null-terminated and so have extra length to account for, unlike other c array literals

except that the c++ version returns integers instead of a mask. an efficient version that produces a mask is similar, but on each iteration it pushes `k`, then iterates backwards through that result to replace substrs of 1 2 3...n by 1 0 0...0:

[source,c]
-----------------------------------------------------------------------------------------------------------
char z[sizeof(x)-1];
const uint n=sizeof(y)-1;
for(char e,i=0,k=0;i<sizeof(x)-1;i++)z[i]=(k=(e=y[k]==x[i])*(k+e));
for(uint f=0,i=sizeof(z);i>0;i--)
  switch(z[i]){
    case 0: f=0;    break;
    case 1: z[i]=f; break;
    case n: f=1;
    default:z[i]=0; break;
  }
-----------------------------------------------------------------------------------------------------------

NOTE: `v` is now `char z[sizeof(x)-1]`

assessment:

* if we were to mark the end index of matches then the code would be one very simple loop.
* `f` ("flag") is a loop-scoped var that changes only on some iterations. it passes info among iterations, and thus, to express the loop functionally would require a fold or stateful map.
* despite what i'd said about "you may as well use a parser at this point", perhaps not; this is a simple, efficient, common case.
* it's beautifully simple & efficient c code. c makes easy the semi-regular relationship of pointers—for example here, that i relate `x[i]` & `y[k]`, where `k` obeys a simple arithmetic update expr per iteration, but where i must specify that update expr. you won't find a combinator that supports this kind of relation! it's so simple & direct, though. that's what's good about c: it allows natural directness to remain direct, whereas anything more complex or contrived (e.g. apl, factor, haskell, or even java, since java doesn't use ℤ/2 for bools) doesn't support expressing directly; their more-complex primitives necessarily mean more-roundabout expressions! well, this is actually not necessarily true; it could be that you use more-complex primitves, but fewer of them. this is common in j compared to c. to succeed in coding this requires knowing how to convert between c & j, which requires knowing the computation's information [info theory]. i'm sure that i could find many examples that are elegant in sql & c, though obviously sql has _very_ few primitives,...and frankly, none of them is complex!
* i'm curious to compare this definition to the one currently used in j.
* if we're not using the value of `x` again, then we can simply overwrite `x`, never needing to allocate for `z`
* it's very neat that i can use numbers to measure the extent of equality, with `k==n` being total equality. using "count of equality" is much easier than saying "these elements equal" b/c it has less info, and thus less info to worry about preserving. i clearly don't concern the elements after i've tested them for equality.
* the `for` loop can, but i want to prove that it never should, have wild traversals e.g. by, even in addition to the usual `i++` in the header, in the body, conditionally resetting `i` to 0 or incrementing it again, so that some loops we effectively do `i+=2`.

translating this efficient code into k:

the fact of pushing `k` unconditionally on every iteration while updating `k` makes this easily represented by a scan...of _two_ iteration variables. so i don't want to use scan to represent this in k. indeed, "big loops" are ugly in k; so i'll just let the arithmetic guide me: `e=y[k]==x[i]`. without yet considering how `i` or `k` update, but knowing their range (`i.#x` & `i.#y`), i'll assume all their values. thus the information for `e` is contained in `x=\:y`, and hopefully this computation preserves information needed to distinguish any distinct subsets. i'll call this informational superset of `e` _ε_. `k` is defined in terms of `e`, so i can compute it from ε. that `k` is defined in terms of itself implies that we must at least fold, but i'll use a scan because i know that i want all k values through all iterations. i see that k increments by `e` (whose range is [0,1]) and is multipled thereby, so 1. k is a natural number, and 2. k only increases or resets to 0. anyway, that leaves us with `e{x*y+x}\0`. ah, it's `{x*y+x}\` yet again!

having identified all the facts, it's time to figure-out how to code this, starting with how to convert ε to `e`. ε is a table^[1]^, not a vector, so i can't just run `{x*y+x}\` on it. i need a variant: with fold var `a` starting at 0, and with `y` being the current row, `a:e*a+e:a=y a`; `{0{e*x+e:y x}\x=\:y}` produces e.g. 0 0 1 2 0 0 0 0 0 0 1 2 3 4 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 7 8 9 0N 0 0 0 0 0 0. this corresponds to the first c loop. note the `0N` btw; k's treatment of nulls sees the code work without me having to account for oob/modulus. cool.

^[1]^TODO: the table looks very similar to self-classify, `=/1`, in j, which is informationally equivalent to `(i.])`; thus i should be able to use it instead of the array. also i note the structure of `(i.])`: monotonically increasing, akin to `+/\@~:`! irregular increment is informationally similar to irregular succession through a sequence!

to translate the 2nd loop, `case n` can't elegantly be put in a k scan since there `f`, the scan's control argument, differs from the output value (iirc this wouldn't be an issue for j's `F.`). so we'll have to do something other than just a fold. `f` & `z[i]` are defined in terms of each other. when it comes to rephrasing, it's often best to think about fresh solutions that preserve the essential invariants, which in this case is that we must mark 1's followed by ``#y``'s differently from other 1's. and again, we must use a scan for this because we're relating elements of the same array. a little pondering and i find that `|0{(L=y)|(y>0)&x}\|` (where L is the length of the query) produces runs of 1's where there are matches. to select only the first of each run, do `0>':`. in total, the whole c solution is thus expressed in k: `{0>':|0{(z=y)|(y>0)&x}[;;#y]\|0{e*x+e:y x}\x=\:y}`.

array langs have no idiomatic way to relate 3+ things—here, `x`, `y`, & `k`; i must break the relation into binary ones then relate those relations, which means that i must break `k` into multiple variables, each containing a partition of ``k``'s information. i must break `k` because it alone—actually, specifically `e`—is already _defined_ in terms of `x`, `y`, & `k`! i must break `e`. it seems that there was no way to avoid starting from something as crude (containing extraneous information) as `x=\:y`.

summary and lessons learned: _translating_ sucks, but _converting_ is fine; one should practice the skill of recognizing the essential computational information of data & traversals: uniqueness, characteristic information (which distinguishes it apart from others), ranges, and order. forget the _variables_; see only this _information_ then code it elegantly per your coding system of choice. oh, and of course, converting from apl (or sql) code into anything else is much easier than the other way around, since it relates definite units, so relational and decomplected!

i guess what i'm really questioning about or seeking is the fundamental desirable properties of (natural) numbers, which namely are, again: uniqueness (enables set inclusion, linear search, and reducing search space by 1 per iteration), order (enables binary search, and reducing search space by distance to inf/sup per iteration), or these in the context of accumulation or disintegration. i'll be studying link:https://en.wikipedia.org/wiki/Coding_theory[coding theory] and number theory via the pdf link:https://www.shoup.net/ntb/[_a computational introduction to number theory and algebra_ by victor shoup] whose table of contents is just the loveliest. my consideration of info theory is one that considers the essential meaning of data, rather than assuming that all input has meaning which should be [mostly] retained through [lossy] compression, so it's really like a mix of num & info theories.

TODO: #/2 &, related, I./1, which both duplicate or remove, and are commonly used for masks

so for the most part, we can well express all computations as a loop whose state is a treeset with optional associated values (for nub, grade, key, bins), the current element(s) (multiple if iterating over multiple streams together i.e. `neach`), and the current iteration number.

TODO: consider how j's `^:` is used for both while and if. this is natural, and in prolog they're one form, but in non-declarative style, to express while as if is nice. it's b/c in prolog, everything is `until`; `until` is the same as `if` if it satisfies on the first iteration. `if` supports `else`, but i don't confidently recall any language supporting `until...else`, though it easily could. in most langs that wouldn't be useful control flow, but it's perfect for prolog which uses backtracking to match a predicate until it's satisfied or exhausted.

so to convert between the array and loop models simply requires knowing the fundamentals.

TODO: discuss the importance of scans and how they well preserve information for successive appar ops. revisit my k notes (or wherever it is that i do that "produce"-style k code with effective short-circuit on emergent loop values)

comparing verbs like prefix & suffix against haskell foldl & foldr is easy but unhelpful; compare them directly against c loops. indeed, even suffix being akin to `foldr` is a total coincidence! in j it's b/c j evals rtl, whereas in haskell it's b/c thunks are built of other thunks and lists are null-terminated on the right/innermost. yes, their parenthecized expressions are equivalent, but the causes for that equivalence differ!

this is what makes sql so powerful. we see this in j:

* cut & bins, both of which take an ascending vector of frets as their control argument (though cut takes it as a mask whereas bins takes it as indices)
  ** be/head, cur/tail, take, & drop all can be expressed in terms of cut; they're simply more convenient forms since each of them takes exclusively-either the inf or sup.
* both of `i.` & ``i:``'s unary & binary forms: the unary forms produce intervals, and the binary forms give either an inf or sup.
* floor & ceiling snap to inf or sup
* signum (ℝ dom) can be expressed as bins with intervals -ℝ^-^,0,ℝ^+^
