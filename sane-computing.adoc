== sane computing

TODO: check-out link:https://fossil-scm.org/home/doc/trunk/www/index.wiki[fossil], a vcs bespoke for sqlite, that uses a sqlite db instead of `.git/`

see also _sane os_, _best paradigms_, and _just use lists_. this document discusses the user's experince with using a computer—common tasks like writing & reading information, configuring their system, communication among code, guarding themselves against incorrect operations, recovering from accidents, communicating with other machines, extending the functionality of one's machine, &c.

this document expects all users to program, but for programming to be no more difficult than the problem that the program aims to solve i.e. the programming itself is trivially simple, and in fact quite fun, since it should use notation much superior to natural language, and is beautiful. anything else is inadmissable.

=== ideal 

living, organic, self-sustaining, self-repairing, context [environment]-sensitive, helpful, adaptive, polymorphic information manipulation & parsing system. it seeks to _help understand my will, then help me fulfill it, rather than being a dumb, dangerous machine that takes my orders._

* forget _programs_; think of just _code_
  ** all code should execute in shared memory and have direct access to hardware
* forget _languages_; think of [notations for] simple abstract machines (e.g. stack, register, array, or list machines)
  ** your notation [graphical representation] should be elegant and terse
* forget text, and forget _editors_; have a single data manipulatiion program. data may be parsed by any pattern, expressed by any form (graphical, audial, &c) and manipulated by any gesture, where _any_ means _whatever appropriate_.
* never represent non-textual data by text. for example, never should the text string "1245.657" exist; that should always be a number! parse bytes, not text!
* hierarchical filesystems suck; as with most sucky things, they impose unnecessary constraint: namely, for starters, being a tree rather than general graph; however, even a graph is overconstrained: it assumes discrete nodes, whereas data is continuous. prefer a database.
  ** blobs still have their place; some things, like media or books, are to be viewed by themselves, and so should be stored with definite bounds (considered as closed sets rather than an open ones.)
* no thing's constraint should affect other things' constraints, unless a constrained group of things is considered, in which case some subset of those things must modify their constraints in order to satisfy the group's constraint.
* _better than web_ paradigm: rather than http, requests are db queries & responses are some form readable by the db. the *data viewer* program can present & manipulate these responses just as it does of any data.
* code should be able to be paused. this may happen manually or when an error occurs. a paused program should be 1. manipulable while running, and 2. writable to persistant media; then resumable from either (1) or (2).
* "data soup" (implicit definition): specify things without regard to order, then allow any order(s) to be calculated. examples:
  ** a la prolog, specify states (by predicate, even if that predicate is equality with an arbitrary nominal label, "λx. label(x) = such-and-such-state") and maps from states to successor states
  ** hooks are implicit definition: rather than insert an operation at a particular point of execution / control flow in a program, a hook specifies a relation between a (named) point (generally 1+ point(s) satisfying a predicate) of execution context and some function. the hooks of a given execution context usually run in indeterminate order, btw.
  ** put into dbs instead of into a file. for files, you must seek to the correct place (assuming that only one exists) to insert the data; dbs have no such restriction.
* messaging (async communication among code) is good e.g. π-calculus or racket's thread mailboxes. in fact, generally concurrent systems are best: they're parallel and generalize to being distributed over multiple machines, but most importantly concurrent programs that work tend to have better designs than non-concurrent programs.
* persistent and volatile storage should be shown to programs as a single thing. TODO: how? link:http://metamodular.com/Common-Lisp/lispos.html[apparently] this was first done in the 1960's.
* all specification of constrained data must specify, as you're entering it, and with tab-completion/antiparsing, the set of valid elements, so that any ultimate input from the user is guaranteed to be sensible
* proglang uses free & bound vars, requiring disambiguation / binding only as necessary

===== benefits of dbs

* acid resists malformed states/data.
* there's no difference, in code, between a local vs remote db
* supports information soup, non-unique data, and overlapping data
* optimized for storage, access, and modification
* often serverized, so machines can communicate
  ** supports the _better than web_ paradigm
* no _pipes_; pipes (at least as currently used) support only linear pipelines of byte streams.
* rows are parsable as collections of distinct attributes
* can modify more efficiently than files? but databases are stored as files on filesystems; TODO: how can dbs more efficiently reshape themselves than filesystems or files (e.g. editing a csv file, which is a [linear] blob instead of a tree or rope)?

==== tech currently nearest to the ideal

* TODO: is there anything better than linux?
* on *nix systems:
  ** for db fs, make a [virtual] fs; open, write, &c are simply sql statements. this allows one to use a db instead of hierarchical fs, but serves as an interface for current h.fs programs (e.g. picture viewers) so that you can continue to use them as they are.
* programs can communicate via http/tcp, udp, unix sockets, websockets, cmdline, pipes, or writing to & reading from shared state e.g. files, dbs
  ** to make a program interactive, daemonize / serverize it; allow it to run in the background, and accept input from socket, stdin, reading from a file &c
* if it's appropriate to use a poor language, then at least there's probabily a lisp or scheme of that lang.
* because often data is represented by strings, you should know a good parser well. parsing expression grammars (pegs) are very good. however, the only lang that i know of that uses them is janet, which is a poor language. haskell has good parsers (megaparsec) but haskell is a bit high-level. racket has megaparsec but its implementation seems to be buggy/flawed. fortunately, if you know programming well, then writing parsers is trivial. in fact, ease of writing parsers is a good measure of how good a programmer you are, specifically how well you understand ad-hoc and/vs symmetric structures.
* for text ui, use kakoune to manipulate text, then pass text to a backend program that actually parses it and performs an action and/or modifies the buffer.

=== design principles

.non-technical

* computers don't do much: they just change [byte] values at given locations in memory. any other presumption about what computers do is merely an interpretation that's only appropriate if accurate and useful. remember that you can always choose to just move bytes; you don't need to burden yourself with unnecessary suppositions. now, naturally, since we're dealing generally with ordered information, you'll want to encode information by groups of bytes and define transforms (or an algebra) on them.
* eschew unnecessities & redundancies
* code should be easy to refactor; this is afforded by making small subsets of code meaningful, where they, if abstract in meaning, are concretely meaningful in the contexts in which they're placed, e.g. a sentence fragment is syntax that is meaningful by itself, but is not a complete thought; it can be associated with other sentence fragments of particular form in order to constitute o complete thought.
* language is a tool, a representation of truth, not the truth itself! do not allow language to mislead you! indeed, choose a _notation_: a _direct representation_ of truth by symbols, so that you can manipulate truth while benefitting from the ease of reasoning enabled by symbolic reasoning!
* keep particular _principles_, techniques, or other _abstractions_, not particular tools (including langs)!

.technical

* _constraint_ has two forms: ad-hoc and symmetric. ad-hoc is arbitrary grouping. symmetric is whether a thing follows a predicate or not. constraints are the domain of a "branching" map (really _partitions_), whose cod is any object. partition functions are the basis for *parsing*.
* _code_ is order (vs chaos). code is not necessarily executable. however, as any (orderly) thing may permit multiple interpretations, one of those interpretations may be as executable instructions.
* the order of the structure (i.e. the form of a structure considered as a duple of mass & form) directly corresponds/represents operations on / traversals over the structure.
* use metaprogramming i.e. use a framework that does not distinguish between executable & non-executable code. avoid macros (as e.g. picolisp does) if you can.
* trees are isomorphic with nested lists. this is universal, not particular: `cons` (ad-hoc binary association) is the primitve association operator; trees are the result of recursing on `cons` produces binary trees, any subset of which may be interpreted as a list. `cons` is the mechanism that enables grouping physical data; sets defined by predicates define abstract & symbolic data.
* create/identify structures that increase brevity by encoding symmetry implicitly in the structure's algebaric axioms. matrices are such an example.

.princples

* seek elegance; minimalism & beauty always follow, though seeking the latter two do not guarantee elegance.
* seek simplicity; safety will follow. seeking safety will not guarantee simplicity.
* ignore how things are done; consider only naïve ideals, then identify an optimized version thereof, constrained by any [currently] inescapable constraints (namely constraints of the implementing system)
* resist data types; store everything as groups of bytes, and allow any group multiple interpretations. if data should be interpreted particularly, then make it difficult to interpret (parse) it as (into) any other data.
  ** magic numbers are easy solutions
  ** if a thing fails to match a predicate, then it should fail to match as early as possible
* maximize unambiguous polymorphism

.useful particularities

* using delimiters instead of indentation means that anything can be a one-liner. this is often useful when mixing languages, e.g. `ls -1 | janet -e '(loop [l :in (string/split "\n" (file/read stdin :all))] (when (string/has-suffix? "adoc" l) (print l)))'`. this example is not so good because it uses both starting and ending delimiters, which can be unruly to keep properly nested; instead, a stack lang would be much better for one-liners.

=== using non-ideal systems

* use others' code, if available, rather than writing your own, unless you can implementat (more) elegantly, quickly, efficiently, and easily enough.
* use external invocation (`execl` in c, `system` in racket, `os.execute` in python) and stdin & stdout, or sockets, servers &c to wire dataflows independent of language. if you can't call fns directly, then wrap the fn in a main method that accepts (from stdin, a file, db, cmdline arg &c) the data that you want.
* to resume from a crash, write program state to a db or file.

.stability & sanity

programming as a field is always seeing new tools, people, techniques. often we're expected to know them because new, useful software uses them, or because an employer or customer demands so, or because we're collaborating with others who use these novel things. keeping up with it all is hopeless: there's too much, and much of it isn't even useful! often "new" technologies are just common ones being marketed differently. for example, currently blockchain, machine learning, and orchestrated containerization are being applied _everywhere_, though they're needed (or even useful/appropriate) in few places.

we find ease in the things that do not change: algorithms, data or abstract structures, and even common software that's been around for a very long time and/or is known to be reliable.

.prefer (sql) databases

databases are the most advanced common software. they implement all the most difficult aspects of programming:

* concurrency
* atomicity
* optimization for both speed and memory for large datasets
* memory (databases are assumed to be much larger than RAM, and their operations account for this)

and they implement some less-difficult yet appreciable conveniences:

* sorting & grouping
* union & intersection
* repl (effectively, by transactions)

therefore to use a database is to make an efficient program. the only places where databases are as good as general purpose proglangs are:

* certain algorithms
* IPC or interaction with remote services
* stateful imperative logics
* hardware interaction

basically, databases are good for everything that involves data, but inappropriate or unaccomodating to everything else (namely anything involving i/o.) not only this, but databases may work locally as a program, or run as a server, which makes database code automatically work for either single-host or distributed use cases.

.beneficial imperfection, and non-symmetric exploitation

know when you need to program for perfection or not. for example _linearize_ (use a linear approximation of) mathematical expressions, or estimate mathematical expressions over reals by a series of bitshift and linear algebra operations. know when it's better to use a hard-coded lookup table or use an algorithm to produce values. code for your purpose rather than a "good" implementation. for example, your situation may call for random numbers. your choices are a random number source like `/dev/urandom` or a pseudo-random number generation algorithm. you can use the former if it provides enough data. if using an algorithm, then it only needs to be seemingly random—something that depends on what the value is to be used for. don't waste your time making a super-unpredictable algorithm if no user will notice the difference. an algorithm may be convincing enough for pseudo-random game events but horribly obviously not truly random for producing a grayscale image of white noise.

remember: this is coding, not mathematics. we often can't afford perfect mathematical precision, whether it be real analysis or combinatronics. for most applications it's better to use approximate solutions then adjust their results for sensibility, than to calculate as exact a solution as could be considered reasonable.

this may seem obvious, and maybe it's only a problem for few people, but please resist any inclination to make the best solution that you can simply because it's the best and you can; prefer simpler, faster, lesser yet sufficient solutions (except when you're uncertain about how the solution may need to generalize in the future. this can be tricky to predict, and is very particular to each situation.)

.fundamental computer science

programming is just recursion, lists & maps / alists (i.e. lists of pairs) / tagged unions (lua shows that these are all the same structure,) and concurrency. computer science is implementing mathematics by these. vectors, lists, stacks,...they're implementation details, which can be important, but only for efficiency rather than result state. graphs are the most general data structure (though not the most general mathematical structure) but are implemented in terms of arrays & maps. ADTs are useful, but they're expressible recursively by lists and maps—more general and thus more flexible structures. strictly, the cons pair is the smallest data structure. it corresponds to the fundamental mathematical principle of _association/relation_—the basis for all super-singleton structures.

given pairs' fundamentality, we see that every structure can be considered or traversed as: itself naturally; a tensor/matrix; a graph. if you're familiar with these structures, it should be clear how databases or parallelized GPU operations can be very useful here.

again, *keep it basic*. much of programming or computer work today—even what's considered brilliant and popular—is really just about making needlessly complicated things simpler—even though they end-up being still overly-complicated (or limited, or difficult to use outside a very specific use case.) let's not forget how simple things are, and be very careful when promoting anything more complex than maps & lists. and guard yourself against anything more complex! there are many such things, and they sound good, and they do work, and so they're tempting! it's very easy to accidentally find yourself in an ocean of complexion, wondering how you strayed so far from simplicity. obviously this is true only for large programs/systems. however, i encourage that you not go too much out of your way to try to discover/learn the hottest tech or try to learn all the tech in order to make yourself seem versatile. there's too much, and it'll corrupt your mind. however, on that note, i do encourage, if you're so inclined & capable (i'll offer a course later on this,) to consider mathematical structures' applications to computer science, such as universal algebra / category theory, linear types, or using tensors for general computations; or cs-specific things like AVL trees. considering these problems and solutions will improve your programming. again, though—generally—mathematics affects how the program is described, whereas cs affects the efficiency of the program.

everything (all data, and functions) can be represented by *pairs/lists* as used in scheme. maps (isomorphic with *alists*) are structures composed of pairs. *tagged unions* are isomorphic to maps from symbols to values. lua is a good language (semantically) because its one structure is a list/table. these are the same structure: a table is another term for a map: lookup values by indices (of any type.) a list (again, specifically in lua) is just a table whose indices are always positive integers. javascript has objects that are similar, and so javascript would be (and used to be) as good as lua; however, recent revisions of javascript introduce special semantics and syntaxes that void that elegance of simplicity.

all programs can be described by the lambda calculus, wherein functions are represented by _lambdas_: simple mappings from inputs to outputs, e.g. `(lambda (a b) (* 2 b (+ a 3)))`. the meaning is obvious. the fact that this is an s-expression implies that it is data—namely it's isomorphic to its quoted form in its evaluation context.

so whenever someone mentions something like chef, ansible, kubernetes, or any of many popular softwares whose name gives absolutely no hint whatsoever as to what it does, and you go to each's respective website, and you encounter astonishingly vague language, or it describes some revolutionary new system or some junk, ask yourself: how do i express this thing as a graph, table, list, or abstract mathematical structure? for example, ansible is basically `map`, but maps stateful modifications over a list/set of machines. nix is a system for executing arbitrary pure functions (usually to an executable program or a library) whose domain is dependencies, with caching support. dependencies is a graph (specifically a DAG.) people love telling what you can do with their software, but that's hardly a concern for us hackers, since hackers understand structures (including functions) and muse about all the different ways that they can use them. besides this, a software's ability tells us nothing about what it is, how to think about it, etc.

this thinking removes all mystery. for example, scheme continuations are usually difficult to learn, but if you realize that all programs (and very clearly lisp programs) are trees (viz ASTs) and that there's a map (table) from identifiers to syntax contexts with values, then continuations are very simple to understand: they're just nodes in a tree, and moving around continuations is just looking-up in a map. despite being moot, continuations' brilliance is that the objects of the table and map are execution contexts! that's the kicker. haskell is a relatively good language simply because it associates data with types, and types are logical constructs that support implication and testing. the _association_ and _logic_ make it good. that's the magic. how is the logic implemented? there's a loop over a couple sets of logical propositions. that's a significant portion of the implementation of a professional programming language! programming isn't hard. the only reason that programming (or using computers) is difficult is because either 1) you're using bad tools or techniques; 2) the problem is inherently tricky, even if not initially obvious. for example, computing the integral of e^(x^1) is easy, but e^(x^2) is not. in other words: we typically consider a solution to a problem, but encounter trouble when expanding it to a general solution. while you should always strive to know how general your solution needs to be, predicting future needs can be very difficult, so just do your best with what you have. though not particularly covered in this course, there is a technique to design systems for flexible generalizing. i might offer that in another course, but it requires a strong foundation in a variety of mathematics that i alone have identified and haven't finished my seminal book on.

almost always, the more that software obscures the simple structures that underlie it, the worse the software is: it's difficult to keep track of options, there are more options than appropriate, the options or operations do not compose well (or at all,) and there's a decent chance that the software will make certain operations easier than others, which may or may not be a problem for you depending on your use case.

.special techniques

* fuzzing
* parsers & antiparsers
* typing (note that types are predicates, i.e. logical statements)
  * composing types and seeing which programs they beget, e.g. a list or tree or dag or graph editor, which would work on bookmarks, spreadsheets, playlists, etc
* streaming
* parallelization
  * MIMD is better than parallel threads
* concurrency
* purity
* memoization

.saneware

software is only as good as it is when it fails. when software works like it's supposed to, then that's good, but it should be expected that it'll fail (or that you'll want to use it in an uncommon way,) and when that happens, if you can't overcome that error or find a way to implement your desired behavior, then the software is worthless.

these wares follow the description of sane computing: simple, serverized or main-shimmed, use funcalls and standard ports. these wares use self-descriptive names and have neither special usage nor installation guides. furthermore, as a practical consideration, these wares do not suck (they do what all they're supposed to and have no needless quirks.) each program does one thing, and for programs that are commonly used work together, any new user does not need to know about these common usages in order to use any subset of tools together.

* link:https://github.com/mawww/kakoune/blob/master/doc/design.asciidoc[kakoune]
* language server protocol (lsp)
* link:https://w3c.github.io/webdriver/[webdriver]
* link:https://nyxt.atlas.engineer/article/technical-design.org[nyxt] (uses xml-rpc to bridge controller (nyxt/lisp) & view (webkit))
